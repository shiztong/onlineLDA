{{Cleanup|date=October 2011}} An '''abstract machine''', also called an '''abstract computer''', is a theoretical model of a [[computer]] hardware or software system used in [[automata theory]]. Abstraction of computing processes is used in both the [[computer science]] and [[computer engineering]] disciplines and usually assumes [[discrete time]] [[paradigm]].  In the [[theory of computation]], abstract machines are often used in [[thought experiments]] regarding computability or to analyze the complexity of algorithms (''see'' [[computational complexity theory]]). A typical abstract machine consists of a definition in terms of input, output, and the set of allowable operations used to turn the former into the latter. The best-known example is the [[Turing machine]].  More complex definitions create abstract machines with full [[instruction set]]s, [[processor register|register]]s and [[memory hierarchy|models of memory]]. One popular model more similar to real modern machines is the [[RAM model]], which allows [[random access]] to indexed memory locations. As the performance difference between different levels of [[cache memory]] grows, cache-sensitive models such as the [[external-memory model]] and [[cache-oblivious model]] are growing in importance.  An abstract machine can also refer to a [[microprocessor]] design which has yet to be (or is not intended to be) implemented as hardware. An abstract machine implemented as a software simulation, or for which an [[Interpreter (computing)|interpreter]] exists, is called a [[virtual machine]].  Through the use of abstract machines it is possible to compute the amount of resources (time, memory, etc.) necessary to perform a particular operation without having to construct an actual system to do it.  ==Articles concerning Turing-equivalent sequential abstract machine models== An approach is to take a somewhat formal taxonomic approach to classify the [[Turing completeness|Turing equivalent]] abstract machines. This taxonomy does not include [[finite automata]]:  '''Family: Turing-equivalent (TE) abstract machine''':  '''Subfamilies:''' :Subfamily (1) Sequential TE abstract machine  :Subfamily (2) Parallel TE abstract machine  <!-- BUT, see OPS! sec. on the Talk page -->  '''Subfamily (1)-- ''Sequential'' TE abstract machine model:''' There are two classes (genera) of Sequential TE abstract machine models currently in use (cf van Emde Boas, for example):  :Genus (1.1) Tape-based [[Turing machine]] model  :Genus (1.2) Register-based [[register machine]]  '''Genus (1.1) -- Tape-based Turing machine model:''' This includes the following "species": :  { single tape, [[Multi-tape Turing machine]], [[deterministic Turing machine]], [[Non-deterministic Turing machine]], [[Wang B-machine]], [[Post-Turing machine]], [[Oracle machine]], [[Universal Turing machine]] }  '''Genus (1.2)-- The register machine model''': This includes (at least) the following four "species" (others are mentioned by van Emde Boas): : { (1.2.1) [[Counter machine]], (1.2.2) [[Random access machine]] RAM, (1.2.3) [[Random access stored program machine]] RASP, (1.2.4) [[Pointer machine]] }  :'''Species (1.2.1) -- Counter machine model''': ::{ abacus machine, Lambek machine, Melzak model, Minsky machine, Shepherdson-Sturgis machine, program machine, etc. } :'''Species (1.2.2) -- Random access machine (RAM) model''': ::{ any counter-machine model with additional ''indirect addressing'', but with instructions in the state machine in the [[Harvard architecture]]; any model with an "accumulator" with additional indirect addressing but instructions in the state machine in the Harvard architecture } :'''Species (1.2.3) -- Random access stored program machine''' (RASP) model includes :: { any RAM with program stored in the registers similar to the Universal Turing machine i.e. in the [[von Neumann architecture]] } :'''Species (1.2.4)-- Pointer machine''' model includes the following: ::= { Schönhage Storage Modification Machine SMM, Kolmogorov-Uspensky KU-machine, Knuth linking automaton }  <!--Within each of the model-species there are varieties.  Within a species, ne possible classification can be by the number of parameters used. The way to figure this out formally is to actually build the models (e.g. in an Excel spreadsheet) and see how many operands (i.e. Excel cells) are required to specify an instruction. In the following the "jump-to" address as an additional operand. So for instance the Schönhage model has no operands excepting the jump-to address in an extra goto instruction:  *0- & 1- operand: Schonage model *1- & 2- operand: Minsky (1967) model *2- & 3- operand: Minsky (1961), Lambek (1961) *3- & 4- operand: Malzek (1961)  There are models with "convenience instructions" (e.g. unconditional-J xxx, CLR h; CPY a,b ) including in particular: * 2-operand: Shepherdson-Sturgis (1963), Minsky (1967) -->  ==Other abstract machines== *[[ABC programming language]] *[[Abstract Machine Notation]] *[[Algebraic Logic Functional programming language]] *[[Categorical Abstract Machine Language]] *[[Context-free grammar]] *[[Finite automata]] *[[Specification and Design Language]] * Historycal/Simplicity Abstract Machines for [[Prolog]]: **1.[[Vienna Abstract Machine]] ([[VAM Prolog]]) **2.[[Warren Abstract Machine]] ([[WAM Prolog]]) **3.[[Berkeley Abstract Machine]] ([[BAM Prolog]]). *[[MMIX]] *[[MikroSim]] *[[SECD abstract machine]] *[[Ten15]] *[[TenDRA Distribution Format]] *[[Zinc abstract machine]]  ==See also== *[[Abstraction (computer science)]] *[[Abstract interpretation]] *[[Discrete time]] *[[State space]] *[[Computability#Formal models of computation]]  ==References== *{{MathWorld|title=Abstract Machine|urlname=AbstractMachine|author=Macura, Wiktor K.}} *Peter van Emde Boas, ''Machine Models and Simulations'' pp.&nbsp;3–66, appearing in: ::[[Jan van Leeuwen]], ed. "Handbook of Theoretical Computer Science. Volume A: Algorithms and Complexity'', The MIT PRESS/Elsevier, 1990. ISBN 0-444-88071-2 (volume A). QA 76.H279 1990. * Stephan Diehl, Pieter Hartel and Peter Sestoft, ''Abstract Machines for Programming Language Implementation'', Future Generation Computer Systems, Vol. 16(7), Elsevier, 2000.  {{FOLDOC}}  {{Refimprove|date=October 2009}}  {{DEFAULTSORT:Abstract Machine}} [[Category:Automata theory]] [[Category:Models of computation]]  [[ar:آلة مجردة]] [[de:Automat (Informatik)]] [[es:Máquina abstracta]] [[ko:추상 기계]] [[hr:Apstraktni stroj]] [[ja:計算模型]] [[pt:Autômato]] [[fi:Abstrakti kone]] [[zh:抽象機器]]
{{more footnotes|date=March 2010}}  In [[computer science]], the '''analysis of algorithms''' is the determination of the number of resources (such as time and storage) necessary to execute them. Most [[algorithm]]s are designed to work with inputs of arbitrary length. Usually the efficiency or running time of an algorithm is stated as a function relating the input length to the number of steps ([[time complexity]]) or storage locations (space complexity).  Algorithm analysis is an important part of a broader [[computational complexity theory]], which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.  In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. [[Big O notation]], [[omega notation]] and [[theta notation]] are used to this end. For instance, [[binary search]] is said to run in a number of steps proportional to the logarithm of the length of the list being searched, or in O(log(n)), colloquially "in [[logarithmic time]]". Usually [[Asymptotic analysis|asymptotic]] estimates are used because different [[implementation]]s of the same algorithm may differ in efficiency. However the efficiencies of any two "reasonable" implementations of a given algorithm are related by a constant multiplicative factor  called a ''hidden constant''.  Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called [[model of computation]]. A model of computation may be defined in terms of an [[abstract machine|abstract computer]], e.g., [[Turing machine]], and/or by postulating that certain operations are executed in unit time. For example, if the sorted list to which we apply binary search has ''n'' elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log<sub>2</sub> ''n''   1 time units are needed to return an answer. <!-- Exact measures of efficiency are useful to the people who actually implement and use algorithms, because they are more precise and thus enable them to know how much time they can expect to spend in execution. To some people (e.g. game programmers), a hidden constant can make all the difference between success and failure.-->  == Cost models == Time efficiency estimates depend on what we define to be a step. For the analysis to correspond usefully to the actual execution time, the time required to perform a step must be guaranteed to be bounded above by a constant. One must be careful here; for instance, some analyses count an addition of two numbers as one step. This assumption may not be warranted in certain contexts. For example, if the numbers involved in a computation may be arbitrarily large, the time required by a single addition can no longer be assumed to be constant.  Two cost models are generally used:<ref name="AhoHopcroft1974">{{cite book|author1=Alfred V. Aho|author2=John E. Hopcroft|author3=Jeffrey D. Ullman|title=The design and analysis of computer algorithms|year=1974|publisher=Addison-Wesley Pub. Co.}}, section 1.3</ref><ref name="Hromkovič2004">{{cite book|author=Juraj Hromkovič|title=Theoretical computer science: introduction to Automata, computability, complexity, algorithmics, randomization, communication, and cryptography|url=http://books.google.com/books?id=KpNet-n262QC&pg=PA177|year=2004|publisher=Springer|isbn=978-3-540-14015-3|pages=177–178}}</ref><ref name="Ausiello1999">{{cite book|author=Giorgio Ausiello|title=Complexity and approximation: combinatorial optimization problems and their approximability properties|url=http://books.google.com/books?id=Yxxw90d9AuMC&pg=PA3|year=1999|publisher=Springer|isbn=978-3-540-65431-5|pages=3–8}}</ref><ref name=Wegener20>{{Citation|last1=Wegener|first1=Ingo|title=Complexity theory: exploring the limits of efficient algorithms|publisher=[[Springer-Verlag]]|location=Berlin, New York|isbn=978-3-540-21045-0|year=2005|page=20|url=http://books.google.com/books?id=u7DZSDSUYlQC&pg=PA20}}</ref><ref name="Tarjan1983">{{cite book|author=[[Robert Endre Tarjan]]|title=Data structures and network algorithms|url=http://books.google.com/books?id=JiC7mIqg-X4C&pg=PA3|year=1983|publisher=SIAM|isbn=978-0-89871-187-5|pages=3–7}}</ref> * the '''uniform cost model''', also called '''uniform-cost measurement''' (and similar variations), assigns a constant cost to every machine operation, regardless of the size of the numbers involved * the '''logarithmic cost model''', also called '''logarithmic-cost measurement''' (and variations thereof), assigns a cost to every machine operation proportional to the number of bits involved  The latter is more cumbersome to use, so it's only employed when necessary, for example in the analysis of [[arbitrary-precision arithmetic]] algorithms, like those used in [[cryptography]].  A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice and therefore there are algorithms that are faster than what would naively be thought possible.<ref>[http://cstheory.stackexchange.com/questions/608/examples-of-the-price-of-abstraction Examples of the price of abstraction?], cstheory.stackexchange.com</ref>  ==Run-time analysis== Run-time analysis is a theoretical classification that estimates and anticipates the increase in ''[[DTIME|running time]]'' (or run-time) of an [[algorithm]] as its ''[[Information|input size]]'' (usually denoted as ''n'') increases.  Run-time efficiency is a topic of great interest in [[computer science]]:  A [[Computer program|program]] can take seconds, hours or even years to finish executing, depending on which algorithm it implements (see also [[performance analysis]], which is the analysis of an algorithm's run-time in practice).  ===Shortcomings of empirical metrics===  Since algorithms are [[platform-independent]] (i.e. a given algorithm can be implemented in an arbitrary [[programming language]] on an arbitrary [[computer]] running an arbitrary [[operating system]]), there are significant drawbacks to using an [[empirical]] approach to gauge the comparative performance of a given set of algorithms.  Take as an example a program that looks up a specific entry in a [[collation|sorted]] [[list (computing)|list]] of size ''n''.  Suppose this program were implemented on Computer A, a state-of-the-art machine, using a [[linear search]] algorithm, and on Computer B, a much slower machine, using a [[binary search]] algorithm.  [[benchmark (computing)|Benchmark testing]] on the two computers running their respective programs might look something like the following:  {| class="wikitable" |- ! ''n'' (list size) ! Computer A run-time<br />(in [[nanosecond]]s) ! Computer B run-time<br />(in [[nanosecond]]s) |- | 15 | 7  | 100,000  |- | 65 | 32  | 150,000  |- | 250 | 125  | 200,000  |- | 1,000 | 500  | 250,000  |}  Based on these metrics, it would be easy to jump to the conclusion that ''Computer A'' is running an algorithm that is far superior in efficiency to that of ''Computer B''.  However, if the size of the input-list is increased to a sufficient number, that conclusion is dramatically demonstrated to be in error:  {| class="wikitable" |- ! ''n'' (list size) ! Computer A run-time<br />(in [[nanosecond]]s) ! Computer B run-time<br />(in [[nanosecond]]s) |- | 15 | 7  | 100,000  |- | 65 | 32  | 150,000  |- | 250 | 125  | 200,000  |- | 1,000 | 500  | 250,000  |- | ... | ... | ... |- | 1,000,000 | 500,000 | 500,000 |- | 4,000,000 | 2,000,000 | 550,000 |- | 16,000,000 | 8,000,000 | 600,000 |- | ... | ... | ... |- | 63,072 &times; 10<sup>12</sup> | 31,536 &times; 10<sup>12</sup> ns,<br />or 1 year | 1,375,000 ns,<br />or 1.375 milliseconds |}  Computer A, running the linear search program, exhibits a [[linear]] growth rate.  The program's run-time is directly proportional to its input size.  Doubling the input size doubles the run time, quadrupling the input size quadruples the run-time, and so forth.  On the other hand, Computer B, running the binary search program, exhibits a [[logarithm]]ic growth rate.  Doubling the input size only increases the run time by a [[wiktionary:Constant|constant]] amount (in this example, 25,000 ns).  Even though Computer A is ostensibly a faster machine, Computer B will inevitably surpass Computer A in run-time because it's running an algorithm with a much slower growth rate.  ===Orders of growth=== {{main|Big O notation}} Informally, an algorithm can be said to exhibit a growth rate on the order of a [[Function (mathematics)|mathematical function]] if beyond a certain input size ''n'', the function ''f(n)'' times a positive constant provides an [[Asymptotic analysis|upper bound or limit]] for the run-time of that algorithm.  In other words, for a given input size ''n'' greater than some ''n<sub>0</sub>'' and a constant ''c'', the running time of that algorithm will never be larger than ''c &times; f(n)''.  This concept is frequently expressed using Big O notation.  For example, since the run-time of [[insertion sort]] [[quadratic growth|grows quadratically]] as its input size increases, insertion sort can be said to be of order ''O(n²)''.  Big O notation is a convenient way to express the [[Best, worst and average case|worst-case scenario]] for a given algorithm, although it can also be used to express the average-case &mdash; for example, the worst-case scenario for [[quicksort]] is ''O(n²)'', but the average-case run-time is ''O(n log n)''.<ref>The term ''lg'' is often used as shorthand for log<sub>2</sub></ref>  ===Empirical orders of growth===  Assuming the order of growth follows power rule, <math>O(n^a)</math>, the coefficient ''a'' can be found by taking empirical measurements (of run time, say), <math>\{t1, t2\}</math> at some problem-size points <math>\{n1, n2\}</math>, and calculating <math>t_2/t_1 = (n_2/n_1)^a</math> so that <math>a = \log(t_2/t_1) / \log(n_2/n_1)</math>. If the order of growth indeed follows the power rule, the empirical value of ''a'' will  stay constant at different ranges, and if not, it will change - but still could serve for comparison of any two given algorithms as to their ''empirical local orders of growth'' behaviour. Applied to the above table:  {| class="wikitable" |- ! ''n'' (list size) ! Computer A run-time<br />(in [[nanosecond]]s) ! Local order of growth<br />(n^_) ! Computer B run-time<br />(in [[nanosecond]]s) ! Local order of growth<br />(n^_) |- | 15 | 7  |  | 100,000  |  |- | 65 | 32  | 1.04 | 150,000  | 0.28 |- | 250 | 125  | 1.01 | 200,000  | 0.21 |- | 1,000 | 500  | 1.00 | 250,000  | 0.16 |- | ... | ... | ... | ... | ... |- | 1,000,000 | 500,000 |  | 500,000 |  |- | 4,000,000 | 2,000,000 | 1.00 | 550,000 | 0.07 |- | 16,000,000 | 8,000,000 | 1.00 | 600,000 | 0.06 |- | ... | ... | ... | ... | ... |}  It is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule. The empirical values for the second one are diminishing rapidly, suggesting it follows another rule of growth and in any case has much lower local order of growth (and improving further still), empirically, than the first one.  === Evaluating run-time complexity === The run-time complexity for the worst-case scenario of a given algorithm can sometimes be evaluated by examining the structure of the algorithm and making some simplifying assumptions.  Consider the following [[pseudocode]]:   1    ''get a positive integer from input''  2    '''if''' n > 10  3        '''print''' "This might take a while..."  4    '''for''' i = 1 '''to''' n  5        '''for''' j = 1 '''to''' i  6            '''print''' i * j  7    '''print''' "Done!"  A given computer will take a [[DTIME|discrete amount of time]] to execute each of the [[Instruction (computer science)|instructions]] involved with carrying out this algorithm.  The specific amount of time to carry out a given instruction will vary depending on which instruction is being executed and which computer is executing it, but on a conventional computer, this amount will be [[Deterministic system (mathematics)|deterministic]].<ref>However, this is not the case with a [[quantum computer]]</ref>  Say that the actions carried out in step 1 are considered to consume time T<sub>1</sub>, step 2 uses time T<sub>2</sub>, and so forth.  In the algorithm above, steps 1, 2 and 7 will only be run once.  For a worst-case evaluation, it should be assumed that step 3 will be run as well.  Thus the total amount of time to run steps 1-3 and step 7 is:  :<math>T_1   T_2   T_3   T_7. \,</math>  The [[program loop|loops]] in steps 4, 5 and 6 are trickier to evaluate.  The outer loop test in step 4 will execute ( n   1 ) times (note that an extra step is required to terminate the for loop, hence n   1 and not n executions), which will consume T<sub>4</sub>( n   1 ) time.  The inner loop, on the other hand, is governed by the value of i, which [[iteration|iterates]] from 1 to n.  On the first pass through the outer loop, j iterates from 1 to 1:  The inner loop makes one pass, so running the inner loop body (step 6) consumes T<sub>6</sub> time, and the inner loop test (step 5) consumes 2T<sub>5</sub> time.  During the next pass through the outer loop, j iterates from 1 to 2:  the inner loop makes two passes, so running the inner loop body (step 6) consumes 2T<sub>6</sub> time, and the inner loop test (step 5) consumes 3T<sub>5</sub> time.  Altogether, the total time required to run the inner loop body can be expressed as an [[arithmetic progression]]:  :<math>T_6   2T_6   3T_6   \cdots   (n-1) T_6   n T_6</math>  which can be [[factorization|factored]]<ref>It can be proven by [[Mathematical induction|induction]] that <math>1   2   3   \cdots   (n-1)   n = \frac{n(n 1)}{2}</math></ref> as  :<math>T_6 \left[ 1   2   3   \cdots   (n-1)   n \right] = T_6 \left[ \frac{1}{2} (n^2   n) \right] </math>  The total time required to run the inner loop test can be evaluated similarly:  :<math>2T_5   3T_5   4T_5   \cdots   (n-1) T_5   n T_5   (n   1) T_5</math> :<br /><math> = T_5   2T_5   3T_5   4T_5   \cdots   (n-1)T_5   nT_5   (n 1)T_5 - T_5</math>  which can be factored as  :<math>T_5 \left[ 1 2 3 \cdots   (n-1)   n   (n   1) \right] - T_5 = T_5 \left[ \frac{1}{2} (n^2   3n   2) \right] - T_5</math>  Therefore the total running time for this algorithm is:  :<math>f(n) = T_1   T_2   T_3   T_7   (n   1)T_4   \left[ \frac{1}{2} (n^2   n) \right] T_6   \left[ \frac{1}{2} (n^2 3n 2) \right] T_5 - T_5</math>  which [[reduction (mathematics)|reduces]] to  :<math>f(n) = \left[ \frac{1}{2} (n^2   n) \right] T_6   \left[ \frac{1}{2} (n^2   3n) \right] T_5   (n   1)T_4   T_1   T_2   T_3   T_7 </math>  As a [[rule-of-thumb]], one can assume that the highest-order term in any given function dominates its rate of growth and thus defines its run-time order.  In this example, n² is the highest-order term, so one can conclude that f(n) = O(n²).  Formally this can be proven as follows:<blockquote>Prove that <math>\left[ \frac{1}{2} (n^2   n) \right] T_6   \left[ \frac{1}{2} (n^2   3n) \right] T_5   (n   1)T_4   T_1   T_2   T_3   T_7 \le cn^2,\ n \ge n_0</math> <br /><br /> <math>\left[ \frac{1}{2} (n^2   n) \right] T_6   \left[ \frac{1}{2} (n^2   3n) \right] T_5   (n   1)T_4   T_1   T_2   T_3   T_7</math>  <br /><math>\le ( n^2   n )T_6   ( n^2   3n )T_5   (n   1)T_4   T_1   T_2   T_3   T_7</math> (''for n ≥ 0'')  <br /><br />Let k be a constant greater than or equal to [T<sub>1</sub>..T<sub>7</sub>] <br /><br /><math>T_6( n^2   n )   T_5( n^2   3n )   (n   1)T_4   T_1   T_2   T_3   T_7 \le k( n^2   n )   k( n^2   3n )   kn   5k</math> <br /><math>= 2kn^2   5kn   5k \le 2kn^2   5kn^2   5kn^2</math> (''for n ≥ 1'') <math>= 12kn^2</math>  <br /><br />Therefore <math>\left[ \frac{1}{2} (n^2   n) \right] T_6   \left[ \frac{1}{2} (n^2   3n) \right] T_5   (n   1)T_4   T_1   T_2   T_3   T_7 \le cn^2, n \ge n_0</math> for <math>c = 12k, n_0 = 1</math></blockquote>  A more [[elegance|elegant]] approach to analyzing this algorithm would be to declare that [T<sub>1</sub>..T<sub>7</sub>] are all equal to one unit of time greater than or equal to [T<sub>1</sub>..T<sub>7</sub>].{{Clarify|date=April 2011}}  This would mean that the algorithm's running time breaks down as follows:<ref>This approach, unlike the above approach, neglects the constant time consumed by the loop tests which terminate their respective loops, but it is [[Trivial (mathematics)|trivial]] to prove that such omission does not affect the final result</ref><blockquote><math>4 \sum_{i=1}^n i\leq 4 \sum_{i=1}^n n=4 n^2\leq5n^2</math> (''for n ≥ 1'') <math>=O(n^2).</math></blockquote>  ===Growth rate analysis of other resources=== The methodology of run-time analysis can also be utilized for predicting other growth rates, such as consumption of [[DSPACE|memory space]].  As an example, consider the following pseudocode which manages and reallocates memory usage by a program based on the size of a [[computer file|file]] which that program manages:   '''while''' (''file still open'')      '''let''' n = ''size of file''      '''for''' ''every 100,000 [[kilobyte]]s of increase in file size''          ''double the amount of memory reserved''  In this instance, as the file size n increases, memory will be consumed at an [[exponential growth]] rate, which is order O(2<sup>n</sup>).<ref>Note that this is an extremely rapid and most likely unmanageable growth rate for consumption of memory [[Resource (computer science)|resources]]</ref>  ==Relevance== Algorithm analysis is important in practice because the accidental or unintentional use of an inefficient algorithm can significantly impact system performance. In time-sensitive applications, an algorithm taking too long to run can render its results outdated or useless. An inefficient algorithm can also end up requiring an uneconomical amount of computing power or storage in order to run, again rendering it practically useless.  ==See also== * [[Amortized analysis]] * [[Asymptotic analysis]] * [[Asymptotic computational complexity]] * [[Best, worst and average case]] * [[Big O notation]] * [[Computational complexity theory]] * [[Master theorem]] * [[NP-Complete]] * [[Numerical analysis]] * [[Polynomial time]] * [[Program optimization]] * [[Profiling (computer programming)]] * [[Smoothed analysis]] * [[Time complexity]] — includes table of orders of growth for common algorithms  ==Notes== {{reflist}}  ==References== *{{Cite book |authorlink=Thomas H. Cormen |first=Thomas H. |last=Cormen |authorlink2=Charles E. Leiserson |first2=Charles E. |last2=Leiserson |authorlink3=Ronald L. Rivest |first3=Ronald L. |last3=Rivest |lastauthoramp=yes |authorlink4=Clifford Stein |first4=Clifford |last4=Stein |title=[[Introduction to Algorithms]] |edition=Second |publisher=MIT Press and McGraw-Hill |location=Cambridge, MA |year=2001 |isbn=0-262-03293-7 |others=Chapter 1: Foundations |pages=3–122 }} *{{Cite book |title=Algorithms in C, Parts 1-4: Fundamentals, Data Structures, Sorting, Searching |edition=3rd |authorlink=Robert Sedgewick (computer scientist) |first=Robert |last=Sedgewick |location=Reading, MA |publisher=Addison-Wesley Professional |year=1998 |isbn=978-0-201-31452-6 }} *{{Cite book |title=[[The Art of Computer Programming]] |authorlink=Donald Knuth |first=Donald |last=Knuth |location= |publisher=Addison-Wesley |isbn= |year= }} *{{Cite book |first=Daniel A. |last=Greene |first2=Donald E. |last2=Knuth |title=Mathematics for the Analysis of Algorithms |edition=Second |location= |publisher=Birkhäuser |year=1982 |isbn=3-7463-3102-X {{Please check ISBN|reason=Check digit (X) does not correspond to calculated figure.}} }} *{{Cite book |authorlink=Oded Goldreich |first=Oded |last=Goldreich |title=Computational Complexity: A Conceptual Perspective |location= |publisher=[[Cambridge University Press]] |year=2010 |isbn=978-0-521-88473-0 }}  {{DEFAULTSORT:Analysis Of Algorithms}} [[Category:Computational complexity theory]] [[Category:Analysis of algorithms| ]]  [[ar:تحليل الخوارزميات]] [[es:Análisis de algoritmos]] [[fa:تحلیل الگوریتم‌ها]] [[ja:アルゴリズム解析]] [[pl:Analiza algorytmów]] [[pt:Análise de algoritmos]] [[sl:Časovna zahtevnost]] [[tr:Algoritma çözümlemesi]]
[[Image:ALU symbol.svg|right|thumb|Arithmetic Logic Unit schematic symbol]] [[Image:KL Texas Instruments ALU SN74AS888.jpg|right|thumb|180px|Cascadable 8 Bit ALU Texas Instruments SN74AS888]]  In [[computing]], an '''arithmetic and logic unit''' (''ALU'') is a [[digital circuit]] that performs [[arithmetic]] and [[logical]] operations.  The ALU is a fundamental building block of the [[central processing unit]] of a computer, and even the simplest [[microprocessor]]s contain one for purposes such as maintaining timers. The processors found inside modern CPUs and graphics processing units ([[Graphics processing unit|GPUs]]) accommodate very powerful and very complex ALUs; a single component may contain a number of ALUs.  Mathematician [[John von Neumann]] proposed the ALU concept in 1945, when he wrote a report on the foundations for a new computer called the [[EDVAC]].  Research into ALUs remains an important part of [[computer science]], falling under '''Arithmetic and logic structures''' in the [[ACM Computing Classification System]].  ==Numerical systems== {{Main|Signed number representations}} An ALU must process numbers using the same format as the rest of the digital circuit. The format of modern processors is almost always the [[two's complement]] binary number representation. Early computers used a wide variety of number systems, including [[Signed number representations#Ones' complement|ones' complement]], [[Signed number representations#Two's complement|two's complement]], [[Signed number representations#Sign-and-magnitude method|sign-magnitude]] format, and even true decimal systems, with various{{#tag:ref|Including Binary-Coded Decimal (BCD) in 4 bits, 2-out-of-5 coding in five bits,<ref>[http://bitsavers.org/pdf/ibm/7070/A22-7003-01_7070_Reference_Jan60.pdf Reference Manual, 7070 Data Processing System, A22-7003-01]</ref> 5-bit biquinary<ref name=biquinary group=NB>IBM and UNIVAC used the term biquinqary with different meanings.</ref> encoding,<ref>[http://bitsavers.org/pdf/univac/uss/SolidState90_Specs.pdf UNIVAC SOLID-STATE 90 Specification Features]</ref> and 2-out-of-seven biquinary<ref name=biquinary group=NB/> encoding in 7 bits<ref>[http://bitsavers.org/pdf/ibm/650/22-6060-2_650_OperMan.pdf 650 Data Procssing Machine Manual of Operation, 22-6060-2]</ref>|name=name|group=NB}} representation of the digits. ALUs for each one of these that makes it easier for the ALUs to calculate additions and subtractions.{{Citation needed|date=October 2007}}  The ones' complement and two's complement number systems allow for subtraction to be accomplished by adding the negative of a number in a very simple way which negates the need for specialized circuits to do subtraction; however, calculating the negative in two's complement requires adding a one to the low order bit and propagating the carry. An alternative way to do two's complement subtraction of A−B is to present a one to the carry input of the adder and use ¬B rather than B as the second input.  ==Practical overview==  Most of a processor's operations are performed by one or more ALUs. An ALU loads data from [[processor register|input register]]s, an external [[Control Unit]] then tells the ALU what operation to perform on that data, and then the ALU stores its result into an output register. The [[Control Unit]] is responsible for moving the processed data between these registers, ALU and memory.  ===Complex operations=== Engineers can design an Arithmetic Logic Unit to calculate any<!-- anything at all?? --> operation. The more complex the operation, the more expensive the ALU is, the more space it uses in the processor, and the more power it dissipates. Therefore, engineers compromise. They make the ALU powerful enough to make the processor fast, yet not so complex as to become prohibitive. For example, computing the square root of a number might use:  # '''Calculation in a single clock''' Design an extraordinarily complex ALU that calculates the square root of any number in a single step. # '''Calculation pipeline''' Design a very complex ALU that calculates the square root of any number in several steps. The intermediate results go through a series of circuits arranged like a factory production line. The ALU can accept new numbers to calculate even before having finished the previous ones. The ALU can now produce numbers as fast as a single-clock ALU, although the results start to flow out of the ALU only after an initial delay. # '''Iterative calculation''' Design a complex ALU that calculates the square root through several steps. This usually relies on control from a complex [[control unit]] with built-in [[microcode]]. # '''Co-processor''' Design a simple ALU in the processor, and sell a separate specialized and costly processor that the customer can install just beside this one, and implements one of the options above. # '''Software libraries''' Tell the programmers<!-- who am "I" supposed to be here? --> that there is no [[co-processor]] and there is no [[emulator|emulation]], so they will have to write their own algorithms to calculate square roots by software. # '''Software emulation''' [[Emulate]] the existence of the [[co-processor]], that is, whenever a program attempts to perform the square root calculation, make the processor check if there is a [[co-processor]] present and use it if there is one; if there isn't one, [[interrupt]] the processing of the program and invoke the [[operating system]] to perform the square root calculation through some software algorithm.  The options above go from the fastest and most expensive one to the slowest and least expensive one. Therefore, while even the simplest computer can calculate the most complicated formula, the simplest computers will usually take a long time doing that because of the several steps for calculating the formula.  Powerful processors like the [[Intel Core]] and [[AMD64]] implement option #1 for several simple operations<!-- such as? -->, #2 for the most common complex operations<!-- like? --> and #3 for the extremely complex operations<!-- ? -->.  ===Inputs and outputs=== The inputs to the ALU are the data to be operated on (called [[operand]]s) and a code from the [[control unit]] indicating which operation to perform. Its output is the result of the computation.  One thing designers must keep in mind is whether the ALU will operate on [[Endianness|big-endian]] or little-endian numbers.  In many designs the ALU also takes or generates as inputs or outputs a set of condition codes from or to a [[status register]].  These codes are used to indicate cases such as [[carry (arithmetic)|carry]]-in or carry-out, [[arithmetic overflow|overflow]], [[division by zero|divide-by-zero]], etc.<!-- <ref name="Stallings page 290-291"/> -->    A [[floating-point unit]] also performs arithmetic operations between two values, but they do so for numbers in [[Floating point|floating-point]] representation, which is much more complicated than the [[two's complement]] representation used in a typical ALU. In order to do these calculations, a [[Floating-point unit|FPU]] has several complex circuits built-in, including some internal ALUs.  In modern practice, engineers typically refer to the ALU as the circuit that performs integer arithmetic operations (like [[two's complement]] and [[Binary-coded decimal|BCD]]). Circuits that calculate more complex formats like [[floating point]], [[complex numbers]], etc. usually receive a more specific name such as FPU.  ==See also== *[[7400 series]] *[[74181]] *[[Adder (electronics)]] *[[Multiplication ALU]] *[[Digital circuit]] *[[Division (electronics)]] *[[Control Unit]]  ==Notes== {{Reflist|group=NB}}  ==References== <!--This article uses the Cite.php citation mechanism. If you would like more information on how to add footnotes to this article, please see http://meta.wikimedia.org/wiki/Cite/Cite.php --> {{Reflist}} *{{Cite book | first=Enoch| last=Hwang| year=2006| title=Digital Logic and Microprocessor Design with VHDL| publisher=Thomson| isbn=0-534-46593-5| url=http://faculty.lasierra.edu/~ehwang/digitaldesign| authorlink=Enoch Hwang}} *{{Cite book | first=William| last=Stallings| year=2006| title=Computer Organization & Architecture: Designing for Performance 7th ed| publisher=Pearson Prentice Hall| isbn=0-13-185644-8| url=http://williamstallings.com/COA/COA7e.html| authorlink=William Stallings}}  ==External links== *[http://www.fullchipdesign.com/tyh/alu_arithmetic_logical_unit.htm ALU and its Micro-operations: Bitwise, Arithmetic and Shift] *[http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=12762&objectType=FILE A Simulator of Complex ALU in MATLAB]   {{CPU technologies}}  ***************************************************************************************  {{DEFAULTSORT:Arithmetic Logic Unit}} [[Category:Digital circuits]] [[Category:Central processing unit]] [[Category:Computer arithmetic]]  [[ar:وحدة الحساب والمنطق]] [[bg:Аритметично-логическо устройство]] [[ca:Unitat aritmeticològica]] [[cs:Aritmeticko-logická jednotka]] [[de:Arithmetisch-logische Einheit]] [[et:Aritmeetika-loogikaplokk]] [[el:Αριθμητική και Λογική Μονάδα]] [[es:Unidad aritmético lógica]] [[eu:Unitate aritmetiko-logiko]] [[fa:واحد محاسبه و منطق]] [[fr:Unité arithmétique et logique]] [[gl:Unidade aritmético-lóxica]] [[ko:산술 논리 장치]] [[hr:Artimetičko-logička jedinica]] [[id:ALU]] [[it:Unità aritmetica e logica]] [[he:יחידה אריתמטית-לוגית]] [[la:Apparatus arithmeticus et logicus]] [[lv:Aritmētiski loģiskā ierīce]] [[lb:Aritmethic Logic Unit]] [[hu:Aritmetikai-logikai egység]] [[ms:Unit aritmetik dan logik]] [[nl:Arithmetic logic unit]] [[ja:演算装置]] [[no:Aritmetisk logisk enhet]] [[pl:Jednostka arytmetyczno-logiczna]] [[pt:Unidade lógica e aritmética]] [[ro:Unitate aritmetică-logică]] [[ru:Арифметическо-логическое устройство]] [[sq:ALU]] [[simple:Arithmetic logic unit]] [[sk:Aritmeticko-logická jednotka]] [[sv:Aritmetisk logisk enhet]] [[th:หน่วยคำนวณและตรรกะ]] [[tr:Aritmetik mantık birimi]] [[zh:算術邏輯單元]]
{{Redirect|AI||Ai (disambiguation){{!}}Ai}} {{Other uses}} {{merge|Synthetic intelligence|discuss=Talk:Artificial intelligence#Merger proposal|date=July 2012}} <!-- DEFINITIONS --> '''Artificial intelligence''' ('''AI''') is the [[intelligence]] of machines and the branch of [[computer science]] that aims to create it. AI textbooks define the field as "the study and design of intelligent agents"<ref name="Definition of AI"/> where an [[intelligent agent]] is a system that perceives its environment and takes actions that maximize its chances of success.<ref name="Intelligent agents"/> [[John McCarthy (computer scientist)|John McCarthy]], who coined the term in 1955,<ref name="Coining of the term AI"/> defines it as "the science and engineering of making intelligent machines."<ref name="McCarthy's definition of AI"/>   <!-- SUMMARIZING PROBLEMS, APPROACHES, TOOLS and (TODO:) APPLICATIONS --> AI research is highly technical and specialized, deeply divided into subfields that often fail to communicate with each other.<ref name="Fragmentation of AI"/> Some of the division is due to social and cultural factors: subfields have grown up around particular institutions and the work of individual researchers. AI research is also divided by several technical issues. There are subfields which are focussed on the solution of specific [[#Problems|problems]], on one of several possible [[#Approaches|approaches]], on the use of widely differing [[#Tools|tools]] and towards the accomplishment of particular [[#Applications|applications]]. The central problems of AI include such traits as reasoning, knowledge, planning, learning, communication, perception and the ability to move and manipulate objects.<ref name="Problems of AI"/> General intelligence (or "[[strong AI]]") is still among the field's long term goals.<ref name="General intelligence"/> Currently popular approaches include [[#Statistical|statistical methods]],  [[#Sub-symbolic|computational intelligence]] and [[#Symbolic|traditional symbolic AI]]. There are an enormous number of tools used in AI, including versions of [[#Search and optimization|search and mathematical optimization]], [[#Logic|logic]], [[#Probabilistic methods for uncertain reasoning|methods based on probability and economics]], and many others.  <!-- SUMMARIZING FICTION/SPECULATION, PHILOSOPHY, HISTORY --> The field was founded on the claim that a central property of humans, intelligence—the [[sapience]] of ''[[Homo sapiens]]''—can be so precisely described that it can be simulated by a machine.<ref>See the [[Dartmouth conference|Dartmouth proposal]], under [[#Philosophy|Philosophy]], below.</ref> This raises philosophical issues about the nature of the [[mind]] and the ethics of creating artificial beings, issues which have been addressed by [[History of AI#AI in myth, fiction and speculation|myth]], [[artificial intelligence in fiction|fiction]] and [[philosophy of AI|philosophy]] since antiquity.<ref name="McCorduck's thesis"/> Artificial intelligence has been the subject of optimism,<ref>The optimism referred to includes the predictions of early AI researchers (see [[History of AI#The optimism|optimism in the history of AI]]) as well as the ideas of modern [[transhumanism|transhumanists]] such as [[Ray Kurzweil]].</ref> but has also suffered [[AI winter|setbacks]]<ref>The "setbacks" referred to include the [[AI winter#Machine translation and the ALPAC report of 1966|ALPAC report]] of 1966, the abandonment of [[perceptrons]] in 1970, [[AI winter#The Lighthill report|the Lighthill Report]] of 1973 and the [[AI winter#The collapse of the Lisp machine market in 1987|collapse of the lisp machine market]] in 1987.</ref> and, today, has become an essential part of the technology industry, providing the heavy lifting for many of the most difficult problems in computer science.<ref name="AI widely used"/>  ==History== <!-- THIS IS A SOCIAL HISTORY. TECHNICAL HISTORY IS COVERED IN THE "APPROACHES" AND "TOOLS" SECTIONS. --> {{Main|History of artificial intelligence|Timeline of artificial intelligence}}  <!-- AN ANCIENT DREAM (MCCORDUCK'S THESIS) --> Thinking machines and artificial beings appear in [[Greek myth]]s, such as [[Talos]] of [[Crete]], the bronze robot of [[Hephaestus]], and [[Pygmalion (mythology)|Pygmalion's]] [[Galatea (mythology)|Galatea]].<ref name="AI in myth"/> Human likenesses believed to have intelligence were built in every major civilization: animated [[cult image]]s were worshipped in [[Egypt]] and [[Greece]]<ref name="Cult images as artificial intelligence"/> and humanoid [[automaton]]s were built by [[King Mu of Zhou#Automaton|Yan Shi]], [[Hero of Alexandria]] and [[Al-Jazari]].<ref name="Humanoid automata"/> It was also widely believed that artificial beings had been created by [[Jābir ibn Hayyān]], [[Judah Loew]] and [[Paracelsus]].<ref name="Artificial beings"/> By the 19th and 20th centuries, artificial beings had become a common feature in fiction, as in [[Mary Shelley]]'s ''[[Frankenstein]]'' or [[Karel Čapek]]'s ''[[R.U.R. (Rossum's Universal Robots)]]''.<!-- PLEASE DON'T ADD MORE EXAMPLES. THIS IS ENOUGH. SEE SECTION AT BOTTOM OF ARTICLE ON SPECULATION.--><ref name="AI in early science fiction"/> [[Pamela McCorduck]] argues that all of these are examples of an ancient urge, as she describes it, "to forge the gods".<ref name="McCorduck's thesis"/> Stories of these creatures and their fates discuss many of the same hopes, fears and [[ethics of artificial intelligence|ethical concerns]] that are presented by artificial intelligence.  <!-- MAJOR INTELLECTUAL PRECURSORS: LOGIC, THEORY OF COMPUTATION, CYBERNETICS, INFORMATION THEORY, EARLY NEURAL NETS --> Mechanical or [[formal reasoning|"formal" reasoning]] has been developed by philosophers and mathematicians since antiquity. The study of logic led directly to the invention of the [[computer|programmable digital electronic computer]], based on the work of mathematician [[Alan Turing]] and others. Turing's [[theory of computation]] suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable (imaginable) act of mathematical deduction.<ref>This insight, that digital computers can simulate any process of formal reasoning, is known as the [[Church–Turing thesis]].</ref><ref name="Formal reasoning"/> This, along with concurrent discoveries in [[neurology]], [[information theory]] and [[cybernetic]]s, inspired a small group of researchers to begin to seriously consider the possibility of building an electronic brain.<ref name="AI's immediate precursors"/>  <!-- THE "GOLDEN YEARS" 1956-1974 --> The field of AI research was founded at [[Dartmouth Conferences|a conference]] on the campus of [[Dartmouth College]] in the summer of 1956.<ref name="Dartmouth conference"/> The attendees, including [[John McCarthy (computer scientist)|John McCarthy]], [[Marvin Minsky]], [[Allen Newell]] and [[Herbert A. Simon|Herbert Simon]], became the leaders of AI research for many decades.<ref name="Hegemony of the Dartmouth conference attendees"/> They and their students wrote programs that were, to most people, simply astonishing:<ref>Russell and Norvig write "it was astonishing whenever a computer did anything kind of smartish." {{Harvnb|Russell|Norvig|2003|p=18}}</ref> Computers were solving word problems in algebra, proving logical theorems and speaking English.<ref name="Golden years of AI"/> By the middle of the 1960s, research in the U.S. was heavily funded by the [[DARPA|Department of Defense]]<ref name="AI funding in the 60s"/> and laboratories had been established around the world.<ref name="AI in England"/> AI's founders were profoundly optimistic about the future of the new field: [[Herbert A. Simon|Herbert Simon]] predicted that "machines will be capable, within twenty years, of doing any work a man can do" and [[Marvin Minsky]] agreed, writing that "within a generation&nbsp;... the problem of creating 'artificial intelligence' will substantially be solved".<ref name="Optimism of early AI"/>  <!-- FIRST AI WINTER --> They had failed to recognize the difficulty of some of the problems they faced.<ref>See {{See section|History of artificial intelligence|The problems}}</ref> In 1974, in response to the criticism of [[Sir James Lighthill]] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off all undirected exploratory research in AI. The next few years, when funding for projects was hard to find, would later be called the "[[AI winter]]".<ref name="First AI winter"/>  <!-- BOOM OF THE 1980s, SECOND AI WINTER --> In the early 1980s, AI research was revived by the commercial success of [[expert systems]],<ref name="Expert systems"/> a form of AI program that simulated the knowledge and analytical skills of one or more human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's [[fifth generation computer]] project inspired the U.S and British governments to restore funding for academic research in the field.<ref name="AI in the 80s"/> However, beginning with the collapse of the [[Lisp Machine]] market in 1987, AI once again fell into disrepute, and a second, longer lasting [[AI winter]] began.<ref name="Second AI winter"/>  <!-- TO THE PRESENT --> <!-- Deleted image removed: [[File:Kasparov v Deepblue.gif|thumb|left|Garry Kasparov versus Deep Blue, New York, 1997.]]   --> In the 1990s and early 21st century, AI achieved its greatest successes, albeit somewhat behind the scenes. Artificial intelligence is used for logistics, [[data mining]], [[medical diagnosis]] and many other areas throughout the technology industry.<ref name="AI widely used"/> The success was due to several factors: the increasing computational power of computers (see [[Moore's law]]), a greater emphasis on solving specific subproblems, the creation of new ties between AI and other fields working on similar problems, and a new commitment by researchers to solid mathematical methods and rigorous scientific standards.<ref name="Formal methods in AI"/>  On 11 May 1997, [[IBM Deep Blue|Deep Blue]] became the first computer chess-playing system to beat a reigning world chess champion, [[Garry Kasparov]].<ref>{{Harvnb|McCorduck|2004|pp=480–483}}</ref> In 2005, a Stanford robot won the [[DARPA Grand Challenge]] by driving autonomously for 131 miles along an unrehearsed desert trail.<ref>[http://www.darpa.mil/grandchallenge/ DARPA Grand Challenge – home page]</ref> Two years later, a team from CMU won the [[DARPA Urban Challenge]] when their vehicle autonomously navigated 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws.<ref>{{cite web|url=http://archive.darpa.mil/grandchallenge/ |title=Welcome |publisher=Archive.darpa.mil |accessdate=31 October 2011}}</ref>  In February 2011, in a [[Jeopardy!]] [[quiz show]] exhibition match, [[IBM]]'s [[question answering system]], [[Watson (artificial intelligence software)|Watson]], defeated the two greatest Jeopardy! champions, [[Brad Rutter]] and [[Ken Jennings]], by a significant margin.<ref>{{cite news| url=http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html | work=The New York Times | first=John | last=Markoff | title=On 'Jeopardy!' Watson Win Is All but Trivial | date=16 February 2011}}</ref>  The leading-edge definition of artificial intelligence research is changing over time. One pragmatic definition is: "AI research is that which computing scientists do not know how to do cost-effectively today." For example, in 1956 [[optical character recognition]] (OCR) was considered AI, but today, sophisticated OCR software with a context-sensitive [[spell checker]] and [[grammar checker]] software comes for free with most [[image scanner]]s. No one would any longer consider already-solved computing science problems like OCR "artificial intelligence" today.  Low-cost entertaining chess-playing software is commonly available for tablet computers. [[DARPA]] no longer provides significant funding for chess-playing computing system development. The [[Kinect]] which provides a 3D body–motion interface for the [[Xbox 360]] uses algorithms that emerged from lengthy AI research,<ref>[http://www.i-programmer.info/news/105-artificial-intelligence/2176-kinects-ai-breakthrough-explained.html Kinect's AI breakthrough explained]</ref> but few consumers realize the technology source.  AI applications are no longer the exclusive domain of [[U.S. Department of Defense]] R&D, but are now commonplace consumer items and inexpensive intelligent toys.  In common usage, the term "AI" no longer seems to apply to off-the-shelf solved computing-science problems, which may have originally emerged out of years of AI research. {{break}}  == Problems ==<!--- This is linked to in the introduction to the article and to the "AI research" section --> The general problem of simulating (or creating) intelligence has been broken down into a number of specific [[History of AI#The problems|sub-problems]]. These consist of particular traits or capabilities that researchers would like an intelligent system to display. The traits described below have received the most attention.<ref name="Problems of AI"/>  ===Deduction, reasoning, problem solving===<!-- This is linked to in the introduction --> <!-- SOLVED PROBLEMS --> Early AI researchers developed algorithms that imitated the step-by-step reasoning that humans use when they solve puzzles or make logical deductions.<ref name="Reasoning"/> By the late 1980s and '90s, AI research had also developed highly successful methods for dealing with [[uncertainty|uncertain]] or incomplete information, employing concepts from [[probability]] and economics.<ref name="Uncertain reasoning"/>  <!-- TWO IMPORTANT UNSOLVED PROBLEMS --> For difficult problems, most of these algorithms can require enormous computational resources – most experience a "[[combinatorial explosion]]": the amount of memory or computer time required becomes astronomical when the problem goes beyond a certain size. The search for more efficient problem-solving algorithms is a high priority for AI research.<ref name="Intractability"/>  Human beings solve most of their problems using fast, intuitive judgements rather than the conscious, step-by-step deduction that early AI research was able to model.<ref name="Psychological evidence of sub-symbolic reasoning"/> AI has made some progress at imitating this kind of "sub-symbolic" problem solving: [[embodied agent]] approaches emphasize the importance of [[Sensory-motor coupling|sensorimotor]] skills to higher reasoning; [[neural net]] research attempts to simulate the structures inside human and animal brains that give rise to this skill; [[#Statistical|statistical approaches to AI]] mimic the probabilistic nature of the human ability to guess.  ===Knowledge representation===<!-- This is linked to in the introduction --> [[Image:GFO taxonomy tree.png|right|thumb|An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts.]] {{Main|Knowledge representation|Commonsense knowledge}}  [[Knowledge representation]]<ref name="Knowledge representation"/> and [[knowledge engineering]]<ref name="Knowledge engineering"/> are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects;<ref name="Representing categories and relations"/> situations, events, states and time;<ref name="Representing time"/> causes and effects;<ref name="Representing causation"/> knowledge about knowledge (what we know about what other people know);<ref name="Representing knowledge about knowledge"/> and many other, less well researched domains. A representation of "what exists" is an [[ontology (computer science)|ontology]] (borrowing a word from traditional philosophy), of which the most general are called [[upper ontology|upper ontologies]].<ref name="Ontology"/>  Among the most difficult problems in knowledge representation are:  ;[[Default reasoning]] and the [[qualification problem]]: Many of the things people know take the form of "working assumptions." For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. None of these things are true about all birds. [[John McCarthy (computer scientist)|John McCarthy]] identified this problem in 1969<ref name="Qualification problem"/> as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.<ref name="Default reasoning and non-monotonic logic"/>  ;The breadth of [[commonsense knowledge]]: The number of atomic facts that the average person knows is astronomical. Research projects that attempt to build a complete knowledge base of [[commonsense knowledge]] (e.g., [[Cyc]]) require enormous amounts of laborious [[ontology engineering|ontological engineering]] — they must be built, by hand, one complicated concept at a time.<ref name="Breadth of commonsense knowledge"/> A major goal is to have the computer understand enough concepts to be able to learn by reading from sources like the internet, and thus be able to add to its own ontology.{{Citation needed|date=October 2010}}  ;The subsymbolic form of some [[commonsense knowledge]]: Much of what people know is not represented as "facts" or "statements" that they could express verbally. For example, a chess master will avoid a particular chess position because it "feels too exposed"<ref>{{Harvnb|Dreyfus|Dreyfus|1986}}</ref> or an art critic can take one look at a statue and instantly realize that it is a fake.<ref>{{Harvnb|Gladwell|2005}}</ref> These are intuitions or tendencies that are represented in the brain non-consciously and sub-symbolically.<ref name="Intuition"/> Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related problem of sub-symbolic reasoning, it is hoped that [[situated artificial intelligence|situated AI]], [[computational intelligence]], or [[#Statistical|statistical AI]] will provide ways to represent this kind of knowledge.<ref name="Intuition"/>  ===Planning===<!-- This is linked to in the introduction --> [[Image:Hierarchical-control-system.svg|thumb| A [[hierarchical control system]] is a form of [[control system]] in which a set of devices and governing software is arranged in a hierarchy.]]  {{Main|Automated planning and scheduling}}  Intelligent agents must be able to set goals and achieve them.<ref name="Planning"/> They need a way to visualize the future (they must have a representation of the state of the world and be able to make predictions about how their actions will change it) and be able to make choices that maximize the [[utility]] (or "value") of the available choices.<ref name="Information value theory"/>  In classical planning problems, the agent can assume that it is the only thing acting on the world and it can be certain what the consequences of its actions may be.<ref name="Classical planning"/> However, if the agent is not the only actor, it must periodically ascertain whether the world matches its predictions and it must change its plan as this becomes necessary, requiring the agent to reason under uncertainty.<ref name="Non-deterministic planning"/>  [[Multi-agent planning]] uses the [[cooperation]] and competition of many agents to achieve a given goal. [[Emergent behavior]] such as this is used by [[evolutionary algorithms]] and [[swarm intelligence]].<ref name="Multi-agent planning"/>  ===Learning===<!-- This is linked to in the introduction --> {{Main|Machine learning}}  [[Machine learning]]<ref name="Machine learning"/> has been central to AI research from the beginning.<ref> [[Alan Turing]] discussed the centrality of learning as early as 1950, in his classic paper [[Computing Machinery and Intelligence]]. {{Harv|Turing|1950}}</ref> In 1956, at the original Dartmouth AI summer conference, [[Ray Solomonoff]] wrote a report on unsupervised probabilistic machine learning: "An Inductive Inference Machine".<ref>[http://world.std.com/~rjs/indinf56.pdf (pdf scanned copy of the original)] (version published in 1957, An Inductive Inference Machine," IRE Convention Record, Section on Information Theory, Part 2, pp. 56–62)</ref> [[Unsupervised learning]] is the ability to find patterns in a stream of input. [[Supervised learning]] includes both [[statistical classification|classification]] and numerical [[Regression analysis|regression]]. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In [[reinforcement learning]]<ref name="Reinforcement learning"/> the agent is rewarded for good responses and punished for bad ones. These can be analyzed in terms of [[decision theory]], using concepts like [[utility (economics)|utility]]. The mathematical analysis of machine learning algorithms and their performance is a branch of [[theoretical computer science]] known as [[computational learning theory]].<ref name="Computational learning theory"/>  ===Natural language processing===<!-- This is linked to in the introduction --> [[File:ParseTree.svg|thumb| A [[parse tree]] represents the [[syntax|syntactic]] structure of a sentence according to some [[formal grammar]].]] {{Main|Natural language processing}}  [[Natural language processing]]<ref name="Natural language processing"/> gives machines the ability to read and understand the languages that humans speak. A sufficiently powerful natural language processing system would enable [[natural language user interface]]s and the acquisition of knowledge directly from human-written sources, such as Internet texts. Some straightforward applications of natural language processing include [[information retrieval]] (or [[text mining]]) and [[machine translation]].<ref name="Applications of natural language processing"/>  A common method of processing and extracting meaning from natural language is through semantic indexing.  Increases in processing speeds and the drop in the cost of data storage makes indexing large volumes of abstractions of the users input much more efficient.  ===Motion and manipulation===<!-- This is linked to in the introduction --> {{Main|Robotics}}  The field of [[robotics]]<ref name="Robotics"/> is closely related to AI. Intelligence is required for robots to be able to handle such tasks as object manipulation<ref name="Configuration space"/> and [[motion planning|navigation]], with sub-problems of [[Robot localization|localization]] (knowing where you are, or finding out where other things are), [[robotic mapping|mapping]] (learning what is around you, building a map of the environment), and [[motion planning]] (figuring out how to get there) or path planning (going from one point in space to another point, which may involve compliant motion - where the robot moves while maintaining physical contact with an object).<ref>Tecuci, G. (2012), Artificial intelligence. WIREs Comp Stat, 4: 168–180. doi: 10.1002/wics.200</ref><ref name="Robotic mapping"/>  ===Perception===<!-- This is linked to in the introduction --> {{Main|Machine perception|Computer vision|Speech recognition}}  [[Machine perception]]<ref name="Machine perception"/> is the ability to use input from sensors (such as cameras, microphones, sonar and others more exotic) to deduce aspects of the world. [[Computer vision]]<ref name="Computer vision"/> is the ability to analyze visual input. A few selected subproblems are [[speech recognition]],<ref name="Speech recognition"/> [[facial recognition system|facial recognition]] and [[object recognition]].<ref name="Object recognition"/>  ===Social intelligence===<!-- This is linked to in the introduction --> {{Main|Affective computing}} [[File:Kismet robot at MIT Museum.jpg|thumb|[[Kismet (robot)|Kismet]], a robot with rudimentary social skills<ref>{{cite web | url=http://www.ai.mit.edu/projects/humanoid-robotics-group/kismet/kismet.html | title=Kismet | publisher=MIT Artificial Intelligence Laboratory, Humanoid Robotics Group}}</ref>]]  Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human [[Affect (psychology)|affects]].<ref>{{cite book|last=Thro|first=Ellen|title=Robotics|year=1993|location=New York}}</ref><ref>{{cite book|last=Edelson|first=Edward|title=The Nervous System|year=1991|publisher=Remmel Nunn|location=New York}}</ref> It is an interdisciplinary field spanning [[computer sciences]], [[psychology]], and [[cognitive science]].<ref name=TaoTan>{{cite conference |first=Jianhua |last=Tao |coauthors=Tieniu Tan |title=Affective Computing: A Review |booktitle=Affective Computing and Intelligent Interaction |volume=[[LNCS]] 3784 |pages=981–995 |publisher=Springer |year=2005 |doi=10.1007/11573548 }}</ref> While the origins of the field may be traced as far back as to early philosophical enquiries into [[Emotion#The_James-Lange_Theory|emotion]],<ref>{{cite  journal|last=James|first=William|year=1884|title=What is Emotion|journal=Mind|volume=9|pages=188–205|doi=10.1093/mind/os-IX.34.188}} Cited by Tao and Tan.</ref> the more modern branch of computer science originated with [[Rosalind Picard]]'s 1995 paper<ref>[http://affect.media.mit.edu/pdfs/95.picard.pdf "Affective Computing"] MIT Technical Report #321 ([http://vismod.media.mit.edu/pub/tech-reports/TR-321-ABSTRACT.html Abstract]), 1995</ref> on affective computing.<ref> {{cite web |url= http://ls12-www.cs.tu-dortmund.de//~fink/lectures/SS06/human-robot-interaction/Emotion-RecognitionAndSimulation.pdf |title= Recognition and Simulation of Emotions |accessdate=13 May 2008 |last= Kleine-Cosack |first= Christian |year= 2006 |month= October |format= PDF |quote= The introduction of emotion to computer science was done by Pickard (sic) who created the field of affective computing. |archiveurl = http://web.archive.org/web/20080528135730/http://ls12-www.cs.tu-dortmund.de/~fink/lectures/SS06/human-robot-interaction/Emotion-RecognitionAndSimulation.pdf <!-- Bot retrieved archive --> |archivedate = 28 May 2008}} </ref><ref> {{cite web |url= http://www.wired.com/wired/archive/11.12/love.html |title= The Love Machine; Building computers that care |accessdate=13 May 2008 |last= Diamond |first= David |year= 2003 |month= December |publisher= Wired |quote= Rosalind Picard, a genial MIT professor, is the field's godmother; her 1997 book, Affective Computing, triggered an explosion of interest in the emotional side of computers and their users. | archiveurl= http://web.archive.org/web/20080518185630/http://www.wired.com/wired/archive/11.12/love.html| archivedate= 18 May 2008 <!--DASHBot-->| deadurl= no}} </ref> A motivation for the research is the ability to simulate [[empathy]]. The machine should interpret the emotional state of humans and adapt its behaviour to them, giving an appropriate response for those emotions.  Emotion and social skills<ref name="Emotion and affective computing"/> play two roles for an intelligent agent. First, it must be able to predict the actions of others, by understanding their motives and emotional states. (This involves elements of [[game theory]], [[decision theory]], as well as the ability to model human emotions and the perceptual skills to detect emotions.) Also, in an effort to facilitate [[human-computer interaction]], an intelligent machine might want to be able to ''display'' emotions—even if it does not actually experience them itself—in order to appear sensitive to the emotional dynamics of human interaction.  ===Creativity===<!-- This is linked to in the introduction --> {{Main|Computational creativity}}  A sub-field of AI addresses [[creativity]] both theoretically (from a philosophical and psychological perspective) and practically (via specific implementations of systems that generate outputs that can be considered creative, or systems that identify and assess creativity). Related areas of computational research are [[Artificial intuition]] and [[Artificial imagination]].{{citation needed|date=January 2011}}  ===General intelligence===<!-- This is linked to in the introduction --> {{Main|Strong AI|AI-complete}}  Most researchers hope that their work will eventually be incorporated into a machine with ''general'' intelligence (known as [[strong AI]]), combining all the skills above and exceeding human abilities at most or all of them.<ref name="General intelligence"/> A few believe that [[anthropomorphic]] features like [[artificial consciousness]] or an [[artificial brain]] may be required for such a project.<ref name="Artificial consciousness"/><ref name="Brain simulation"/>  Many of the problems above are considered [[AI-complete]]: to solve one problem, you must solve them all. For example, even a straightforward, specific task like [[machine translation]] requires that the machine follow the author's argument ([[#Deduction, reasoning, problem solving|reason]]), know what is being talked about ([[#Knowledge representation|knowledge]]), and faithfully reproduce the author's intention ([[#Social intelligence|social intelligence]]). [[Machine translation]], therefore, is believed to be AI-complete: it may require [[strong AI]] to be done as well as humans can do it.<ref name="AI complete"/>  == Approaches == There is no established unifying theory or [[paradigm]] that guides AI research. Researchers disagree about many issues.<ref>[[Nils Nilsson (researcher)|Nils Nilsson]] writes: "Simply put, there is wide disagreement in the field about what AI is all about" {{Harv|Nilsson|1983|p=10}}.</ref> A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying [[psychology]] or [[neurology]]? Or is human biology as irrelevant to AI research as bird biology is to [[aeronautical engineering]]?<ref name="Biological intelligence vs. intelligence in general"/> Can intelligent behavior be described using simple, elegant principles (such as [[logic]] or [[optimization (mathematics)|optimization]])? Or does it necessarily require solving a large number of completely unrelated problems?<ref name="Neats vs. scruffies"/> Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require "sub-symbolic" processing?<ref name="Symbolic vs. sub-symbolic"/> John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as [[synthetic intelligence]],{{sfn|Haugeland|1985|p=255}} a term which has since been adopted by some non-GOFAI researchers.<ref>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.8384&rep=rep1&type=pdf</ref><ref name="Wang2008">{{cite book|author=Pei Wang|title=Artificial general intelligence, 2008: proceedings of the First AGI Conference|url=http://books.google.com/books?id=a_ZR81Z25z0C&pg=PA63|accessdate=31 October 2011|year=2008|publisher=IOS Press|isbn=978-1-58603-833-5|page=63}}</ref>  === Cybernetics and brain simulation === {{Main|Cybernetics|Computational neuroscience}} In the 1940s and 1950s, a number of researchers explored the connection between [[neurology]], [[information theory]], and [[cybernetics]]. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as [[W. Grey Walter]]'s [[turtle (robot)|turtles]] and the [[Johns Hopkins Beast]]. Many of these researchers gathered for meetings of the Teleological Society at [[Princeton University]] and the [[Ratio Club]] in England.<ref name="AI's immediate precursors"/> By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.  === Symbolic === {{Main|GOFAI}} When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: [[Carnegie Mellon University|CMU]], [[Stanford]] and [[MIT]], and each one developed its own style of research. [[John Haugeland]] named these approaches to AI "good old fashioned AI" or "[[GOFAI]]".<ref name="GOFAI"/> During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on [[cybernetics]] or [[neural network]]s were abandoned or pushed into the background.<ref>The most dramatic case of sub-symbolic AI being pushed into the background was the devastating critique of [[perceptron]]s by [[Marvin Minsky]] and [[Seymour Papert]] in 1969. See [[History of AI]], [[AI winter]], or [[Frank Rosenblatt]].</ref> Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with [[artificial general intelligence]] and considered this the goal of their field.   ; Cognitive simulation: Economist [[Herbert A. Simon|Herbert Simon]] and [[Allen Newell]] studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as [[cognitive science]], [[operations research]] and [[management science]]. Their research team used the results of [[psychology|psychological]] experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at [[Carnegie Mellon University]] would eventually culminate in the development of the [[Soar (cognitive architecture)|Soar]] architecture in the middle 80s.<ref name="AI at CMU in the 60s"/><ref name="Soar"/>  ; Logic-based: Unlike [[Allen Newell|Newell]] and [[Herbert A. Simon|Simon]], [[John McCarthy (computer scientist)|John McCarthy]] felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms.<ref name="Biological intelligence vs. intelligence in general"/> His laboratory at [[Stanford University|Stanford]] ([[Stanford Artificial Intelligence Laboratory|SAIL]]) focused on using formal [[logic]] to solve a wide variety of problems, including [[knowledge representation]], [[automated planning and scheduling|planning]] and [[machine learning|learning]].<ref name="AI at Stanford in the 60s"/> Logic was also focus of the work at the [[University of Edinburgh]] and elsewhere in Europe which led to the development of the programming language [[Prolog]] and the science of [[logic programming]].<ref name="AI at Edinburgh and France in the 60s"/>  ; "Anti-logic" or "scruffy": Researchers at [[MIT]] (such as [[Marvin Minsky]] and [[Seymour Papert]])<ref name="AI at MIT in the 60s"/> found that solving difficult problems in [[computer vision|vision]] and [[natural language processing]] required ad-hoc solutions – they argued that there was no simple and general principle (like [[logic]]) that would capture all the aspects of intelligent behavior. [[Roger Schank]] described their "anti-logic" approaches as "[[Neats vs. scruffies|scruffy]]" (as opposed to the "[[neats vs. scruffies|neat]]" paradigms at [[Carnegie Mellon University|CMU]] and [[Stanford]]).<ref name="Neats vs. scruffies"/> [[Commonsense knowledge bases]] (such as [[Doug Lenat]]'s [[Cyc]]) are an example of "scruffy" AI, since they must be built by hand, one complicated concept at a time.<ref name="Cyc"/>  ; Knowledge-based: When computers with large memories became available around 1970, researchers from all three traditions began to build [[knowledge representation|knowledge]] into AI applications.<ref name="Knowledge revolution"/> This "knowledge revolution" led to the development and deployment of [[expert system]]s (introduced by [[Edward Feigenbaum]]), the first truly successful form of AI software.<ref name="Expert systems"/> The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.  === Sub-symbolic === By the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially [[machine perception|perception]], [[robotics]], [[machine learning|learning]] and [[pattern recognition]]. A number of researchers began to look into "sub-symbolic" approaches to specific AI problems.<ref name="Symbolic vs. sub-symbolic"/>  ; Bottom-up, [[embodied agent|embodied]], [[situated]], [[behavior-based AI|behavior-based]] or [[nouvelle AI]]: Researchers from the related field of [[robotics]], such as [[Rodney Brooks]], rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive.<ref name="Embodied AI"/> Their work revived the non-symbolic viewpoint of the early [[cybernetic]]s researchers of the 50s and reintroduced the use of [[control theory]] in AI. This coincided with the development of the [[embodied mind thesis]] in the related field of [[cognitive science]]: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.  ; Computational Intelligence: Interest in [[neural networks]] and "[[connectionism]]" was revived by [[David Rumelhart]] and others in the middle 1980s.<ref name="Revival of connectionism"/> These and other sub-symbolic approaches, such as [[fuzzy system]]s and [[evolutionary computation]], are now studied collectively by the emerging discipline of [[computational intelligence]].<ref name="Computational intelligence"/>  ===Statistical=== In the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly [[scientific method|scientific]], in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like [[mathematics]], economics or [[operations research]]). [[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] describe this movement as nothing less than a "revolution" and "the victory of the [[neats and scruffies|neats]]."<ref name="Formal methods in AI"/> Critics argue that these techniques are too focused on particular problems and have failed to address the long term goal of general intelligence.<ref>Pat Langley, [http://www.springerlink.com/content/j067h855n8223338/ "The changing science of machine learning"], ''Machine Learning'', Volume 82, Number 3, 275–279, {{doi|10.1007/s10994-011-5242-y}}</ref>  === Integrating the approaches === ;Intelligent agent paradigm: An [[intelligent agent]] is a system that perceives its environment and takes actions which maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as [[firm]]s). The paradigm gives researchers license to study isolated problems and find solutions that are both verifiable and useful, without agreeing on one single approach. An agent that solves a specific problem can use any approach that works – some agents are symbolic and logical, some are sub-symbolic [[neural network]]s and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fields—such as [[decision theory]] and economics—that also use concepts of abstract agents. The intelligent agent paradigm became widely accepted during the 1990s.<ref name="Intelligent agents"/>  ;[[Agent architecture]]s and [[cognitive architecture]]s: Researchers have designed systems to build intelligent systems out of interacting [[intelligent agents]] in a [[multi-agent system]].<ref name="Agent architectures"/> A system with both symbolic and sub-symbolic components is a [[hybrid intelligent system]], and the study of such systems is [[artificial intelligence systems integration]]. A [[hierarchical control system]] provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modelling.<ref name="Hierarchical control system"/> [[Rodney Brooks]]' [[subsumption architecture]] was an early proposal for such a hierarchical system.<ref name="Subsumption architecture"/> <!-- This should also describe [[Soar (cognitive architecture)]] briefly. --->  == Tools == In the course of 50 years of research, AI has developed a large number of tools to solve the most difficult problems in [[computer science]]. A few of the most general of these methods are discussed below.  === Search and optimization === {{Main|Search algorithm|Mathematical optimization|Evolutionary computation}}  Many problems in AI can be solved in theory by intelligently searching through many possible solutions:<ref name="Search"/> [[:#Deduction, reasoning, problem solving|Reasoning]] can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from [[premise]]s to [[Logical consequence|conclusion]]s, where each step is the application of an [[inference rule]].<ref name="Logic as search"/> [[Automated planning and scheduling|Planning]] algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called [[means-ends analysis]].<ref name="Planning as search"/> [[Robotics]] algorithms for moving limbs and grasping objects use [[local search (optimization)|local searches]] in [[configuration space]].<ref name="Configuration space" /> Many [[machine learning|learning]] algorithms use search algorithms based on [[optimization (mathematics)|optimization]].  Simple exhaustive searches<ref name="Uninformed search"/> are rarely sufficient for most real world problems: the [[search algorithm|search space]] (the number of places to search) quickly grows to [[astronomical]] numbers. The result is a search that is [[Computation time|too slow]] or never completes. The solution, for many problems, is to use "[[heuristics]]" or "rules of thumb" that eliminate choices that are unlikely to lead to the goal (called "[[pruning (algorithm)|pruning]] the [[search tree]]"). [[Heuristics]] supply the program with a "best guess" for the path on which the solution lies.<ref name="Informed search"/>  A very different kind of search came to prominence in the 1990s, based on the mathematical theory of [[optimization (mathematics)|optimization]]. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind [[hill climbing]]: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are [[simulated annealing]], [[beam search]] and [[random optimization]].<ref name="Optimization search"/>  [[Evolutionary computation]] uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, [[natural selection|selecting]] only the fittest to survive each generation (refining the guesses). Forms of [[evolutionary computation]] include [[swarm intelligence]] algorithms (such as [[ant colony optimization|ant colony]] or [[particle swarm optimization]])<ref name="Society based learning"/> and [[evolutionary algorithms]] (such as [[genetic algorithms]] and [[genetic programming]]).<ref name="Genetic programming"/>  === Logic === {{Main|Logic programming|Automated reasoning}}  [[Logic]]<ref name="Logic"/> is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the [[satplan]] algorithm uses logic for [[automated planning and scheduling|planning]]<ref name="Satplan"/> and [[inductive logic programming]] is a method for [[machine learning|learning]].<ref name="Symbolic learning techniques"/>  Several different forms of logic are used in AI research. [[Propositional logic|Propositional]] or [[sentential logic]]<ref name="Propositional logic"/> is the logic of statements which can be true or false. [[First-order logic]]<ref name="First-order logic"/> also allows the use of [[quantifier]]s and [[predicate (mathematical logic)|predicate]]s, and can express facts about objects, their properties, and their relations with each other. [[Fuzzy logic]],<ref name="Fuzzy logic"/> is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). [[Fuzzy system]]s can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. [[Subjective logic]]<ref name="Subjective logic"/> models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief   disbelief   uncertainty = 1 within a [[Beta distribution]]. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.  [[Default logic]]s, [[non-monotonic logic]]s and [[circumscription (logic)|circumscription]]<ref name="Default reasoning and non-monotonic logic"/> are forms of logic designed to help with default reasoning and the [[qualification problem]]. Several extensions of logic have been designed to handle specific domains of [[knowledge representation|knowledge]], such as: [[description logic]]s;<ref name="Representing categories and relations"/> [[situation calculus]], [[event calculus]] and [[fluent calculus]] (for representing events and time);<ref name="Representing time"/> [[Causality#Causal calculus|causal calculus]];<ref name="Representing causation"/> belief calculus; and [[modal logic]]s.<ref name="Representing knowledge about knowledge"/>  ===Probabilistic methods for uncertain reasoning=== {{Main|Bayesian network|Hidden Markov model|Kalman filter|Decision theory|Utility theory}}  Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from [[probability]] theory and economics.<ref name="Stochastic methods for uncertain reasoning"/>  [[Bayesian network]]s<ref name="Bayesian networks"/> are a very general tool that can be used for a large number of problems: reasoning (using the [[Bayesian inference]] algorithm),<ref name="Bayesian inference"/> [[Machine learning|learning]] (using the [[expectation-maximization algorithm]]),<ref name="Bayesian learning"/> [[Automated planning and scheduling|planning]] (using [[decision network]]s)<ref name="Bayesian decision networks"/> and [[machine perception|perception]] (using [[dynamic Bayesian network]]s).<ref name="Stochastic temporal models"/> Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping [[machine perception|perception]] systems to analyze processes that occur over time (e.g., [[hidden Markov model]]s or [[Kalman filter]]s).<ref name="Stochastic temporal models"/>  A key concept from the science of economics is "[[utility]]": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using [[decision theory]], [[decision analysis]],<ref name="Decisions theory and analysis"/> [[applied information economics|information value theory]].<ref name="Information value theory"/> These tools include models such as [[Markov decision process]]es,<ref name="Markov decision process"/> dynamic [[decision network]]s,<ref name="Stochastic temporal models" /> [[game theory]] and [[mechanism design]].<ref name="Game theory and mechanism design"/>  === Classifiers and statistical learning methods === {{Main|Classifier (mathematics)|Statistical classification|Machine learning}}  The simplest AI applications can be divided into two types: classifiers ("if shiny then diamond") and controllers ("if shiny then pick up"). Controllers do however also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. [[Classifier (mathematics)|Classifiers]] are functions that use [[pattern matching]] to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.<ref name="Classifiers"/>  A classifier can be trained in various ways; there are many statistical and [[machine learning]] approaches. The most widely used classifiers are the [[Artificial neural network|neural network]],<ref name="Neural networks" /> [[kernel methods]] such as the [[support vector machine]],<ref name="Kernel methods"/> [[k-nearest neighbor algorithm]],<ref name="K-nearest neighbor algorithm"/> [[Gaussian mixture model]],<ref name="Guassian mixture model"/> [[naive Bayes classifier]],<ref name="Naive Bayes classifier"/> and [[decision tree learning|decision tree]].<ref name="Decision tree"/> The performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the "[[No free lunch in search and optimization|no free lunch]]" theorem. Determining a suitable classifier for a given problem is still more an art than science.<ref name="Classifier performance"/>  === Neural networks === {{Main|Neural network|Connectionism}} [[File:Artificial neural network.svg|thumb|A neural network is an interconnected group of nodes, akin to the vast network of [[neuron]]s in the [[human brain]].]]  The study of [[artificial neural network]]s<ref name="Neural networks"/> began in the decade before the field AI research was founded, in the work of [[Walter Pitts]] and [[Warren McCullough]]. Other important early researchers were [[Frank Rosenblatt]], who invented the [[perceptron]] and [[Paul Werbos]] who developed the [[backpropagation]] algorithm.<ref name="Backpropagation"/>  The main categories of networks are acyclic or [[feedforward neural network]]s (where the signal passes in only one direction) and [[recurrent neural network]]s (which allow feedback). Among the most popular feedforward networks are [[perceptron]]s, [[multi-layer perceptron]]s and [[radial basis network]]s.<ref name="Feedforward neural networks"/> Among recurrent networks, the most famous is the [[Hopfield net]], a form of attractor network, which was first described by [[John Hopfield]] in 1982.<ref name="Recurrent neural networks"/> Neural networks can be applied to the problem of [[intelligent control]] (for robotics) or [[machine learning|learning]], using such techniques as [[Hebbian learning]] and [[competitive learning]].<ref name="Learning in neural networks"/>  [[Hierarchical temporal memory]] is an approach that models some of the structural and algorithmic properties of the [[neocortex]].<ref name="Hierarchical temporal memory"/>  === Control theory === {{Main|Intelligent control}} [[Control theory]], the grandchild of [[cybernetics]], has many important applications, especially in [[robotics]].<ref name="Control theory"/>  === Languages === {{Main|List of programming languages for artificial intelligence}}  <!-- Not sure where to put this in the article. I think it deserves no more weight than this single sentence. To be improved. --> AI researchers have developed several specialized languages for AI research, including [[Lisp programming language|Lisp]]<ref name="Lisp"/> and [[Prolog]].<ref name="Prolog"/>  ==Evaluating progress== {{Main|Progress in artificial intelligence}} In 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the [[Turing test]]. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.<ref name="Turing test"/>  Artificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed [[subject matter expert Turing test]]s. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.<ref name="Subject matter expert Turing test"/>  The broad classes of outcome for an AI test are<ref name="Progress in Artificial Intelligence"/>:   # Optimal: it is not possible to perform better. # Strong super-human: performs better than all humans. # Super-human: performs better than most humans. # Sub-human: performs worse than most humans.  For example, performance at [[draughts]] is optimal,<ref name="Game AI"/> performance at chess is super-human and nearing strong super-human (see [[Computer chess#Computers versus humans]]) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.  A quite different approach measures machine intelligence through tests which are developed from ''mathematical'' definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from [[Kolmogorov complexity]] and [[data compression]].<ref name="Mathematical definitions of intelligence"/> Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.  == Applications == [[File:Automated online assistant.png|thumb|An [[automated online assistant]] providing customer service on a web page – one of many very primitive applications of artificial intelligence.]] {{expand section|talksection=Todo: Applications|date=January 2011}} {{Main|Applications of artificial intelligence}}  Artificial intelligence techniques are pervasive and are too numerous to list.  Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the [[AI effect]].<ref> {{cite news   | coauthors =   | title = AI set to exceed human brain power   |publisher=CNN   | date = 26 July 2006   | url = http://www.cnn.com/2006/TECH/science/07/24/ai.bostrom/   | format = web article   | accessdate =26 February 2008| archiveurl= http://web.archive.org/web/20080219001624/http://www.cnn.com/2006/TECH/science/07/24/ai.bostrom/| archivedate= 19 February 2008 <!--DASHBot-->| deadurl= no}} </ref>  === Competitions and prizes === {{Main|Competitions and prizes in artificial intelligence}} There are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, driverless cars, robot soccer and games.  === Platforms === A [[platform (computing)|platform]] (or "[[computing platform]]") is defined as "some sort of hardware architecture or software framework (including application frameworks), that allows software to run." As Rodney Brooks<ref>Brooks, R.A., "How to build complete creatures rather than isolated cognitive simulators," in K. VanLehn (ed.), Architectures for Intelligence, pp. 225–239, Lawrence Erlbaum Associates, Hillsdale, NJ, 1991.</ref> pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.  A wide variety of platforms has allowed different aspects of AI to develop, ranging from [[expert systems]], albeit [[Personal Computer|PC]]-based but still an entire real-world system, to various robot platforms such as the widely available [[Roomba]] with open interface.<ref>[http://hackingroomba.com/?s=atmel Hacking Roomba » Search Results » atmel<!-- Bot generated title -->]</ref>  == Philosophy == {{Main|Philosophy of artificial intelligence}}  Artificial intelligence, by claiming to be able to recreate the capabilities of the human [[mind]], is both a challenge and an inspiration for philosophy. Are there limits to how intelligent machines can be? Is there an essential difference between human intelligence and artificial intelligence? Can a machine have a [[mind]] and [[consciousness]]? A few of the most influential answers to these questions are given below.<ref name="Philosophy of AI"/>  [[Computing Machinery and Intelligence|Turing's "polite convention"]]: We need not decide if a machine can "think"; we need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the [[Turing test]].<ref name="Turing test"/>   The [[Dartmouth Conferences|Dartmouth proposal]]: "Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it." This conjecture was printed in the proposal for the [[Dartmouth Conferences|Dartmouth Conference]] of 1956, and represents the position of most working AI researchers.<ref name="Dartmouth proposal"/>  [[Physical symbol system|Newell and Simon's physical symbol system hypothesis]]: "A physical symbol system has the necessary and sufficient means of general intelligent action." Newell and Simon argue that intelligences consist of formal operations on symbols.<ref name="Physical symbol system hypothesis"/> [[Hubert Dreyfus]] argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a "feel" for the situation rather than explicit symbolic knowledge. (See [[Dreyfus' critique of AI]].)<ref> Dreyfus criticized the [[necessary and sufficient|necessary]] condition of the [[physical symbol system]] hypothesis, which he called the "psychological assumption": "The mind can be viewed as a device operating on bits of information according to formal rules". {{Harv|Dreyfus|1992|p=156}}</ref><ref name="Dreyfus' critique"/>  [[Gödel's incompleteness theorem]]: A [[formal system]] (such as a computer program) cannot prove all true statements.<ref>This is a paraphrase of the relevant implication of Gödel's theorems.</ref> [[Roger Penrose]] is among those who claim that Gödel's theorem limits what machines can do. (See ''[[The Emperor's New Mind]]''.)<ref name="The mathematical objection"/>  [[Strong AI hypothesis|Searle's strong AI hypothesis]]: "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."<ref name="Searle's strong AI"/> John Searle counters this assertion with his [[Chinese room]] argument, which asks us to look ''inside'' the computer and try to find where the "mind" might be.<ref name="Chinese room"/>  The [[artificial brain]] argument: The brain can be simulated. [[Hans Moravec]], [[Ray Kurzweil]] and others have argued that it is technologically feasible to copy the brain directly into hardware and software, and that such a simulation will be essentially identical to the original.<ref name="Brain simulation"/>  == Predictions and ethics == {{Main|Artificial intelligence in fiction|Ethics of artificial intelligence|Transhumanism|Technological singularity}}  Artificial Intelligence is a common topic in both science fiction and projections about the future of technology and society. The existence of an artificial intelligence that rivals human intelligence raises difficult ethical issues, and the potential power of the technology inspires both hopes and fears.  <!-- IMPACT ON SOCIETY: FICTION --> In fiction, Artificial Intelligence has appeared fulfilling many roles, including a servant ([[R2D2]] in ''[[Star Wars]]''), a law enforcer ([[K.I.T.T.]] "[[Knight Rider (1982 TV series)|Knight Rider]]"), a comrade ([[Data (Star Trek)|Lt. Commander Data]] in ''[[Star Trek: The Next Generation]]''), a conqueror/overlord (''[[The Matrix]]''), a dictator (''[[With Folded Hands]]''), benevolent provider/de facto rulers of society (''[[Culture (series)|Culture]]'' series), an assassin (''[[Terminator (series)|Terminator]]''), a sentient race (''[[Battlestar Galactica (re-imagining)|Battlestar Galactica]]''/[[Transformers]]/''[[Mass Effect]]''), an extension to human abilities (''[[Ghost in the Shell]]'') and the savior of the human race ([[R. Daneel Olivaw]] in [[Isaac Asimov]]'s [[Robot series (Asimov)|''Robot'' series]]).  <!--ROBOT RIGHTS / THE ABILITY TO SUFFER / SENTIENCE --> [[Mary Shelley]]'s ''[[Frankenstein]]'' considers a key issue in the [[ethics of artificial intelligence]]: if a machine can be created that has intelligence, could it also ''[[sentience|feel]]''? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, including the films ''[[I Robot (film)|I Robot]]'', ''[[Blade Runner]]'' and ''[[A.I.: Artificial Intelligence]]'', in which humanoid machines have the ability to feel human emotions. This issue, now known as "[[robot rights]]", is currently being considered by, for example, California's [[Institute for the Future]], although many critics believe that the discussion is premature.<ref name="Robot rights"/> The subject is profoundly discussed in the 2010 documentary film ''[[Plug & Pray]]''.<ref>[http://www.plugandpray-film.de/en/content.html Independent documentary Plug & Pray, featuring Joseph Weizenbaum and Raymond Kurzweil]</ref>  <!-- CONSEQUENCES 1: DECREASED DEMAND FOR HUMAN LABOR --> Martin Ford, author of ''The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future'',<ref name="Ford2009Lights">{{Ford 2009 The lights in the tunnel}}</ref> and others argue that specialized artificial intelligence applications, robotics and other forms of automation will ultimately result in significant unemployment as machines begin to match and exceed the capability of workers to perform most routine and repetitive jobs. Ford predicts that many knowledge-based occupations—and in particular entry level jobs—will be increasingly susceptible to automation via expert systems, machine learning<ref>[http://econfuture.wordpress.com/2011/04/14/machine-learning-a-job-killer/ "Machine Learning: A Job Killer?"]</ref> and other AI-enhanced applications. AI-based applications may also be used to amplify the capabilities of low-wage offshore workers, making it more feasible to [[outsource]] knowledge work.<ref name="Replaced by machines"/>  <!-- CONSEQUENCES 2: DEVALUATION OF HUMANITY --> [[Joseph Weizenbaum]] wrote that AI applications can not, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as [[customer service]] or [[psychotherapy]]<ref>In the early 70s, [[Kenneth Colby]] presented a version of Weizenbaum's [[ELIZA]] known as DOCTOR which he promoted as a serious therapeutic tool. {{Harv|Crevier|1993|pp=132–144}}</ref> was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as [[computationalism]]). To Weizenbaum these points suggest that AI research devalues human life.<ref name="Weizenbaum's critique"/>  <!-- CONSEQUENCES 3: INSTANTANEOUS PROGRESS --> Many futurists believe that artificial intelligence will ultimately transcend the limits of progress. [[Ray Kurzweil]] has used [[Moore's law]] (which describes the relentless exponential improvement in digital technology) to calculate that [[desktop computer]]s will have the same processing power as human brains by the year 2029. He also predicts that by 2045 artificial intelligence will reach a point where it is able to improve ''itself'' at a rate that far exceeds anything conceivable in the past, a scenario that science fiction writer [[Vernor Vinge]] named the "[[technological singularity|singularity]]".<ref name=Singularity/>  <!-- CONSEQUENCES 4: TRANSHUMANISM --> Robot designer [[Hans Moravec]], cyberneticist [[Kevin Warwick]] and inventor [[Ray Kurzweil]] have predicted that humans and machines will merge in the future into [[cyborg]]s that are more capable and powerful than either.<ref name="Transhumanism"/> This idea, called [[transhumanism]], which has roots in [[Aldous Huxley]] and [[Robert Ettinger]], has been illustrated in fiction as well, for example in the [[manga]] ''[[Ghost in the Shell]]'' and the science-fiction series ''[[Dune (novel)|Dune]]''.  <!-- CONSEQUENCES 5: DIFFICULTIES FOR FRIENDLY AI --> Political scientist [[Charles T. Rubin]] believes that AI can be neither designed nor guaranteed to be [[Friendly AI|friendly]].<ref>{{cite journal|last=Rubin|first=Charles|authorlink=Charles T. Rubin|date=Spring 2003|title=Artificial Intelligence and Human Nature|journal=The New Atlantis|volume=1|pages=88–100|url=http://www.thenewatlantis.com/publications/artificial-intelligence-and-human-nature}}</ref> He argues that "any sufficiently advanced benevolence may be indistinguishable from malevolence." Humans should not assume machines or robots would treat us favorably, because there is no ''a priori'' reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share).  <!-- AI AS EVOLUTION --> [[Edward Fredkin]] argues that "artificial intelligence is the next stage in evolution", an idea first proposed by [[Samuel Butler (novelist)|Samuel Butler]]'s "[[Darwin among the Machines]]" (1863), and expanded upon by [[George Dyson (science historian)|George Dyson]] in his book of the same name in 1998.<ref name="AI as evolution"/>  [[Pamela McCorduck]] writes that all these scenarios are expressions of the ancient human desire to, as she calls it, "forge the gods".<ref name="McCorduck's thesis"/>  ==See also== {{Portal|AI|Mind and Brain|Chess|Strategy games}}  {{main|Outline of artificial intelligence}} * [[AI-complete]] * [[Artificial intelligence in fiction]] * [[Artificial Intelligence (journal)]] * [[Artificial intelligence (video games)]] * [[Synthetic intelligence]] * [[Cognitive sciences]] * [[Computer Go]] * [[Human Cognome Project]] * [[Friendly artificial intelligence]] * [[List of basic artificial intelligence topics]] * [[:Category:Artificial intelligence researchers|List of AI researchers]] * [[List of important publications in computer science#Artificial intelligence|List of important AI publications]] * [[List of notable artificial intelligence projects|List of AI projects]] * [[List of machine learning algorithms]] * [[List of emerging technologies]] * [[List of scientific journals]] * [[Philosophy of mind]] * [[Technological singularity]] * [[Never-Ending Language Learning]]  == References == === Notes === {{reflist|30em|refs=  <!-- unused ref <ref name="TOPIO"> [[TOPIO]]: * {{Cite news|url = http://www.popsci.com/technology/article/2010-02/ping-pong-playing-terminator|title=A Ping-Pong-Playing Terminator|publisher=Popular Science}} * {{Cite news|url = http://www.gadgetrivia.com/8164-best_robot_international_robot_exhibition|title=Best robot 2009|publisher=gadgetrivia.com}}{{dead link|date=February 2011}}</ref> -->  <!-- INTRODUCTION ------------------------------------------------------------------------------>  <ref name="Definition of AI"> Definition of AI as the study of [[intelligent agents]]: * {{Harvnb|Poole|Mackworth|Goebel|1998|loc=[http://people.cs.ubc.ca/~poole/ci/ch1.pdf p. 1]}}, which provides the version that is used in this article. Note that they use the term "computational intelligence" as a synonym for artificial intelligence. * {{Harvtxt|Russell|Norvig|2003}} (who prefer the term "rational agent") and write "The whole-agent view is now widely accepted in the field" {{Harv|Russell|Norvig|2003|p=55}}. * {{Harvnb|Nilsson|1998}} <!--These textbooks are the most widely used in academic AI.--> </ref>  <ref name="Coining of the term AI"> Although there is some controversy on this point (see {{Harvtxt|Crevier|1993|p=50}}), [[John McCarthy (computer scientist)|McCarthy]] states unequivocally "I came up with the term" in a c|net interview. {{Harv|Skillings|2006}} </ref>  <ref name="McCarthy's definition of AI"> [[John McCarthy (computer scientist)|McCarthy]]'s definition of AI: * {{Harvnb|McCarthy|2007}} </ref>  <ref name="McCorduck's thesis"> This is a central idea of [[Pamela McCorduck]]'s ''Machines That Think''. She writes: "I like to think of artificial intelligence as the scientific apotheosis of a venerable cultural tradition." {{Harv|McCorduck|2004|p=34}} "Artificial intelligence in one form or another is an idea that has pervaded Western intellectual history, a dream in urgent need of being realized." {{Harv|McCorduck|2004|p=xviii}} "Our history is full of attempts—nutty, eerie, comical, earnest, legendary and real—to make artificial intelligences, to reproduce what is the essential us—bypassing the ordinary means. Back and forth between myth and reality, our imaginations supplying what our workshops couldn't, we have engaged for a long time in this odd form of self-reproduction." {{Harv|McCorduck|2004|p=3}} She traces the desire back to its [[Hellenistic]] roots and calls it the urge to "forge the Gods." {{Harv|McCorduck|2004|pp=340–400}} </ref>  <ref name="AI widely used"> AI applications widely used behind the scenes: * {{Harvnb|Russell|Norvig|2003|p=28}} * {{Harvnb|Kurzweil|2005|p=265}} * {{Harvnb|NRC|1999|pp=216–222}} </ref>  <ref name="Fragmentation of AI"> Pamela {{Harvtxt|McCorduck|2004|pp=424}} writes of "the rough shattering of AI in subfields—vision, natural language, decision theory, genetic algorithms, robotics ... and these with own sub-subfield—that would hardly have anything to say to each other." </ref>  <ref name="Problems of AI"> This list of intelligent traits is based on the topics covered by the major AI textbooks, including: * {{Harvnb|Russell|Norvig|2003}} * {{Harvnb|Luger|Stubblefield|2004}} * {{Harvnb|Poole|Mackworth|Goebel|1998}} * {{Harvnb|Nilsson|1998}} </ref>  <ref name="General intelligence"> General intelligence ([[strong AI]]) is discussed in popular introductions to AI: * {{Harvnb|Kurzweil|1999}} and {{Harvnb|Kurzweil|2005}} </ref>  <!-- History --------------------------------------------------------------------------------------------------->  <ref name="AI in myth"> AI in myth: * {{Harvnb|McCorduck|2004|pp=4–5}} * {{Harvnb|Russell|Norvig|2003|p=939}} </ref>  <ref name="Cult images as artificial intelligence"> [[Cult image]]s as artificial intelligence: * {{Harvtxt|Crevier|1993|p=1}} (statue of [[Amun]]) * {{Harvtxt|McCorduck|2004|pp=6–9}} These were the first machines to be believed to have true intelligence and consciousness. [[Hermes Trismegistus]] expressed the common belief that with these statues, craftsman had reproduced "the true nature of the gods", their ''sensus'' and ''spiritus''. McCorduck makes the connection between sacred automatons and [[613 Commandments|Mosaic law]] (developed around the same time), which expressly forbids the worship of robots {{Harv|McCorduck|2004|pp=6–9}} </ref>  <ref name="Humanoid automata"> Humanoid automata:<br> [[King Mu of Zhou|Yan Shi]]: * {{Harvnb|Needham|1986|p=53}} [[Hero of Alexandria]]: * {{Harvnb|McCorduck|2004|p=6}} [[Al-Jazari]]: * {{cite web|url=http://www.shef.ac.uk/marcoms/eview/articles58/robot.html |title=A Thirteenth Century Programmable Robot |publisher=Shef.ac.uk |accessdate=25 April 2009}} [[Wolfgang von Kempelen]]: * {{Harvnb|McCorduck|2004|p=17}} </ref>  <ref name="Artificial beings"> Artificial beings:<br> [[Jābir ibn Hayyān]]'s [[Takwin]]: * {{Cite journal |author=O'Connor, Kathleen Malone |title=The alchemical creation of life (takwin) and other concepts of Genesis in medieval Islam|publisher=University of Pennsylvania |year=1994 |url=http://repository.upenn.edu/dissertations/AAI9503804 |accessdate=10 January 2007 |ref=harv}} [[Judah Loew]]'s [[Golem]]: * {{Harvnb|McCorduck|2004|pp=15–16}} * {{Harvnb|Buchanan|2005|p=50}} [[Paracelsus]]' Homunculus: * {{Harvnb|McCorduck|2004|pp=13–14}} </ref>  <ref name="AI in early science fiction"> AI in early science fiction. * {{Harvnb|McCorduck|2004|pp=17–25}} </ref>  <ref name="Formal reasoning"> Formal reasoning: * {{cite book | first = David | last = Berlinski | year = 2000 | title =The Advent of the Algorithm| publisher = Harcourt Books |author-link=David Berlinski | isbn=0-15-601391-6 | oclc = 46890682 }} </ref>  <ref name="AI's immediate precursors"> AI's immediate precursors: * {{Harvnb|McCorduck|2004|pp=51–107}} * {{Harvnb|Crevier|1993|pp=27–32}} * {{Harvnb|Russell|Norvig|2003|pp=15, 940}} * {{Harvnb|Moravec|1988|p=3}} See also {{See section|History of artificial intelligence|Cybernetics and early neural networks}}. Among the researchers who laid the foundations of AI were [[Alan Turing]], [[John Von Neumann]], [[Norbert Wiener]], [[Claude Shannon]], [[Warren McCullough]], [[Walter Pitts]] and [[Donald Hebb]]. </ref>  <ref name="Dartmouth conference"> [[Dartmouth conference]]: * {{Harvnb|McCorduck|2004|pp=111–136}} * {{Harvnb|Crevier|1993|pp=47–49}}, who writes "the conference is generally recognized as the official birthdate of the new science." * {{Harvnb|Russell|Norvig|2003|p=17}}, who call the conference "the birth of artificial intelligence." * {{Harvnb|NRC|1999|pp=200–201}} </ref>  <ref name="Hegemony of the Dartmouth conference attendees"> Hegemony of the Dartmouth conference attendees: * {{Harvnb|Russell|Norvig|2003|p=17}}, who write "for the next 20 years the field would be dominated by these people and their students." * {{Harvnb|McCorduck|2004|pp=129–130}} </ref>  <ref name="Golden years of AI"> "[[History of AI#The golden years 1956−1974|Golden years]]" of AI (successful symbolic reasoning programs 1956–1973): * {{Harvnb|McCorduck|2004|pp=243–252}} * {{Harvnb|Crevier|1993|pp=52–107}} * {{Harvnb|Moravec|1988|p=9}} * {{Harvnb|Russell|Norvig|2003|pp=18–21}} The programs described are [[Daniel Bobrow]]'s [[STUDENT (computer program)|STUDENT]], [[Allen Newell|Newell]] and [[Herbert A. Simon|Simon]]'s [[Logic Theorist]] and [[Terry Winograd]]'s [[SHRDLU]]. </ref>  <ref name="AI funding in the 60s"> [[DARPA]] pours money into undirected pure research into AI during the 1960s: * {{Harvnb|McCorduck|2004|pp=131}} * {{Harvnb|Crevier|1993|pp=51, 64–65}} * {{Harvnb|NRC|1999|pp=204–205}} </ref>  <ref name="AI in England"> AI in England: * {{Harvnb|Howe|1994}} </ref>  <ref name="Optimism of early AI"> Optimism of early AI: * [[Herbert A. Simon|Herbert Simon]] quote: {{Harvnb|Simon|1965|p=96}} quoted in {{Harvnb|Crevier|1993|p=109}}. * [[Marvin Minsky]] quote: {{Harvnb|Minsky|1967|p=2}} quoted in {{Harvnb|Crevier|1993|p=109}}. </ref>  <ref name="First AI winter"> First [[AI Winter]], [[Mansfield Amendment]], [[Lighthill report]] * {{Harvnb|Crevier|1993|pp=115–117}} * {{Harvnb|Russell|Norvig|2003|p=22}} * {{Harvnb|NRC|1999|pp=212–213}} * {{Harvnb|Howe|1994}} </ref>  <ref name="Expert systems"> Expert systems: * {{Harvnb|ACM|1998|loc=I.2.1}}, * {{Harvnb|Russell|Norvig|2003|pp=22–24}} * {{Harvnb|Luger|Stubblefield|2004|pp=227–331}}, * {{Harvnb|Nilsson|1998|loc=chpt. 17.4}} * {{Harvnb|McCorduck|2004|pp=327–335, 434–435}} * {{Harvnb|Crevier|1993|pp=145–62, 197–203}} </ref>  <ref name="AI in the 80s"> Boom of the 1980s: rise of [[expert systems]], [[Fifth generation computer|Fifth Generation Project]], [[Alvey]], [[Microelectronics and Computer Technology Corporation|MCC]], [[Strategic Computing Initiative|SCI]]: * {{Harvnb|McCorduck|2004|pp=426–441}} * {{Harvnb|Crevier|1993|pp=161–162,197–203, 211, 240}} * {{Harvnb|Russell|Norvig|2003|p=24}} * {{Harvnb|NRC|1999|pp=210–211}} </ref>  <ref name="Second AI winter"> Second [[AI winter]]: * {{Harvnb|McCorduck|2004|pp=430–435}} * {{Harvnb|Crevier|1993|pp=209–210}} * {{Harvnb|NRC|1999|pp=214–216}} </ref>  <ref name="Formal methods in AI"> Formal methods are now preferred ("Victory of the [[neats vs. scruffies|neats]]"): * {{Harvnb|Russell|Norvig|2003|pp=25–26}} * {{Harvnb|McCorduck|2004|pp=486–487}} </ref>  <!---- PROBLEMS ------------------------------------------------------------------------------------------>  <ref name="Reasoning"> Problem solving, puzzle solving, game playing and deduction: * {{Harvnb|Russell|Norvig|2003|loc=chpt. 3–9}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|loc=chpt. 2,3,7,9}}, * {{Harvnb|Luger|Stubblefield|2004|loc=chpt. 3,4,6,8}}, * {{Harvnb|Nilsson|1998|loc=chpt. 7–12}} </ref>  <ref name="Uncertain reasoning"> Uncertain reasoning: * {{Harvnb|Russell|Norvig|2003|pp=452–644}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=345–395}}, * {{Harvnb|Luger|Stubblefield|2004|pp=333–381}}, * {{Harvnb|Nilsson|1998|loc=chpt. 19}} </ref>  <ref name="Intractability"> [[Intractably|Intractability and efficiency]] and the [[combinatorial explosion]]: * {{Harvnb|Russell|Norvig|2003|pp=9, 21–22}} </ref>  <ref name="Psychological evidence of sub-symbolic reasoning"> Psychological evidence of sub-symbolic reasoning: * {{Harvtxt|Wason|Shapiro|1966}} showed that people do poorly on completely abstract problems, but if the problem is restated to allow the use of intuitive [[social intelligence]], performance dramatically improves. (See [[Wason selection task]]) * {{Harvtxt|Kahneman|Slovic|Tversky|1982}} have shown that people are terrible at elementary problems that involve uncertain reasoning. (See [[list of cognitive biases]] for several examples). * {{Harvtxt|Lakoff|Núñez|2000}} have controversially argued that even our skills at mathematics depend on knowledge and skills that come from "the body", i.e. sensorimotor and perceptual skills. (See [[Where Mathematics Comes From]]) </ref>  <ref name="Knowledge representation"> [[Knowledge representation]]: * {{Harvnb|ACM|1998|loc=I.2.4}}, * {{Harvnb|Russell|Norvig|2003|pp=320–363}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=23–46, 69–81, 169–196, 235–277, 281–298, 319–345}}, * {{Harvnb|Luger|Stubblefield|2004|pp=227–243}}, * {{Harvnb|Nilsson|1998|loc=chpt. 18}} </ref>  <ref name="Knowledge engineering"> [[Knowledge engineering]]: * {{Harvnb|Russell|Norvig|2003|pp=260–266}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=199–233}}, * {{Harvnb|Nilsson|1998|loc=chpt. ~17.1–17.4}} </ref>  <ref name="Representing categories and relations"> Representing categories and relations: [[Semantic network]]s, [[description logic]]s, [[inheritance (computer science)|inheritance]] (including [[frame (artificial intelligence)|frame]]s and [[scripts (artificial intelligence)|scripts]]): * {{Harvnb|Russell|Norvig|2003|pp=349–354}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=174–177}}, * {{Harvnb|Luger|Stubblefield|2004|pp=248–258}}, * {{Harvnb|Nilsson|1998|loc=chpt. 18.3}} </ref>  <ref name="Representing time"> Representing events and time:[[Situation calculus]], [[event calculus]], [[fluent calculus]] (including solving the [[frame problem]]): * {{Harvnb|Russell|Norvig|2003|pp=328–341}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=281–298}}, * {{Harvnb|Nilsson|1998|loc=chpt. 18.2}} </ref>  <ref name="Representing causation"> [[Causality#Causal calculus|Causal calculus]]: * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=335–337}} </ref>  <ref name="Representing knowledge about knowledge"> Representing knowledge about knowledge: [[Belief calculus]], [[modal logic]]s: * {{Harvnb|Russell|Norvig|2003|pp=341–344}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=275–277}} </ref>  <ref name="Ontology"> [[Ontology (computer science)|Ontology]]: * {{Harvnb|Russell|Norvig|2003|pp=320–328}} </ref>  <ref name="Qualification problem"> [[Qualification problem]]: * {{Harvnb|McCarthy|Hayes|1969}} * {{Harvnb|Russell|Norvig|2003}}{{Page needed|date=February 2011}}<!-- We really need to know where they say this, because it's kind of wrong --> While McCarthy was primarily concerned with issues in the logical representation of actions, {{Harvnb|Russell|Norvig|2003}} apply the term to the more general issue of default reasoning in the vast network of assumptions underlying all our commonsense knowledge. </ref>  <ref name="Default reasoning and non-monotonic logic"> Default reasoning and [[default logic]], [[non-monotonic logic]]s, [[circumscription (logic)|circumscription]], [[closed world assumption]], [[abductive reasoning|abduction]] (Poole ''et al.'' places abduction under "default reasoning". Luger ''et al.'' places this under "uncertain reasoning"): * {{Harvnb|Russell|Norvig|2003|pp=354–360}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=248–256, 323–335}}, * {{Harvnb|Luger|Stubblefield|2004|pp=335–363}}, * {{Harvnb|Nilsson|1998|loc=~18.3.3}} </ref>  <ref name="Breadth of commonsense knowledge"> Breadth of commonsense knowledge: * {{Harvnb|Russell|Norvig|2003|p=21}}, * {{Harvnb|Crevier|1993|pp=113–114}}, * {{Harvnb|Moravec|1988|p=13}}, * {{Harvnb|Lenat|Guha|1989}} (Introduction) </ref>  <ref name="Intuition"> Expert knowledge as [[embodied cognition|embodied]] intuition: * {{Harvnb|Dreyfus|Dreyfus|1986}} ([[Hubert Dreyfus]] is a philosopher and critic of AI who was among the first to argue that most useful human knowledge was encoded sub-symbolically. See [[Dreyfus' critique of AI]]) * {{Harvnb|Gladwell|2005}} (Gladwell's ''[[Blink (book)|Blink]]'' is a popular introduction to sub-symbolic reasoning and knowledge.) * {{Harvnb|Hawkins|Blakeslee|2005}} (Hawkins argues that sub-symbolic knowledge should be the primary focus of AI research.) </ref>  <ref name="Planning"> [[automated planning and scheduling|Planning]]: * {{Harvnb|ACM|1998|loc=~I.2.8}}, * {{Harvnb|Russell|Norvig|2003|pp= 375–459}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=281–316}}, * {{Harvnb|Luger|Stubblefield|2004|pp=314–329}}, * {{Harvnb|Nilsson|1998|loc=chpt. 10.1–2, 22}} </ref>  <ref name="Information value theory"> [[Applied information economics|Information value theory]]: * {{Harvnb|Russell|Norvig|2003|pp=600–604}} </ref>  <ref name="Classical planning"> Classical planning: * {{Harvnb|Russell|Norvig|2003|pp=375–430}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=281–315}}, * {{Harvnb|Luger|Stubblefield|2004|pp=314–329}}, * {{Harvnb|Nilsson|1998|loc=chpt. 10.1–2, 22}} </ref>  <ref name="Non-deterministic planning"> Planning and acting in non-deterministic domains: conditional planning, execution monitoring, replanning and continuous planning: * {{Harvnb|Russell|Norvig|2003|pp=430–449}} </ref>  <ref name="Multi-agent planning"> Multi-agent planning and emergent behavior: * {{Harvnb|Russell|Norvig|2003|pp=449–455}} </ref>  <ref name="Machine learning"> [[machine learning|Learning]]: * {{Harvnb|ACM|1998|loc=I.2.6}}, * {{Harvnb|Russell|Norvig|2003|pp=649–788}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=397–438}}, * {{Harvnb|Luger|Stubblefield|2004|pp=385–542}}, * {{Harvnb|Nilsson|1998|loc=chpt. 3.3 , 10.3, 17.5, 20}} </ref>  <ref name="Reinforcement learning"> [[Reinforcement learning]]: *{{Harvnb|Russell|Norvig|2003|pp=763–788}} * {{Harvnb|Luger|Stubblefield|2004|pp=442–449}} </ref>  <ref name="Computational learning theory"> [[Computational learning theory]]: * CITATION IN PROGRESS.{{citation needed|date=January 2011}} </ref>  <ref name="Natural language processing"> [[Natural language processing]]: * {{Harvnb|ACM|1998|loc=I.2.7}} * {{Harvnb|Russell|Norvig|2003|pp=790–831}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=91–104}} * {{Harvnb|Luger|Stubblefield|2004|pp=591–632}} </ref>  <ref name="Applications of natural language processing"> Applications of natural language processing, including [[information retrieval]] (i.e. [[text mining]]) and [[machine translation]]: * {{Harvnb|Russell|Norvig|2003|pp=840–857}}, * {{Harvnb|Luger|Stubblefield|2004|pp=623–630}} </ref>  <ref name="Robotics"> [[Robotic]]s: * {{Harvnb|ACM|1998|loc=I.2.9}}, * {{Harvnb|Russell|Norvig|2003|pp=901–942}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=443–460}} </ref>  <ref name="Configuration space"> Moving and [[configuration space]]: * {{Harvnb|Russell|Norvig|2003|pp=916–932}} </ref>  <ref name="Robotic mapping"> [[Robotic mapping]] (localization, etc): *{{Harvnb|Russell|Norvig|2003|pp=908–915}} </ref>  <ref name="Machine perception"> [[Machine perception]]: * {{Harvnb|Russell|Norvig|2003|pp=537–581, 863–898}} * {{Harvnb|Nilsson|1998|loc=~chpt. 6}} </ref>  <ref name="Computer vision"> [[Computer vision]]: * {{Harvnb|ACM|1998|loc=I.2.10}} * {{Harvnb|Russell|Norvig|2003|pp=863–898}} * {{Harvnb|Nilsson|1998|loc=chpt. 6}} </ref>  <ref name="Speech recognition"> [[Speech recognition]]: * {{Harvnb|ACM|1998|loc=~I.2.7}} *{{Harvnb|Russell|Norvig|2003|pp=568–578}} </ref> [[facial recognition system|facial recognition]] and [[object recognition]].  <ref name="Object recognition"> [[Object recognition]]: *{{Harvnb|Russell|Norvig|2003|pp=885–892}} </ref>  <ref name="Emotion and affective computing"> Emotion and [[affective computing]]: * {{Harvnb|Minsky|2006}} </ref>  <ref name="Artificial consciousness"> [[Gerald Edelman]], [[Igor Aleksander]] and others have both argued that [[artificial consciousness]] is required for strong AI. ({{Harvnb|Aleksander|1995}}; {{Harvnb|Edelman|2007}}) </ref>  <ref name="Brain simulation"> [[Artificial brain]] arguments: AI requires a simulation of the operation of the human brain * {{Harvnb|Russell|Norvig|2003|p=957}} * {{Harvnb|Crevier|1993|pp=271 and 279}} A few of the people who make some form of the argument: * {{Harvnb|Moravec|1988}} * {{Harvnb|Kurzweil|2005|p=262}} * {{Harvnb|Hawkins|Blakeslee|2005}} The most extreme form of this argument (the brain replacement scenario) was put forward by [[Clark Glymour]] in the mid-70s and was touched on by [[Zenon Pylyshyn]] and [[John Searle]] in 1980. </ref>  <ref name="AI complete"> [[AI complete]]: {{Harvnb|Shapiro|1992|p=9}} </ref>  <!---- APPROACHES -----------------------------------------------------------------------------------> <ref name="Biological intelligence vs. intelligence in general"> Biological intelligence vs. intelligence in general: * {{Harvnb|Russell|Norvig|2003|pp=2–3}}, who make the analogy with [[aeronautical engineering]]. * {{Harvnb|McCorduck|2004|pp=100–101}}, who writes that there are "two major branches of artificial intelligence: one aimed at producing intelligent behavior regardless of how it was accomplioshed, and the other aimed at modeling intelligent processes found in nature, particularly human ones." * {{Harvnb|Kolata|1982}}, a paper in ''[[Science (journal)|Science]]'', which describes [[John McCarthy (computer scientist)|McCathy's]] indifference to biological models. Kolata quotes McCarthy as writing: "This is AI, so we don't care if it's psychologically real"[http://books.google.com/books?id=PEkqAAAAMAAJ&q=%22we don't care if it's psychologically real%22&dq=%22we don't care if it's psychologically real%22&output=html&pgis=1 ]. McCarthy recently reiterated his position at the [[AI@50]] conference where he said "Artificial intelligence is not, by definition, simulation of human intelligence" {{Harv|Maker|2006}}. </ref>  <ref name="Neats vs. scruffies"> [[Neats vs. scruffies]]: * {{Harvnb|McCorduck|2004|pp=421–424, 486–489}} * {{Harvnb|Crevier|1993|pp=168}} * {{Harvnb|Nilsson|1983|pp=10–11}} </ref>  <ref name="Symbolic vs. sub-symbolic"> Symbolic vs. sub-symbolic AI: * {{Harvtxt|Nilsson|1998|p=7}}, who uses the term "sub-symbolic". </ref>  <ref name="GOFAI"> {{Harvnb|Haugeland|1985|pp=112–117}} </ref>  <ref name="AI at CMU in the 60s"> Cognitive simulation, [[Allen Newell|Newell]] and [[Herbert A. Simon|Simon]], AI at [[Carnegie Mellon University|CMU]] (then called [[Carnegie Tech]]): * {{Harvnb|McCorduck|2004|pp=139–179, 245–250, 322–323 (EPAM)}} * {{Harvnb|Crevier|1993|pp=145–149}} </ref>  <ref name="Soar"> [[Soar (cognitive architecture)|Soar]] (history): * {{Harvnb|McCorduck|2004|pp=450–451}} * {{Harvnb|Crevier|1993|pp=258–263}} </ref>  <ref name="AI at Stanford in the 60s"> [[John McCarthy (computer scientist)|McCarthy]] and AI research at [[Stanford Artificial Intelligence Laboratory|SAIL]] and [[SRI International]]: * {{Harvnb|McCorduck|2004|pp=251–259}} * {{Harvnb|Crevier|1993}}<!-- Page number needed --> </ref>  <ref name="AI at Edinburgh and France in the 60s"> AI research at [[University of Edinburgh|Edinburgh]] and in France, birth of [[Prolog]]: * {{Harvnb|Crevier|1993|pp=193–196}} * {{Harvnb|Howe|1994}} </ref>  <ref name="AI at MIT in the 60s"> [[AI]] at [[MIT]] under [[Marvin Minsky]] in the 1960s : * {{Harvnb|McCorduck|2004|pp=259–305}} * {{Harvnb|Crevier|1993|pp=83–102, 163–176}} * {{Harvnb|Russell|Norvig|2003|p=19}} </ref>  <ref name="Cyc"> [[Cyc]]: * {{Harvnb|McCorduck|2004|p=489}}, who calls it "a determinedly scruffy enterprise" * {{Harvnb|Crevier|1993|pp=239–243}} * {{Harvnb|Russell|Norvig|2003|p=363−365}} * {{Harvnb|Lenat|Guha|1989}} </ref>  <ref name="Knowledge revolution"> Knowledge revolution: * {{Harvnb|McCorduck|2004|pp=266–276, 298–300, 314, 421}} * {{Harvnb|Russell|Norvig|2003|pp=22–23}} </ref>  <ref name="Embodied AI"> [[Embodied agent|Embodied]] approaches to AI: * {{Harvnb|McCorduck|2004|pp=454–462}} * {{Harvnb|Brooks|1990}} * {{Harvnb|Moravec|1988}} </ref>  <ref name="Revival of connectionism"> Revival of [[connectionism]]: * {{Harvnb|Crevier|1993|pp=214–215}} * {{Harvnb|Russell|Norvig|2003|p=25}} </ref>  <ref name="Computational intelligence"> [[Computational intelligence]] * [http://www.ieee-cis.org/ IEEE Computational Intelligence Society] </ref>  <ref name="Intelligent agents"> The [[intelligent agent]] paradigm: * {{Harvnb|Russell|Norvig|2003|pp=27, 32–58, 968–972}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=7–21}} * {{Harvnb|Luger|Stubblefield|2004|pp=235–240}} The definition used in this article, in terms of goals, actions, perception and environment, is due to {{Harvtxt|Russell|Norvig|2003}}. Other definitions also include knowledge and learning as additional criteria. </ref>  <ref name="Agent architectures"> [[Agent architecture]]s, [[hybrid intelligent system]]s: * {{Harvtxt|Russell|Norvig|2003|pp=27, 932, 970–972}} * {{Harvtxt|Nilsson|1998|loc=chpt. 25}} </ref>  <ref name="Hierarchical control system"> [[Hierarchical control system]]: * Albus, J. S. [http://www.isd.mel.nist.gov/documents/albus/4DRCS.pdf 4-D/RCS reference model architecture for unmanned ground vehicles.] In G Gerhart, R Gunderson, and C Shoemaker, editors, Proceedings of the SPIE AeroSense Session on Unmanned Ground Vehicle Technology, volume 3693, pages 11—20 </ref>  <ref name="Subsumption architecture"> [[Subsumption architecture]]: * CITATION IN PROGRESS.{{citation needed|date=January 2011}} </ref>  <!---- TOOLS --------------------------------------------------------------------------------->  <ref name="Search"> [[Search algorithm]]s: * {{Harvnb|Russell|Norvig|2003|pp=59–189}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=113–163}} * {{Harvnb|Luger|Stubblefield|2004|pp=79–164, 193–219}} * {{Harvnb|Nilsson|1998|loc=chpt. 7–12}} </ref>  <ref name="Logic as search"> [[Forward chaining]], [[backward chaining]], [[Horn clause]]s, and logical deduction as search: * {{Harvnb|Russell|Norvig|2003|pp=217–225, 280–294}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=~46–52}} * {{Harvnb|Luger|Stubblefield|2004|pp=62–73}} * {{Harvnb|Nilsson|1998|loc=chpt. 4.2, 7.2}} </ref>  <ref name="Planning as search"> [[State space search]] and [[automated planning and scheduling|planning]]: * {{Harvnb|Russell|Norvig|2003|pp=382–387}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=298–305}} * {{Harvnb|Nilsson|1998|loc=chpt. 10.1–2}} </ref>  <ref name="Uninformed search"> Uninformed searches ([[breadth first search]], [[depth first search]] and general [[state space search]]): * {{Harvnb|Russell|Norvig|2003|pp=59–93}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=113–132}} * {{Harvnb|Luger|Stubblefield|2004|pp=79–121}} * {{Harvnb|Nilsson|1998|loc=chpt. 8}} </ref>  <ref name="Informed search"> [[Heuristic]] or informed searches (e.g., greedy [[best-first search|best first]] and [[A*]]): * {{Harvnb|Russell|Norvig|2003|pp= 94–109}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=pp. 132–147}}, * {{Harvnb|Luger|Stubblefield|2004|pp= 133–150}}, * {{Harvnb|Nilsson|1998|loc=chpt. 9}} </ref>  <ref name="Optimization search"> [[optimization (mathematics)|Optimization]] searches: * {{Harvnb|Russell|Norvig|2003|pp=110–116,120–129}} * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=56–163}} * {{Harvnb|Luger|Stubblefield|2004|pp= 127–133}} </ref>  <ref name="Society based learning"> [[Artificial life]] and society based learning: * {{Harvnb|Luger|Stubblefield|2004|pp=530–541}} </ref>  ).<ref name="Genetic programming"> [[Genetic programming]] and [[genetic algorithms]]: * {{Harvnb|Luger|Stubblefield|2004|pp=509–530}}, * {{Harvnb|Nilsson|1998|loc=chpt. 4.2}}. * {{cite book |last=Holland |first=John H. |year=1975 |title=Adaptation in Natural and Artificial Systems | publisher=University of Michigan Press | isbn = 0-262-58111-6}} * {{cite book |last=Koza|first=John R. |year=1992 |title=Genetic Programming| subtitle=On the Programming of Computers by Means of Natural Selection | publisher=MIT Press |isbn=0-262-11170-5}} * {{cite book | author=Poli, R., Langdon, W. B., McPhee, N. F. |year=2008 |title=A Field Guide to Genetic Programming | publisher=Lulu.com, freely available from http://www.gp-field-guide.org.uk/  | isbn = 978-1-4092-0073-4}} </ref>  <ref name="Logic"> [[Logic]]: * {{Harvnb|ACM|1998|loc=~I.2.3}}, * {{Harvnb|Russell|Norvig|2003|pp=194–310}}, * {{Harvnb|Luger|Stubblefield|2004|pp=35–77}}, * {{Harvnb|Nilsson|1998|loc=chpt. 13–16}} </ref>  <ref name="Satplan"> [[Satplan]]: * {{Harvnb|Russell|Norvig|2003|pp=402–407}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=300–301}}, * {{Harvnb|Nilsson|1998|loc=chpt. 21}} </ref>  <ref name="Symbolic learning techniques"> [[Explanation based learning]], [[relevance based learning]], [[inductive logic programming]], [[case based reasoning]]: * {{Harvnb|Russell|Norvig|2003|pp=678–710}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=414–416}}, * {{Harvnb|Luger|Stubblefield|2004|pp=~422–442}}, * {{Harvnb|Nilsson|1998|loc=chpt. 10.3, 17.5}} </ref>  <ref name="Propositional logic"> [[Propositional logic]]: * {{Harvnb|Russell|Norvig|2003|pp=204–233}}, * {{Harvnb|Luger|Stubblefield|2004|pp=45–50}} * {{Harvnb|Nilsson|1998|loc=chpt. 13}} </ref>  <ref name="First-order logic"> [[First-order logic]] and features such as [[equality (mathematics)|equality]]: * {{Harvnb|ACM|1998|loc=~I.2.4}}, * {{Harvnb|Russell|Norvig|2003|pp=240–310}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=268–275}}, * {{Harvnb|Luger|Stubblefield|2004|pp=50–62}}, * {{Harvnb|Nilsson|1998|loc=chpt. 15}} </ref>  <ref name="Fuzzy logic"> [[Fuzzy logic]]: * {{Harvnb|Russell|Norvig|2003|pp=526–527}} </ref>  <ref name="Subjective logic"> [[Subjective logic]]: * CITATION IN PROGRESS.{{citation needed|date=January 2011}} </ref>  <ref name="Stochastic methods for uncertain reasoning"> Stochastic methods for uncertain reasoning: * {{Harvnb|ACM|1998|loc=~I.2.3}}, * {{Harvnb|Russell|Norvig|2003|pp=462–644}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=345–395}}, * {{Harvnb|Luger|Stubblefield|2004|pp=165–191, 333–381}}, * {{Harvnb|Nilsson|1998|loc=chpt. 19}} </ref>  <ref name="Bayesian networks"> [[Bayesian network]]s: * {{Harvnb|Russell|Norvig|2003|pp=492–523}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=361–381}}, * {{Harvnb|Luger|Stubblefield|2004|pp=~182–190, ~363–379}}, * {{Harvnb|Nilsson|1998|loc=chpt. 19.3–4}} </ref>  <ref name="Bayesian inference"> [[Bayesian inference]] algorithm: * {{Harvnb|Russell|Norvig|2003|pp=504–519}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=361–381}}, * {{Harvnb|Luger|Stubblefield|2004|pp=~363–379}}, * {{Harvnb|Nilsson|1998|loc=chpt. 19.4 & 7}} </ref>  <ref name="Bayesian learning"> [[Bayesian learning]] and the [[expectation-maximization algorithm]]: * {{Harvnb|Russell|Norvig|2003|pp=712–724}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=424–433}}, * {{Harvnb|Nilsson|1998|loc=chpt. 20}} </ref>  <ref name="Bayesian decision networks"> [[Bayesian decision theory]] and Bayesian [[decision network]]s: * {{Harvnb|Russell|Norvig|2003|pp=597–600}} </ref>  <ref name="Stochastic temporal models"> Stochastic temporal models: * {{Harvnb|Russell|Norvig|2003|pp=537–581}} [[Dynamic Bayesian network]]s: * {{Harvnb|Russell|Norvig|2003|pp=551–557}} [[Hidden Markov model]]: * {{Harv|Russell|Norvig|2003|pp=549–551}} [[Kalman filter]]s: * {{Harvnb|Russell|Norvig|2003|pp=551–557}} </ref>  <ref name="Decisions theory and analysis"> [[decision theory]] and [[decision analysis]]: * {{Harvnb|Russell|Norvig|2003|pp=584–597}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=381–394}} </ref>  <ref name="Markov decision process" > [[Markov decision process]]es and dynamic [[decision network]]s: * {{Harvnb|Russell|Norvig|2003|pp=613–631}} </ref>  <ref name="Game theory and mechanism design"> [[Game theory]] and [[mechanism design]]: * {{Harvnb|Russell|Norvig|2003|pp=631–643}} </ref>  <ref name="Classifiers"> Statistical learning methods and [[classifier (mathematics)|classifiers]]: * {{Harvnb|Russell|Norvig|2003|pp=712–754}}, * {{Harvnb|Luger|Stubblefield|2004|pp=453–541}} </ref>  <ref name="Kernel methods"> [[kernel methods]] such as the [[support vector machine]], [[Kernel methods]]: * {{Harvnb|Russell|Norvig|2003|pp=749–752}} </ref>  <ref name="K-nearest neighbor algorithm"> [[K-nearest neighbor algorithm]]: * {{Harvnb|Russell|Norvig|2003|pp=733–736}} </ref>  <ref name="Guassian mixture model"> [[Gaussian mixture model]]: * {{Harvnb|Russell|Norvig|2003|pp=725–727}} </ref>  <ref name="Naive Bayes classifier"> [[Naive Bayes classifier]]: * {{Harvnb|Russell|Norvig|2003|pp=718}} </ref>  <ref name="Decision tree"> [[Alternating decision tree|Decision tree]]: * {{Harvnb|Russell|Norvig|2003|pp=653–664}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=403–408}}, * {{Harvnb|Luger|Stubblefield|2004|pp=408–417}} </ref>  <ref name="Classifier performance" > Classifier performance: * {{Harvnb|van der Walt|Bernard|2006}} </ref>  <ref name="Neural networks"> Neural networks and connectionism: * {{Harvnb|Russell|Norvig|2003|pp=736–748}}, * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=408–414}}, * {{Harvnb|Luger|Stubblefield|2004|pp=453–505}}, * {{Harvnb|Nilsson|1998|loc=chpt. 3}} </ref>  <ref name="Backpropagation"> [[Backpropagation]]: * {{Harvnb|Russell|Norvig|2003|pp=744–748}}, * {{Harvnb|Luger|Stubblefield|2004|pp=467–474}}, * {{Harvnb|Nilsson|1998|loc=chpt. 3.3}} </ref>  <ref name="Feedforward neural networks"> [[Feedforward neural network]]s, [[perceptron]]s and [[radial basis network]]s: * {{Harvnb|Russell|Norvig|2003|pp=739–748, 758}} * {{Harvnb|Luger|Stubblefield|2004|pp=458–467}} </ref>  <ref name="Recurrent neural networks"> [[Recurrent neural networks]], [[Hopfield nets]]: * {{Harvnb|Russell|Norvig|2003|p=758}} * {{Harvnb|Luger|Stubblefield|2004|pp=474–505}} </ref>  <ref name="Learning in neural networks"> [[Competitive learning]], [[Hebbian theory|Hebbian]] coincidence learning, [[Hopfield network]]s and attractor networks: * {{Harvnb|Luger|Stubblefield|2004|pp=474–505}} </ref>  <ref name="Hierarchical temporal memory"> [[Hierarchical temporal memory]]: * {{Harvnb|Hawkins|Blakeslee|2005}} </ref>  <ref name="Control theory"> [[Control theory]]: * {{Harvnb|ACM|1998|loc=~I.2.8}}, * {{Harvnb|Russell|Norvig|2003|pp=926–932}} </ref>  <ref name="Lisp"> [[Lisp (programming language)|Lisp]]: * {{Harvnb|Luger|Stubblefield|2004|pp=723–821}} * {{Harvnb|Crevier|1993|pp=59–62}}, * {{Harvnb|Russell|Norvig|2003|p=18}} </ref>  <ref name="Prolog"> [[Prolog]]: * {{Harvnb|Poole|Mackworth|Goebel|1998|pp=477–491}}, * {{Harvnb|Luger|Stubblefield|2004|pp=641–676, 575–581}} </ref>  <!---- PROGRESS ----------------------------------------------------------------------------------------------------->  <ref name="Turing test"> The [[Turing test]]:<br> Turing's original publication: * {{Harvnb|Turing|1950}} Historical influence and philosophical implications: * {{Harvnb|Haugeland|1985|pp=6–9}} * {{Harvnb|Crevier|1993|p=24}} * {{Harvnb|McCorduck|2004|pp=70–71}} * {{Harvnb|Russell|Norvig|2003|pp=2–3 and 948}} </ref>  <ref name="Subject matter expert Turing test"> [[Subject matter expert Turing test]]: * CITATION IN PROGRESS.{{citation needed|date=January 2011}} </ref>  <ref name="Mathematical definitions of intelligence"> Mathematical definitions of intelligence: * {{cite journal | title = Beyond the Turing Test | journal = Journal of Logic, Language and Information | author = Jose Hernandez-Orallo | id = {{citeseerx|10.1.1.44.8943}} |  accessdate =21 July 2009 | year = 2000 | pages = 447–466 | volume = 9  | issue = 4 | doi = 10.1023/A:1008367325700 | ref = harv }} * {{cite journal | title = A computational extension to the Turing Test | journal = Proceedings of the 4th Conference of the Australasian Cognitive Science jSociety | author =  D L Dowe and A R Hajek | url = http://www.csse.monash.edu.au/publications/1997/tr-cs97-322-abs.html |  accessdate =21 July 2009 | year = 1997 | ref = harv }} * {{cite journal | title = Measuring Universal Intelligence: Towards an Anytime Intelligence Test | journal = Artificial Intelligence Journal | author =  J Hernandez-Orallo and D L Dowe |  accessdate =1 October 2010 | year = 2010| ref = harv | doi=10.1016/j.artint.2010.09.006 | volume = 174 | issue = 18 | pages = 1508–1539}} </ref>  <ref name="Game AI"> [[Game AI]]: * CITATION IN PROGRESS.{{citation needed|date=January 2011}} </ref>  <!------ PHILOSOPHY ----------------------------------------------------------------------------------------------------->  <ref name="Philosophy of AI"> [[Philosophy of AI]]. All of these positions in this section are mentioned in standard discussions of the subject, such as: * {{Harvnb|Russell|Norvig|2003|pp=947–960}} * {{Harvnb|Fearn|2007|pp=38–55}} </ref>  <ref name="Dartmouth proposal"> [[Dartmouth Conferences|Dartmouth proposal]]: * {{Harvnb|McCarthy|Minsky|Rochester|Shannon|1955}} (the original proposal) * {{Harvnb|Crevier|1993|p=49}} (historical significance) </ref>  <ref name="Physical symbol system hypothesis"> The [[physical symbol system]]s hypothesis: * {{Harvnb|Newell|Simon|1976|p=116}} * {{Harvnb|McCorduck|2004|p=153}} * {{Harvnb|Russell|Norvig|2003|p=18}} </ref>  <ref name="Dreyfus' critique"> [[Dreyfus' critique of artificial intelligence]]: * {{Harvnb|Dreyfus|1972}}, {{Harvnb|Dreyfus|Dreyfus|1986}} * {{Harvnb|Crevier|1993|pp=120–132}} * {{Harvnb|McCorduck|2004|pp=211–239}} * {{Harvnb|Russell|Norvig|2003|pp=950–952}}, </ref>  <ref name="The mathematical objection"> The Mathematical Objection: * {{Harvnb|Russell|Norvig|2003|p=949}} * {{Harvnb|McCorduck|2004|pp=448–449}} Making the Mathematical Objection: * {{Harvnb|Lucas|1961}} * {{Harvnb|Penrose|1989}} Refuting Mathematical Objection: * {{Harvnb|Turing|1950}} under "(2) The Mathematical Objection" * {{Harvnb|Hofstadter|1979}} Background: * {{Harvnb|Ref=none|Gödel|1931}}, {{Harvnb|Ref=none|Church|1936}}, {{Harvnb|Ref=none|Kleene|1935}}, {{Harvnb|Ref=none|Turing|1937}} </ref>  <ref name="Searle's strong AI"> This version is from {{Harvtxt|Searle|1999}}, and is also quoted in {{Harvnb|Dennett|1991|p=435}}. Searle's original formulation was "The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states."  {{Harv|Searle|1980|p=1}}. Strong AI is defined similarly by {{Harvtxt|Russell|Norvig|2003|p=947}}: "The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis." </ref>  <ref name="Chinese room"> Searle's [[Chinese room]] argument: * {{Harvnb|Searle|1980}}. Searle's original presentation of the thought experiment. * {{Harvnb|Searle|1999}}. Discussion: * {{Harvnb|Russell|Norvig|2003|pp=958–960}} * {{Harvnb|McCorduck|2004|pp=443–445}} * {{Harvnb|Crevier|1993|pp=269–271}} </ref>  <!---- PREDICTIONS -------------------------------------------------------------------------------------------------------->  <ref name="Robot rights"> [[Robot rights]]: * {{Harvnb|Russell|Norvig|2003|p=964}} * {{cite news|url=http://news.bbc.co.uk/2/hi/technology/6200005.stm | date=21 December 2006 | title=Robots could demand legal rights |work=BBC News | accessdate=3 February 2011 }} Prematurity of: * {{cite news|url=http://www.timesonline.co.uk/tol/news/uk/science/article1695546.ece | title=Human rights for robots? We're getting carried away | work=The Times Online | location=London | first=Mark | last=Henderson | date=24 April 2007}}{{Dead link|date=February 2011}} In fiction: * {{Harvtxt|McCorduck|2004|p=190-25}} discusses ''[[Frankenstein]]'' and identifies the key ethical issues as scientific hubris and the suffering of the monster, i.e. [[robot rights]]. </ref>  <ref name="Replaced by machines"> AI could decrease the demand for human labor: * {{harvnb|Russell|Norvig|2003|pp=960–961}} * {{cite book| last=Ford | first=Martin | title=The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future | publisher=Acculant Publishing |year=2009 | isbn=978-1-4486-5981-4 | url=http://www.thelightsinthetunnel.com}} </ref>  <ref name="Weizenbaum's critique"> [[Joseph Weizenbaum]]'s critique of AI: * {{Harvnb|Weizenbaum|1976}} * {{Harvnb|Crevier|1993|pp=132–144}} * {{Harvnb|McCorduck|2004|pp=356–373}} * {{Harvnb|Russell|Norvig|2003|p=961}} Weizenbaum (the AI researcher who developed the first [[chatterbot]] program, [[ELIZA]]) argued in 1976 that the misuse of artificial intelligence has the potential to devalue human life. </ref>  <ref name=Singularity> [[Technological singularity]]: * {{Harvnb|Vinge|1993}} * {{Harvnb|Kurzweil|2005}} * {{Harvnb|Russell|Norvig|2003|p=963}} </ref>  <ref name="Transhumanism"> [[Transhumanism]]: * {{Harvnb|Moravec|1988}} * {{Harvnb|Kurzweil|2005}} * {{Harvnb|Russell|Norvig|2003|p=963}} </ref>  <ref name="AI as evolution"> AI as evolution: * [[Edward Fredkin]] is quoted in {{Harvtxt|McCorduck|2004|p=401}}. * {{Cite news|last=Butler|first=Samuel|authorlink=Samuel Butler (novelist)|contribution=[[Darwin among the Machines]]|date=13 June 1863|newspaper=the Press|place=Christchurch, New Zealand|url=http://www.nzetc.org/tm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html|ref=harv|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}, Letter to the Editor.<!-- I've asked if anyone knows how to format this right at Template talk:Citation/core --> * {{cite book|last=Dyson|first=George|authorlink=George Dyson (science historian)|title=Darwin among the Machiens|year=1998|publisher=Allan Lane Science|isbn= 0-7382-0030-1}} </ref>}}  ==References== ===AI textbooks=== {{refbegin}} * {{cite book |ref=harv | first=George | last=Luger | author-link=George Luger | first2=William | last2=Stubblefield | author2-link=William Stubblefield | year=2004 | title=Artificial Intelligence: Structures and Strategies for Complex Problem Solving|edition=5th | publisher=The Benjamin/Cummings Publishing Company, Inc. | isbn=0-8053-4780-1 | url=http://www.cs.unm.edu/~luger/ai-final/tocfull.html}} * {{cite book |ref=harv | first=Richard | last=Neapolitan  | first2=Xia | last2=Jiang  | year=2012 | title=Contemporary Artificial Intelligence  | publisher=Chapman & Hall/CRC | url=http://www.crcpress.com/product/isbn/9781439844694 | isbn=978-143984-469-4}} * {{cite book |ref=harv | last=Nilsson | first=Nils | author-link=Nils Nilsson (researcher) | year=1998 | title=Artificial Intelligence: A New Synthesis|publisher=Morgan Kaufmann Publishers | isbn=978-1-55860-467-4}} * {{Russell Norvig 2003}} * {{cite book |ref=harv | first = David | last = Poole | author-link=David Poole (researcher)| first2 = Alan | last2 = Mackworth | author2-link=Alan Mackworth | first3 = Randy | last3 = Goebel | author3-link=Randy Goebel | publisher = Oxford University Press | place = New York | year = 1998 | title = Computational Intelligence: A Logical Approach | url = http://www.cs.ubc.ca/spider/poole/ci.html |isbn=0-19-510270-3 }} * {{cite book |ref=harv | first = Patrick Henry | last = Winston | author-link = Patrick Winston | publisher = Addison-Wesley | year = 1984 | place = Reading, Massachusetts | isbn = 0-201-08259-4 | title = Artificial Intelligence}} {{refend}}  === History of AI === {{refbegin}} * {{Crevier 1993}} * {{McCorduck 2004}} {{refend}} Nilsson, Nils (2010), The Quest for Artificial Intelligence: A History of Ideas and Achievements, New York, NY: Cambridge University Press, ISBN 978052112293 {{Please check ISBN|reason=Invalid length.}}  === Other sources === {{refbegin|colwidth=60em}} * {{cite web |ref={{harvid|ACM|1998}} | publisher=[[Association of Computing Machinery|ACM]] | year=1998 | title=ACM Computing Classification System: Artificial intelligence | url=http://www.acm.org/class/1998/I.2.html | accessdate=30 August 2007 }} * {{Cite book | last = Aleksander | first= Igor | authorlink = Igor Aleksander | year=1995 | title= Artificial Neuroconsciousness: An Update | publisher=IWANN | url = http://www.ee.ic.ac.uk/research/neural/publications/iwann.html | archiveurl = http://web.archive.org/web/19970302014628/http://www.ee.ic.ac.uk/research/neural/publications/iwann.html | archivedate = 2 March 1997 | ref = harv }} [http://dblp.uni-trier.de/rec/bibtex/conf/iwann/Aleksander95 BibTex] [http://web.archive.org/web/19970302014628/http://www.ee.ic.ac.uk/research/neural/publications/iwann.html Internet Archive] * {{Cite journal |ref=harv | last=Brooks | first=Rodney | authorlink=Rodney Brooks | year =1990 | title = Elephants Don't Play Chess | journal = Robotics and Autonomous Systems | volume=6 | pages=3–15 | url=http://people.csail.mit.edu/brooks/papers/elephants.pdf | format=PDF | accessdate=30 August 2007 | doi=10.1016/S0921-8890(05)80025-9 | archiveurl= http://web.archive.org/web/20070809020912/http://people.csail.mit.edu/brooks/papers/elephants.pdf| archivedate= 9 August 2007 <!--DASHBot-->| deadurl= no}}. * {{Cite journal |ref=harv | last=Buchanan | first = Bruce G. | title = A (Very) Brief History of Artificial Intelligence | magazine = AI Magazine | year =2005 | pages=53–60 | url=http://www.aaai.org/AITopics/assets/PDF/AIMag26-04-016.pdf | format=PDF | accessdate=30 August 2007 | archiveurl= http://web.archive.org/web/20070926023314/http://www.aaai.org/AITopics/assets/PDF/AIMag26-04-016.pdf| archivedate= 26 September 2007 <!--DASHBot-->| deadurl= no}} * {{cite book |ref=harv | last=Dennett | first=Daniel | author-link=Daniel Dennett | year=1991 | title=[[Consciousness Explained]] | publisher=The Penguin Press | isbn= 0-7139-9037-6 }} * {{cite book |ref=harv | last=Dreyfus | first=Hubert | authorlink = Hubert Dreyfus | year = 1972 | title = [[What Computers Can't Do]] | publisher = MIT Press | location = New York | isbn = 0-06-011082-1 }} * {{cite book |ref=harv | last=Dreyfus | first=Hubert | authorlink = Hubert Dreyfus | year =1979 | title = What Computers ''Still'' Can't Do | publisher = MIT Press | location = New York | isbn=0-262-04134-0 }} * {{cite book |ref=harv | last=Dreyfus | first=Hubert  | authorlink = Hubert Dreyfus | last2 = Dreyfus | first2 = Stuart | year = 1986 | title = Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer | publisher = Blackwell | location = Oxford, UK | isbn=0-02-908060-6 }} * {{cite book |ref=harv | last=Dreyfus | first=Hubert | authorlink = Hubert Dreyfus | year =1992 | title = What Computers ''Still'' Can't Do | publisher = MIT Press | location = New York | isbn=0-262-54067-3 }} * {{cite web |ref=harv | last = Edelman | first = Gerald | authorlink = Gerald Edelman | date = 23 November 2007 | title = Gerald Edelman – Neural Darwinism and Brain-based Devices | url = http://lis.epfl.ch/resources/podcast/2007/11/gerald-edelman-neural-darwinism-and.html | publisher = Talking Robots }} * {{cite book |ref=harv | last=Fearn | first = Nicholas | year =2007 | title= The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers | publisher = Grove Press | location=New York |isbn=0-8021-1839-9 }} * {{cite web |ref=harv  | last = Forster | first =  Dion | authorlink= Dion Forster  | year = 2006  | title = Self validating consciousness in strong artificial intelligence: An African theological contribution  | url = http://www.spirituality.org.za/files/D%20Forster%20doctorate.pdf  | publisher = University of South Africa | location = Pretoria }} * {{cite book |ref=harv  | last = Gladwell | first =  Malcolm | authorlink= Malcolm Gladwell  | year = 2005  | title = [[Blink (book)|Blink]]  | isbn = 0-316-17232-4  | publisher = Little, Brown and Co. | location = New York }} * {{cite book |ref=harv | last=Haugeland | first=John | author-link = John Haugeland | year = 1985 | title = Artificial Intelligence: The Very Idea | publisher=MIT Press| location= Cambridge, Mass. | isbn=0-262-08153-9 }} * {{cite book |ref=harv | last=Hawkins | first=Jeff | author-link=Jeff Hawkins | last2=Blakeslee | first2=Sandra | year=2005 | title=[[On Intelligence]] | publisher=Owl Books | location=New York, NY | isbn=0-8050-7853-3 }} * {{cite book |ref=harv | last=Hofstadter | first = Douglas | author-link = Douglas Hofstadter | year = 1979 | title = [[Gödel, Escher, Bach|Gödel, Escher, Bach: an Eternal Golden Braid]] | isbn=0-394-74502-7 | publisher=Vintage Books | location=New York, NY }} * {{cite web |ref=harv | first = J. | last = Howe | date = November 1994 | title = Artificial Intelligence at Edinburgh University: a Perspective | url=http://www.inf.ed.ac.uk/about/AIhistory.html | accessdate=30 August 2007 }}. * {{cite book |ref=harv | last=Kahneman | first=Daniel | author-link=Daniel Kahneman | last2=Slovic | first2= D. | last3=Tversky | first3=Amos | author3-link=Amos Tversky | year=1982 | title=Judgment under uncertainty: Heuristics and biases | publisher=Cambridge University Press | location=New York | isbn=0-521-28414-7 }} * {{Cite journal |ref=harv | first = G. | last=Kolata | year=1982 | title=How can computers get common sense? | journal=Science | issue=  4566| pages=1237–1238 | doi = 10.1126/science.217.4566.1237 | volume = 217 |pmid=17837639  }} * {{cite book |ref=harv | last=Kurzweil | first=Ray | author-link=Ray Kurzweil | year=1999 | title=[[The Age of Spiritual Machines]] | publisher=Penguin Books | isbn=0-670-88217-8 }} * {{cite book |ref=harv | last=Kurzweil | first=Ray | author-link=Ray Kurzweil | year=2005 | title=[[The Singularity is Near]] | publisher=Penguin Books | isbn=0-670-03384-7 }} * {{cite book |ref=harv | first = George | last = Lakoff |author-link=George Lakoff | year = 1987 | title = Women, Fire, and Dangerous Things: What Categories Reveal About the Mind | publisher = University of Chicago Press | isbn= 0-226-46804-6 }} * {{cite book |ref=harv | last=Lakoff | first=George | author-link=George Lakoff | last2=Núñez | first2=Rafael E. | author2-link=Rafael E. Núñez| year=2000 | title=[[Where Mathematics Comes From|Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being]] | publisher=Basic Books | isbn= 0-465-03771-2 }}. * {{cite book |ref=harv | last=Lenat | first=Douglas | author-link=Douglas Lenat | last2=Guha | first2=R. V. | year = 1989 | title = Building Large Knowledge-Based Systems | publisher = Addison-Wesley | isbn=0-201-51752-3 }} * {{Cite journal |ref=harv | last=Lighthill | first = Professor Sir James | author-link=James Lighthill | year = 1973 | contribution= Artificial Intelligence: A General Survey | title = Artificial Intelligence: a paper symposium | publisher = Science Research Council }} * {{cite book |ref=harv | last=Lucas | first= John | author-link = John Lucas (philosopher) | year = 1961 | contribution=Minds, Machines and Gödel | editor-last = Anderson | editor-first =A.R. | title=Minds and Machines | url = http://users.ox.ac.uk/~jrlucas/Godel/mmg.html | accessdate=30 August 2007 | archiveurl= http://web.archive.org/web/20070819165214/http://users.ox.ac.uk/~jrlucas/Godel/mmg.html| archivedate= 19 August 2007 <!--DASHBot-->| deadurl= no}} * {{cite web |ref=harv | last = Maker | first = Meg Houston | year = 2006 | title = AI@50: AI Past, Present, Future | location=Dartmouth College | url = http://www.engagingexperience.com/2006/07/ai50_ai_past_pr.html | accessdate=16 October 2008 | archiveurl= http://web.archive.org/web/20081008120238/http://www.engagingexperience.com/2006/07/ai50_ai_past_pr.html| archivedate= 8 October 2008 <!--DASHBot-->| deadurl= no}} * {{cite web |ref=harv | last1 = McCarthy | first1 = John | authorlink1 = John McCarthy (computer scientist) | last2 = Minsky | first2 = Marvin | authorlink2 = Marvin Minsky | last3 = Rochester | first3 = Nathan | authorlink3 = Nathan Rochester | last4 = Shannon | first4 = Claude | authorlink4 = Claude Shannon | year = 1955 | title = A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence | url = http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html | accessdate=30 August 2007 | archiveurl= http://web.archive.org/web/20070826230310/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html| archivedate= 26 August 2007 <!--DASHBot-->| deadurl= no}}. * {{cite journal |ref=harv | last1 = McCarthy | first1 = John | author-link = John McCarthy (computer scientist) | last2 = Hayes | first2=P. J. | year = 1969 | title= Some philosophical problems from the standpoint of artificial intelligence | journal =Machine Intelligence | volume= 4 | pages = 463–502 | url=http://www-formal.stanford.edu/jmc/mcchay69.html | accessdate=30 August 2007 | archiveurl= http://web.archive.org/web/20070810233856/http://www-formal.stanford.edu/jmc/mcchay69.html| archivedate= 10 August 2007 <!--DASHBot-->| deadurl= no}} * {{cite web |ref=harv | last=McCarthy | first=John | authorlink=John McCarthy (computer scientist) | title=What Is Artificial Intelligence? | date=12 November 2007 | url=http://www-formal.stanford.edu/jmc/whatisai/whatisai.html }} * {{cite book |ref=harv | last=Minsky | first=Marvin | author-link=Marvin Minsky | year = 1967 | title = Computation: Finite and Infinite Machines | publisher = Prentice-Hall | location=Englewood Cliffs, N.J. | isbn=0-13-165449-7 }} * {{cite book |ref=harv | last=Minsky | first=Marvin | author-link=Marvin Minsky | year = 2006 | title = [[The Emotion Machine]] | publisher = Simon & Schusterl | publication-place=New York, NY | isbn=0-7432-7663-9 }} * {{cite web |ref=harv | last=Moravec | first=Hans | authorlink=Hans Moravec | year = 1976 | title = The Role of Raw Power in Intelligence | url= http://www.frc.ri.cmu.edu/users/hpm/project.archive/general.articles/1975/Raw.Power.html  | accessdate=30 August 2007 }} * {{cite book |ref=harv | last=Moravec | first=Hans | author-link=Hans Moravec | year = 1988 | title = Mind Children | publisher = Harvard University Press | isbn=0-674-57616-0 }} * {{cite book |ref=harv | last=NRC | first=(United States National Research Council) | author-link=United States National Research Council | year=1999 | chapter=Developments in Artificial Intelligence | title=Funding a Revolution: Government Support for Computing Research | publisher=National Academy Press }} * {{cite book |ref=harv | last=Needham | first=Joseph | authorlink = Joseph Needham | year=1986 | title=[[Science and Civilization in China]]: Volume 2 | publisher=Caves Books Ltd. }} * {{cite book |ref=harv | last = Newell | first = Allen  | author-link=Allen Newell | last2 = Simon | first2=H. A. | year = 1963 | contribution=GPS: A Program that Simulates Human Thought | editor-last= Feigenbaum | editor-first= E.A. | editor2-last= Feldman | editor2-first= J. | title=Computers and Thought | publisher= McGraw-Hill | authorlink2=Herbert A. Simon | publication-place= New York }} * {{cite journal|ref=harv | last = Newell | first = Allen  | author-link=Allen Newell | last2 = Simon | first2=H. A. | year = 1976 | contribution = Computer Science as Empirical Inquiry: Symbols and Search | title = Communications of the ACM | volume= 19 | url = http://www.rci.rutgers.edu/~cfs/472_html/AI_SEARCH/PSS/PSSH4.html | authorlink2=Herbert A. Simon | issue=3 }}. * {{Citation<!-- using Citation template (rather than "cite magazine") to avoid the final period --> | last = Nilsson | first = Nils | author-link = Nils Nilsson (researcher) | year = 1983 | title = Artificial Intelligence Prepares for 2001 | journal = AI Magazine | volume = 1 | number = 1 | url = http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General%20Essays/AIMag04-04-002.pdf }}, Presidential Address to the [[Association for the Advancement of Artificial Intelligence]]. * {{Cite book | ref=harv | last = Penrose | first = Roger | author-link = Roger Penrose | year = 1989 | title = The Emperor's New Mind: Concerning Computer, Minds and The Laws of Physics | publisher =  [[Oxford University Press]] | isbn = 0-19-851973-7 }} * {{Cite journal |ref=harv | last = Searle | first = John | author-link=John Searle | year = 1980 | title = Minds, Brains and Programs | journal = Behavioral and Brain Sciences | volume = 3 | issue = 3 | pages= 417–457 | url = http://www.bbsonline.org/documents/a/00/00/04/84/bbs00000484-00/bbs.searle2.html |doi=10.1017/S0140525X00005756 }} * {{cite book |ref=harv | last=Searle | first=John | author-link=John Searle | year = 1999 | title = Mind, language and society | publisher = Basic Books | location = New York, NY | isbn = 0-465-04521-9 | oclc = 231867665 43689264 }} * {{Cite journal |ref=harv | last1 = Serenko | first1 = Alexander | last2 = Detlor | first2 = Brian | year = 2004 | title = Intelligent agents as innovations | journal = AI and Society | volume = 18 | issue = 4 | pages= 364–381 | url =http://foba.lakeheadu.ca/serenko/papers/Serenko_Detlor_AI_and_Society.pdf |doi=10.1007/s00146-004-0310-5 }} * {{Cite journal |ref=harv | last1 = Serenko | first1 = Alexander | last2 = Ruhi | first2 = Umar | last3 = Cocosila | first3 = Mihail | year = 2007 | title = Unplanned effects of intelligent agents on Internet use: Social Informatics approach | journal = AI and Society | volume = 21 | issue = 1–2 | pages= 141–166 | url =http://foba.lakeheadu.ca/serenko/papers/AI_Society_Serenko_Social_Impacts_of_Agents.pdf |doi=10.1007/s00146-006-0051-8 }} * {{cite book |ref=harv | last=Shapiro | first= Stuart C. | editor-first=Stuart C. | editor-last=Shapiro | year=1992 | contribution=Artificial Intelligence | title=Encyclopedia of Artificial Intelligence | edition=2nd | pages=54–57 | url=http://www.cse.buffalo.edu/~shapiro/Papers/ai.pdf | publisher= John Wiley | location=New York | isbn=0-471-50306-1 }} * {{cite book |ref=harv | last=Simon | first = H. A. | author-link=Herbert A. Simon | year = 1965 | title=The Shape of Automation for Men and Management | publisher = Harper & Row | publication-place = New York }} * {{cite web |ref=harv | last=Skillings | first=Jonathan | url=http://news.cnet.com/Getting-machines-to-think-like-us/2008-11394_3-6090207.html | title=Getting Machines to Think Like Us | work=cnet | date=3 July 2006 | accessdate=3 February 2011 }} *  {{cite journal |ref=harv |last=Tecuci |first=Gheorghe |date= March/April 2012 |title = Artificial Intelligence |journal=Wiley Interdisciplinary Reviews: Computational Statistics |volume=4 |issue=2 |pages=168–180 |publisher=Wiley |doi=10.1002/wics.200 |pmid= |pmc= |url= |accessdate=6 May 2012 }} * {{Turing 1950}}. * {{cite web|ref=harv | last=van der Walt | first=Christiaan  |  last2=Bernard | first2 =Etienne | year=2006<!––year is presumed based on acknowledgements at the end of the article––> | title= Data characteristics that determine classifier performance | url=http://www.patternrecognition.co.za/publications/cvdwalt_data_characteristics_classifiers.pdf|format=PDF | accessdate =5 August 2009 }} * {{cite web | ref=harv | last=Vinge | first=Vernor | authorlink=Vernor Vinge | year=1993 | title=The Coming Technological Singularity: How to Survive in the Post-Human Era | url=http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html }} * {{cite book |ref=harv | last1=Wason | first1=P. C. | author-link=Peter Cathcart Wason | last2=Shapiro | first2=D. | editor=Foss, B. M. | year=1966 | title=New horizons in psychology | location=Harmondsworth | publisher=Penguin | chapter=Reasoning }} * {{cite book |ref=harv | last=Weizenbaum | first = Joseph | authorlink=Joseph Weizenbaum | year = 1976 | title = [[Computer Power and Human Reason]] | publisher = W.H. Freeman & Company | location = San Francisco | isbn = 0-7167-0464-1 }}  == Further reading == *TechCast Article Series, John Sagi, [http://www.techcast.org/Upload/PDFs/634146249446122137_Consciousness-Sagifinalversion.pdf Framing Consciousness] * [[Boden, Margaret]], Mind As Machine, [[Oxford University Press]], 2006 * Johnston, John (2008) "The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI", MIT Press * Myers, Courtney Boyd ed. (2009). [http://www.forbes.com/2009/06/22/singularity-robots-computers-opinions-contributors-artificial-intelligence-09_land.html The AI Report]. Forbes June 2009 * {{cite journal | last1 = Serenko | first1 = Alexander | year = 2010 | title = The development of an AI journal ranking based on the revealed preference approach | url = http://foba.lakeheadu.ca/serenko/papers/JOI_Serenko_AI_Journal_Ranking_Published.pdf | format = PDF | journal = Journal of Informetrics | volume = 4 | issue = 4| pages = 447–459 | doi = 10.1016/j.joi.2010.04.001 }} * Sun, R. & Bookman, L. (eds.), ''Computational Architectures: Integrating Neural and Symbolic Processes''. Kluwer Academic Publishers, Needham, MA. 1994.  == External links == {{Sister project links|Artificial Intelligence}} * [http://www-formal.stanford.edu/jmc/whatisai/whatisai.html What Is AI?] — An introduction to artificial intelligence by AI founder [[John McCarthy (computer scientist)|John McCarthy]]. * {{SEP|logic-ai|Logic and Artificial Intelligence|Richmond Thomason}} * {{dmoz|Computers/Artificial_Intelligence/|AI}} * [http://aaai.org/AITopics/ AITopics] — A large directory of links and other resources maintained by the [[Association for the Advancement of Artificial Intelligence]], the leading organization of academic AI researchers. * [https://www.researchgate.net/group/Artificial_Intelligence Artificial Intelligence Discussion group]  {{Clear}}<!-- cmd to lower template out of another template's space --> {{John McCarthy navbox}} {{Technology}} {{philosophy of science}} {{philosophy of mind}} {{Robotics}} {{Emerging technologies}} {{Use dmy dates|date=July 2012}}  {{DEFAULTSORT:Artificial Intelligence}} [[Category:Artificial intelligence| ]] [[Category:Cybernetics]] [[Category:Formal sciences]] [[Category:Technology in society]] [[Category:Computational neuroscience]] [[Category:Emerging technologies]] [[Category:Open problems]]  {{Link GA|es}} {{Link FA|tt}} [[ar:ذكاء اصطناعي]] [[an:Intelichencia artificial]] [[az:Süni intellekt]] [[bn:কৃত্রিম বুদ্ধিমত্তা]] [[zh-min-nan:Jîn-kang tì-lêng]] [[be:Штучны інтэлект]] [[be-x-old:Штучны інтэлект]] [[bg:Изкуствен интелект]] [[bs:Vještačka inteligencija]] [[ca:Intel·ligència artificial]] [[cs:Umělá inteligence]] [[da:Kunstig intelligens]] [[de:Künstliche Intelligenz]] [[et:Tehisintellekt]] [[el:Τεχνητή νοημοσύνη]] [[es:Inteligencia artificial]] [[eo:Artefarita inteligenteco]] [[eu:Adimen artifizial]] [[fa:هوش مصنوعی]] [[fr:Intelligence artificielle]] [[fur:Inteligjence artificiâl]] [[gl:Intelixencia artificial]] [[gan:人工智能]] [[ko:인공지능]] [[hi:कृत्रिम बुद्धिमत्ता]] [[hr:Umjetna inteligencija]] [[io:Artifical inteligenteso]] [[id:Kecerdasan buatan]] [[ia:Intelligentia artificial]] [[is:Gervigreind]] [[it:Intelligenza artificiale]] [[he:בינה מלאכותית]] [[jv:Kacerdhasan gawéyan]] [[kn:ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ]] [[ka:ხელოვნური ინტელექტი]] [[ky:Жасалма интеллект]] [[la:Intellegentia artificialis]] [[lv:Mākslīgais intelekts]] [[lt:Dirbtinis intelektas]] [[jbo:rutni menli]] [[hu:Mesterséges intelligencia]] [[ml:കൃത്രിമബുദ്ധി]] [[mr:कृत्रिम बुद्धिमत्ता]] [[arz:ذكاء صناعى]] [[ms:Kecerdasan buatan]] [[mn:Хиймэл оюун ухаан]] [[my:ဉာဏ်တု]] [[nl:Kunstmatige intelligentie]] [[new:आर्टिफिसियल इन्टेलिजेन्स]] [[ja:人工知能]] [[no:Kunstig intelligens]] [[nn:Kunstig intelligens]] [[oc:Intelligéncia artificiala]] [[pnb:بنائی گئی ذہانت]] [[pl:Sztuczna inteligencja]] [[pt:Inteligência artificial]] [[ksh:Artificial Intelligence]] [[ro:Inteligență artificială]] [[ru:Искусственный интеллект]] [[sah:Оҥоһуу интеллект]] [[sq:Inteligjenca artificiale]] [[simple:Artificial intelligence]] [[sk:Umelá inteligencia]] [[sl:Umetna inteligenca]] [[ckb:ژیریی دەستکرد]] [[sr:Вјештачка интелигенција]] [[sh:Umjetna inteligencija]] [[fi:Tekoäly]] [[sv:Artificiell intelligens]] [[tl:Intelihensiyang artipisyal]] [[ta:செயற்கை அறிவுத்திறன்]] [[tt:Ясалма интеллект]] [[te:కృత్రిమ మేధస్సు]] [[th:ปัญญาประดิษฐ์]] [[tr:Yapay zekâ]] [[tk:Ýasama akyl]] [[uk:Штучний інтелект]] [[ur:مصنوعی ذہانت]] [[vec:Inteligensa artificial]] [[vi:Trí tuệ nhân tạo]] [[fiu-vro:Kunstmõistus]] [[war:Artipisyal nga intelihensya]] [[yi:קינסטלעכע אינטעליגענץ]] [[zh-yue:人工智能]] [[bat-smg:Dėrbtėns intelekts]] [[zh:人工智能]]
{{Use dmy dates|date=July 2012}} {{for|the journal|Bioinformatics (journal)}} [[Image:Genome viewer screenshot small.png|thumbnail|right|220px|'''Map of the human X chromosome''' (from the [[National Center for Biotechnology Information|NCBI]] website). Assembly of the [[human genome]] is one of the greatest achievements of bioinformatics.]] {{Evolutionary biology}}  ''' Bioinformatics ''' {{IPAc-en|audio=en-us-bioinformatics.ogg|ˌ|b|aɪ|.|oʊ|ˌ|ɪ|n|f|ɚ|ˈ|m|æ|t|ɪ|k|s}} is a branch of [[biological]] science which deals with the study of methods for storing, retrieving and analyzing biological data, such as [[nucleic acid]] (DNA/RNA) and protein [[sequence (biology)|sequence]], [[protein structure|structure]], function, [[signal transduction|pathways]] and [[epistasis|genetic interactions]]. It generates new knowledge that is useful in such fields as [[drug design]] and development of new software tools to create that knowledge. Bioinformatics also deals with [[algorithm]]s, databases and information systems, web technologies, [[artificial intelligence]] and [[soft computing]], information and computation theory, [[structural biology]], software engineering, data mining, image processing, modeling and simulation, [[discrete mathematics]], control and system theory, circuit theory, and statistics.    Commonly used software tools and technologies in this field include [[Java (programming language)|Java]], [[XML]], [[Perl]], [[C (programming language)|C]], [[C  ]], [[Python (programming language)|Python]], [[R (programming language)|R]], [[MySQL]], [[SQL]], [[CUDA]], [[MATLAB]], and [[Microsoft Excel]].  ==Introduction== Building on the recognition of the importance of information transmission, accumulation and processing in biological systems, in 1978 [[Paulien Hogeweg]], coined the termed "Bioinformatics" to refer to the study of information processes in biotic systems <ref>{{cite web|title=The roots of Bioinformatics in Theoretical Biology|url=http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002021|author=Paulien Hogeweg|year=2011}}</ref>. This definition placed bioinformatics as field parallel to biophysics and biochemistry. Examples of relevant biological information processes studied in the early days of bioinformatics are the formation of complex social interaction structures by simple behavioral rules, and the information accumulation and maintenance in models of prebiotic evolution.   At the beginning of the "genomic revolution", the term bioinformatics was re-discovered to refer to the creation and maintenance of a database to store biological information such as [[nucleotide sequences]] and [[amino acid sequence]]s. Development of this type of database involved not only design issues but the development of complex interfaces whereby researchers could access existing data as well as submit new or revised data.  In order to study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data.  This includes nucleotide and amino acid sequences, protein domains, and protein structures.<ref>{{cite web|title=Concepts, Historical Milestones and the Central Place of Bioinformatics in Modern Biology: A European Perspective|url=http://www.intechopen.com/articles/show/title/concepts-historical-milestones-and-the-central-place-of-bioinformatics-in-modern-biology-a-european-|work=Bioinformatics - Trends and Methodologies|publisher= InTech |accessdate=8 Jan 2012|author = Attwood T.K., Gisel A., Eriksson N-E. and Bongcam-Rudloff E.|year = 2011}}</ref> The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include: * the development and implementation of tools that enable efficient access to, and use and management of, various types of information. * the development of new algorithms (mathematical formulas) and statistics with which to assess relationships among members of large data sets.  For example, methods to locate a gene within a sequence, predict protein structure and/or function, and cluster protein sequences into families of related sequences.  The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: [[pattern recognition]],  [[data mining]], [[machine learning]] algorithms, and [[Biological Data Visualization|visualization]]. Major research efforts in the field include [[sequence alignment]], [[gene finding]], [[genome assembly]], [[drug design]], [[drug discovery]], [[protein structural alignment|protein structure alignment]], [[protein structure prediction]], prediction of [[gene expression]] and [[protein–protein interactions]], [[genome-wide association studies]] and the modeling of [[evolution]].  Interestingly, the term ''bioinformatics'' was coined before the "genomic revolution". [[Paulien Hogeweg]] and Ben Hesper defined the term in 1978 to refer to "the study of information processes in biotic systems".<ref>Hesper B., Hogeweg P. (1970.) "Bioinformatica: een werkconcept", ''Kameleon'', 1(6): 28–29.</ref><ref>{{cite doi|10.1177/003754977803100305}}</ref><ref name="Hogeweg2011">{{cite pmid|21483479}}</ref>  This definition placed bioinformatics as a field parallel to biophysics or biochemistry (biochemistry is the study of chemical processes in biological systems).<ref name="Hogeweg2011" /> However, its primary use since at least the late 1980s has been to describe the application of computer science and information sciences to the analysis of biological data, particularly in those areas of genomics involving large-scale [[DNA sequencing]].  Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques and theory to solve formal and practical problems arising from the management and analysis of biological data.  Over the past few decades rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology.   Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.  Common activities in bioinformatics include mapping and analyzing [[DNA]] and protein sequences, aligning different DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.  There are two fundamental ways of modelling a Biological system (e.g., living cell) both coming under Bioinformatic approaches. *Static **Sequences – Proteins, Nucleic acids and Peptides **Structures – Proteins, Nucleic acids, Ligands (including metabolites and drugs) and Peptides **Interaction data among the above entities including microarray data and Networks of proteins, metabolites *Dynamic **Systems Biology comes under this category including reaction fluxes and variable concentrations of metabolites **Multi-Agent Based modelling approaches capturing cellular events such as signalling, transcription and reaction dynamics  A broad sub-category under bioinformatics is [[structural bioinformatics]].  ==Major research areas== ===Sequence analysis=== {{main|Sequence alignment|Sequence database}}  Since the [[Phi X 174|Phage Φ-X174]] was [[sequencing|sequenced]] in 1977,<ref name="pmid870828">{{cite journal |author=Sanger F, Air GM, Barrell BG, Brown NL, Coulson AR, Fiddes CA, Hutchison CA, Slocombe PM, Smith M |title=Nucleotide sequence of bacteriophage phi X174 DNA |journal=Nature |volume=265 |issue=5596 |pages=687–95 |year=1977 |month=February |pmid=870828 |doi= 10.1038/265687a0|url=}}</ref> the [[DNA sequence]]s of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode [[polypeptides]] ([[proteins]]), RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a [[species]] or between different species can show similarities between protein functions, or relations between species (the use of [[molecular systematics]] to construct [[phylogenetic tree]]s). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, [[computer program]]s such as [[BLAST]] are used daily to search sequences from more than 260 000 organisms, containing over 190 billion [[nucleotides]].<ref name="pmid18073190">{{cite journal |author=Benson DA, Karsch-Mizrachi I, Lipman DJ, Ostell J, Wheeler DL |title=GenBank |journal=Nucleic Acids Res. |volume=36 |issue=Database issue |pages=D25–30 |year=2008 |month=January |pmid=18073190 |pmc=2238942 |doi=10.1093/nar/gkm929 |url=}}</ref> These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this [[sequence alignment]] is used in the sequencing process itself. The so-called [[shotgun sequencing]] technique (which was used, for example, by [[The Institute for Genomic Research]] to sequence the first bacterial genome, ''[[Haemophilus influenzae]]'')<ref name="pmid7542800">{{cite journal |author=Fleischmann RD, Adams MD, White O, Clayton RA, Kirkness EF, Kerlavage AR, Bult CJ, Tomb JF, Dougherty BA, Merrick JM |title=Whole-genome random sequencing and assembly of Haemophilus influenzae Rd |journal=Science |volume=269 |issue=5223 |pages=496–512 |year=1995 |month=July |pmid=7542800 |doi= 10.1126/science.7542800|url=}}</ref> does not produce entire chromosomes. Instead it generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the [[human genome]], it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly will usually contain numerous gaps that have to be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.  Another aspect of bioinformatics in sequence analysis is annotation. This involves computational [[gene finding]] to search for protein-coding genes, RNA genes, and other functional sequences within a genome. Not all of the nucleotides within a genome are part of genes. Within the genomes of higher organisms, large parts of the DNA do not serve any obvious purpose. This so-called [[junk DNA]] may, however, contain unrecognized functional elements. Bioinformatics helps to bridge the gap between genome and [[proteome]] projects — for example, in the use of DNA sequences for protein identification.  {{see also|sequence analysis|sequence mining|sequence profiling tool|sequence motif}}  ===Genome annotation=== {{main|Gene finding}}  In the context of [[genomics]], [[Genome project#Genome annotation|annotation]] is the process of marking the genes and other biological features in a DNA sequence. The first genome annotation software system was designed in 1995 by Dr. Owen White, who was part of the team at [[The Institute for Genomic Research]] that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium ''[[Haemophilus influenzae]]''. Dr. White built a software system to find the genes (places in the DNA sequence that encode a protein), the transfer RNA, and other features, and to make initial assignments of function to those genes. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA are constantly changing and improving.  ===Computational evolutionary biology=== [[Evolutionary biology]] is the study of the origin and descent of [[species]], as well as their change over time. [[Informatics (academic field)|Informatics]] has assisted evolutionary biologists in several key ways; it has enabled researchers to: * trace the evolution of a large number of organisms by measuring changes in their [[DNA]], rather than through physical taxonomy or physiological observations alone, * more recently, compare entire [[genomes]], which permits the study of more complex evolutionary events, such as [[gene duplication]], [[horizontal gene transfer]], and the prediction of factors important in bacterial [[speciation]], * build complex computational models of populations to predict the outcome of the system over time <ref>{{cite web|title=Simulation of Genes and Genomes Forward in Time|url=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2851118/|work=Current Genomics|publisher= Bentham Science Publishers Ltd. |accessdate=16 April 2012|author = Antonio Carvajal-Rodríguez|year = 2012}}</ref>  * track and share information on an increasingly large number of species and organisms Future work endeavours to reconstruct the now more complex [[Evolutionary tree|tree of life]].  The area of research within [[computer science]] that uses [[genetic algorithm]]s is sometimes confused with [[computational evolutionary biology]], but the two areas are not necessarily related.  ===Literature analysis=== {{main|Text mining}}  The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed subfields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example: * abbreviation recognition - identify the long-form and abbreviation of biological terms, * named entity recognition - recognizing biological terms such as gene names * protein-protein interaction - identify which proteins interact with which proteins from text  The area of research draws from [[statistics]] and [[computational linguistics]].  ===Analysis of gene expression=== The [[gene expression|expression]] of many genes can be determined by measuring [[Messenger RNA|mRNA]] levels with multiple techniques including [[DNA microarray|microarrays]], [[expressed sequence tag|expressed cDNA sequence tag]] (EST) sequencing, [[serial analysis of gene expression]] (SAGE) tag sequencing, [[massively parallel signature sequencing]] (MPSS), [[RNA-Seq]], also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate [[signal (information theory)|signal]] from [[noise]] in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous [[epithelial]] cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.  ===Analysis of regulation=== Regulation is the complex orchestration of events starting with an extracellular signal such as a [[hormone]] and leading to an increase or decrease in the activity of one or more [[protein]]s. Bioinformatics techniques have been applied to explore various steps in this process. For example, [[promoter analysis]] involves the identification and study of [[sequence motif]]s in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Expression data can be used to infer gene regulation: one might compare [[microarray]] data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the [[cell cycle]], along with various stress conditions (heat shock, starvation, etc.). One can then apply [[cluster analysis|clustering algorithms]] to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented [[regulatory elements]].  ===Analysis of protein expression=== [[Protein microarray]]s and high throughput (HT) [[mass spectrometry]] (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected.  ===Analysis of mutations in cancer=== In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown [[point mutation]]s in a variety of [[gene]]s in [[cancer]]. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of [[human genome]] sequences and [[germline]] polymorphisms. New physical detection technologies are employed, such as [[oligonucleotide]] microarrays to identify chromosomal gains and losses (called [[comparative genomic hybridization]]), and [[single-nucleotide polymorphism]] arrays to detect known ''point mutations''. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate [[terabyte]]s of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or [[noise]], and thus [[Hidden Markov model]] and change-point analysis methods are being developed to infer real [[copy number variation|copy number]] changes.  Another type of data that requires novel informatics development is the analysis of [[lesion]]s found to be recurrent among many tumors<!-- this is specific enough, compared to general knowledge in previous paragraph, that it would be nice to have a source or example -->.  ===Comparative genomics=== {{main|Comparative genomics}}  The core of comparative genome analysis is the establishment of the correspondence between [[genes]] ([[orthology]] analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Ultimately, whole genomes are involved in processes of hybridization, polyploidization and [[endosymbiosis]], often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectra of algorithmic, statistical and mathematical techniques, ranging from exact, [[heuristics]], fixed parameter and [[approximation algorithms]] for problems based on parsimony models to [[Markov Chain Monte Carlo]] algorithms for [[Bayesian analysis]] of problems based on probabilistic models.  Many of these studies are based on the homology detection and protein families computation.  ===Modeling biological systems=== {{main|Computational systems biology}}  Systems biology involves the use of [[computer simulation]]s of [[cell (biology)|cellular]] subsystems (such as the [[metabolic network|networks of metabolites]] and [[enzyme]]s which comprise [[metabolism]], [[signal transduction]] pathways and [[gene regulatory network]]s) to both analyze and visualize the complex connections of these cellular processes. [[Artificial life]] or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.  ===High-throughput image analysis=== Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content [[biomedical imagery]]. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving [[accuracy]], [[Objectivity (science)|objectivity]], or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both [[diagnostics]] and research. Some examples are: * high-throughput and high-fidelity quantification and sub-cellular localization ([[high-content screening]], cytohistopathology, [[Bioimage informatics]]) * [[morphometrics]] * clinical image analysis and visualization * determining the real-time air-flow patterns in breathing lungs of living animals * quantifying occlusion size in real-time imagery from the development of and recovery during arterial injury * making behavioral observations from extended video recordings of laboratory animals * infrared measurements for metabolic activity determination * inferring clone overlaps in [[Gene mapping|DNA mapping]], e.g. the [[Sulston score]]  ===Structural bioinformatic approaches=== ====Prediction of protein structure==== {{main|Protein structure prediction}}  Protein structure prediction is another important application of bioinformatics. The [[amino acid]] sequence of a protein, the so-called [[primary structure]], can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the [[bovine spongiform encephalopathy]] – a.k.a. [[Mad Cow Disease]] – [[prion]].) Knowledge of this structure is vital in understanding the function of the protein. For lack of better terms, structural information is usually classified as one of ''[[secondary structure|secondary]]'', ''[[tertiary structure|tertiary]]'' and ''[[quaternary structure|quaternary]]'' structure. A viable general solution to such predictions remains an open problem. As of now, most efforts have been directed towards heuristics that work most of the time.  One of the key ideas in bioinformatics is the notion of [[homology (biology)|homology]]. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene ''A'', whose function is known, is homologous to the sequence of gene ''B,'' whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.  One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes ([[leghemoglobin]]). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.  Other techniques for predicting protein structure include protein threading and ''de novo'' (from scratch) physics-based modeling.  ''See also:'' [[structural motif]] and [[structural domain]].  ====Molecular Interaction==== Efficient software is available today for studying interactions among proteins, ligands and peptides. Types of interactions most often encountered in the field include – Protein–ligand (including drug), protein–protein and protein–peptide.  Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed '''docking algorithms''' for studying molecular interactions.  ''See also:'' [[protein–protein interaction prediction]].  =====Docking algorithms===== {{main|Protein–protein docking}} In the last two decades, tens of thousands of protein three-dimensional structures have been determined by [[X-ray crystallography]] and [[Protein nuclear magnetic resonance spectroscopy]] (protein NMR). One central question for the biological scientist is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without doing [[protein–protein interaction]] experiments. A variety of methods have been developed to tackle the [[Protein–protein docking]] problem, though it seems that there is still much work to be done in this field.  ==Software and tools== [[Software]] tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various [[List of bioinformatics companies|bioinformatics companies]] or public institutions.  ===Open source bioinformatics software=== Many [[free and open source software]] tools have existed and continued to grow since the 1980s.<ref name=obf-main>{{cite web|title=Open Bioinformatics Foundation: About us|url=http://www.open-bio.org/wiki/Main_Page|work=Official website|publisher=[[Open Bioinformatics Foundation]]|accessdate=10 May 2011}}</ref> The combination of a continued need for new [[algorithm]]s for the analysis of emerging types of biological readouts, the potential for innovative ''[[in silico]]'' experiments, and freely available [[open code]] bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported [[Plug-in (computing)|plug-ins]] in commercial applications. They may also provide ''[[de facto]]'' standards and shared object models for assisting with the challenge of bioinformation integration.    The [[List of open source bioinformatics software|range of open source software packages]] includes titles such as [[Bioconductor]], [[BioPerl]], [[Biopython]], [[BioJava]], [[BioRuby]], [[Bioclipse]], [[EMBOSS]], [[Taverna workbench]], and [[UGENE]]. In order to maintain this tradition and create further opportunities, the non-profit [[Open Bioinformatics Foundation]]<ref name=obf-main /> have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.<ref name=obf-bosc>{{cite web|title=Open Bioinformatics Foundation: BOSC|url=http://www.open-bio.org/wiki/BOSC|work=Official website|publisher=[[Open Bioinformatics Foundation]]|accessdate=10 May 2011}}</ref>  ===Web services in bioinformatics=== [[SOAP]] and [[REST]]-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.  Basic bioinformatics services are classified by the [[European Bioinformatics Institute|EBI]] into three categories: [[Sequence alignment software|SSS]] (Sequence Search Services), [[Multiple sequence alignment|MSA]] (Multiple Sequence Alignment) and [[Bioinformatics#Sequence analysis|BSA]] (Biological Sequence Analysis). The availability of these [[Service-orientation|service-oriented]] bioinformatics resources demonstrate the applicability of web based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible [[bioinformatics workflow management systems]].  ===Bioinformatics workflow management systems=== {{main|Bioinformatics workflow management systems}}  A [[Bioinformatics workflow management systems|Bioinformatics workflow management system]] is a specialized form of a [[workflow management system]] designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application.  Such systems are designed to  * provide an easy-to-use environment for individual application scientists themselves to create their own workflows * provide interactive tools for the scientists enabling them to execute their workflows and view their results in real-time * simplify the process of sharing and reusing workflows between the scientists. * enable scientists to track the [[provenance]] of the workflow execution results and the workflow creation steps.  Currently, there are two platforms giving this service: [[Galaxy (computational biology)|Galaxy]] and [[Taverna workbench|Taverna]].  ==See also== {{columns-list|2| * [[Bioinformatics companies]] * [[Health informatics]] * [[Computational biomodeling]] * [[Computational genomics]] * [[Functional genomics]] * [[Margaret Oakley Dayhoff]] * [[List of scientific journals in bioinformatics]] * [[List of open source bioinformatics software]] * [[Phylogenetics]] }}  ==References== {{reflist}}  ==Further reading== <!-- It's possible that some of these were used as the original sources for the article. --> {{refbegin}} * Achuthsankar S Nair [http://print.achuth.googlepages.com/BINFTutorialV5.0CSI07.pdf Computational Biology & Bioinformatics – A gentle Overview], Communications of Computer Society of India, January 2007 * Aluru, Srinivas, ed. ''Handbook of Computational Molecular Biology''. Chapman & Hall/Crc, 2006. ISBN 1-58488-406-1 (Chapman & Hall/Crc Computer and Information Science Series) * Baldi, P and Brunak, S, ''Bioinformatics: The Machine Learning Approach'', 2nd edition. MIT Press, 2001. ISBN 0-262-02506-X * Barnes, M.R. and Gray, I.C., eds., ''Bioinformatics for Geneticists'', first edition. Wiley, 2003. ISBN 0-470-84394-2 * Baxevanis, A.D. and  Ouellette, B.F.F., eds., ''Bioinformatics: A Practical Guide to the Analysis of Genes and Proteins'', third edition. Wiley, 2005. ISBN 0-471-47878-4 * Baxevanis, A.D., Petsko, G.A., Stein, L.D., and Stormo, G.D., eds., ''[[Current Protocols]] in Bioinformatics''. Wiley, 2007. ISBN 0-471-25093-7 * Claverie, J.M. and C. Notredame, ''Bioinformatics for Dummies''. Wiley, 2003. ISBN 0-7645-1696-5 * Cristianini, N. and Hahn, M. [http://www.computational-genomics.net/ ''Introduction to Computational Genomics''], Cambridge University Press, 2006. (ISBN 9780521671910 | ISBN 0-521-67191-4) * Durbin, R., S. Eddy, A. Krogh and G. Mitchison, ''Biological sequence analysis''. Cambridge University Press, 1998. ISBN 0-521-62971-3 * Gilbert, D. [http://bib.oxfordjournals.org/cgi/content/abstract/5/3/300 ''Bioinformatics software resources'']. Briefings in Bioinformatics, Briefings in Bioinformatics, 2004 5(3):300–304. * Keedwell, E., ''Intelligent Bioinformatics: The Application of Artificial Intelligence Techniques to Bioinformatics Problems''. Wiley, 2005. ISBN 0-470-02175-6 * Kohane, et al. ''Microarrays for an Integrative Genomics.'' The MIT Press, 2002. ISBN 0-262-11271-X * Lund, O. et al. ''Immunological Bioinformatics.'' The MIT Press, 2005. ISBN 0-262-12280-4 * Michael S. Waterman, ''Introduction to Computational Biology: Sequences, Maps and Genomes''. CRC Press, 1995. ISBN 0-412-99391-0 * Mount, David W. ''Bioinformatics: Sequence and Genome Analysis'' Spring Harbor Press, May 2002. ISBN 0-87969-608-7 * Pachter, Lior and [[Bernd Sturmfels|Sturmfels, Bernd]]. "Algebraic Statistics for Computational Biology" Cambridge University Press, 2005. ISBN 0-521-85700-7 * Pevzner, Pavel A. ''Computational Molecular Biology: An Algorithmic Approach'' The MIT Press, 2000. ISBN 0-262-16197-4 * Soinov, L. [http://jprr.org/index.php/jprr/article/view/8/5 Bioinformatics and Pattern Recognition Come Together] Journal of Pattern Recognition Research ([http://www.jprr.org JPRR]), Vol 1 (1) 2006 p.&nbsp;37–41 * Tisdall, James. "Beginning Perl for Bioinformatics" O'Reilly, 2001. ISBN 0-596-00080-4 * [http://publishing.royalsociety.org/bioinformatics Dedicated issue of ''Philosophical Transactions B'' on Bioinformatics freely available] * [http://www.nap.edu/catalog/11480.html Catalyzing Inquiry at the Interface of Computing and Biology (2005) CSTB report] * [http://www.nap.edu/catalog/2121.html Calculating the Secrets of Life: Contributions of the Mathematical Sciences and computing to Molecular Biology (1995)] * [http://ocw.mit.edu/OcwWeb/Biology/7-91JSpring2004/LectureNotes/index.htm Foundations of Computational and Systems Biology MIT Course] * [http://compbio.mit.edu/6.047/ Computational Biology: Genomes, Networks, Evolution Free MIT Course] * [http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-096Spring-2005/CourseHome/index.htm Algorithms for Computational Biology Free MIT Course]  ==External links== {{Spoken Wikipedia|En-Bioinformatics.ogg|2011-11-25}} {{Wiktionary|bioinformatics}} {{WVD}} <!-- Please use the talk page to propose any additions to this section. If you do not do this, the link will almost certainly be deleted. Also, do not list bioinformatics research groups or centers.--> * [http://bioinformatics.org/ Bioinformatics Organization] * [http://www.embnet.org/ EMBnet] * [http://www.open-bio.org/ Open Bioinformatics Foundation]  {{genomics-footer}} {{Biology-footer}} {{Technology}} {{Computer science}}  [[Category:Bioinformatics|*]] [[Category:Bioengineering]] [[Category:Biostatistics]] [[Category:Mathematical and theoretical biology]] [[Category:Computational biology]] [[Category:Formal sciences]]  [[ar:معلوماتية حيوية]] [[bn:জৈব তথ্যবিজ্ঞান]] [[be:Біяінфарматыка]] [[bg:Биоинформатика]] [[bs:Bioinformatika]] [[ca:Bioinformàtica]] [[cs:Bioinformatika]] [[da:Bioinformatik]] [[de:Bioinformatik]] [[et:Bioinformaatika]] [[el:Βιοπληροφορική]] [[es:Bioinformática]] [[eo:Biokomputiko]] [[fa:بیوانفورماتیک]] [[fr:Bio-informatique]] [[ko:생물정보학]] [[hi:जैव सूचना विज्ञान]] [[id:Bioinformatika]] [[is:Lífupplýsingafræði]] [[it:Bioinformatica]] [[he:ביואינפורמטיקה]] [[jv:Bioinformatika]] [[kn:ಬಯೋಇನ್‌ಫರ್ಮ್ಯಾಟಿಕ್ಸ್‌]] [[la:Informatio genetica]] [[lv:Bioinformātika]] [[lb:Bioinformatik]] [[lt:Bioinformatika]] [[li:Bioinformatica]] [[hu:Bioinformatika]] [[ml:ബയോ-ഇൻഫർമാറ്റിക്സ്‌]] [[ms:Bioinformasi]] [[nl:Bio-informatica]] [[ja:バイオインフォマティクス]] [[no:Bioinformatikk]] [[nov:Bioinformatike]] [[pl:Bioinformatyka]] [[pt:Bioinformática]] [[ru:Биоинформатика]] [[si:ජෛව ‍තොරතුරු විද්‍යාව]] [[simple:Bioinformatics]] [[sk:Bioinformatika]] [[sr:Биоинформатика]] [[sh:Bioinformatika]] [[fi:Bioinformatiikka]] [[sv:Bioinformatik]] [[tl:Biyoimpormatika]] [[ta:உயிர் தகவலியல்]] [[te:బయోఇన్ఫర్మేటిక్స్]] [[th:ชีวสารสนเทศศาสตร์]] [[tr:Biyoenformatik]] [[uk:Біоінформатика]] [[ur:معلوماتیۂ حیاتیات]] [[vi:Tin sinh học]] [[war:Biyoimpormatika]] [[yi:ביאאינפארמאטיק]] [[zh:生物信息学]]
{{Redirect|CPU}} {{Multiple images |direction = vertical |width = 220 |image1 = Intel 80486DX2 top.jpg |caption1 = An [[Intel 80486DX2]] CPU from above |image2 = Intel 80486DX2 bottom.jpg |caption2 =  An Intel 80486DX2 from below}} The '''central processing unit''' ('''CPU''', occasionally '''central processor unit'''<ref name="espin2">{{cite book | author = [[Envos Corporation]] |title= 1108 USER'S GUIDE | year = 1988 | month = September | url= http://www.bitsavers.org/pdf/xerox/interlisp/400004_1108UsersGuide_Sep88.pdf | chapter = 1. INTRODUCTION |format= PDF |accessdate= May 24, 2012 |type= Manual |publisher= Envos Corporation |page= 1 }}</ref>) is the hardware within a [[computer]] system which carries out the [[Instruction (computer science)|instruction]]s of a [[computer program]] by performing the basic arithmetical, logical, and [[input/output]] operations of the system. The CPU plays a role somewhat{{vague|date=July 2012}} analogous to the [[Human brain|brain]] in the computer. The term has been in use in the computer industry at least since the early 1960s.<ref name="weik1961">{{cite journal | author = Weik, Martin H. | title = A Third Survey of Domestic Electronic Digital Computing Systems | publisher = [[Ballistics Research Laboratory|Ballistic Research Laboratories]] | url = http://ed-thelen.org/comp-hist/BRL61.html | year = 1961 }}</ref> The form, design, and implementation of CPUs have changed over the course of their history, but their fundamental operation remains much the same.  On large machines, CPUs require one or more [[printed circuit board]]s. On [[personal computers]] and small workstations, the CPU is housed in a single [[silicon chip]] called a [[microprocessor]]. Since the 1970s the microprocessor class of CPUs has almost completely overtaken all other CPU implementations. Modern CPUs are large scale [[integrated circuit]]s in packages typically less than four centimeters square, with hundreds of connecting pins.  Two typical components of a CPU are the [[arithmetic logic unit]] (ALU), which performs arithmetic and logical operations, and the [[control unit]] (CU), which extracts instructions from [[Memory (computers)|memory]] and decodes and executes them, calling on the ALU when necessary.  Not all computational systems rely on a central processing unit. An array processor or [[vector processor]] has multiple parallel computing elements, with no one unit considered the "center". In the [[distributed computing]] model, problems are solved by a distributed interconnected set of processors.  ==History== {{Main|History of general purpose CPUs}} [[Image:Edvac.jpg|thumb|[[EDVAC]], one of the first stored program computers]]  Computers such as the [[ENIAC]] had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers." Since the term "CPU" is generally defined as a device for [[software]] (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the [[stored-program computer]].  The idea of a stored-program computer was already present in the design of [[J. Presper Eckert]] and [[John William Mauchly]]'s [[ENIAC]], but was initially omitted so that it could be finished sooner. On June&nbsp;30, 1945, before ENIAC was made, mathematician [[John von Neumann]] distributed the paper entitled ''First Draft of a Report on the [[EDVAC]]''. It was the outline of a stored-program computer that would eventually be completed in August 1949.<ref>{{cite journal | author =  | title = First Draft of a Report on the EDVAC | publisher = [[Moore School of Electrical Engineering]], [[University of Pennsylvania]] | url = http://www.virtualtravelog.net/entries/2003-08-TheFirstDraft.pdf | year = 1945 }}</ref> EDVAC was designed to perform a certain number of instructions (or operations) of various types.  These instructions could be combined to create useful programs for the EDVAC to run.  Significantly, the programs written for EDVAC were stored in high-speed [[Memory (computers)|computer memory]] rather than specified by the physical wiring of the computer.  This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program, or software, that [[EDVAC]] ran could be changed simply by changing the contents of the memory.  Early CPUs were custom-designed as a part of a larger, sometimes one-of-a-kind, computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of mass-produced processors that are made for many purposes. This standardization began in the era of discrete [[transistor]] [[Mainframe computer|mainframes]] and [[minicomputer]]s and has rapidly accelerated with the popularization of the [[integrated circuit]]&nbsp;(IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of [[nanometer]]s. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in everything from [[automobile]]s to [[cell phone]]s and children's toys.  While von Neumann is most often credited with the design of the stored-program computer because of his design of [[EDVAC]], others before him, such as [[Konrad Zuse]], had suggested and implemented similar ideas. The so-called [[Harvard architecture]] of the [[Harvard Mark I]], which was completed before EDVAC, also utilized a stored-program design using [[Punched tape|punched paper tape]] rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but elements of the Harvard architecture are commonly seen as well.{{citation needed|date=August 2012}}  [[Relay]]s and [[vacuum tube]]s (thermionic valves) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices.  The overall speed of a system is dependent on the speed of the switches. Tube computers like [[EDVAC]] tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) [[Harvard Mark I]] failed very rarely.<ref name="weik1961" /> In the end, tube based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low [[clock rate]]s compared to modern microelectronic designs (see below for a discussion of clock rate). Clock signal frequencies ranging from 100 [[Hertz|kHz]] to 4&nbsp;MHz were very common at this time, limited largely by the speed of the switching devices they were built with.  ===Transistor and integrated circuit CPUs=== [[Image:PDP-8i cpu.jpg|thumb|CPU, [[Magnetic core memory|core memory]], and [[external bus interface]] of a DEC [[PDP-8]]/I. Made of medium-scale integrated circuits]]  The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices.  The first such improvement came with the advent of the [[transistor]].  Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like [[vacuum tube]]s and [[Relay|electrical relays]].  With this improvement more complex and reliable CPUs were built onto one or several [[printed circuit board]]s containing discrete (individual) components.  During this period, a method of manufacturing many interconnected transistors in a compact space was developed. The [[integrated circuit]] (IC) allowed a large number of transistors to be manufactured on a single [[semiconductor]]-based [[Die (integrated circuit)|die]], or "chip."  At first only very basic non-specialized digital circuits such as [[NOR gate]]s were miniaturized into ICs.  CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices.  SSI ICs, such as the ones used in the [[Apollo guidance computer]], usually contained up to a few score transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.  As microelectronic technology advanced, an increasing number of transistors were placed on ICs, thus decreasing the quantity of individual ICs needed for a complete CPU. MSI and LSI (medium- and large-scale integration) ICs increased transistor counts to hundreds, and then thousands.  In 1964 [[IBM]] introduced its [[System/360]] computer architecture which was used in a series of computers that could run the same programs with different speed and performance.  This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer.  To facilitate this improvement, IBM utilized the concept of a [[microprogram]] (often called "microcode"), which still sees widespread usage in modern CPUs.<ref name="amdahl1964">{{cite journal | author = [[Gene Amdahl|Amdahl, G. M.]], Blaauw, G. A., & Brooks, F. P. Jr. | title = Architecture of the IBM System/360 | publisher = IBM Research | year = 1964 | url = http://www.research.ibm.com/journal/rd/441/amdahl.pdf }}</ref> The System/360 architecture was so popular that it dominated the [[mainframe computer]] market for decades and left a legacy that is still continued by similar modern computers like the IBM [[zSeries]].  In the same year (1964), [[Digital Equipment Corporation]] (DEC) introduced another influential computer aimed at the scientific and research markets, the [[PDP-8]].  DEC would later introduce the extremely popular [[PDP-11]] line that originally was built with SSI ICs but was eventually implemented with LSI components once these became practical.  In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.<ref>{{cite book | author = [[Digital Equipment Corporation]] | year = 1975 | month = November | title = LSI-11, PDP-11/03 user's manual | chapter = LSI-11 Module Descriptions | edition = 2nd | pages = 4–3 | publisher = Digital Equipment Corporation | location = Maynard, Massachusetts | url = http://www.classiccmp.org/bitsavers/pdf/dec/pdp11/1103/EK-LSI11-TM-002.pdf }}</ref>  Transistor-based computers had several distinct advantages over their predecessors.  Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay.  Thanks to both the increased reliability as well as the dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were obtained during this period.  Additionally while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like [[SIMD]] (Single Instruction Multiple Data) [[vector processor]]s began to appear.  These early experimental designs later gave rise to the era of specialized [[supercomputer]]s like those made by [[Cray Inc.]]  ===Microprocessors=== {{Main|Microprocessor}} {{Multiple images |width = 220 |direction = vertical |image1 = 80486dx2-large.jpg |caption1 = [[Die (integrated circuit)|Die]] of an [[Intel 80486DX2]] microprocessor (actual size: 12×6.75&nbsp;mm) in its packaging |image2 = EBIntel Corei5.JPG |caption2 = [[Intel]] Core i5 CPU on a Vaio E series laptop motherboard (on the right, beneath the [[heat pipe]]). }}  In the 1970s the fundamental inventions by [[Federico Faggin]] (Silicon Gate MOS ICs with self aligned gates along with his new random logic design methodology) changed the design and implementation of CPUs forever.  Since the introduction of the first commercially available microprocessor (the [[Intel 4004]]) in 1970, and the first widely used [[microprocessor]] (the [[Intel 8080]]) in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older [[computer architecture]]s, and eventually produced [[instruction set]] compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous [[personal computer]], the term ''CPU'' is now applied almost exclusively{{whom?|date=July 2012}} to microprocessors. Several CPUs can be combined in a single processing chip.  Previous generations of CPUs were implemented as discrete components and numerous small [[integrated circuit]]s (ICs) on one or more circuit boards.  Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one.  The overall smaller CPU size as a result of being implemented on a single die means faster switching time because of physical factors like decreased gate [[parasitic capacitance]].  This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz.  Additionally, as the ability to construct exceedingly small transistors on an IC has increased, the complexity and number of transistors in a single CPU has increased many fold.  This widely observed trend is described by [[Moore's law]], which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity.<ref>{{cite journal | title=Excerpts from A Conversation with Gordon Moore: Moore's Law | publisher=Intel | year=2005 | url=ftp://download.intel.com/museum/Moores_Law/Video-Transcripts/Excepts_A_Conversation_with_Gordon_Moore.pdf | format=PDF | accessdate=2012-07-25}}</ref>  While the complexity, size, construction, and general form of CPUs have changed enormously since 1950, it is notable that the basic design and function has not changed much at all.  Almost all common CPUs today can be very accurately{{whom?|date=July 2012}} described as von Neumann stored-program machines. As the aforementioned Moore's law continues to hold true{{whom?|date=July 2012}}, concerns have arisen about the limits of integrated circuit transistor technology.  Extreme miniaturization of electronic gates is causing the effects of phenomena like [[electromigration]] and [[subthreshold leakage]] to become much more significant.  These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the [[quantum computer]], as well as to expand the usage of [[Parallel computing|parallelism]] and other methods that extend the usefulness of the classical von Neumann model.  ==Operation== The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions called a program. The program is represented by a series of numbers that are kept in some kind of [[Memory (computers)|computer memory]].  There are four steps that nearly all CPUs use in their operation: fetch, decode, execute, and writeback.  The first step, fetch, involves retrieving an [[instruction (computer science)|instruction]] (which is represented by a number or sequence of numbers) from program memory.  The location in program memory is determined by a [[program counter]] (PC), which stores a number that identifies the current position in the program. After an instruction is fetched, the PC is incremented by the length of the instruction word in terms of memory units.<ref>Since the program counter counts ''memory addresses'' and not ''instructions,'' it is incremented by the number of memory units that the instruction word contains. In the case of simple fixed-length instruction word ISAs, this is always the same number.  For example, a fixed-length 32-bit instruction word ISA that uses 8-bit memory words would always increment the PC by 4 (except in the case of jumps).  ISAs that use variable length instruction words,increment the PC by the number of memory words corresponding to the last instruction's length.</ref>  Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned.  This issue is largely addressed in modern processors by caches and pipeline architectures (see below).  The instruction that the CPU fetches from memory is used to determine what the CPU is to do.  In the decode step, the instruction is broken up into parts that have significance to other portions of the CPU.  The way in which the numerical instruction value is interpreted is defined by the CPU's instruction set architecture (ISA).<ref>Because the instruction set architecture of a CPU is fundamental to its interface and usage, it is often used as a classification of the "type" of CPU.  For example, a "PowerPC CPU" uses some variant of the PowerPC ISA. A system can execute a different ISA by running an emulator.</ref>  Often, one group of numbers in the instruction, called the opcode, indicates which operation to perform.  The remaining parts of the number usually provide information required for that instruction, such as operands for an addition operation.  Such operands may be given as a constant value (called an immediate value), or as a place to locate a value: a [[processor register|register]] or a memory address, as determined by some [[addressing mode]].  In older designs the portions of the CPU responsible for instruction decoding were unchangeable hardware devices.  However, in more abstract and complicated CPUs and ISAs, a [[microprogram]] is often used to assist in translating instructions into various configuration signals for the CPU.  This microprogram is sometimes rewritable so that it can be modified to change the way the CPU decodes instructions even after it has been manufactured.  After the fetch and decode steps, the execute step is performed.  During this step, various portions of the CPU are connected so they can perform the desired operation.  If, for instance, an addition operation was requested, the [[arithmetic logic unit]] (ALU) will be connected to a set of inputs and a set of outputs.  The inputs provide the numbers to be added, and the outputs will contain the final sum.  The ALU contains the circuitry to perform simple arithmetic and logical operations on the inputs (like addition and [[bitwise operations]]).  If the addition operation produces a result too large for the CPU to handle, an arithmetic overflow flag in a flags register may also be set.  The final step, writeback, simply "writes back" the results of the execute step to some form of memory.  Very often the results are written to some internal CPU register for quick access by subsequent instructions.  In other cases results may be written to slower, but cheaper and larger, [[Random access memory|main memory]].  Some types of instructions manipulate the program counter rather than directly produce result data. These are generally called "jumps" and facilitate behavior like [[Control flow#Loops|loops]], conditional program execution (through the use of a conditional jump), and [[Subroutine|functions]] in programs.<ref>Some early computers like the Harvard Mark I did not support any kind of "jump" instruction, effectively limiting the complexity of the programs they could run.  It is largely for this reason that these computers are often not considered to contain a CPU proper, despite their close similarity as stored program computers.</ref>  Many instructions will also change the state of digits in a "flags" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations.  For example, one type of "compare" instruction considers two values and sets a number in the flags register according to which one is greater.  This flag could then be used by a later jump instruction to determine program flow.  After the execution of the instruction and writeback of the resulting data, the entire process repeats, with the next [[instruction cycle]] normally fetching the next-in-sequence instruction because of the incremented value in the program counter.  If the completed instruction was a jump, the program counter will be modified to contain the address of the instruction that was jumped to, and program execution continues normally.  In more complex CPUs than the one described here, multiple instructions can be fetched, decoded, and executed simultaneously.  This section describes what is generally referred to as the "[[classic RISC pipeline]]", which in fact is quite common among the simple CPUs used in many electronic devices (often called microcontroller). It largely ignores the important role of [[CPU cache]], and therefore the access stage of the pipeline.  ==Design and implementation== {{Main|CPU design}}  The basic concept of a CPU is as follows:  Hardwired into a CPU's design is a list of basic operations it can perform, called an [[instruction set]].  Such operations may include adding or subtracting two numbers, comparing numbers, or jumping to a different part of a program.  Each of these basic operations is represented by a particular sequence of [[bit]]s; this sequence is called the [[opcode]] for that particular operation.  Sending a particular opcode to a CPU will cause it to perform the operation represented by that opcode.  To execute an instruction in a computer program, the CPU uses the opcode for that instruction as well as its arguments (for instance the two numbers to be added, in the case of an addition operation).  A [[computer program]] is therefore a sequence of instructions, with each instruction including an opcode and that operation's arguments.  The actual mathematical operation for each instruction is performed by a subunit of the CPU known as the [[arithmetic logic unit]] or ALU.  In addition to using its ALU to perform operations, a CPU is also responsible for reading the next instruction from memory, reading data specified in arguments from memory, and writing results to memory.  In many CPU designs, an instruction set will clearly differentiate between operations that load data from memory, and those that perform math.  In this case the data loaded from memory is stored in [[processor register|registers]], and a mathematical operation takes no arguments but simply performs the math on the data in the registers and writes it to a new register, whose value a separate operation may then write to memory.  ===Control unit=== {{Main|Control unit}} The control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit must communicate with both the arithmetic/logic unit and memory.  ===Integer range=== The way a CPU represents numbers is a design choice that affects the most basic ways in which the device functions.  Some early digital computers used an electrical model of the common [[decimal]] (base ten) [[numeral system]] to represent numbers internally.  A few other computers have used more exotic numeral systems like [[Balanced ternary|ternary]] (base three). Nearly all modern CPUs represent numbers in [[Binary numeral system|binary]] form, with each digit being represented by some two-valued physical quantity such as a "high" or "low" [[volt]]age.<ref>The physical concept of [[voltage]] is an analog one by its nature, practically having an infinite range of possible values.  For the purpose of physical representation of binary numbers, set ranges of voltages are defined as one or zero.  These ranges are usually influenced by the circuit designs and operational parameters of the switching elements used to create the CPU, such as a [[transistor]]'s threshold level.</ref>  [[Image:MOS 6502AD 4585 top.jpg|thumb|left|[[MOS Technology 6502|MOS 6502]] microprocessor in a [[dual in-line package]], an extremely popular 8-bit design]] Related to number representation is the size and precision of numbers that a CPU can represent.  In the case of a binary CPU, a ''bit'' refers to one significant place in the numbers a CPU deals with. The number of bits (or numeral places) a CPU uses to represent numbers is often called "[[Word (data type)|word size]]", "bit width", "data path width", or "integer precision" when dealing with strictly integer numbers (as opposed to [[floating point]]). This number differs between architectures, and often within different parts of the very same CPU. For example, an [[8-bit]] CPU deals with a range of numbers that can be represented by eight binary digits (each digit having two possible values), that is, 2<sup>8</sup> or 256 discrete numbers.  In effect, integer size sets a hardware limit on the range of integers the software run by the CPU can utilize.<ref>While a CPU's integer size sets a limit on integer ranges, this can (and often is) overcome using a combination of software and hardware techniques.  By using additional memory, software can represent integers many magnitudes larger than the CPU can.  Sometimes the CPU's ISA will even facilitate operations on integers larger than it can natively represent by providing instructions to make large integer arithmetic relatively quick.  While this method of dealing with large integers is somewhat slower than utilizing a CPU with higher integer size, it is a reasonable trade-off in cases where natively supporting the full integer range needed would be cost-prohibitive.  See [[Arbitrary-precision arithmetic]] for more details on purely software-supported arbitrary-sized integers.</ref>  Integer range can also affect the number of locations in memory the CPU can address (locate).  For example, if a binary CPU uses 32 bits to represent a memory address, and each memory address represents one [[octet (computing)|octet]] (8&nbsp;bits), the maximum quantity of memory that CPU can address is 2<sup>32</sup> octets, or 4 [[GiB]].  This is a very simple view of CPU [[address space]], and many designs use more complex addressing methods like [[Bank switching|paging]] to locate more memory than their integer range would allow with a flat address space.  Higher levels of integer range require more structures to deal with the additional digits, and therefore more complexity, size, power usage, and general expense.  It is not at all uncommon, therefore, to see 4- or 8-bit [[microcontroller]]s used in modern applications, even though CPUs with much higher range (such as 16, 32, 64, even 128-bit) are available.  The simpler microcontrollers are usually cheaper, use less power, and therefore generate less heat, all of which can be major design considerations for electronic devices.  However, in higher-end applications, the benefits afforded by the extra range (most often the additional address space) are more significant and often affect design choices.  To gain some of the advantages afforded by both lower and higher bit lengths, many CPUs are designed with different bit widths for different portions of the device.  For example, the IBM [[System/370]] used a CPU that was primarily 32 bit, but it used 128-bit precision inside its [[floating point]] units to facilitate greater accuracy and range in floating point numbers.<ref name="amdahl1964" /> Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.  ===Clock rate=== {{Main|Clock rate}}  The clock rate is the speed at which a microprocessor executes instructions. Every computer contains an internal clock that regulates the rate at which instructions are executed and synchronizes all the various computer components. The CPU requires a fixed number of clock ticks (or clock cycles) to execute each instruction. The faster the clock, the more instructions the CPU can execute per second.  Most CPUs, and indeed most [[sequential logic]] devices, are [[Synchronous circuit|synchronous]] in nature.<ref>In fact, all synchronous CPUs use a combination of [[sequential logic]] and [[combinational logic]].  (See [[boolean logic]])</ref>  That is, they are designed and operate on assumptions about a synchronization signal.  This signal, known as a [[clock signal]], usually takes the form of a periodic [[square wave]].  By calculating the maximum time that electrical signals can move in various branches of a CPU's many circuits, the designers can select an appropriate [[Frequency|period]] for the clock signal.  This period must be longer than the amount of time it takes for a signal to move, or propagate, in the worst-case scenario.  In setting the clock period to a value well above the worst-case [[propagation delay]], it is possible to design the entire CPU and the way it moves data around the "edges" of the rising and falling clock signal.  This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective.  However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster.  This limitation has largely been compensated for by various methods of increasing CPU parallelism. (see below)  However, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs.  For example, a clock signal is subject to the delays of any other electrical signal.  Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit.  This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction.  Another major issue as clock rates increase dramatically is the amount of heat that is dissipated by the CPU.  The constantly changing clock causes many components to switch regardless of whether they are being used at that time.  In general, a component that is switching uses more energy than an element in a static state.  Therefore, as clock rate increases, so does heat dissipation, causing the CPU to require more effective cooling solutions.  One method of dealing with the switching of unneeded components is called [[clock gating]], which involves turning off the clock signal to unneeded components (effectively disabling them).  However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable late CPU design that uses clock gating is that of the IBM [[PowerPC]]-based [[Xbox 360]].  It utilizes extensive clock gating to reduce the power requirements of the aforementioned videogame console in which it is used.<ref>{{cite web | last = Brown | first = Jeffery | title = Application-customized CPU design | publisher = IBM developerWorks | url = http://www-128.ibm.com/developerworks/power/library/pa-fpfxbox/?ca=dgr-lnxw07XBoxDesign | year = 2005 | accessdate = 2005-12-17 }}</ref>  Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether.  While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs.  While somewhat uncommon, entire [[Asynchronous_circuit#Asynchronous_CPU|asynchronous CPU]]s have been built without utilizing a global clock signal.  Two notable examples of this are the [[ARM architecture|ARM]] compliant [[AMULET microprocessor|AMULET]] and the [[MIPS architecture|MIPS]] R3000 compatible MiniMIPS.  Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous [[Arithmetic logic unit|ALUs]] in conjunction with superscalar pipelining to achieve some arithmetic performance gains.  While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations.  This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for [[embedded computer]]s.<ref>{{cite journal | author = Garside, J. D., Furber, S. B., & Chung, S-H | title = AMULET3 Revealed | publisher = [[University of Manchester]] Computer Science Department | year = 1999 | url = http://www.cs.manchester.ac.uk/apt/publications/papers/async99_A3.php }}</ref>  ===Parallelism=== {{Main|Parallel computing}} [[Image:Nopipeline.png|thumb|Model of a subscalar CPU. Notice that it takes fifteen cycles to complete three instructions.]]  The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take.  This type of CPU, usually referred to as ''subscalar'', operates on and executes one instruction on one or two pieces of data at a time.  This process gives rise to an inherent inefficiency in subscalar CPUs.  Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction.  As a result, the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution.  Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased.  This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach ''scalar'' performance (one instruction per clock). However, the performance is nearly always subscalar (less than one instruction per cycle).  Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel.  When referring to parallelism in CPUs, two terms are generally used to classify these design techniques.  [[Instruction level parallelism]] (ILP) seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the utilization of on-die execution resources), and [[thread level parallelism]] (TLP) purposes to increase the number of [[Thread (computer science)|threads]] (effectively individual programs) that a CPU can execute simultaneously.  Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.<ref>Neither [[Instruction level parallelism|ILP]] nor [[Thread level parallelism|TLP]] is inherently superior over the other; they are simply different means by which to increase CPU parallelism.  As such, they both have advantages and disadvantages, which are often determined by the type of software that the processor is intended to run.  High-TLP CPUs are often used in applications that lend themselves well to being split up into numerous smaller applications, so-called "[[embarrassingly parallel]] problems".  Frequently, a computational problem that can be solved quickly with high TLP design strategies like SMP take significantly more time on high ILP devices like superscalar CPUs, and vice versa.</ref>  ====Instruction level parallelism==== {{Main|Instruction pipelining|Superscalar}} [[Image:Fivestagespipeline.png|thumb|left|Basic five-stage pipeline. In the best case scenario, this pipeline can sustain a completion rate of one instruction per cycle.]]  One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing.  This is the simplest form of a technique known as [[instruction pipelining]], and is utilized in almost all modern general-purpose CPUs.  Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.  Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict.  To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs.  Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).  [[Image:Superscalarpipeline.svg|thumb|Simple superscalar pipeline. By fetching and dispatching two instructions at a time, a maximum of two instructions per cycle can be completed.]]  Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further.  Designs that are said to be ''superscalar'' include a long instruction pipeline and multiple identical execution units.<ref>{{cite web | last = Huynh | first = Jack | title = The AMD Athlon XP Processor with 512KB L2 Cache | publisher = University of Illinois&nbsp;— Urbana-Champaign | pages = 6–11 | url = http://courses.ece.uiuc.edu/ece512/Papers/Athlon.pdf | year = 2003 | accessdate = 2007-10-06 }}</ref> In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously).  If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously.  In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.  Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher.  The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible.  This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of [[CPU cache]].  It also makes [[Hazard (computer architecture)|hazard]]-avoiding techniques like [[branch prediction]], [[speculative execution]], and [[out-of-order execution]] crucial to maintaining high levels of performance.  By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed.  Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes.  Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of Single Instructions Multiple Data&nbsp;— a case when a lot of data from the same type has to be processed, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.  In the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The Intel [[P5 (microarchitecture)|P5]] [[Pentium (brand)|Pentium]] had two superscalar ALUs which could accept one instruction per clock each, but its FPU could not accept one instruction per clock. Thus the P5 was integer superscalar but not floating point superscalar.  Intel's successor to the P5 architecture, [[P6 (microarchitecture)|P6]], added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.  Both simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per cycle (IPC).<ref>Best-case scenario (or peak) IPC rates in very superscalar architectures are difficult to maintain since it is impossible to keep the instruction pipeline filled all the time.  Therefore, in highly superscalar CPUs, average sustained IPC is often discussed rather than peak IPC.</ref>  Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar.  In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or [[Instruction set|ISA]].  The strategy of the [[very long instruction word]] (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity.  ====Thread-level parallelism==== Another strategy of achieving performance is to execute multiple programs or [[thread (computer science)|threads]] in parallel. This area of research is known as [[parallel computing]]. In [[Flynn's taxonomy]], this strategy is known as Multiple Instructions-Multiple Data or MIMD.  One technology used for this purpose was [[multiprocessing]] (MP). The initial flavor of this technology is known as [[symmetric multiprocessing]] (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as [[non-uniform memory access]] (NUMA) and [[directory-based coherence protocols]] were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single silicon chip, the technology is known as a [[Multi-core (computing)|multi-core]] microprocessor.  It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented [[input/output]] processing such as [[direct memory access]] as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as [[Multithreading (computer architecture)|multi-threading]] (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT.  One type of MT that was implemented is known as block multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the [[UltraSPARC T1|UltraSPARC]] Technology. Another type of MT is known as [[simultaneous multithreading]], where instructions of multiple threads are executed in parallel within one CPU clock cycle.  For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel [[Pentium 4]]. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.  CPU designers then borrowed ideas from commercial computing markets such as [[transaction processing]], where the aggregate performance of multiple programs, also known as [[throughput]] computing, was more important than the performance of a single thread or program.  This reversal of emphasis is evidenced by the proliferation of dual and multiple core CMP (chip-level multiprocessing) designs and notably, Intel's newer designs resembling its less superscalar [[P6 (microarchitecture)|P6]] architecture.  Late designs in several processor families exhibit CMP, including the [[x86-64]] [[Opteron]] and [[Athlon 64 X2]], the [[SPARC]] [[UltraSPARC T1]], IBM [[POWER4]] and [[POWER5]], as well as several [[video game console]] CPUs like the [[Xbox 360]]'s triple-core PowerPC design, and the [[PS3]]'s 7-core [[Cell (microprocessor)|Cell microprocessor]].  ====Data parallelism==== {{Main|Vector processor|SIMD}}  A less common but increasingly important paradigm of CPUs (and indeed, computing in general) deals with data parallelism.  The processors discussed earlier are all referred to as some type of scalar device.<ref>Earlier the term scalar was used to compare the IPC (instructions per cycle) count afforded by various ILP methods.  Here the term is used in the strictly mathematical sense to contrast with vectors.  See [[scalar (mathematics)]] and [[Vector (geometric)]].</ref>  As the name implies, vector processors deal with multiple pieces of data in the context of one instruction.  This contrasts with scalar processors, which deal with one piece of data for every instruction.  Using [[Flynn's taxonomy]], these two schemes of dealing with data are generally referred to as [[SIMD]] (single instruction, multiple data) and [[SISD]] (single instruction, single data), respectively.  The great utility in creating CPUs that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a [[dot product]]) to be performed on a large set of data.  Some classic examples of these types of tasks are [[multimedia]] applications (images, video, and sound), as well as many types of [[Scientific computing|scientific]] and engineering tasks.  Whereas a scalar CPU must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector CPU can perform a single operation on a comparatively large set of data with one instruction.  Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.  Most early vector CPUs, such as the [[Cray-1]], were associated almost exclusively with scientific research and [[cryptography]] applications.  However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose CPUs has become significant.  Shortly after inclusion of [[Floating point unit|floating point execution units]] started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose CPUs.  Some of these early SIMD specifications like HP's [[Multimedia Acceleration eXtensions]] (MAX) and Intel's [[MMX (instruction set)|MMX]] were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with [[floating point]] numbers. Progressively, these early designs were refined and remade into some of the common, modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples are Intel's [[Streaming SIMD Extensions|SSE]] and the PowerPC-related [[AltiVec]] (also known as VMX).<ref>Although SSE/SSE2/SSE3 have superseded MMX in Intel's general purpose CPUs, later [[IA-32]] designs still support MMX.  This is usually accomplished by providing most of the MMX functionality with the same hardware that supports the much more expansive SSE instruction sets.</ref>  ==Performance== {{main | computer performance}} The ''performance'' or ''speed'' of a processor depends on the clock rate (generally given in multiples of [[hertz]]) and the instructions per clock (IPC), which together are the factors for the [[instructions per second]] (IPS) that the CPU can perform.<ref name='Freq'>{{Cite web   | title = CPU Frequency   | work = CPU World Glossary   | publisher = CPU World   | date = 25 March 2008   | url = http://www.cpu-world.com/Glossary/C/CPU_Frequency.html   | accessdate = 1 January 2010 }}</ref> Many reported IPS values have represented "peak" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the [[memory hierarchy]] also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called [[benchmark (computing)|"benchmarks"]] for this purpose—such as [[SPECint]] -- have been developed to attempt to measure the real effective performance in commonly used applications.  Processing performance of computers is increased by using [[multi-core processor]]s, which essentially is plugging two or more individual processors (called ''cores'' in this sense) into one [[integrated circuit]].<ref name="tt">{{Cite web   | title = What is (a) multi-core processor?   | work = Data Center Definitions   | publisher = SearchDataCenter.com   | date = 27 March 2007   | url = http://searchdatacenter.techtarget.com/sDefinition/0,,sid80_gci1015740,00.html   | accessdate = 1 January 2010 }}</ref> Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, however, the performance gain is far less, only about 50%,<ref name="tt" /> due to imperfect software algorithms and implementation.  Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that a computer can handle.  This means that the processor can now handle numerous asynchronous events, Interrupts, etc. which can take a toll on the [[CPU]] (Central Processing Unit) when overwhelmed.  It is best to think of these numerous cores as different floors in a processing plant, with each floor handling a different task.  Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information to prevent a [[crash (computing)|crash]].  == Integrated heat spreader ==  IHS is usually made of copper covered with a nickel plating.  ==See also== <div style="-moz-column-count:4; column-count:4;"> * [[Accelerated Processing Unit]] * [[Addressing mode]] * [[Complex instruction set computer|CISC]] * [[Computer bus]] * [[Computer engineering]] * [[CPU cooling]] * [[CPU core voltage]] * [[CPU design]] * [[CPU power dissipation]] * [[CPU socket]] * [[Digital signal processor]] * [[Execution unit]] * [[Instruction pipeline]] * [[List of CPU architectures]] * [[Ring (computer security)]] * [[RISC]] * [[Stream processing]] * [[True Performance Index]] * [[Wait state]] </div>  ==Notes== {{reflist|colwidth=30em}}  ==References== <div class="references-small"> <references />  ==External links== {{Spoken Wikipedia-2|2006-06-13|Central Processing Unit (Part 1).ogg|Central Processing Unit (Part 2).ogg}} {{Commons category|Microprocessors}} {{wikiversity|Introduction to Computers/Processor}} ;Microprocessor designers *[http://www.amd.com/ Advanced Micro Devices] - [[Advanced Micro Devices]], a designer of primarily [[x86]]-compatible personal computer CPUs. *[http://www.arm.com/ ARM Ltd] - [[ARM Ltd]], one of the few CPU designers that profits solely by licensing their designs rather than manufacturing them.  [[ARM architecture]] microprocessors are among the most popular in the world for embedded applications. *[http://www.freescale.com/ Freescale Semiconductor] (formerly of [[Motorola]]) - [[Freescale Semiconductor]], designer of several embedded and [[System-on-a-chip|SoC]] PowerPC based processors. *[http://www-03.ibm.com/chips/ IBM Microelectronics] - Microelectronics division of [[IBM]], which is responsible for many [[IBM POWER|POWER]] and [[PowerPC]] based designs, including many of the CPUs utilized in late [[video game console]]s. *[http://www.intel.com/ Intel Corp] - [[Intel]], a maker of several notable CPU lines, including [[IA-32]] and [[IA-64]].  Also a producer of various peripheral chips for use with their CPUs. *[http://www.microchip.com/ Microchip Technology Inc.] - [[Microchip Technology|Microchip]], developers of the 8 and 16-bit short pipleine [[RISC]] and [[Digital Signal Processor|DSP]] microcontrollers. *[http://www.mips.com/ MIPS Technologies] - [[MIPS Technologies]], developers of the [[MIPS architecture]], a pioneer in [[RISC]] designs. *[http://www.am.necel.com/ NEC Electronics] - [http://www.am.necel.com/ NEC Electronics], developers of the [http://www.am.necel.com/micro/product/all_8_general.html/ 78K0 8-bit Architecture], [http://www.am.necel.com/micro/product/all_16_general.html/ 78K0R 16-bit Architecture], and [http://www.am.necel.com/micro/product/all_32_general.html/ V850 32-bit Architecture]. *[http://www.sun.com/ Sun Microsystems] - [[Sun Microsystems]], developers of the [[SPARC]] architecture, a RISC design. *[http://www.ti.com/home_p_allsc Texas Instruments] - [[Texas Instruments]] semiconductor division.  Designs and manufactures several types of low-power microcontrollers among their many other semiconductor products. *[http://www.transmeta.com/ Transmeta] - [[Transmeta]] Corporation. Creators of low-power x86 compatibles like [[Transmeta Crusoe|Crusoe]] and [[Efficeon]]. *[http://www.viatech.com/ VIA Technologies] - Taiwanese maker of low-power x86-compatible CPUs.  ;Further reading * {{HowStuffWorks|microprocessor|How Microprocessors Work}} *[http://spectrum.ieee.org/25chips 25 Microchips that shook the world] - an article by the [[Institute of Electrical and Electronics Engineers]]  {{CPU technologies}} {{Basic computer components}}  {{DEFAULTSORT:Central Processing Unit}} [[Category:Central processing unit| ]] {{Link FA|tt}}  [[af:Sentrale verwerkingseenheid]] [[als:Central Processing Unit]] [[ar:وحدة المعالجة المركزية]] [[an:Unidat central de procesamiento]] [[ba:Үҙәк процессор]] [[be:Цэнтральны працэсар]] [[be-x-old:Цэнтральны працэсар]] [[bg:Централен процесор]] [[bs:Procesor]] [[ca:Unitat central de processament]] [[cv:Процессор]] [[cs:Procesor]] [[da:CPU]] [[de:Hauptprozessor]] [[et:Keskprotsessor]] [[el:Κεντρική Μονάδα Επεξεργασίας]] [[es:Unidad central de procesamiento]] [[eo:Procesoro]] [[eu:Prozesatzeko unitate zentral]] [[fa:واحد پردازش مرکزی]] [[fr:Processeur]] [[fur:CPU]] [[ga:Láraonad próiseála]] [[gl:CPU]] [[ko:중앙 처리 장치]] [[hy:Մշակիչ]] [[hr:Procesor]] [[id:Unit Pemroses Sentral]] [[ia:Processator central]] [[is:Miðverk]] [[it:CPU]] [[he:מעבד]] [[jv:Piranti Pamrosésan Sentral]] [[kn:ಕೇಂದ್ರ ಸಂಸ್ಕರಣ ಘಟಕ]] [[krc:Процессор]] [[kk:Процессор]] [[sw:Bongo kuu (kompyuta)]] [[ky:Процессор]] [[la:Processorium medium]] [[lv:Centrālais procesors]] [[lt:Procesorius]] [[ln:Bɔngɔ́ (elektroníki)]] [[hu:Central processing unit]] [[mk:Централна обработувачка единица]] [[ml:സെന്‍ട്രല്‍ പ്രൊസസിങ് യൂണിറ്റ്]] [[arz:بروسيسور]] [[ms:Unit pemprosesan pusat]] [[mn:Төв процессор]] [[nl:Processor (computer)]] [[ja:CPU]] [[no:CPU]] [[nn:CPU]] [[oc:Processor]] [[mhr:Рӱдӧ процессор]] [[pnb:پروسیسر]] [[nds:Perzesser]] [[pl:Procesor]] [[pnt:Κεντρικόν μονάδα επεξεργασίας]] [[pt:Unidade central de processamento]] [[rue:Централный процесор]] [[ru:Процессор]] [[sah:Киин процессор]] [[sq:Njësia qendrore e përpunimit]] [[si:මධ්‍යම සැකසුම් ඒකකය (CPU)]] [[simple:Central processing unit]] [[sk:CPU]] [[sl:Procesor]] [[so:CPU]] [[ckb:یەکەی ناوەندیی پێوەئاژۆیی]] [[sr:Процесор]] [[sh:Procesor]] [[fi:Suoritin]] [[sv:Central Processing Unit]] [[tl:Sentral na nagpoprosesong unit]] [[ta:மையச் செயற்பகுதி]] [[tt:Үзәк эшкәрткеч җайланма]] [[te:సెంట్రల్ ప్రాసెసింగ్ యూనిట్]] [[th:หน่วยประมวลผลกลาง]] [[tg:Протсессори марказӣ]] [[tr:Merkezi işlem birimi]] [[uk:Центральний процесор]] [[ur:خرد عملیہ]] [[vi:CPU]] [[war:Processor]] [[yi:פראצעסאר]] [[zh-yue:處理器]] [[zh:中央处理器]]
{{cleanup-rewrite|date=February 2010}} '''Coding theory''' is the study of the properties of [[code]]s and their fitness for a specific application. Codes are used for [[data compression]], [[cryptography]], [[error-correction]] and more recently also for [[network coding]]. Codes are studied by various scientific disciplines—such as [[information theory]], [[electrical engineering]],  [[mathematics]], and [[computer science]]—for the purpose of designing efficient and reliable [[data transmission]] methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.   There are essentially two aspects to Coding theory: #   Data compression (or, ''source coding'') # [[Error-correction code|Error correction]] (or, ''[[channel coding]]'').  These two aspects may be [[Joint source and channel coding|studied in combination]]. Source encoding attempts to compress the data from a source in order to transmit it more efficiently.  This practice is found every day on the Internet where the common [[Zip (file format)|Zip data compression]] is used to reduce the network load and make files smaller. The second, channel encoding, adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using channel coding. A typical music CD uses the [[Reed-Solomon error correction#Data storage|Reed-Solomon]] code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and [[NASA]] all employ channel coding techniques to get the bits through, for example the [[turbo code]] and [[LDPC code]]s.  ==Source coding== {{main|data compression}} The aim of source coding is to take the source data and make it smaller.  ===Principle=== [[Entropy (information theory)|Entropy]] of a source is the measure of information. Basically source codes try to reduce the redundancy present in the source, and represent the source with fewer bits that carry more information.  Data compression which explicitly tries to minimize the average length of messages according to a particular assumed probability model is called [[entropy encoding]].  Various techniques used by source coding schemes try to achieve the limit of Entropy of the source. ''C''(''x'') ≥ ''H''(''x''), where ''H''(''x'') is entropy of source (bitrate), and  ''C''(''x'') is the bitrate after compression. In particular, no source coding scheme can be better than the entropy of the source.  ===Example=== [[FAX|Facsimile]] transmission uses a simple [[Run-length encoding|run length code]]. Source coding removes all data superfluous to the need of the transmitter, decreasing the bandwidth required for transmission.  ==Channel coding== {{main|Forward error correction}} The aim of channel coding theory is to find codes which transmit quickly, contain many valid [[code word]]s and can correct or at least [[error detection|detect]] many errors. While not mutually exclusive, performance in these areas is a trade off. So, different codes are optimal for different applications. The needed properties of this code mainly depend on the probability of errors happening during transmission. In a typical CD, the impairment is mainly dust or scratches. Thus codes are used in an interleaved manner.{{Citation needed|date=July 2008}} The data is spread out over the disk. Although not a very good code, a simple repeat code can serve as an understandable example. Suppose we take a block of data bits (representing sound) and send it three times. At the receiver we will examine the three repetitions bit by bit and take a majority vote. The twist on this is that we don't merely send the bits in order. We interleave them. The block of data bits is first divided into 4 smaller blocks. Then we cycle through the block and send one bit from the first, then the second, etc. This is done three times to spread the data out over the surface of the disk. In the context of the simple repeat code, this may not appear effective. However, there are more powerful codes known which are very effective at correcting the "burst" error of a scratch or a dust spot when this interleaving technique is used.  Other codes are more appropriate for different applications. Deep space communications are limited by the [[thermal noise]] of the receiver which is more of a continuous nature than a bursty nature. Likewise, narrowband modems are limited by the noise, present in the telephone network and  also modeled better as a continuous disturbance.{{Citation needed|date=July 2008}} Cell phones are subject to rapid fading. The high frequencies used can cause rapid fading of the signal even if the receiver is moved a few inches. Again there are a class of channel codes that are designed to combat fading.{{Citation needed|date=July 2008}}  ===Linear codes=== {{Main|Linear code}} The term '''algebraic coding theory''' denotes the sub-field of coding theory where the properties of codes are expressed in algebraic terms and then further researched.{{Citation needed|date=July 2008}}  Algebraic coding theory is basically divided into two major types of codes:{{Citation needed|date=July 2008}} # Linear block codes # Convolutional codes.  It analyzes the following three properties of a code – mainly:{{Citation needed|date=July 2008}} *code word length *total number of valid code words *the minimum [[distance]] between two valid code words, using mainly the [[Hamming distance]], sometimes also other distances like the [[Lee distance]].  ====Linear block codes==== {{Main|Block code}} Linear block codes have the property of [[linearity]], i.e. the sum of any two codewords is also a code word, and they are applied to the source bits in blocks, hence the name linear block codes. There are block codes that are not linear, but it is difficult to prove that a code is a good one without this property.<ref name=terras/>  Linear block codes are summarized by their symbol alphabets (e.g., binary or ternary) and parameters (''n'',''m'',''d<sub>min</sub>'')<ref name=blahut/> where  # n  is the length of the codeword, in symbols, # m is the number of source symbols that will be used for encoding at once, # ''d<sub>min</sub>'' is the minimum hamming distance for the code.  {{Merge to |Block code |date=July 2010}} There are many types of linear block codes, such as  # [[Cyclic code]]s (e.g., [[Hamming code]]s) # [[Repetition code]]s # [[Parity bit|Parity codes]] # [[Polynomial code]]s (e.g., [[BCH code]]s) # [[Reed-Solomon error correction|Reed–Solomon codes]] # [[Algebraic geometric code]]s # [[Reed–Muller code]]s # [[Hamming bound|Perfect codes]].  Block codes are tied to the [[sphere packing]] problem, which has received some attention over the years. In two dimensions, it is easy to visualize. Take a bunch of pennies flat on the table and push them together. The result is a hexagon pattern like a bee's nest. But block codes rely on more dimensions which cannot easily be visualized. The powerful (24,12) [[Binary Golay code|Golay code]] used in deep space communications uses 24 dimensions. If used as a binary code (which it usually is) the dimensions refer to the length of the codeword as defined above.  The theory of coding uses the ''N''-dimensional sphere model. For example, how many pennies can be packed into a circle on a tabletop, or in 3 dimensions, how many marbles can be packed into a globe. Other considerations enter the choice of a code. For example, hexagon packing into the constraint of a rectangular box will leave empty space at the corners. As the dimensions get larger, the percentage of empty space grows smaller. But at certain dimensions, the packing uses all the space and these codes are the so-called "perfect" codes. The only nontrivial and useful perfect codes are the distance-3 Hamming codes with parameters satisfying (2<sup>''r''</sub> – 1, 2<sup>''r''</sub> – 1 – ''r'', 3), and the [23,12,7] binary and [11,6,5] ternary Golay codes.<ref name=terras>{{cite book | title = Fourier Analysis on Finite Groups and Applications | author = Audrey Terras |authorlink=Audrey Terras| publisher = [[Cambridge University Press]] | year = 1999 | isbn = 0-521-45718-1 | url = http://books.google.com/books?id=-B2TA669dJMC&pg=PA195&dq=linear block code difficult prove nonlinear#PPA195,M1 }}</ref><ref name=blahut>{{cite book | title = Algebraic Codes for Data Transmission | author = Richard E. Blahut | publisher = Cambridge University Press | year = 2003 | isbn = 0-521-55374-1 | url = http://books.google.com/books?id=n0XHMY58tL8C&pg=PA60&dq=golay hamming only perfect}}</ref>  Another code property is the number of neighbors that a single codeword may have.<ref name=schlegel> {{cite book  | title = Trellis and turbo coding  | author = Christian Schlegel and Lance Pérez  | publisher = Wiley-IEEE  | year = 2004  | isbn = 978-0-471-22755-7  | page = 73  | url = http://books.google.com/books?id=9wRCjfGAaEcC&pg=PA73  }}</ref> Again, consider pennies as an example. First we pack the pennies in a rectangular grid. Each penny will have 4 near neighbors (and 4 at the corners which are farther away). In a hexagon, each penny will have 6 near neighbors. When we increase the dimensions, the number of near neighbors increases very rapidly.  The result is the number of ways for noise to make the receiver choose a neighbor (hence an error) grows as well. This is a fundamental limitation of block codes, and indeed all codes. It may be harder to cause an error to a single neighbor, but the number of neighbors can be large enough so the total error probability actually suffers.<ref name=schlegel/>  Properties of linear block codes are used in many applications.  For example, the syndrome-coset uniqueness property of linear block codes is  used in trellis shaping,<ref>[[Dave Forney|G.D. Forney, Jr.]] (March 1992), [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=119687 ''Trellis shaping''], [[IEEE]] Transactions on Information Theory, Vol. 38, Issue 2, Part 2.</ref> one of the best known [[shaping codes]]. This same property is used in sensor networks for distributed source coding  ====Convolutional codes==== {{Main|Convolutional code}}  The idea behind a convolutional code is to make every codeword symbol be the weighted sum of the various input message symbols. This is like [[convolution]] used in [[linear time invariant|LTI]] systems to find the output of a system, when you know the input and impulse response.  So we generally find the output of the system convolutional encoder, which is the convolution of the input bit, against the states of the convolution encoder, registers.  Fundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. The encoder is usually a simple circuit which has state memory and some feedback logic, normally XOR gates. The decoder can be implemented in software or firmware.  The [[Viterbi algorithm]] is the optimum algorithm used to decode convolutional codes. There are simplifications to reduce the computational load. They rely on searching only the most likely paths. Although not optimum, they have generally been found to give good results in the lower noise environments.  Convolutional codes are used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices.  ==Other applications of coding theory== {{misleading|date=August 2012}} Another concern of coding theory is designing codes that help [[synchronization]]. A code may be designed so that a [[Phase (waves)|phase shift]] can be easily detected and corrected and that multiple signals can be sent on the same channel.{{Citation needed|date=July 2008}}  Another application of codes, used in some mobile phone systems, is [[code-division multiple access]] (CDMA). Each phone is assigned a code sequence that is approximately uncorrelated with the codes of other phones.{{Citation needed|date=July 2008}} When transmitting, the code word is used to modulate the data bits representing the voice message. At the receiver, a demodulation process is performed to recover the data. The properties of this class of codes allow many users (with different codes) to use the same radio channel at the same time. To the receiver, the signals of other users will appear to the demodulator only as a low-level noise.{{Citation needed|date=July 2008}}  Another general class of codes are the [[automatic repeat-request]] (ARQ) codes. In these codes the sender adds redundancy to each message for error checking, usually by adding check bits. If the check bits are not consistent with the rest of the message when it arrives, the receiver will ask the sender to retransmit the message. All but the simplest [[wide area network]] protocols use ARQ. Common protocols include [[Synchronous Data Link Control|SDLC]] (IBM), [[Transmission Control Protocol|TCP]] (Internet), [[X.25]] (International) and many others. There is an extensive field of research on this topic because of the problem of matching a rejected packet against a new packet. Is it a new one or is it a retransmission? Typically numbering schemes are used, as in TCP.{{cite web |url= http://tools.ietf.org/html/rfc793 |title= RFC793 |work= RFCs|publisher= [[Internet Engineering Task Force]] (IETF) |date= 1981-09}}  === Group Testing === [[Group testing]] uses codes in a different way. Consider a large group of items in which a very few are different in a particular way (for e.g. Defective products or infected test subjects). The idea of group testing is to determine which items are "different" by using as few tests as possible. The origin of the problem has its roots in the [[Second World War]] when the [[United States Army Air Forces]] needed to test its soldiers for [[Syphilis]]. It originated from a ground-breaking paper by [[Robert Dorfman]].  ===Analog coding===  Information is encoded analogously in the [[neural network]]s of [[brain]]s, in [[analog signal processing]], and [[analog electronics]]. Aspects of [http://scholar.google.com/scholar?&q=%22analog code%22 OR %22analogue code%22 OR %22analogue coding%22 OR %22analog coding%22 analog coding] include [http://scholar.google.com/scholar?q=%22analog error correction%22 analog error correction],<ref>{{cite journal | title = Analog Error-Correcting Codes Based on Chaotic Dynamical Systems | id = {{citeseerx|10.1.1.30.4093}} | first1 = Brian | last1 = Chen | first2 = Gregory W. | last2 = Wornell | journal = IEEE TRANSACTIONS ON COMMUNICATIONS | volume = 46 | issue = 7 | month = July | year = 1998 }}</ref> [http://scholar.google.com/scholar?q=%22analog data compression%22 analog data compression].<ref>{{cite conference | title = On Analog Signature Analysis | id = {{citeseerx|10.1.1.142.5853}} | first1 = Franc Novak Bojan | last1 = Hvala | first2 = Sandi | last2 = Klavžar | booktitle = Proceedings of the conference on Design, automation and test in Europe | year = 1999 | isbn = 1-58113-121-6 }}</ref> [http://scholar.google.com/scholar?hl=en&q=%22analog encryption%22 analog encryption]<ref>[http://www.hooklee.com/Papers/IEEETCASI2008.pdf Cryptanalyzing an Encryption Scheme Based on Blind Source Separation], Shujun Li, Chengqing Li, Kwok-Tung Lo, Guanrong Chen, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 55, NO. 4, PAGES 1055-1063, APRIL 2008</ref>  ==Neural coding== [[Neural coding]] is a [[neuroscience]]-related field concerned with how sensory and other information is represented in the [[brain]] by [[neural network|networks]] of [[neurons]]. The main goal of studying neural coding is to characterize the relationship between the [[Stimulus (physiology)|stimulus]] and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble.<ref name="Brown">Brown EN, Kass RE, and Mitra PP. 2004. Multiple neural spike train data analysis: state-of-the-art and future challenges. ''Nature Neuroscience'' 7:456-61</ref> It is thought that neurons can encode both [[digital]] and [[analog signal|analog]] information,<ref>[http://pop.cerco.ups-tlse.fr/fr_vers/documents/thorpe_sj_90_91.pdf Spike arrival times: A highly efficient coding scheme for neural networks], SJ Thorpe - Parallel processing in neural systems, 1990</ref> and that neurons follow the principles of information theory and compress information,<ref>{{cite web | title = Information Distortion and Neural Coding | id = {{citeseerx|10.1.1.5.6365}} | first1 = Tomáš | last1 = Gedeon | first2 = Albert E. | last2 = Parker | first3 = Alexander G. | last3 = Dimitrov }}</ref> and detect and correct<ref>[http://arxiv.org/pdf/q-bio/0501021 Spike timing precision and neural error correction: Local behavior], M Stiber - Neural computation, 2005</ref> errors in the signals that are sent throughout the brain and wider nervous system.  ==See also== *[[Coding gain]] *[[Covering code]] *[[Error-correcting code]] *[[Group testing]] *[[Hamming distance]], [[Hamming weight]] *[[Information theory]] *[[Lee distance]] * Spatial coding and [[MIMO]] in [[multiple antenna research]] ** [[Space–time code|Spatial diversity coding]] is spatial coding that transmits replicas of the information signal along different spatial paths, so as to increase the reliability of the data transmission. ** [[Dirty paper coding (DPC)|Spatial interference cancellation coding]] ** [[Spatial multiplexing|Spatial multiplex coding]] *[[Timeline of information theory|Timeline of information theory, data compression, and error correcting codes]] *[[List of algebraic coding theory topics]] *[[Folded Reed–Solomon codes]] *[[ABNNR and AEL codes]]  ==Notes== {{Refimprove|date=January 2007}} {{reflist|2}}  ==References== *[[Vera Pless]] (1982), ''Introduction to the Theory of Error-Correcting Codes'', John Wiley & Sons, Inc., ISBN 0-471-08684-3. *[[Elwyn R. Berlekamp]] (1984), ''Algebraic Coding Theory'', Aegean Park Press (revised edition), ISBN 0-89412-063-8, ISBN 978-0-89412-063-3. *Randy Yates, ''[http://www.digitalsignallabs.com/tutorial.pdf A Coding Theory Tutorial]''.  {{DEFAULTSORT:Coding Theory}} [[Category:Coding theory| ]] [[Category:Error detection and correction]]  [[ar:نظرية الترميز]] [[da:Kodeteori]] [[de:Kodierungstheorie]] [[es:Teoría de códigos]] [[fa:نظریه کدگذاری]] [[fr:Théorie des codes]] [[it:Teoria dei codici]] [[he:תורת הקודים]] [[nl:Coderingstheorie]] [[ja:符号理論]] [[nn:Kodeteori]] [[pt:Teoria de códigos]] [[fi:Koodausteoria]] [[sv:Kodningsteori]] [[uk:Теорія кодування]] [[vi:Lý thuyết mã hóa]] [[zh:编码理论]]
'''Computational complexity theory''' is a branch of the [[theory of computation]] in [[theoretical computer science]] and [[mathematics]] that focuses on classifying [[computational problems]] according to their inherent difficulty, and relating those [[Complexity class|classes]] to each other. In this context, a computational problem is understood to be a task that is in principle amenable to being solved by a computer (which basically means that the problem can be stated by a set of mathematical instructions). Informally, a computational problem consists of problem instances and solutions to these problem instances. For example, [[primality testing]] is the problem of determining whether a given number is [[prime number|prime]] or not. The instances of this problem are [[natural numbers]], and the solution to an instance is ''yes'' or ''no'' based on whether the number is prime or not.  A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical [[models of computation]] to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in [[communication complexity]]), the number of [[logic gate|gates]] in a circuit (used in [[circuit complexity]]) and the number of processors (used in [[parallel computing]]). One of the roles of computational complexity theory is to determine the practical limits on what [[computer]]s can and cannot do.  Closely related fields in theoretical computer science are [[analysis of algorithms]] and [[computability theory]]. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.  ==Computational problems== [[Image:TSP Deutschland 3.png|thumb|200px|An optimal traveling salesperson tour through [[Germany]]’s 15 largest cities. It is the shortest among 43,589,145,600<ref group="nb">Take one city, and take all possible orders of the other 14 cities. Then divide by two because it does not matter in which direction in time they come after each other: 14!/2 = 43,589,145,600.</ref> possible tours visiting each city exactly once.]]  ===Problem instances===  A [[computational problem]] can be viewed as an infinite collection of ''instances'' together with a ''solution'' for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of [[primality testing]]. The instance is a number (e.g. 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case "no"). Stated another way, the ''instance'' is a particular input to the problem, and the ''solution'' is the output corresponding to the given input.  To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the [[traveling salesman problem]]: Is there a route of at most 2000 kilometres in length passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in [[Milan]] whose total length is at most 10&nbsp;km. For this reason, complexity theory addresses computational problems and not particular problem instances.  ===Representing problem instances=== When considering computational problems, a problem instance is a [[string (computer science)|string]] over an [[Alphabet (computer science)|alphabet]]. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are [[bitstring]]s. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, [[integer]]s can be represented in [[binary notation]], and [[graph (mathematics)|graphs]] can be encoded directly via their [[adjacency matrix|adjacency matrices]], or by encoding their [[adjacency list]]s in binary.  Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.  ===Decision problems as formal languages=== [[Image:Decision Problem.svg|thumb|200px|A [[decision problem]] has only two possible outputs, ''yes'' or ''no'' (or alternately 1 or 0) on any input.]] [[Decision problem]]s are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either ''yes'' or ''no'', or alternately either 1 or 0. A decision problem can be viewed as a [[formal language]], where the members of the language are instances whose answer is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an [[algorithm]], whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer ''yes'', the algorithm is said to accept the input string, otherwise it is said to reject the input.  An example of a decision problem is the following. The input is an arbitrary [[graph (mathematics)|graph]]. The problem consists in deciding whether the given graph is [[connectivity (graph theory)|connected]], or not. The formal language associated with this decision problem is then the set of all connected graphs&mdash;of course, to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.  ===Function problems=== A [[function problem]] is a computational problem where a single output (of a [[total function]]) is expected for every input, but the output is more complex than that of a [[decision problem]], that is, it isn't just yes or no. Notable examples include the [[traveling salesman problem]] and the [[integer factorization problem]].  It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the [[multiplication]] of two integers can be expressed as the set of triples (''a'',&nbsp;''b'',&nbsp;''c'') such that the relation ''a''&nbsp;×&nbsp;''b''&nbsp;=&nbsp;''c'' holds. Deciding whether a given triple is member of this set corresponds to solving the problem of multiplying two numbers.  ===Measuring the size of an instance=== To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2''n'' vertices compared to the time taken for a graph with ''n'' vertices?  If the input size is ''n'', the time taken can be expressed as a function of ''n''. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(''n'') is defined to be the maximum time taken over all inputs of size ''n''. If T(''n'') is a polynomial in ''n'', then the algorithm is said to be a [[polynomial time]] algorithm. [[Cobham's thesis]] says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.  ==Machine models and complexity measures== ===Turing Machine=== [[Image:Maquina.png|thumb|An artistic representation of a Turing machine]] {{main|Turing machine}} A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the [[Church–Turing thesis]]. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a [[RAM machine]], [[Conway's Game of Life]], [[cellular automata]] or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.  Many types of Turing machines are used to define complexity classes, such as [[deterministic Turing machine]]s, [[probabilistic Turing machine]]s, [[non-deterministic Turing machine]]s, [[quantum Turing machine]]s, [[symmetric Turing machine]]s and [[alternating Turing machine]]s. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.  A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called [[randomized algorithm]]s. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see [[nondeterministic algorithm]].  ===Other machine models=== Many machine models different from the standard [[Turing machine equivalents#Multi-tape Turing machines|multi-tape Turing machines]] have been proposed in the literature, for example [[random access machine]]s. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary.<ref>See {{harvnb|Arora|Barak|2009|loc=Chapter 1: The computational model and why it doesn't matter}}</ref> What all these models have in common is that the machines operate [[deterministic algorithm|deterministically]].  However, some computational problems are easier to analyze in terms of more unusual resources. For example, a [[nondeterministic Turing machine]] is a computational model that is allowed to branch out to check many different possibilities at once. The nondeterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that [[nondeterministic time]] is a very important resource in analyzing computational problems.  ===Complexity measures=== For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the [[deterministic Turing machine]] is used. The ''time required'' by a deterministic Turing machine ''M'' on input ''x'' is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ("yes" or "no"). A Turing machine ''M'' is said to operate within time ''f''(''n''), if the time required by ''M'' on each input of length ''n'' is at most ''f''(''n''). A decision problem ''A'' can be solved in time ''f''(''n'') if there exists a Turing machine operating in time ''f''(''n'') that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time ''f''(''n'') on a deterministic Turing machine is then denoted by [[DTIME]](''f''(''n'')).  Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any [[complexity measure]] can be viewed as a computational resource. Complexity measures are very generally defined by the [[Blum complexity axioms]]. Other complexity measures used in complexity theory include [[communication complexity]], [[circuit complexity]], and [[decision tree complexity]].  ===Best, worst and average case complexity=== [[File:Sorting quicksort anim.gif|thumb|250px|Visualization of the [[quicksort]] [[algorithm]] that has [[Best, worst and average case|average case performance]] <math>\Theta(n\log n)</math>.]] The [[best, worst and average case]] complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size ''n'' may be faster to solve than others, we define the following complexities: *Best-case complexity: This is the complexity of solving the problem for the best input of size ''n''. *Worst-case complexity: This is the complexity of solving the problem for the worst input of size ''n''. *Average-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a [[probability distribution]] over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size ''n''.  For example, consider the dc sorting algorithm [[quicksort]]. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(''n''<sup>2</sup>) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(''n'' log ''n''). The best case occurs when each pivoting divides the list in half, also needing O(''n'' log ''n'') time.  ===Upper and lower bounds on the complexity of problems=== To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the minimum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of [[analysis of algorithms]]. To show an upper bound ''T''(''n'') on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most ''T''(''n''). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase "all possible algorithms" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of ''T''(''n'') for a problem requires showing that no algorithm can have time complexity lower than ''T''(''n'').  Upper and lower bounds are usually stated using the [[big O notation]], which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if ''T''(''n'')&nbsp;=&nbsp;7''n''<sup>2</sup>&nbsp; &nbsp;15''n''&nbsp; &nbsp;40, in big O notation one would write ''T''(''n'')&nbsp;=&nbsp;O(''n''<sup>2</sup>).  ==Complexity classes== ===Defining complexity classes=== A '''complexity class''' is a set of problems of related complexity. Simpler complexity classes are defined by the following factors: * The type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on [[function problem]]s, [[counting problem (complexity)|counting problem]]s, [[optimization problem]]s, [[promise problem]]s, etc. * The model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on [[nondeterministic Turing machine]]s, [[Boolean circuit]]s, [[quantum Turing machine]]s, [[monotone circuit]]s, etc. * The resource (or resources) that are being bounded and the bounds: These two properties are usually stated together, such as "polynomial time", "logarithmic space", "constant depth", etc.  Of course, some complexity classes have complex definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:  :The set of decision problems solvable by a deterministic Turing machine within time ''f''(''n''). (This complexity class is known as DTIME(''f''(''n'')).)  But bounding the computation time above by some concrete function ''f''(''n'') often yields complexity classes that depend on the chosen machine model. For instance, the language {''xx'' | ''x'' is any binary string} can be solved in [[linear time]] on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, [[Cobham's thesis|Cobham-Edmonds thesis]] states that "the time complexities in any two reasonable and general models of computation are polynomially related" {{Harv|Goldreich|2008|loc=Chapter 1.2}}. This forms the basis for the complexity class [[P (complexity)|P]], which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is [[FP (complexity)|FP]].  ===Important complexity classes=== [[Image:Complexity subsets pspace.svg|300px|thumb|right|A representation of the relation among complexity classes]] Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:  {| class="wikitable" |-  ! Complexity class  ! Model of computation  ! Resource constraint |- | [[DTIME]](''f''(''n'')) | Deterministic Turing machine | Time ''f''(''n'') |- | [[P (complexity)|P]] | Deterministic Turing machine | Time poly(''n'') |- | [[EXPTIME]] | Deterministic Turing machine | Time 2<sup>poly(''n'')</sup> |- | [[NTIME]](''f''(''n'')) | Non-deterministic Turing machine | Time ''f''(''n'') |- | [[NP (complexity)|NP]] | Non-deterministic Turing machine | Time poly(''n'') |- | [[NEXPTIME]] | Non-deterministic Turing machine | Time 2<sup>poly(''n'')</sup> |- | [[DSPACE]](''f''(''n'')) | Deterministic Turing machine | Space ''f''(''n'') |- | [[L (complexity)|L]] | Deterministic Turing machine | Space O(log ''n'') |- | [[PSPACE]] | Deterministic Turing machine | Space poly(''n'') |- | [[EXPSPACE]] | Deterministic Turing machine | Space 2<sup>poly(''n'')</sup> |- | [[NSPACE]](''f''(''n'')) | Non-deterministic Turing machine | Space ''f''(''n'') |- | [[NL (complexity)|NL]] | Non-deterministic Turing machine | Space O(log ''n'') |- | [[NPSPACE]] | Non-deterministic Turing machine | Space poly(''n'') |- | [[NEXPSPACE]] | Non-deterministic Turing machine | Space 2<sup>poly(''n'')</sup> |}  It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by [[Savitch's theorem]].  Other important complexity classes include [[BPP (complexity)|BPP]], [[ZPP (complexity)|ZPP]] and [[RP (complexity)|RP]], which are defined using [[probabilistic Turing machine]]s; [[AC (complexity)|AC]] and [[NC (complexity)|NC]], which are defined using Boolean circuits and [[BQP]] and [[QMA]], which are defined using quantum Turing machines. [[Sharp-P|#P]] is an important complexity class of counting problems (not decision problems). Classes like [[IP (complexity)|IP]] and [[AM (complexity)|AM]] are defined using [[Interactive proof system]]s. [[ALL (complexity)|ALL]] is the class of all decision problems.  ===Hierarchy theorems=== {{main|time hierarchy theorem|space hierarchy theorem}} For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(''n'') is contained in DTIME(''n''<sup>2</sup>), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.  More precisely, the [[time hierarchy theorem]] states that :<math>\operatorname{DTIME}\big(f(n) \big) \subsetneq \operatorname{DTIME} \big(f(n) \sdot \log^{2}(f(n)) \big)</math>.  The [[space hierarchy theorem]] states that :<math>\operatorname{DSPACE}\big(f(n)\big) \subsetneq \operatorname{DSPACE} \big(f(n) \sdot \log(f(n)) \big)</math>.  The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.  ===Reduction=== {{main|Reduction (complexity)}} Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at least as difficult as another problem. For instance, if a problem ''X'' can be solved using an algorithm for ''Y'', ''X'' is no more difficult than ''Y'', and we say that ''X'' ''reduces'' to ''Y''. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as [[polynomial-time reduction]]s or [[log-space reduction]]s.  The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.  This motivates the concept of a problem being hard for a complexity class. A problem ''X'' is ''hard'' for a class of problems ''C'' if every problem in ''C'' can be reduced to ''X''. Thus no problem in ''C'' is harder than ''X'', since an algorithm for ''X'' allows us to solve any problem in ''C''. Of course, the notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of [[NP-hard]] problems.  If a problem ''X'' is in ''C'' and hard for ''C'', then ''X'' is said to be ''[[complete (complexity)|complete]]'' for ''C''. This means that ''X'' is the hardest problem in ''C''. (Since many problems could be equally hard, one might say that ''X'' is one of the hardest problems in ''C''.) Thus the class of [[NP-complete]] problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P&nbsp;=&nbsp;NP is not solved, being able to reduce a known NP-complete problem, Π<sub>2</sub>, to another problem, Π<sub>1</sub>, would indicate that there is no known polynomial-time solution for Π<sub>1</sub>. This is because a polynomial-time solution to Π<sub>1</sub> would yield a polynomial-time solution to Π<sub>2</sub>. Similarly, because all NP problems can be reduced to the set, finding an [[NP-complete]] problem that can be solved in polynomial time would mean that P&nbsp;=&nbsp;NP.<ref name="Sipser2006"/>  ==Important open problems== [[Image:Complexity classes.svg|thumb|250px|Diagram of complexity classes provided that P&nbsp;≠&nbsp;NP. The existence of problems in NP outside both P and NP-complete in this case was established by Ladner.<ref name="Ladner75">{{Citation|last=Ladner|first=Richard E.|title=On the structure of polynomial time reducibility|journal=[[Journal of the ACM]] (JACM)|volume=22|year=1975|pages=151–171|url=http://delivery.acm.org/10.1145/330000/321877/p155-ladner.pdf?key1=321877&key2=7146531911&coll=&dl=ACM&CFID=15151515&CFTOKEN=6184618|format=PDF|doi=10.1145/321864.321877|issue=1|postscript=.}}</ref>]]  ===P versus NP problem=== {{Main|P versus NP problem}}  The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the [[Cobham–Edmonds thesis]]. The complexity class [[NP (complexity)|NP]], on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the [[Boolean satisfiability problem]], the [[Hamiltonian path problem]] and the [[vertex cover problem]]. Since deterministic Turing machines are special nondeterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.  The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution.<ref name="Sipser2006">See {{harvnb|Sipser|2006|loc= Chapter 7: Time complexity}}</ref> If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of [[integer programming]] problems in [[operations research]], many problems in [[logistics]], [[protein structure prediction]] in [[biology]],<ref>{{Citation|title=Protein folding in the hydrophobic-hydrophilic (HP) model is NP-complete|last=Berger|first=Bonnie A.|journal=Journal of Computational Biology|year=1998|volume=5|number=1|pages=p27–40|pmid=9541869|doi=10.1089/cmb.1998.5.27|last2=Leighton|first2=T|issue=1|postscript=. }}</ref> and the ability to find formal proofs of [[pure mathematics]] theorems.<ref>{{Citation|last=Cook|first=Stephen|authorlink=Stephen Cook|title=The P versus NP Problem|publisher=[[Clay Mathematics Institute]]|year=2000|month=April|url=http://www.claymath.org/millennium/P_vs_NP/Official_Problem_Description.pdf|accessdate=2006-10-18|postscript=.}}</ref> The P versus NP problem is one of the [[Millennium Prize Problems]] proposed by the [[Clay Mathematics Institute]]. There is a US$1,000,000 prize for resolving the problem.<ref>{{Citation|title=The Millennium Grand Challenge in Mathematics|last=Jaffe|first=Arthur M.|authorlink=Arthur Jaffe|year=2006|journal=Notices of the AMS|volume=53|issue=6|url=http://www.ams.org/notices/200606/fea-jaffe.pdf|accessdate=2006-10-18|postscript=.}}</ref>  ===Problems in NP not known to be in P or NP-complete===  It was shown by Ladner that if '''P''' ≠ '''NP''' then there exist problems in '''NP''' that are neither in '''P''' nor '''NP-complete'''.<ref name="Ladner75" /> Such problems are called [[NP-intermediate]] problems. The [[graph isomorphism problem]], the [[discrete logarithm problem]] and the [[integer factorization problem]] are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in '''P''' or to be '''NP-complete'''.  The [[graph isomorphism problem]] is the computational problem of determining whether two finite [[Graph (mathematics)|graph]]s are [[graph isomorphism|isomorphic]]. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in '''P''', '''NP-complete''', or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete.<ref name="AK06">{{Citation  | first1 = Vikraman  | last1 = Arvind  | first2 = Piyush P.  | last2 = Kurur  | title = Graph isomorphism is in SPP  | journal = Information and Computation  | volume = 204  | issue = 5  | year = 2006  | pages = 835–852  | doi = 10.1016/j.ic.2006.02.002  | postscript = .}}</ref> If graph isomorphism is NP-complete, the [[polynomial time hierarchy]] collapses to its second level.<ref>[[Uwe Schöning]], "Graph isomorphism is in the low hierarchy", Proceedings of the 4th Annual [[Symposium on Theoretical Aspects of Computer Science]], 1987, 114–124; also: ''Journal of Computer and System Sciences'', vol. 37 (1988), 312–323</ref> Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to [[Laszlo Babai]] and [[Eugene Luks]] has run time 2<sup>O(√(''n'' log(''n'')))</sup> for graphs with ''n'' vertices.  The [[integer factorization problem]] is the computational problem of determining the [[prime factorization]] of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than ''k''. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the [[RSA (algorithm)|RSA]] algorithm. The integer factorization problem is in '''NP''' and in '''co-NP''' (and even in UP and co-UP<ref>[[Lance Fortnow]]. Computational Complexity Blog: Complexity Class of the Week: Factoring. September 13, 2002. http://weblog.fortnow.com/2002/09/complexity-class-of-week-factoring.html</ref>). If the problem is '''NP-complete''', the polynomial time hierarchy will collapse to its first level (i.e., '''NP''' will equal '''co-NP'''). The best known algorithm for integer factorization is the [[general number field sieve]], which takes time O(e<sup>(64/9)<sup>1/3</sup>(''n''.log 2)<sup>1/3</sup>(log (''n''.log 2))<sup>2/3</sup></sup>) to factor an ''n''-bit integer. However, the best known [[quantum algorithm]] for this problem, [[Shor's algorithm]], does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.  ===Separations between other complexity classes===  Many known complexity classes are suspected to be unequal, but this has not been proved. For instance '''P''' ⊆ '''NP''' ⊆ [[PP (complexity)|'''PP''']] ⊆ '''PSPACE''', but it is possible that '''P''' = '''PSPACE'''. If '''P''' is not equal to '''NP''', then '''P''' is not equal to '''PSPACE''' either. Since there are many known complexity classes between '''P''' and '''PSPACE''', such as '''RP''', '''BPP''', '''PP''', '''BQP''', '''MA''', '''PH''', etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.  Along the same lines, '''[[co-NP]]''' is the class containing the [[Complement (complexity)|complement]] problems (i.e. problems with the ''yes''/''no'' answers reversed) of '''NP''' problems. It is believed<ref>[http://www.cs.princeton.edu/courses/archive/spr06/cos522/ Boaz Barak's course on Computational Complexity] [http://www.cs.princeton.edu/courses/archive/spr06/cos522/lec2.pdf Lecture 2]</ref> that '''NP''' is not equal to '''co-NP'''; however, it has not yet been proven. It has been shown that if these two complexity classes are not equal then '''P''' is not equal to '''NP'''.  Similarly, it is not known if '''L''' (the set of all problems that can be solved in logarithmic space) is strictly contained in '''P''' or equal to '''P'''. Again, there are many complexity classes between the two, such as '''NL''' and '''NC''', and it is not known if they are distinct or equal classes.  It is suspected that '''P''' and '''BPP''' are equal. However, it is currently open if '''BPP''' = '''NEXP'''.  ==Intractability== <!-- This section is linked from [[Minimax]], [[Intractability]], [[Intractable]] --> {{See also|Combinatorial explosion}} Problems that can be solved in theory (e.g., given infinite time), but which in practice take too long for their solutions to be useful, are known as ''intractable'' problems.<ref>Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2007) [[Introduction to Automata Theory, Languages, and Computation]], Addison Wesley, Boston/San Francisco/New York (page 368)</ref> In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs. In fact, the [[Cobham–Edmonds thesis]] states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device. Problems that are known to be intractable in this sense include those that are [[EXPTIME]]-hard. If NP is not the same as P, then the NP-complete problems are also intractable in this sense. To see why exponential-time algorithms might be unusable in practice, consider a program that makes 2<sup>''n''</sup> operations before halting. For small ''n'', say 100, and assuming for the sake of example that the computer does 10<sup>12</sup> operations each second, the program would run for about 4&nbsp;×&nbsp;10<sup>10</sup> years, which is roughly the [[age of the universe]]. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. Nevertheless a polynomial time algorithm is not always practical. If its running time is, say, ''n''<sup>15</sup>, it is unreasonable to consider it efficient and it is still useless except on small instances.  What intractability means in practice is open to debate. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example the decision problem in [[Presburger arithmetic]] has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete [[knapsack problem]] over a wide range of sizes in less than quadratic time and [[SAT solver]]s routinely handle large instances of the NP-complete [[Boolean satisfiability problem]].  ==Continuous complexity theory==  Continuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations, as studied in [[numerical analysis]]. One approach to complexity theory of numerical analysis<ref>{{cite journal | title = Complexity Theory and Numerical Analysis | id = {{citeseerx|10.1.1.33.4678}} | first = Steve | last = Smale | journal = Acta Numerica | year = 1997 | publisher = Cambridge Univ Press }}</ref> is [[information based complexity]].  Continuous complexity theory can also refer to complexity theory of the use of [[analog computation]], which uses continuous [[dynamical system]]s and [[differential equation]]s.<ref>[http://arxiv.org/abs/arxiv:0907.3117 A Survey on Continuous Time Computations], Olivier Bournez, Manuel Campagnolo, New Computational Paradigms. Changing Conceptions of What is Computable. (Cooper, S.B. and L{\"o}we, B. and Sorbi, A., Eds.). New York, Springer-Verlag, pages 383-423. 2008</ref> [[Control theory]] can be considered a form of computation and differential equations are used in the modelling of continuous-time and hybrid discrete-continuous-time systems.<ref>{{cite journal | title = Computational Techniques for the Verification of Hybrid Systems | id = {{citeseerx|10.1.1.70.4296}} | first1 = Claire J. | last1 = Tomlin | first2 = Ian | last2 = Mitchell | first3 = Alexandre M. | last3 = Bayen | first4 = Meeko | last4 = Oishi | journal = Proceedings of the IEEE | volume = 91 | issue = 7 | month = July | year = 2003 }}</ref>  ==History==  Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by [[Alan Turing]] in 1936, which turned out to be a very robust and flexible notion of computer.  {{Harvtxt|Fortnow|Homer|2003}} date the beginning of systematic studies in computational complexity to the seminal paper "On the Computational Complexity of Algorithms" by Juris Hartmanis and Richard Stearns (1965), which laid out the definitions of time and space complexity and proved the hierarchy theorems.  According to {{Harvtxt|Fortnow|Homer|2003}}, earlier papers studying problems solvable by Turing machines with specific bounded resources include [[John Myhill]]'s definition of [[linear bounded automata]] (Myhill 1960), [[Raymond Smullyan]]'s study of rudimentary sets (1961), as well as [[Hisao Yamada]]'s paper<ref>{{cite doi|10.1109/TEC.1962.5219459}}</ref> on real-time computations (1962). Somewhat earlier, [[Boris Trakhtenbrot]] (1956), a pioneer in the field from the USSR, studied another specific complexity measure.<ref>Trakhtenbrot, B.A.: Signalizing functions and tabular operators. Uchionnye Zapiski Penzenskogo Pedinstituta (Transactions of the Penza Pedagogoical Institute) 4, 75–87 (1956) (in Russian)</ref> As he remembers: {{quote|However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term "signalizing function", which is nowadays commonly known as "complexity measure". |Boris Trakhtenbrot|From Logic to Theoretical Computer Science – An Update. In: ''Pillars of Computer Science'', LNCS 4800, Springer 2008. }} In 1967, [[Manuel Blum]] developed an axiomatic complexity theory based on his [[Blum axioms|axioms]] and proved an important result, the so called, [[Blum's speedup theorem|speed-up theorem]]. The field really began to flourish when the US researcher [[Stephen Cook]] and, working independently, [[Leonid Levin]] in the USSR, proved that there exist practically relevant problems that are [[NP-complete]]. In 1972, [[Richard Karp]] took this idea a leap forward with his landmark paper, "Reducibility Among Combinatorial Problems", in which he showed that 21 diverse [[combinatorics|combinatorial]] and [[graph theory|graph theoretical]] problems, each infamous for its computational intractability, are NP-complete.<ref>{{Citation | author = Richard M. Karp | chapter = Reducibility Among Combinatorial Problems | chapter-url = http://www.cs.berkeley.edu/~luca/cs172/karp.pdf | title = Complexity of Computer Computations | editor = R. E. Miller and J. W. Thatcher (editors) | publisher = New York: Plenum | pages = 85–103 | year = 1972}}</ref>  ==See also== [[Image:Theoretical computer science.svg|thumb|700px|Relationship between [[computability theory]], complexity theory and [[formal language theory]].]] *[[List of computability and complexity topics]] *[[List of important publications in theoretical computer science]] *[[Unsolved problems in computer science]] *[[:Category:Computational problems]] *[[List of complexity classes]] *[[Structural complexity theory]] *[[Descriptive complexity theory]] *[[Quantum complexity theory]] *[[Context of computational complexity]] *[[Parameterized complexity|Parameterized Complexity]] *[[Game complexity]] *[[Proof complexity]] *[[Transcomputational problem]]  ==Notes== <references group="nb"/>  ==References== {{refbegin|2}} <references />  ===Textbooks=== *{{Citation | last1=Arora | first1=Sanjeev | authorlink1=Sanjeev Arora | last2=Barak | first2=Boaz | title=Computational Complexity: A Modern Approach | url = http://www.cs.princeton.edu/theory/complexity/ | publisher=[[Cambridge University Press|Cambridge]] | year=2009 | isbn=978-0-521-42426-4 | author=Sanjeev Arora, Boaz Barak }} * {{Citation | last1=Downey | first1=Rod | last2=Fellows | first2=Michael | authorlink2=Michael Fellows | title=Parameterized complexity | url=http://www.springer.com/sgw/cda/frontpage/0,11855,5-0-22-1519914-0,00.html?referer=www.springer.de%2Fcgi-bin%2Fsearch_book.pl%3Fisbn%3D0-387-94883-X | publisher=[[Springer-Verlag]] | location=Berlin, New York | year=1999 }} * {{citation | last=Du | first=Ding-Zhu | coauthors=Ko, Ker-I | title=Theory of Computational Complexity | publisher=[[John Wiley & Sons]] | year=2000 | country=US | isbn=978-0-471-34506-0 }} * {{Citation | last=Goldreich | first=Oded | authorlink=Oded Goldreich | url = http://www.wisdom.weizmann.ac.il/~oded/cc-book.html | title = Computational Complexity: A Conceptual Perspective | publisher = Cambridge University Press | year = 2008 }} * {{Citation | editor1-last=van Leeuwen | editor1-first=Jan | editor1-link = Jan van Leeuwen | title=Handbook of theoretical computer science (vol. A): algorithms and complexity | publisher=[[MIT Press]] | isbn=978-0-444-88071-0 | year=1990 }} * {{citation  | last = Papadimitriou  | first = Christos  | authorlink = Christos Papadimitriou  | title = Computational Complexity  | edition = 1st  | year = 1994  | publisher = Addison Wesley  | isbn = 0-201-53082-1 }} * {{Citation |last=Sipser |first=Michael |authorlink=Michael Sipser |title=[[Introduction to the Theory of Computation]] |edition=2nd |year=2006 |publisher=[[Thomson Learning|Thomson Course Technology]] |location=USA |isbn=0-534-95097-3 }} * {{Garey-Johnson}}  ===Surveys=== * {{Citation | last1=Khalil | first1=Hatem | last2=Ulery | first2=Dana | author2-link=Dana Ulery | title=A Review of Current Studies on Complexity of Algorithms for Partial Differential Equations | publisher=ACM '76 Proceedings of the 1976 Annual Conference | year=1976 | pages=197 | url = http://portal.acm.org/citation.cfm?id=800191.805573 | doi=10.1145/800191.805573}} * {{Citation | last1=Cook | first1=Stephen | author1-link=Stephen Cook | title=An overview of computational complexity | publisher=ACM | year=1983 | journal=Commun. ACM | issn=0001-0782 | volume=26 | issue=6 | pages=400–408 | doi=10.1145/358141.358144}} * {{Citation | last1=Fortnow | first1=Lance | last2=Homer | first2=Steven | title=A Short History of Computational Complexity | year=2003 | journal=Bulletin of the EATCS | volume=80 | pages=95–133 | url = http://people.cs.uchicago.edu/~fortnow/papers/history.pdf}} * {{Citation | last1=Mertens | first1=Stephan | title=Computational Complexity for Physicists | publisher=IEEE Educational Activities Department | location=Piscataway, NJ, USA | year=2002 | journal=Computing in Science and Engg. | issn=1521-9615 | volume=4 | issue=3 | pages=31–47 | doi=10.1109/5992.998639 | arxiv=cond-mat/0012185 | unused_data=arXiv}}  {{refend}}  ==External links== * [http://qwiki.stanford.edu/wiki/Complexity_Zoo The Complexity Zoo]  {{ComplexityClasses}}  {{DEFAULTSORT:Computational Complexity Theory}} [[Category:Computational complexity theory| ]]  {{Link FA|de}} [[ar:نظرية التعقيد الحسابي]] [[bn:গণনামূলক জটিলতা তত্ত্ব]] [[bg:Теория на изчислителната сложност]] [[ca:Complexitat computacional]] [[cs:Teorie složitosti]] [[de:Komplexitätstheorie]] [[et:Algoritmiline keerukus]] [[el:Θεωρία πολυπλοκότητας]] [[es:Teoría de la complejidad computacional]] [[fa:نظریه پیچیدگی محاسباتی]] [[fr:Théorie de la complexité des algorithmes]] [[ko:계산 복잡도 이론]] [[hr:Računska teorija složenosti]] [[it:Teoria della complessità computazionale]] [[he:סיבוכיות]] [[lt:Algoritmų sudėtingumas]] [[ms:Teori kekompleksan pengiraan]] [[nl:Computationele complexiteitstheorie]] [[ja:計算複雑性理論]] [[no:Kompleksitetsteori]] [[pl:Złożoność obliczeniowa]] [[pt:Teoria da complexidade]] [[ro:Teoria complexității]] [[ru:Вычислительная сложность]] [[simple:Computational complexity theory]] [[sk:Teória zložitosti]] [[sr:Теорија комплексности]] [[sh:Računska teorija složenosti]] [[fi:Aikavaativuusluokka]] [[sv:Komplexitetsteori]] [[th:ทฤษฎีความซับซ้อนในการคำนวณ]] [[uk:Теорія складності обчислень]] [[vi:Lý thuyết độ phức tạp tính toán]] [[zh:計算複雜性理論]]
{{distinguish|computer science}} {{Science}} '''Computational science''' (or '''scientific computing''') is the subfield of [[computer science]]{{Citation needed|date=March 2012}} concerned with constructing [[scientific modeling|mathematical models]] and [[numerical analysis|quantitative analysis]] techniques and using computers to analyze and solve [[scientific]] problems.<ref>[http://www.nccs.gov/about/ National Center for Computational Science]</ref> In practical use, it is typically the application of [[computer simulation]] and other forms of [[computation]] from theoretical computer science to problems in various scientific disciplines.  The field is a branch{{Citation needed|date=March 2012}} of [[computer science]] (the study of [[computation]], [[computer]]s and [[information processing]]) but is different from theory and experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on [[computer]]s.  Scientists and engineers develop [[computer programs]], [[application software]], that model systems being studied and run these programs with various sets of input parameters. Typically, these models require massive amounts of calculations (usually [[floating-point]]) and are often executed on [[supercomputer]]s or [[distributed computing]] platforms.  [[Numerical analysis]] is an important underpinning for techniques used in computational science.  ==Applications of computational science== Problem domains for computational science/scientific computing include:  ===Numerical simulations=== Numerical simulations have different objectives depending on the nature of the task being simulated: * Reconstruct and understand known events (e.g., earthquake, tsunamis and other natural disasters). * Predict future or unobserved situations (e.g., weather, sub-atomic particle behaviour).  ===Model fitting and data analysis=== * Appropriately tune models or solve equations to reflect observations, subject to model constraints (e.g. oil exploration geophysics, computational linguistics). * Use [[graph theory]] to model networks, such as those connecting individuals, organizations, websites, and biological systems.  ===Computational optimization=== {{main|Mathematical optimization}} * Optimize known scenarios (e.g., technical and manufacturing processes, front-end engineering).  ==Methods and algorithms== Algorithms and mathematical methods used in computational science are varied. Commonly applied methods include:  {{div col|colwidth=30em}} * [[Numerical analysis]] * Application of [[Taylor series]] as convergent and asymptotic series * [[Computing]] derivatives by [[Automatic differentiation]] (AD) * [[Computing]] derivatives by [[finite differences]] * Graph theoretic suites * High order difference approximations via [[Taylor series]] and [[Richardson extrapolation]] * [[Methods of integration]] on a uniform [[Mesh (mathematics)|mesh]]: [[rectangle rule]] (also called ''midpoint rule''), [[trapezoid rule]], [[Simpson's rule]] * [[Runge-Kutta methods|Runge Kutta method]] for solving ordinary differential equations * [[Monte Carlo method]]s * [[Molecular dynamics]] * [[Linear programming]] * [[Branch and cut]] * [[Branch and Bound]] * [[Numerical linear algebra]] * [[Computing]] the [[LU decomposition|LU]] factors by [[Gaussian elimination]] * [[Cholesky decomposition|Cholesky factorizations]] * [[Discrete Fourier transform]] and applications. * [[Newton's method]] * [[Time stepping]] methods for dynamical systems {{div col end}}  Programming languages and [[computer algebra systems]] commonly used for the more mathematical aspects of scientific computing applications include [[R (programming language)]], [[MATLAB]], [[Mathematica]],<ref>[http://www.scientific-computing.com/products/review_details.php?review_id=17 Mathematica 6] Scientific Computing World, May 2007</ref> [[SciLab]], [[GNU Octave]], [[Python (programming language)]] with [[SciPy]], and [[Perl Data Language|PDL]].{{Citation needed|date=December 2008}} The more computationally intensive aspects of scientific computing will often utilize some variation of [[C (programming language)|C]] or [[Fortran]] and optimized algebra libraries such as [[BLAS]] or [[LAPACK]].  Computational science application programs often model real-world changing conditions, such as weather, air flow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving equations that describe how the system operates; and then repeat the process to calculate the next state.  The term [[computational scientist]] is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computers in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering. Scientific computing has increasingly also impacted on other areas including economics, biology and medicine.  Computational science is now commonly considered a third mode of [[science]], complementing and adding to [[experimentation]]/[[observation]] and [[theory]].<ref>[http://www.siam.org/students/resources/report.php Siam.org]</ref> The essence of computational science is numerical algorithm<ref>Nonweiler T. R., 1986. Computational Mathematics: An Introduction to Numerical Approximation, John Wiley and Sons</ref> and/or [[computational mathematics]].<ref>Yang X. S., 2008. Introduction to Computational Mathematics, World Scientific Publishing</ref> In fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop, 2004.<ref>Steeb W.-H., Hardy Y., Hardy A. and Stoop R., 2004. Problems and Solutions in Scientific Computing with C   and Java Simulations, World Scientific Publishing. ISBN 981-256-112-9</ref>  ==Reproducibility and open research computing==  The complexity of computational methods is a thread to the [[reproducibility]] of research. [[Jon Claerbout]] has become prominent for pointing out that ''reproducible research'' requires archiving and documenting all raw data and all code used to obtain a result.<ref>Sergey Fomel and [[Jon Claerbout]], "[http://www.rrplanet.com/reproducible-research-librum/viewtopic.php?f=30&t=372 Guest Editors' Introduction: Reproducible Research]," Computing in Science and Engineering, vol. 11, no. 1, pp. 5–7, Jan./Feb. 2009, {{doi|10.1109/MCSE.2009.14}}</ref><ref>J. B. Buckheit and [[David Donoho|D. L. Donoho]], "[http://www.rrplanet.com/reproducible-research-librum/viewtopic.php?f=30&t=53 WaveLab and Reproducible Research]," Dept. of Statistics, Stanford University, Tech. Rep. 474, 1995.</ref><ref>The Yale Law School Round Table on Data and Core Sharing: "[http://www.computer.org/portal/web/csdl/doi/10.1109/MCSE.2010.113 Reproducible Research]", Computing in Science and Engineering, vol. 12, no. 5, pp. 8–12, Sept/Oct 2010, {{doi|10.1109/MCSE.2010.113}}</ref> Nick Barnes, in the ''Science Code Manifesto'', proposed five principles that should be followed when software is used in open science publication.<ref>http://sciencecodemanifesto.org/</ref> Tomi Kauppinen et al. established and defined ''Linked Open Science'', an approach to interconnect scientific assets to enable transparent, reproducible and transdisciplinary research.<ref name="kauppinen">{{cite doi|10.1016/j.procs.2011.04.076}}</ref>  ==Journals==  Most scientific journals do not accept software papers because a description of a reasonably mature software usually does not meet the criterion of ''novelty''. Outside computer science itself, there are only few journals dedicated to scientific software. Established journals like [[Elsevier]]'s [[Computer Physics Communications]] publish papers that are not open-access (though the described software usually is). To fill this gap, a new journal entitled ''Open research computation'' was announced in 2010;<ref>http://cameronneylon.net/blog/open-research-computation-an-ordinary-journal-with-extraordinary-aims/</ref> it closed in 2012 without having published a single paper, for a lack of submissions probably due to excessive quality requirements.<ref>http://gael-varoquaux.info/blog/?p=166</ref> A new initiative was launched in 2012, the ''Journal of Open Research Software.''<ref>http://openresearchsoftware.metajnl.com/; announced at  software.ac.uk/blog/2012-03-23-announcing-journal-open-research-software-software-metajournal</ref>  ==Education== Scientific computation is most often studied through an [[applied mathematics]] or [[computer science]] program, or within a standard mathematics, sciences, or engineering program. At some institutions a specialization in scientific computation can be earned as a "minor" within another program (which may be at varying levels). However, there are increasingly many [[bachelor's degree|bachelor's]] and master's programs in computational science. Some schools also offer the Ph.D. in computational science, [[computational engineering]], computational science and engineering, or scientific computation.  There are also programs in areas such as [[computational physics]], [[computational chemistry]], etc.  ==Related fields== {{div col|colwidth=30em}} * [[Bioinformatics]] * [[Cheminformatics]] * [[Chemometrics]] * [[Computational biology]] * [[Computational chemistry]] * [[Computational economics]] * [[Computational electromagnetics]] * [[Computational engineering]] * [[Computational finance]] * [[Computational fluid dynamics]] * [[Computational forensics]] * [[Computational geophysics]] * [[Computational intelligence]] * [[Computational linguistics]] * [[Computational mathematics]] * [[Computational mechanics]] * [[Computational neuroscience]] * [[Computational particle physics]] * [[Computational physics]] * [[Computational statistics]] * [[Computer algebra]] * [[Environmental simulation]] * [[Financial modeling]] * [[Geographic information system]] (GIS) * [[High performance computing]] * [[Machine learning]] * [[Network analysis]] * [[Neuroinformatics]] * [[Numerical linear algebra]] * [[Numerical weather prediction]] * [[Pattern recognition]] {{div col end}}  ==See also== {{portalbox|Science|Computing|Mathematics}} * [[Comparison of computer algebra systems]] * [[List of software for molecular mechanics modeling|List of molecular modeling software]] * [[List of numerical analysis software]] * [[List of statistical packages]] * [[Timeline of scientific computing]] * [[Simulated reality]]  ==References== {{reflist}}  ==Additional sources== * G. Hager and G. Wellein, Introduction to High Performance Computing for Scientists and Engineers, [[Chapman and Hall]] (2010) * A.K. Hartmann, [http://www.worldscibooks.com/physics/6988.html Practical Guide to Computer Simulations], [[World Scientific]] (2009) * Journal [http://www.man.poznan.pl/cmst/ Computational Methods in Science and Technology] (open access), [[Polish Academy of Sciences]] * Journal [http://iopscience.iop.org/1749-4699/ Computational Science and Discovery], [[Institute of Physics]] * R.H. Landau, C.C. Bordeianu, and M. Jose Paez, A Survey of Computational Physics: Introductory Computational Science, [[Princeton University Press]] (2008)  ==External links== * [http://www2.fz-juelich.de/nic/ John von Neumann-Institut for Computing (NIC) at Juelich (Germany)] * [http://www.nccs.gov The National Center for Computational Science at Oak Ridge National Laboratory] * [http://www.capital.edu/21424/Computational-Studies/7111/ Educational Materials for Undergraduate Computational Studies] * [http://www.deixismagazine.org/ Computational Science at the National Laboratories]  {{DEFAULTSORT:Computational Science}} [[Category:Computational science| ]] [[Category:Applied mathematics]]  [[ar:حوسبة علمية]] [[de:Wissenschaftliches Rechnen]] [[es:Computación Científica]] [[fa:علم محاسبه]] [[ko:계산과학]] [[id:Ilmu komputasi]] [[it:Scienza computazionale]] [[mn:Тооцон бодох шинжлэх ухаан]] [[ja:計算科学]] [[pt:Computação científica]] [[fi:Laskennallinen tiede]] [[sv:Beräkningsvetenskap]] [[ta:அறிவியல் கணிமை]] [[zh:计算科学]]
[[File:Plasticity.jpg|thumb|Nonlinear static analysis of a 3D structure subjected to plastic deformations]]  '''Computer-aided engineering''' ('''CAE''') is the broad usage of [[computer software]] to aid in [[engineering]] tasks.<ref>{{Citation | last = Laplante | first = Phillip A. | title = Comprehensive dictionary of electrical engineering | page = 136 | publisher = CRC Press | year = 2005 | edition = 2nd | url = http://books.google.com/books?id=_UBzZ4coYMkC&pg=PA136 | isbn = 978-0-8493-3086-5 | postscript =.}}</ref><ref>{{Citation | last = Kreith | first = Frank | title = The CRC handbook of mechanical engineering | page = 15-1<!-- Not a range. --> | publisher = CRC Press | year = 1998 | volume =  | edition =  | url = http://books.google.com/books?id=OpD226SXKisC&pg=PT1972 | isbn = 978-0-8493-9418-8 | postscript =.}}</ref><ref>According to Daintith it does not usually include software engineering.</ref> It includes {{nowrap|[[computer-aided design]] (CAD)}}, {{nowrap|[[computer-aided analysis]] (CAA)}}, {{nowrap|[[computer-integrated manufacturing]] (CIM)}}, {{nowrap|[[computer-aided manufacturing]] (CAM)}}, {{nowrap|[[material requirements planning]] (MRP)}}, and {{nowrap|[[computer-aided planning]] (CAP)}}.<ref>{{Citation | last = Meguid | first = S. A. | title = Integrated computer-aided design of mechanical systems | page = 7 | publisher = Springer | year = 1987 | url = http://books.google.com/books?id=aRNlWPJzsTMC&pg=PA7 | isbn = 978-1-85166-021-6 | postscript =.}}</ref><ref>{{Citation | last = Matthews | first = Clifford | title = Aeronautical engineer's data book | page = 229 | publisher = Butterworth-Heinemann | year = 2005 | edition = 2nd | url = http://books.google.com/books?id=5W9Rqq3qP1QC&pg=PA229 | isbn = 978-0-7506-5125-7 | postscript =.}}</ref><ref>{{Citation | last = Daintith | first = John | title = A dictionary of computing | page = 102 | publisher = Oxford University Press | year = 2004 | edition = 5 | url = http://books.google.com/books?id=Hay6vTsGFAsC&pg=PA102 | isbn = 978-0-19-860877-6 | postscript =.}}</ref>  == Overview == [[Software]] tools that have been developed to support these activities are considered CAE tools. CAE tools are being used, for example, to analyze the robustness and performance of components and assemblies. The term encompasses simulation, [[verification and validation|validation]], and [[Process optimization|optimization]] of products and manufacturing tools. In the future, CAE systems will be major providers of information to help support design teams in decision making.  In regard to [[information network]]s, CAE systems are individually considered a single [[node (networking)|node]] on a total information network and each node may interact with other nodes on the network.  CAE systems can provide support to businesses. This is achieved by the use of reference architectures and their ability to place information views on the business process. Reference architecture is the basis from which information model, especially product and manufacturing models.  The term CAE has also been used by some in the past to describe the use of computer technology within engineering in a broader sense than just engineering analysis. It was in this context that the term was coined by  [[Jason Lemon]], founder of [[SDRC]] in the late 1970s. This definition is however better known today by the terms [[CAx]] and [[Product Lifecycle Management|PLM]].{{citation needed |date= August 2011}}  == CAE fields and phases == CAE areas covered include: *[[Stress analysis]] on components and assemblies using FEA ([[Finite Element Analysis]]); *Thermal and fluid flow analysis [[Computational fluid dynamics]] (CFD); *[[Multibody dynamics]](MBD) & [[Kinematics]]; *Analysis tools for process simulation for operations such as [[casting]], [[molding (process)|molding]], and die press forming. *[[Multidisciplinary design optimization|Optimization]] of the product or process. *Safety analysis of postulate [[loss-of-coolant accident]] in [[nuclear reactor]] using realistic thermal-hydraulics code.   In general, there are three phases in any computer-aided engineering task: *Pre-processing &ndash; defining the model and environmental factors to be applied to it. (typically a finite element model, but facet, [[voxel]] and thin sheet methods are also used) *Analysis solver (usually performed on high powered computers) *Post-processing of results (using visualization tools) This cycle is iterated, often many times, either manually or with the use of [[Multidisciplinary_design_optimization#Commercial_MDO_Tools|commercial optimization software]].  ==CAE in the automotive industry== CAE tools are very widely used in the [[automotive industry]]. In fact, their use has enabled the automakers to reduce product development cost and time while improving the safety, comfort, and durability of the vehicles they produce. The predictive capability of CAE tools has progressed to the point where much of the design verification is now done using computer simulations rather than physical [[prototype]] testing. CAE dependability is based upon all proper assumptions as inputs and must identify critical inputs (BJ). Even though there have been many advances in CAE, and it is widely used in the engineering field, physical testing is still used as a final confirmation for subsystems due to the fact that CAE cannot predict all variables in complex assemblies (i.e. metal stretch, thinning).  ==See also== * [[Computer representation of surfaces]] * [[Electronic design automation]] EDA * [[Finite element analysis]] (FEA/FEM) * [[Multibody dynamics]] (MBD) * [[Applied element analysis]] (AEA/AEM) * [[Multidisciplinary design optimization]] * [[Comparison of CAD editors for CAE]]  == References == {{reflist}} {{Refimprove|date=February 2009}}  == Further reading == * B. Raphael and I.F.C. Smith (2003).'' Fundamentals of computer aided engineering.'' John Wiley. ISBN 978-0-471-48715-9.  == External links == {{Commons category|Computer Aided Engineering (CAE)}} * [http://caejournal.com Computer Aided Engineering Journal] (FEA, CAD, ...) * [http://www.iospress.nl/journal/integrated-computer-aided-engineering Integrated Computer Aided Engineering Journal] * [http://www.eng.fea.ru/ANSYS_LSDYNA_AviGallery.html CAE AVI-gallery at CompMechLab site, Russia] * [http://www.wiley.com/bw/journal.asp?ref=1093-9687 Computer-Aided Civil and Infrastructure Engineering]  {{Metalworking navbox|machopen}} {{Technology}}  [[Category:Computer-aided engineering| ]] [[Category:Product lifecycle management]]  [[ar:هندسة بمساعدة الحاسوب]] [[ca:Enginyeria assistida per ordinador]] [[de:Rechnergestützte Entwicklung]] [[et:Raal-tehnoanalüüs]] [[es:Ingeniería asistida por computadora]] [[fr:Ingénierie assistée par ordinateur]] [[ko:컴퓨터 이용 공학]] [[it:Computer-Aided Engineering]] [[ja:CAE]] [[pl:Computer Aided Engineering]] [[pt:Engenharia assistida por computador]] [[ru:Computer-aided engineering]] [[sk:Inžinierstvo pomocou počítača]] [[zh:CAE]]
{{merge to|Computer memory|date=May 2012}} {{Refimprove|date=June 2011}} [[File:DDR2 ram mounted.jpg|thumb|1 [[Gigabyte|GB]] of [[Synchronous dynamic random access memory|SDRAM]] mounted in a [[personal computer]]. An example of ''primary'' storage.]] [[File:Seagate Hard Disk.jpg|thumb|40 [[Gigabyte|GB]] [[Parallel ATA|PATA]] hard disk drive (HDD); when connected to a computer it serves as ''secondary'' storage.]] [[File:Super DLTtape I.jpg|thumb|160 GB [[Digital Linear Tape|SDLT]] tape cartridge, an example of ''off-line'' storage. When used within a robotic [[tape library]], it is classified as ''tertiary'' storage instead.]]  '''Computer data storage''', often called '''storage''' or '''memory''', is a technology consisting of [[computer]] components and [[Data storage device|recording media]] used to retain digital [[data (computing)|data]]. It is a core function and fundamental component of computers.  In contemporary usage, ''memory'' is usually [[semiconductor]] storage read-write [[random-access memory]], typically [[DRAM]] (Dynamic-RAM) or other forms of fast but temporary storage. ''Storage'' consists of storage devices and their media not directly accessible by the [[CPU]],  ([[Secondary storage|secondary]] or [[tertiary storage]]), typically [[hard disk drive]]s, [[optical disc]] drives, and other devices slower than RAM but are [[non-volatile memory|non-volatile]] (retaining contents when powered down).<ref>''Storage'' as defined in Microsoft Computing Dictionary, 4th Ed. (c)1999 or in The Authoritative Dictionary of IEEE Standard Terms, 7th Ed., (c) 2000.</ref> Historically, ''memory'' has been called ''core'', ''main memory'', ''real storage'' or ''internal memory'' while storage devices have been referred to as ''secondary storage'', ''external memory'' or ''auxiliary/peripheral storage''.  The distinctions are fundamental to the architecture of computers. The distinctions also reflect an  important and significant technical difference between memory and mass storage devices, which has been blurred by the historical usage of the term ''storage''. Nevertheless, this article uses the traditional nomenclature.  Many different forms of storage, based on various natural phenomena, have been invented. So far, no practical universal storage medium exists, and all forms of storage have some drawbacks. Therefore a computer system usually contains several kinds of storage, each with an individual purpose.  A modern [[Computer|digital computer]] represents [[data]] using the [[binary numeral system]]. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of [[bit]]s, or binary digits, each of which has a value of 1 or 0. The most common unit of storage is the [[byte]], equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate ''the binary representation of the piece of information'', or simply [[data (computing)|data]]. For example, the [[complete works of Shakespeare]], about 1250 pages in print, can be stored in about five [[megabyte]]s (forty million bits) with one byte per character.  The defining component of a computer is the [[central processing unit]] (CPU, or simply processor), because it operates on data, performs computations, and controls other components. In [[Von Neumann architecture|the most commonly used computer architecture]], the CPU consists of two main parts: [[Control Unit]] and [[Arithmetic Logic Unit]] (ALU). The former controls the flow of data between the CPU and memory; the latter performs arithmetic and logical operations on data.  Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk [[calculator]]s, [[digital signal processing|digital signal processors]], and other specialised devices. [[von Neumann architecture|Von Neumann]] machines differ in having a memory in which they store their operating [[instruction (computer science)|instruction]]s and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be [[computer programming|reprogrammed]] with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep [[program state|state]] between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.  In practice, almost all computers use a variety of memory types, organized in a [[memory hierarchy|storage hierarchy]] around the CPU, as a trade-off between performance and cost. Generally, the lower a storage is in the hierarchy, the lesser its [[Bandwidth (computing)|bandwidth]] and the greater its access [[latency (engineering)|latency]] is from the CPU. This traditional division of storage to primary, secondary, tertiary and off-line storage is also guided by cost per bit.  == Hierarchy of storage == [[File:Computer storage types.svg|thumb|right|350px|Various forms of storage, divided according to their distance from the [[central processing unit]]. The fundamental components of a general-purpose computer are [[arithmetic logic unit|arithmetic and logic unit]], [[control unit|control circuitry]], storage space, and [[input/output]] devices. Technology and capacity as in common [[home computer]]s around 2005.]] {{see also|Memory hierarchy}}  === Primary storage === <!-- Note that additional DIRECT links point to this section by its name. --> :''Direct links to this section: Primary storage, Main memory, Internal Memory.''  <!-- A *lot* of people will need a direct to link to the subject, and mostly they don't know about # magic. Inform them about the possibility, or they will split this article in no-time. -->  ''Primary storage'' (or ''main memory'' or ''internal memory''), often referred to simply as ''memory'', is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.  Historically, [[History of computing hardware|early computers]] used [[delay line memory|delay lines]], [[Williams tube]]s, or rotating [[drum memory|magnetic drums]] as primary storage. By 1954, those unreliable methods were mostly replaced by [[magnetic core memory]]. Core memory remained dominant until the 1970s, when advances in [[integrated circuit]] technology allowed [[semiconductor memory]] to become economically competitive.<!-- Please DO NOT EXPAND above text! Change but no expand any further. Anyone desiring to know more types, let it be [[twister memory]] or [[bubble memory]] etc, will click the History link. Do not confuse other readers. -->  This led to modern [[random-access memory]] (RAM). It is small-sized, light, but quite expensive at the same time. (The particular types of RAM used for primary storage are also [[volatile memory|volatile]], i.e. they lose the information when not powered).  As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM: * [[Processor register]]s are located inside the processor. Each register typically holds a [[Word (computer architecture)|word]] of data (often 32 or 64 bits). CPU instructions instruct the [[arithmetic logic unit|arithmetic and logic unit]] to perform various calculations or other operations on this data (or with the help of it). Registers are the fastest of all forms of computer data storage. * [[CPU cache|Processor cache]] is an intermediate stage between ultra-fast registers and much slower main memory. It's introduced solely to increase performance of the computer. Most actively used information in the main memory is just duplicated in the cache memory, which is faster, but of much lesser capacity. On the other hand, main memory is much slower, but has a much greater storage capacity than processor registers. Multi-level [[Memory hierarchy|hierarchical cache]] setup is also commonly used—''primary cache'' being smallest, fastest and located inside the processor; ''secondary cache'' being somewhat larger and slower. <!-- Please DO NOT EXPAND above text, especially with L1/L2/etc variants - reader can always click the link. -->  Main memory is directly or indirectly connected to the central processing unit via a ''memory bus''. It is actually two buses (not on the diagram): an [[address bus]] and a [[Bus (computing)|data bus]]. The CPU firstly sends a number through an address bus, a number called [[memory address]], that indicates the desired location of data. Then it reads or writes the data itself using the data bus. Additionally, a [[memory management unit]] (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of [[virtual memory]] or other tasks.  As the RAM types used for primary storage are volatile (cleared at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, [[Non-volatile memory|non-volatile primary storage]] containing a small startup program ([[BIOS]]) is used to [[Bootstrapping (computing)|bootstrap]] the computer, that is, to read a larger program from non-volatile ''secondary'' storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for [[read-only memory]] (the terminology may be somewhat confusing as most ROM types are also capable of ''random access'').  Many types of "ROM" are not literally ''read only'', as updates are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some [[embedded system]]s run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, rather use large capacities of secondary storage, which is non-volatile as well, and not as costly.   Recently, ''primary storage'' and ''secondary storage'' in some uses refer to what was historically called, respectively, ''secondary storage'' and ''tertiary storage''.<ref>[http://searchstorage.techtarget.com/topics/0,295493,sid5_tax298620,00.html "Primary Storage or Storage Hardware" (shows usage of term "primary storage" meaning "hard disk storage")]. Searchstorage.techtarget.com (2011-06-13). Retrieved on 2011-06-18.</ref>  ===Secondary storage=== <!-- Note that additional DIRECT links point to this section by its name. --> [[File:Hard disk platter reflection.jpg|thumb|A [[hard disk drive]] with protective cover removed.]]  ''Secondary storage'' (also known as external memory or auxiliary storage), differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its [[input/output]] channels to access secondary storage and transfers the desired data using [[Data buffer|intermediate area]] in primary storage. Secondary storage does not lose the data when the device is powered down—it is non-volatile. Per unit, it is typically also two orders of magnitude less expensive than primary storage. Consequently, modern computer systems typically have two orders of magnitude more secondary storage than primary storage and data are kept for a longer time there.  In modern computers, [[hard disk drive]]s are usually used as secondary storage. The time taken to access a given byte of information stored on a hard disk is typically a few thousandths of a second, or milliseconds. By contrast, the time taken to access a given byte of information stored in random access memory is measured in billionths of a second, or nanoseconds. This illustrates the  significant access-time difference which distinguishes solid-state memory from rotating magnetic storage devices: hard disks are typically about a million times slower than memory. Rotating [[Optical disc drive|optical storage]] devices, such as [[Compact Disc|CD]] and [[DVD]] drives, have even longer access times. With disk drives, once the disk read/write head reaches the proper placement and the data of interest rotates under it, subsequent data on the track are very fast to access. As a result, in order to hide the initial seek time and rotational latency, data are transferred to and from disks in large contiguous blocks.  When data reside on disk, block access to hide latency offers a ray of hope in designing efficient [[external memory algorithms]]. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based upon sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel in order to increase the bandwidth between primary and secondary memory.<ref>[[J. S. Vitter]], ''[http://faculty.cse.tamu.edu/jsv/Papers/Vit.IO_book.pdf Algorithms and Data Structures for External Memory]'', Series on Foundations and Trends in Theoretical Computer Science, now Publishers, Hanover, MA, 2008, ISBN 978-1-60198-106-6.</ref>  Some other examples of secondary storage technologies are: [[flash memory]] (e.g. [[USB flash drive]]s or keys), [[floppy disk]]s, [[Magnetic tape data storage|magnetic tape]], [[Punched tape|paper tape]], [[punched card]]s, standalone [[RAM disk]]s, and [[Iomega Zip drive]]s.  The secondary storage is often formatted according to a [[file system]] format, which provides the abstraction necessary to organize data into [[Computer file|files]] and [[Folder (computing)|directories]], providing also additional information (called [[metadata]]) describing the owner of a certain file, the access time, the access permissions, and other information.  Most computer [[operating system]]s use the concept of [[virtual memory]], allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (''[[page (computing)|pages]]'') to secondary storage devices (to a [[Paging|swap file]] or [[paging|page file]]), retrieving them later when they are needed. As more of these retrievals from slower secondary storage are necessary, the more the overall system performance is degraded.  === Tertiary storage === <!-- Note that additional DIRECT links point to this section by its name. --> [[File:StorageTek Powderhorn tape library.jpg|thumb|Large [[tape library]]. Tape cartridges placed on shelves in the front, robotic arm moving in the back. Visible height of the library is about 180 cm.]]  ''Tertiary storage'' or ''tertiary memory'',<ref>[http://www.eecs.berkeley.edu/Pubs/TechRpts/1994/CSD-94-847.pdf A thesis on Tertiary storage]. (PDF) . Retrieved on 2011-06-18.</ref> provides a third level of storage. Typically it involves a robotic mechanism which will ''mount'' (insert) and ''dismount'' removable mass storage media into a storage device according to the system's demands; these data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include [[tape library|tape libraries]] and [[optical jukebox]]es.  When a computer needs to read information from the tertiary storage, it will first consult a catalog [[database]] to determine which tape or disc contains the information. Next, the computer will instruct a [[industrial robot|robotic arm]] to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.  === {{Visible anchor|Off-line storage}} === <!-- Additional DIRECT links point to this section by name. --> ''Off-line storage'' is a computer data storage on a medium or a device that is not under the control of a [[central processing unit|processing unit]].<ref>{{Cite document   | last =  National Communications System   | author-link = National Communications System   | title = Federal Standard 1037C – Telecommunications: Glossary of Telecommunication Terms   | publisher = General Services Administration   | year = 1996   | id = FS-1037C   | url = http://www.its.bldrdoc.gov/fs-1037/fs-1037c.htm   | accessdate = 2007-10-08   | postscript =  <!--None--> }} See also article [[Federal Standard 1037C]].</ref> The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.  [[Online and offline|Off-line]] storage is used to [[Data transmission|transfer information]], since the detached medium can be easily physically transported. Additionally, in case a disaster, for example a fire, destroys the original data, a medium in a remote location will probably be unaffected, enabling [[disaster recovery]]. Off-line storage increases general [[information security]], since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.  In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are most popular, and to much lesser extent removable hard disk drives. In enterprise uses, magnetic tape is predominant. Older examples are floppy disks, Zip disks, or punched cards.  == Characteristics of storage == [[File:DDR RAM-2.jpg|thumb|250px|A 1GB DDR [[Random-access memory|RAM]] module (detail)]]  Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressibility. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.  === Volatility === ; [[Non-volatile memory]]: Will retain the stored information even if it is not constantly supplied with electric power. It is suitable for long-term storage of information. ; [[Volatile memory]]: Requires constant power to maintain the stored information. The fastest memory technologies of today are volatile ones (not a universal rule). Since primary storage is required to be very fast, it predominantly uses volatile memory. :; [[Dynamic random-access memory]]: A form of volatile memory which also requires the stored information to be periodically re-read and re-written, or [[memory refresh|refreshed]], otherwise it would vanish. :; [[Static random-access memory]]: A form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied. (It loses its content if power is removed).  === Mutability === ; Read/write storage or mutable storage : Allows information to be overwritten at any time. A computer without some amount of read/write storage for primary storage purposes would be useless for many tasks. Modern computers typically use read/write storage also for secondary storage. ; Read only storage : Retains the information stored at the time of manufacture, and ''write once storage'' ([[Write Once Read Many]]) allows the information to be written only once at some point after manufacture. These are called ''immutable storage''. Immutable storage is used for tertiary and off-line storage. Examples include [[CD-ROM]] and [[CD-R]]. ; Slow write, fast read storage : Read/write storage which allows information to be overwritten multiple times, but with the write operation being much slower than the read operation. Examples include [[CD-RW]] and [[flash memory]].  === Accessibility === ; [[Random access]]: Any location in storage can be accessed at any moment in approximately the same amount of time. Such characteristic is well suited for primary and secondary storage. Most semiconductor memories and disk drives provide random access. ; [[Sequential access]]: The accessing of pieces of information will be in a serial order, one after the other; therefore the time to access a particular piece of information depends upon which piece of information was last accessed. Such characteristic is typical of off-line storage.  === Addressability === ; Location-addressable : Each individually accessible unit of information in storage is selected with its numerical [[memory address]]. In modern computers, location-addressable storage usually limits to primary storage, accessed internally by computer programs, since location-addressability is very efficient, but burdensome for humans. ; [[file system|File addressable]]: Information is divided into ''[[computer file|files]]'' of variable length, and a particular file is selected with [[human-readable]] directory and file names. The underlying device is still location-addressable, but the [[operating system]] of a computer provides the file system [[abstraction (computer science)|abstraction]] to make the operation more understandable. In modern computers, secondary, tertiary and off-line storage use file systems. ; [[content-addressable memory|Content-addressable]]: Each individually accessible unit of information is selected based on the basis of (part of) the contents stored there. Content-addressable storage can be implemented using [[software]] (computer program) or [[Personal computer hardware|hardware]] (computer device), with hardware being faster but more expensive option. Hardware content addressable memory is often used in a computer's [[CPU cache]]. CAS(content-addressable storage) addresses the thinking behind how are we to find and access the information that we currently have or will gather in the future.  === Capacity === ; Raw capacity : The total amount of stored information that a storage device or medium can hold. It is expressed as a quantity of [[bit]]s or [[byte]]s (e.g. 10.4 [[megabyte]]s). ; [[Memory storage density]]: The compactness of stored information. It is the storage capacity of a medium divided with a unit of length, area or volume (e.g. 1.2 megabytes per square inch).  === Performance === ; [[Latency (engineering)|Latency]]: The time it takes to access a particular location in storage. The relevant [[units of measurement|unit of measurement]] is typically [[1 E-9 s|nanosecond]] for primary storage, [[millisecond]] for secondary storage, and [[second]] for tertiary storage. It may make sense to separate read latency and write latency, and in case of sequential access storage, minimum, maximum and average latency. ; [[Throughput]]: The rate at which information can be read from or written to the storage. In computer data storage, throughput is usually expressed in terms of megabytes per second or MB/s, though [[bit rate]] may also be used. As with latency, read rate and write rate may need to be differentiated. Also accessing media sequentially, as opposed to randomly, typically yields maximum throughput.  === Energy use === * Storage devices that reduce fan usage, automatically shut-down during inactivity, and low power hard drives can reduce energy consumption 90 percent.<ref>[http://www.springlightcfl.com/consumer/energy_savings_calculator.aspx Energy Savings Calculator] and [http://www.simpletech.com/content/eco-friendly-redrive Fabric website]</ref> * 2.5 inch hard disk drives often consume less power than larger ones.<ref>{{cite web| title=IS the Silent PC Future 2.5-inches wide?|url=http://www.silentpcreview.com/article145-page1.html|accessdate=2008-08-02|author=Mike Chin|date=8 March 2004}}</ref><ref>{{cite web| url=http://www.silentpcreview.com/article29-page2.html|title=Recommended Hard Drives|accessdate=2008-08-02|author=Mike Chin|date=2002-09-18}}</ref> Low capacity [[solid-state drive]]s have no moving parts and consume less power than hard disks.<ref>[http://techreport.com/articles.x/10334/13 Super Talent's 2.5" IDE Flash hard drive – The Tech Report – Page 13]. The Tech Report. Retrieved on 2011-06-18.</ref><ref>[http://www.tomshardware.com/reviews/conventional-hard-drive-obsoletism,1324-5.html Power Consumption – Tom's Hardware : Conventional Hard Drive Obsoletism? Samsung's 32 GB Flash Drive Previewed]. Tomshardware.com (2006-09-20). Retrieved on 2011-06-18.</ref><ref name=xbitSSDvsHD /> Also, memory may use more power than hard disks.<ref name=xbitSSDvsHD>{{cite web | title = SSD, i-RAM and Traditional Hard Disk Drives | date = 2008-04-23 | url = http://www.xbitlabs.com/articles/storage/display/ssd-iram.html| author = Aleksey Meyev| media = X-bit labs}}</ref>  == Fundamental storage technologies == {{anchor|media}} {{As of|2011}}, the most commonly used data storage technologies are semiconductor, magnetic, and optical, while paper still sees some limited usage. ''Media'' is a common name for what actually holds the data in the storage device. Some other fundamental storage technologies have also been used in the past or are proposed for development.  === Semiconductor === [[Semiconductor memory]] uses [[semiconductor]]-based [[integrated circuit]]s to store information. A semiconductor memory chip may contain millions of tiny [[transistor]]s or [[capacitor]]s. Both ''volatile'' and ''non-volatile'' forms of semiconductor memory exist. In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor memory or [[dynamic random access memory]]. Since the turn of the century, a type of non-volatile semiconductor memory known as [[flash memory]] has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers. As early as 2006, [[Laptop|notebook]] and [[desktop computer]] manufacturers started using flash-based [[solid-state drive]]s (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.<ref>[http://www.extremetech.com/article2/0,1558,1966644,00.asp New Samsung Notebook Replaces Hard Drive With Flash]. ExtremeTech (2006-05-23). Retrieved on 2011-06-18.</ref><ref>[http://www.technewsworld.com/rsstory/60700.html?wlc=1308338527 Welcome to TechNewsWorld]. Technewsworld.com. Retrieved on 2011-06-18.</ref><ref>[http://www.apple.com/macpro/features/storage.html Mac Pro – Storage and RAID options for your Mac Pro]. Apple (2006-07-27). Retrieved on 2011-06-18.</ref><ref>[http://www.apple.com/macbookair/design.html MacBook Air – The best of iPad meets the best of Mac]. Apple. Retrieved on 2011-06-18.</ref><ref>[http://news.inventhelp.com/Articles/Computer/Inventions/apple-macbook-air-12512.aspx MacBook Air Replaces the Standard Notebook Hard Disk for Solid State Flash Storage]. News.inventhelp.com (2010-11-15). Retrieved on 2011-06-18.</ref>  === Magnetic === {{Magnetic storage media}}  [[Magnetic storage]] uses different patterns of [[magnetization]] on a [[magnetism|magnetically]] coated surface to store information. Magnetic storage is ''non-volatile''. The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms: * [[Disk storage|Magnetic disk]] ** [[Floppy disk]], used for off-line storage ** [[Hard disk drive]], used for secondary storage * [[Magnetic tape data storage|Magnetic tape]], used for tertiary and off-line storage  In early computers, magnetic storage was also used as: *Primary storage in a form of [[Drum memory|magnetic memory]], or [[Magnetic core memory|core memory]], [[core rope memory]], [[thin-film memory]] and/or [[twistor memory]]. *Tertiary (e.g. [[NCR CRAM]]) or off line storage in the form of magnetic cards. *Magnetic tape was then often used for secondary storage.  === Optical === {{Optical storage media}}  [[Optical storage]], the typical [[optical disc]], stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a [[laser diode]] and observing the reflection. Optical disc storage is ''non-volatile''. The deformities may be permanent (read only media ), formed once (write once media) or reversible (recordable or read/write media). The following forms are currently in common use:<ref>The [http://www.dvddemystified.com/dvdfaq.html DVD FAQ] is a comprehensive reference of DVD technologies.</ref> * [[Compact Disc|CD]], [[CD-ROM]], [[DVD]], [[Blu-ray Disc|BD-ROM]]: Read only storage, used for mass distribution of digital information (music, video, computer programs) * [[CD-R]], [[DVD-R]], [[DVD R]], [[Blu-ray Disc recordable|BD-R]]: Write once storage, used for tertiary and off-line storage * [[CD-RW]], [[DVD-RW]], [[DVD RW]], [[DVD-RAM]], [[Blu-ray Disc recordable|BD-RE]]: Slow write, fast read storage, used for tertiary and off-line storage * [[Ultra Density Optical]] or UDO is similar in capacity to [[Blu-ray Disc recordable|BD-R or BD-RE]] and is slow write, fast read storage used for tertiary and off-line storage.  [[Magneto-optical drive|Magneto-optical disc storage]] is optical disc storage where the magnetic state on a [[ferromagnetism|ferromagnetic]] surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is ''non-volatile'', ''sequential access'', slow write, fast read storage used for tertiary and off-line storage.  [[3D optical data storage]] has also been proposed.  === Paper === {{Paper data storage media}}  [[Paper data storage]], typically in the form of [[punched tape|paper tape]] or [[punched card]]s, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. A few technologies allow people to make marks on paper that are easily read by machine—these are widely used for tabulating votes and grading standardized tests. [[Barcode]]s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it.  ===Uncommon=== ; Vacuum tube memory : A [[Williams tube]] used a [[cathode ray tube]], and a [[Selectron tube]] used a large [[vacuum tube]] to store information. These primary storage devices were short-lived in the market, since Williams tube was unreliable and the Selectron tube was expensive.  ; Electro-acoustic memory : [[Delay line memory]] used [[Longitudinal wave|sound wave]]s in a substance such as [[mercury (element)|mercury]] to store information. Delay line memory was dynamic volatile, cycle sequential read/write storage, and was used for primary storage.  ; [[Optical tape]]: is a medium for optical storage generally consisting of a long and narrow strip of plastic onto which patterns can be written and from which the patterns can be read back. It shares some technologies with cinema film stock and optical discs, but is compatible with neither. The motivation behind developing this technology was the possibility of far greater storage capacities than either magnetic tape or optical discs.  ; [[Phase-change memory]]: uses different mechanical phases of [[Phase Change Material]] to store information in an X-Y addressable matrix, and reads the information by observing the varying [[electrical resistance]] of the material. Phase-change memory would be non-volatile, random access read/write storage, and might be used for primary, secondary and off-line storage. Most rewritable and many write once optical disks already use phase change material to store information.  ; [[Holographic data storage]]: stores information optically inside [[crystal]]s or [[photopolymer]]s. Holographic storage can utilize the whole volume of the storage medium, unlike optical disc storage which is limited to a small number of surface layers. Holographic storage would be non-volatile, sequential access, and either write once or read/write storage. It might be used for secondary and off-line storage. See [[Holographic Versatile Disc]] (HVD).  ; [[Molecular memory]]: stores information in [[polymer]] that can store electric charge. Molecular memory might be especially suited for primary storage. The theoretical storage capacity of molecular memory is 10 terabits per square inch.<ref>[http://www.sciencedaily.com/releases/2009/02/090219141438.htm New Method Of Self-assembling Nanoscale Elements Could Transform Data Storage Industry]. Sciencedaily.com (2009-03-01). Retrieved on 2011-06-18.</ref>  == Related technologies == === Network connectivity === A secondary or tertiary storage may connect to a computer utilizing [[computer network]]s. This concept does not pertain to the primary storage, which is shared between multiple processors in a much lesser degree. * [[Direct-attached storage]] (DAS) is a traditional mass storage, that does not use any network. This is still a most popular approach. This [[retronym]] was coined recently, together with NAS and SAN. * [[Network-attached storage]] (NAS) is mass storage attached to a computer which another computer can access at file level over a [[local area network]], a private [[wide area network]], or in the case of [[File hosting service|online file storage]], over the [[Internet]]. NAS is commonly associated with the [[Network File System (protocol)|NFS]] and [[Server Message Block|CIFS/SMB]] protocols. * [[Storage area network]] (SAN) is a specialized network, that provides other computers with storage capacity. The crucial difference between NAS and SAN is the former presents and manages file systems to client computers, whilst the latter provides access at block-addressing (raw) level, leaving it to attaching systems to manage data or file systems within the provided capacity. SAN is commonly associated with [[Fibre Channel]] networks.  === Robotic storage === Large quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as [[tape library|tape libraries]], and in optical storage field [[optical jukebox]]es, or optical disk libraries per analogy. Smallest forms of either technology containing just one drive device are referred to as [[autoloader (data storage device)|autoloaders]] or [[autochanger]]s.  Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide [[terabyte]]s or [[petabyte]]s of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.  Robotic storage is used for [[backup]]s, and for high-capacity archives in imaging, medical, and video industries. [[Hierarchical storage management]] is a most known archiving strategy of automatically ''migrating'' long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are ''retrieved'' back to disk.  == See also == {{Commons category|Computer memory}}  === Primary storage topics === * [[Aperture (computer memory)]] * [[Dynamic random access memory]] (DRAM) * [[CAS latency|Memory latency]] * [[Mass storage]] * [[Memory cell (disambiguation)]] <!-- do not remove - disambig of redirect --> * [[Memory management]] ** [[Dynamic memory allocation]] *** [[Memory leak]] ** [[Virtual memory]] * [[Memory protection]] * [[Page address register]] * [[Static random access memory]] (SRAM) * [[Stable storage]]  === Secondary, tertiary and off-line storage topics === * [[Data deduplication]] * [[Data proliferation]] * [[Data storage tag]] used for capturing research data * [[File system]] ** [[List of file formats]] * [[Flash memory]] * [[Information repository]] * [[Removable media]] * [[Solid-state drive]] * [[Hard disk drive#Spindle|Spindle]] * [[Virtual tape library]] * [[Wait state]] * [[Write buffer]] * [[Write protection]]  === Data storage conferences === * [[Storage Networking World]] * [[Storage World Conference]]  == References == {{FS1037C}} {{reflist|30em}}  {{Primary storage technologies}} {{Use dmy dates|date=June 2011}}  {{DEFAULTSORT:Computer Data Storage}} [[Category:Computer storage|*]] [[Category:Computer memory|Storage]] [[Category:Digital electronics]]  [[ar:ذاكرة (حاسوب)]] [[az:Operativ yaddaş]] [[bg:Оперативна памет]] [[ca:Memòria d'ordinador]] [[cs:Elektronická paměť]] [[da:Digital hukommelse]] [[de:Arbeitsspeicher]] [[et:Mälu (arvuti)]] [[el:Μνήμη υπολογιστή]] [[es:Memoria (informática)]] [[eo:Memoro (komputiko)]] [[eu:Memoria (informatika)]] [[fa:حافظه (رایانه)]] [[fr:Mémoire (informatique)]] [[ko:기억 장치]] [[hi:कंप्यूटर स्मृति]] [[id:Penyimpanan data komputer]] [[is:Vinnsluminni]] [[it:Memoria (elettronica)]] [[lt:Atminties įrenginys]] [[hu:Memóriaegység]] [[ms:Storan data komputer]] [[nl:Computergeheugen]] [[ja:記憶装置]] [[no:Datalager]] [[mhr:Шарныш]] [[pl:Pamięć komputerowa]] [[pt:Memória (computador)]] [[ru:Компьютерная память]] [[sq:Memoria]] [[simple:Computer memory]] [[sk:Pamäť (počítač)]] [[sv:Datorminne]] [[th:หน่วยความจำ]] [[uk:Комп'ютерна пам'ять]] [[ur:حافظہ (شمارندہ)]] [[vi:Bộ nhớ]] [[zh:電腦數據存貯器]]
{{About|computer files and file systems in general terms|a more detailed and technical article|File system}} {{refimprove|date=June 2009}} A '''computer file''' is a block of arbitrary information, or resource for storing information, which is available to a [[computer program]] and is usually based on some kind of durable [[computer storage|storage]]. A '''file''' is durable in the sense that it remains available for programs to use after the current program has finished. Computer files can be considered as the modern counterpart of paper [[document]]s which traditionally are kept in offices' and libraries' [[Filing cabinet|files]], and this is the source of the term.  ==History== [[File:PunchCardDecks.agr.jpg|right|thumb|A [[punched card]] file]] [[File:BRL61-IBM 305 RAMAC.jpeg|right|thumb|The twin [[Hard disk drive|hard drives]] of an [[IBM 305]] system]] The word "file" was used publicly in the context of computer storage as early as February, 1950.  In an [[RCA]] (Radio Corporation of America) advertisement in [[Popular Science]] Magazine<ref>[http://books.google.com/books?id=9SwDAAAAMBAJ&lpg=PA210&pg=PA96 Popular Science Magazine, February 1950, page 96]</ref> describing a new "memory" vacuum tube it had developed, RCA stated: :''"...the results of countless computations can be kept "on file" and taken out again.  Such a "file" now exists in a "memory" tube developed at RCA Laboratories.  Electronically it retains figures fed into calculating machines, holds them in storage while it memorizes new ones - speeds intelligent solutions through mazes of mathematics."'' In 1952 "file" was used in referring to information stored on [[punched card]]s.<ref>Robert S. Casey, et al. ''Punched Cards: Their Applications to Science and Industry'', 1952.</ref> In early usage people regarded the underlying hardware (rather than the contents) as the file. For example, the [[IBM 350]] disk drives were called "disk files".<ref>Martin H. Weik. Ballistic Research Laboratories Report #1115. March 1961. [http://ed-thelen.org/comp-hist/BRL61-ibm03.html#IBM-305-RAMAC pp. 314-331].</ref> Systems like the [[Compatible Time-Sharing System]] introduced the concept of a file system, which managed several virtual "files" on one storage device, giving the term its present-day meaning. File names in CTSS had two parts, a user-readable "primary name" and a "secondary name" indicating the file type.<ref>Fernando J. Corbató et al. "[http://larch-www.lcs.mit.edu:8001/~corbato/sjcc62/ An Experimental Time-Sharing System]." May 3, 1962.</ref><ref>Jerome H. Saltzer ''[http://www.lcs.mit.edu/publications/pubs/pdf/MIT-LCS-TR-016.pdf CTSS Technical Notes]''. Project MIT-LCS-TR016</ref> This convention remains in use by several operating systems today, including [[Microsoft Windows]]. Although the current term "[[register file]]" shows the early concept of files, it has largely disappeared.  ==File contents== On most modern [[operating systems]], files are organized into one-dimensional arrays of [[byte]]s. The [[file format|format]] of a file is defined by its content since a file is solely a container for data, although, on some platforms the format is usually indicated by its [[filename extension]], specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file (<tt>.txt</tt> in Windows) are associated with either [[ASCII]] or [[UTF-8]] characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for [[metadata]], which allows a file to carry some basic information about itself.  === File size === At any instant in time, a file might have a size, normally expressed as number of [[byte]]s, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit.  However, the general definition of a file does not require that its instant size has any real meaning, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a [[zero byte file]]; these files are either an accident (a result of an aborted disk operation) or serve as some kind of [[Flag (computing)|flag]] in the file system.  For example, the file to which the link <tt>/bin/ls</tt> points in a typical [[Unix-like]] system probably has a defined size that seldom changed. Compare this with <tt>[[/dev/null]]</tt> which is also a file, but its size may be obscure.  === Organizing the data in a file === Information in a computer file can consist of smaller packets of information (often called "[[Row (database)|records]]" or "lines") that are individually different but share some trait in common. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll—this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a [[BLOB]]) or it may contain an [[executable]].  The way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by [[computer programs]] which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.  In some cases, computer programs manipulate files that are made visible to the computer user. For example, in a [[word-processing program]], the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.  Many applications pack all their data files into a single file called [[archive file]], using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.  === File operations === At the most basic level there are only two types of file operations; read and write. For example: adding text to a document involves; opening the file (read), inputting the text and saving the file (write)  Files on a computer can be created, moved, modified, grown, shrunk and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, [[Microsoft Word]] files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a [[file manager|file manager program]] such as [[Windows Explorer]] (on Windows computers) or by [[command-line interface|command lines]] (CLI).   In [[Unix-like]] systems, user-space processes do not normally deal with files at all; the [[operating system]] provides a level of [[Abstraction (computer science)|abstraction]] which means that almost all interaction with files from user-space is through [[hard links]].  For example, a [[user space]] program cannot [[File deletion|delete a file]]; it can delete a link to a file, and if the kernel determines that there are no more existing hard links to the file, it may then allow the memory location for the deleted file to be allocated for another file.  Because the data remains intact on disk, this creates what is known as [[Data remanence|free space]], which is commonly considered a security risk due to the existence of [[File recovery|file recovery software]].  Such a risk  has given rise to secure deletion programs.  In fact, it really is only the kernel that deals with files, but it serves to handle all user-space interaction with (virtual) files in a manner that is transparent to the user-space programs  ==Semantics== Although the way programs manipulate files varies according to the operating system and file system involved, the following operations are typical: *''Creating'' a file with a given name *Setting ''attributes'' that control operations on the file *''Opening'' a file to use its contents *''Reading'' or ''updating'' the contents *''Committing'' updated contents to durable storage *''Closing'' the file, thereby losing access until it is opened again  ==Identifying and organizing files== [[File:FileFolders.svg|thumb|200px|right|Files and folders arranged in a hierarchy]] In modern computer systems, files are typically accessed using names ([[filename]]s).  In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.  Files (or links to files) can be located in directories. However, more generally, a [[File directory|directory]] can contain either a list of files or a list of links to files.  Within this definition, it is of paramount importance that the term "file" includes directories.  This permits the existence of directory hierarchies, i.e., directories containing sub-directories.  A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory.  However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.  In environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system—no two files can have the same name and path.  Where a file is anonymous, named references to it will exist within a namespace.  In most cases, any name within the namespace will refer to exactly zero or one file.  However, any file may be represented within any namespace by zero, one or more names.  Any string of characters may or may not be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of [[unicode]] letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the [[file system]]. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. [[Microsoft Windows]] supports multiple file systems, each with different policies regarding case-sensitivity. The common [[File Allocation Table|FAT]] file system can have multiple files whose names differ only in case if the user uses a [[disk editor]] to edit the file names in the [[File Allocation Table#Directory table|directory entries]]. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.  Most computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one "master folder" (or "root folder" — the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.  When a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character—such as a slash—is used to separate the file and folder names. For example, in the illustration shown in this article, the path <tt>/Payroll/Salaries/Managers</tt> uniquely identifies a file called <tt>Managers</tt> in a folder called <tt>Salaries</tt>, which in turn is contained in a folder called <tt>Payroll</tt>. The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).  Many (but not all) computer systems use [[filename extension|extensions]] in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of <tt>.txt</tt> identifies a text file; a <tt>.doc</tt> extension identifies any type of document or documentation, commonly in the [[Microsoft Word]] [[file format]]; [[List of file formats|and so on]]. Even when extensions are used in a computer system, the degree to which the computer system recognizes  and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.  ==Protecting files== Many modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement [[file permissions]] to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.  Another protection mechanism implemented in many computers is a ''read-only flag.'' When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a ''[[Hidden file and hidden directory|hidden flag]]'' to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.  ==Storing files== The discussion above describes a file as a concept presented to a user or a high-level operating system.  However, any file that has any useful purpose, outside of a thought experiment, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.   In physical terms, most computer files are stored on some type of data storage device. For example, there is a [[hard disk]], from which most [[operating system]]s run and on which most store their files. Hard disks have been the ubiquitous form of [[non-volatile]] storage since the early 1960s.<ref name="Mee">Magnetic Storage Handbook 2nd Ed., Section 2.1.1, Disk File Technology, Mee and Daniel, (c)1990,</ref> Where files contain only temporary information, they may be stored in [[RAM]]. Computer files can be also stored on other media in some cases, such as [[magnetic tape]]s, [[compact disc]]s, [[Digital Versatile Disc]]s, [[Zip drive]]s, [[USB flash drive]]s, etc.  In Unix-like operating systems, many files have no direct association with a physical storage device: <tt>[[/dev/null]]</tt> is a prime example, as are just about all files under <tt>/dev</tt>, <tt>/proc</tt> and <tt>/sys</tt>.  These can be accessed as files in user space. They are really virtual files that exist, in reality, as objects within the operating system kernel.  ==Backing up files== When computer files contain information that is extremely important, a ''[[back-up]]'' process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.  There are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.  The [[grandfather-father-son backup]] method automatically makes three back ups, the grandfather file is the oldest copy of the file and the son is the current copy.  ==File systems and file managers== The way a computer organizes, names, stores and manipulates files is globally referred to as its ''[[file system]].'' Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of [[MS-DOS]] and old versions of Windows are supported, in addition to the [[NTFS]] file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file "<tt>Payroll records</tt>" in NTFS, but in FAT you would be restricted to something like <tt>payroll.dat</tt> (unless you were using [[Vfat#Long_file_names|VFAT]], a FAT extension allowing long file names).  [[File manager]] programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. Under Windows, the most commonly used file manager program is Windows Explorer.  ==See also== *[[Block (data storage)]] *[[Computer file management]] *[[Data hierarchy]] *[[File Camouflage]] *[[File copying]] *[[File conversion]] *[[File deletion]] *[[File directory]] *[[File manager]] *[[Filename|File name]] *[[File size]] *[[File system]] *[[Flat file database]] *[[Object composition]] *[[Soft copy]]  ==Notes== {{reflist}}  ==External links and references== * {{dmoz|Computers/Data_Formats/ Data Formats}}  {{DEFAULTSORT:Computer File}} [[Category:Files| ]] [[Category:Computer file systems|File]] [[Category:Inter-process communication]]  [[ar:ملف حاسوب]] [[ast:Ficheru informáticu]] [[az:Fayl]] [[be:Камп'ютарны файл]] [[bg:Файл]] [[bs:Datoteka]] [[br:Restr stlennegel]] [[ca:Fitxer informàtic]] [[cv:Файл]] [[cs:Soubor]] [[da:Fil (dataobjekt)]] [[de:Datei]] [[et:Fail]] [[el:Αρχείο (υπολογιστές)]] [[es:Archivo (informática)]] [[eo:Dosiero]] [[eu:Fitxategi (informatika)]] [[fa:پرونده (رایانه)]] [[fr:Fichier informatique]] [[fy:Triem]] [[gl:Ficheiro]] [[ko:컴퓨터 파일]] [[hy:Նիշք]] [[hi:संगणक संचिका]] [[hr:Datoteka]] [[id:Berkas komputer]] [[ia:File]] [[is:Tölvuskrá]] [[it:File]] [[he:קובץ]] [[kk:Файл]] [[lv:Datne]] [[lt:Rinkmena]] [[ln:Kásá]] [[hu:Adatállomány]] [[ms:Fail komputer]] [[nl:Bestand (computer)]] [[ja:ファイル (コンピュータ)]] [[no:Datafil]] [[mhr:Файл]] [[pl:Plik]] [[pt:Arquivo de computador]] [[ro:Fișier]] [[ru:Файл]] [[sah:Билэ]] [[simple:Computer file]] [[sk:Súbor (informatika)]] [[sl:Datoteka]] [[sh:Datoteka]] [[fi:Tiedosto]] [[sv:Fil (data)]] [[ta:கணினிக் கோப்பு]] [[th:แฟ้ม]] [[tr:Dosya]] [[uk:Файл]] [[vi:Tập tin]] [[wo:Dencukaay]] [[yi:קאמפיוטער טעקע]] [[zh:電腦檔案]]
{{other uses}}  [[Image:Blender 2.45 screenshot.jpg|thumb|256px|A [[Blender (software)|Blender]] 2.45 screenshot, displaying the [[3D test model]] [[Suzanne (3D model)|Suzanne]].]]  '''Computer graphics''' are [[graphics]] created using [[computer]]s and, more generally, the representation and manipulation of [[image]] data by a computer with help from specialized [[graphics software|software]] and [[graphics hardware|hardware]].  The development of computer graphics has made computers easier to interact with, and better for understanding and interpreting many types of data. Developments in computer graphics have had a profound impact on many types of media and have revolutionized [[animation]], [[movies]] and the [[video game]] industry.  == Overview == The term computer graphics has been used in a broad sense to describe "almost everything on computers that is not text or sound".<ref>[http://www.graphics.cornell.edu/online/tutorial/ What is Computer Graphics?], Cornell University Program of Computer Graphics. Last updated 04/15/98. Accessed November 17, 2009.</ref> Typically, the term ''computer graphics'' refers to several different things: * the representation and manipulation of image data by a computer * the various [[technologies]] used to create and manipulate images * the sub-field of [[computer science]] which studies methods for digitally synthesizing and manipulating visual content, see [[Computer graphics (computer science)|study of computer graphics]]  Computer graphics is widespread today. Computer imagery is found on television, in newspapers, for example in weather reports, or for example in all kinds of medical investigation and surgical procedures. A well-constructed [[Chart|graph]] can present complex statistics in a form that is easier to understand and interpret. In the media "such graphs are used to illustrate papers, reports, thesis", and other presentation material.<ref name="ISS02">University of Leeds ISS (2002). [http://iss.leeds.ac.uk/info/306/graphics/215/overview_of_computer_graphics/2 "What are computer graphics?"]. Last updated: 22 September 2008</ref>  Many powerful tools have been developed to visualize data. Computer generated imagery can be categorized into several different types: 2D, 3D, and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of [[computer science]] which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like [[information visualization]], and [[scientific visualization]] more concerned with "the visualization of [[Three-dimensional space|three dimensional]] phenomena (architectural, meteorological, medical, [[Biological Data Visualization|biological]], etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component".<ref name = "MF08">[[Michael Friendly]] (2008). [http://www.math.yorku.ca/SCS/Gallery/milestone/milestone.pdf "Milestones in the history of thematic cartography, statistical graphics, and data visualization"].</ref>  == History ==  {{See also|History of computer animation|}}  [[Image:SAGE control room.png|thumb|right|[[SAGE Project|SAGE]] Sector Control Room.]] The phrase “Computer Graphics” was coined in 1960 by [[William Fetter]], a graphic designer for [[Boeing]].<ref name= "WC03">Wayne Carlson (2003) [http://accad.osu.edu/~waynec/history/lessons.html A Critical History of Computer Graphics and Animation]. The Ohio State University</ref>  The field of computer graphics developed with the emergence of computer graphics hardware. Early projects like the [[Whirlwind (computer)|Whirlwind]] and [[SAGE Project]]s introduced the [[Cathode ray tube|CRT]] as a viable [[visual display unit|display]] and interaction interface and introduced the [[light pen]] as an [[input device]].  === Initial 1960s developments === Further advances in computing led to greater advancements in interactive computer graphics. In 1959, the [[TX-2]] computer was developed at [[Lincoln Laboratory|MIT's Lincoln Laboratory]]. The TX-2 integrated a number of new man-machine interfaces. A light pen could be used to draw sketches on the computer using [[Ivan Sutherland]]'s revolutionary [[Sketchpad|Sketchpad software]].<ref name= "WC03"/> Using a light pen, Sketchpad allowed one to draw simple shapes on the computer screen, save them and even recall them later. The light pen itself had a small photoelectric cell in its tip. This cell emitted an electronic pulse whenever it was placed in front of a computer screen and the screen's electron gun fired directly at it. By simply timing the electronic pulse with the current location of the electron gun, it was easy to pinpoint exactly where the pen was on the screen at any given moment. Once that was determined, the computer could then draw a cursor at that location.  Sutherland seemed to find the perfect solution for many of the graphics problems he faced. Even today, many standards of computer graphics interfaces got their start with this early Sketchpad program. One example of this is in drawing constraints. If one wants to draw a square for example, they do not have to worry about drawing four lines perfectly to form the edges of the box. One can simply specify that they want to draw a box, and then specify the location and size of the box. The software will then construct a perfect box, with the right dimensions and at the right location. Another example is that Sutherland's software modeled objects - not just a picture of objects. In other words, with a model of a car, one could change the size of the tires without affecting the rest of the car. It could stretch the body of the car without deforming the tires.  === Further 1961 developments === [[File:Spacewar!-PDP-1-20070512.jpg|thumb|Spacewar! running on the Computer History Museum's [[PDP-1]].]] Also in 1961 another student at MIT, [[Steve Russell]], created the first video game, [[Spacewar!|Spacewar]]. Written for the DEC [[PDP-1]], Spacewar was an instant success and copies started flowing to other PDP-1 owners and eventually even DEC got a copy. The engineers at [[Digital Equipment Corporation|DEC]] used it as a diagnostic program on every new PDP-1 before shipping it. The sales force picked up on this quickly enough and when installing new units, would run the world's first video game for their new customers.  E. E. Zajac, a scientist at [[Bell Labs|Bell Telephone Laboratory]] (BTL), created a film called "Simulation of a two-giro gravity attitude control system" in 1963.<ref>David Salomon (1999). ''Computer graphics and geometric modeling''. p. ix</ref> In this computer generated film, Zajac showed how the attitude of a satellite could be altered as it orbits the Earth. He created the animation on an [[IBM 7090]] mainframe computer. Also at BTL, [[Ken Knowlton]], [[Frank Sindon]] and [[Michael Noll]] started working in the computer graphics field. Sindon created a film called Force, Mass and Motion illustrating Newton's laws of motion in operation. Around the same time, other scientists were creating computer graphics to illustrate their research. At Lawrence Radiation Laboratory, Nelson Max created the films, "Flow of a Viscous Fluid" and "Propagation of Shock Waves in a Solid Form." Boeing Aircraft created a film called "Vibration of an Aircraft."  It was not long before major corporations started taking an interest in computer graphics. [[TRW]], [[Lockheed Corporation|Lockheed-Georgia]], [[General Electric]] and [[Sperry Rand]] are among the many companies that were getting started in computer graphics by the mid-1960s. IBM was quick to respond to this interest by releasing the IBM 2250 graphics terminal, the first commercially available graphics computer. [[Image:Pong.png|thumb|right|170px|''[[Pong]]'' arcade version]] [[Ralph Baer]], a supervising engineer at Sanders Associates, came up with a home [[video game]] in 1966 that was later licensed to [[Magnavox]] and called the [[Magnavox Odyssey|Odyssey]]. While very simplistic, and requiring fairly inexpensive electronic parts, it allowed the player to move points of light around on a screen. It was the first consumer computer graphics product.  [[David C. Evans]] was director of engineering at [[Bendix Corporation]]'s computer division from 1953 to 1962, after which he worked for the next five years as a visiting professor at Berkeley. There he continued his interest in computers and how they interfaced with people. In 1966, the University of Utah recruited Evans to form a computer science program, and computer graphics quickly became his primary interest. This new department would become the world's primary research center for computer graphics.  Also in 1966, Sutherland at MIT invented the first computer controlled [[head-mounted display]] (HMD). Called the Sword of Damocles because of the hardware required for support, it displayed two separate wireframe images, one for each eye. This allowed the viewer to see the computer scene in stereoscopic 3D. After receiving his Ph.D. from MIT, Sutherland became Director of Information Processing at [[DARPA|ARPA]] (Advanced Research Projects Agency), and later became a professor at Harvard.  In 1967 Sutherland was recruited by Evans to join the computer science program at the University of Utah. There he perfected his HMD. Twenty years later, NASA would re-discover his techniques in their virtual reality research. At Utah, Sutherland and Evans were highly sought after consultants by large companies but they were frustrated at the lack of graphics hardware available at the time so they started formulating a plan to start their own company.  In 1969, the [[Association for Computing Machinery|ACM]] initiated A Special Interest Group in Graphics ([[SIGGRAPH]]) which organizes [[List of computer science conferences#Computer graphics|conferences]], graphics standards, and publications within the field of computer graphics. In 1973, the first annual SIGGRAPH conference was held, which has become one of the focuses of the organization. SIGGRAPH has grown in size and importance as the field of computer graphics has expanded over time.  === 1970s === Many of the most important early breakthroughs in computer graphics research occurred at the [[University of Utah]] in the 1970s. A student by the name of [[Edwin Catmull]] started at the University of Utah in 1970 and signed up for [[Ivan Sutherland|Sutherland]]'s computer graphics class. Catmull had just come from The Boeing Company and had been working on his degree in physics. Growing up on Disney, Catmull loved animation yet quickly discovered that he did not have the talent for drawing. Now Catmull (along with many others) saw computers as the natural progression of animation and they wanted to be part of the revolution. The first animation that Catmull saw was his own. He created an animation of his hand opening and closing. It became one of his goals to produce a feature length motion picture using computer graphics. In the same class, [[Fred Parke]] created an animation of his wife's face. Because of Evan's and Sutherland's presence, UU was gaining quite a reputation as the place to be for computer graphics research so Catmull went there to learn 3D animation.  As the UU computer graphics laboratory was attracting people from all over, [[John Warnock]] was one of those early pioneers; he would later found [[Adobe Systems]] and create a revolution in the publishing world with his [[PostScript]] page description language. Tom Stockham led the image processing group at UU which worked closely with the computer graphics lab. Jim Clark was also there; he would later found Silicon Graphics, Inc.  The first major advance in 3D computer graphics was created at UU by these early pioneers, the hidden-surface algorithm. In order to draw a representation of a 3D object on the screen, the computer must determine which surfaces are "behind" the object from the viewer's perspective, and thus should be "hidden" when the computer creates (or renders) the image.  The [[3D Core Graphics System]] (or '''Core''') was the first graphical standard to be developed.  A group of 25 experts of the [[Association for Computing Machinery|ACM]] [[Special Interest Group]] [[SIGGRAPH]] developed this "conceptual framework".  The specifications were published in 1977, and it became a foundation for many future development in the field.  === 1980s === In the early 1980s, the availability of bit-slice and 16-bit microprocessors started to revolutionise high resolution computer graphics terminals which now increasingly became intelligent, semi-standalone and standalone workstations. Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. Typical of the early move to high resolution computer graphics intelligent workstations for the computer-aided engineering market were the Orca 1000, 2000 and 3000 workstations, developed by Orcatech of Ottawa, a spin-off from [[Bell-Northern Research]], and led by an early workstation pioneer [[David John Pearson]]. The Orca 3000 was based on Motorola 68000 and AMD bit-slice processors and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector. Artists and graphic designers began to see the personal computer, particularly the [[Commodore Amiga]] and [[Apple Macintosh|Macintosh]], as a serious design tool, one that could save time and draw more accurately than other methods. In the late 1980s, [[Silicon Graphics|SGI]] computers were used to create some of the first fully computer-generated [[short film]]s at [[Pixar]]. The Macintosh remains a highly popular tool for computer graphics among graphic design studios and businesses. Modern computers, dating from the 1980s often use [[graphical user interfaces]] (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of [[multimedia]] technology.  === 1990s === [[3D graphics]] became more popular in the 1990s in [[Video game|gaming]], [[multimedia]] and [[animation]]. At the end of the 80s and beginning of the nineties were created, in France, the very first computer graphics TV series: "La Vie des bêtes" by studio Mac Guff Ligne (1988), ''Les Fables Géométriques'' J.-Y. Grall, Georges Lacroix and Renato (studio Fantome, 1990–1993) and ''[[Quarxs]]'', the first  HDTV computer graphics series by [[Maurice Benayoun]] and [[François Schuiten]] (studio Z-A production, 1991–1993). In 1995, [[Toy Story]], the first full-length computer-generated animation film, was released in cinemas worldwide. In 1996, [[Quake (video game)|Quake]], one of the first fully 3D [[game]]s, was released. Since then, computer graphics have only become more detailed and realistic, due to more powerful graphics hardware and 3D modeling software.  == Image types == === Two-dimensional === [[Image:Blit dot.gif|thumb|[[Raster graphics|Raster graphic]] [[Sprite (computer graphics)|sprite]]s (left) and masks (right)]] [[2D computer graphics]] are the computer-based generation of [[digital image]]s&mdash;mostly from two-dimensional models, such as [[2D geometric model]]s, text, and digital images, and by techniques specific to them.  2D computer graphics are mainly used in applications that were originally developed upon traditional [[printing]] and [[drawing]] technologies, such as  [[typography]], [[cartography]], [[technical drawing]], [[advertising]], etc.. In those applications, the two-dimensional [[image]] is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred, because they give more direct control of the image than [[3D computer graphics]], whose approach is more akin to [[photography]] than to [[typography]].  ==== Pixel art ==== [[Pixel art]] is a form of [[digital art]], created through the use of [[raster graphics]] [[software]], where images are edited on the [[pixel]] level. Graphics in most old (or relatively limited) computer and video games, [[graphing calculator]] games, and many [[mobile phone]] games are mostly pixel art.  ==== Vector graphics ==== [[Image:VectorBitmapExample.png|thumb|Example showing effect of vector graphics versus raster (bitmap) graphics.]] [[Vector graphics]] formats are complementary to [[raster graphics]]. Raster graphics is the representation of images as an array of [[pixel]]s and is typically used for the representation of photographic images. <ref>{{cite book | title = Processing: Creative Coding and Computational Art | author = Ira Greenberg | publisher = Apress | year = 2007 | isbn = 1-59059-617-X | url = http://books.google.com/books?id=WTl_7H5HUZAC&pg=PA115&dq=raster vector graphics photographic&lr=&as_brr=0&ei=llOVR5LKCJL0iwGZ8-ywBw&sig=YEjfPOYSUDIf1CUbL5S5Jbzs7M8 }}</ref> Vector graphics consists in encoding information about shapes and colors that comprise the image, which can allow for more flexibility in rendering. There are instances when working with vector tools and formats is best practice, and instances when working with raster tools and formats is best practice. There are times when both formats come together. An understanding of the advantages and limitations of each technology and the relationship between them is most likely to result in efficient and effective use of tools.  === Three-dimensional === [[3D computer graphics]] in contrast to [[2D computer graphics]] are graphics that use a [[Cartesian coordinate system#Cartesian coordinates in three dimensions|three-dimensional]] representation of geometric data that is stored in the computer for the purposes of performing calculations and rendering 2D images. Such images may be for later display or for real-time viewing.  Despite these differences, 3D computer graphics rely on many of the same [[algorithm]]s as 2D computer [[vector graphics]] in the [[wire frame model]] and 2D computer [[raster graphics]] in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques.  3D computer graphics are often referred to as [[3D models]]. Apart from the rendered graphic, the model is contained within the graphical data file. However, there are differences. A 3D model is the [[Mathematics|mathematical]] representation of any [[Three-dimensional space|three-dimensional]] object. A model is not technically a graphic until it is visually displayed. Due to [[3D printing]], 3D models are not confined to virtual space. A model can be displayed visually as a two-dimensional image through a process called ''[[3D rendering]],'' or used in non-graphical [[computer simulation]]s and calculations. There are some [[3D computer graphics software]] for users to create 3D images e.g. autocad, photoshop,solidwork,google sketchup etc.  === Computer animation === [[image:Activemarker2.PNG|right|thumb|Example of [[Computer animation]] produced using [[Motion capture]]]] [[File:FractalLandscape.jpg|thumb|right|[[Fractal landscape]], an example of [[computer-generated imagery]]. .]] [[Computer animation]] is the art of creating moving images via the use of [[computer]]s. It is a subfield of computer graphics and [[animation]]. Increasingly it is created by means of [[3D computer graphics]], though [[2D computer graphics]] are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another [[Recording medium|medium]], such as [[film]]. It is also referred to as CGI ([[Computer-generated imagery]] or computer-generated imaging), especially when used in films.  Virtual entities may contain and be controlled by assorted attributes, such as transform values (location, orientation, and scale) stored in an object's [[transformation matrix]]. Animation is the change of an attribute over time. Multiple methods of achieving animation exist; the rudimentary form is based on the creation and editing of [[keyframe]]s, each storing a value at a given time, per attribute to be animated. The 2D/3D graphics software will [[interpolation|interpolate]] between keyframes, creating an editable curve of a value mapped over time, resulting in animation. Other methods of animation include [[procedural animation|procedural]] and [[Expression (mathematics)|expression]]-based techniques: the former consolidates related elements of animated entities into sets of attributes, useful for creating [[particle system|particle]] effects and [[crowd simulation]]s; the latter allows an evaluated result returned from a user-defined logical expression, coupled with mathematics, to automate animation in a predictable way (convenient for controlling bone behavior beyond what a [[hierarchy]] offers in [[skeletal animation|skeletal system]] set up).  To create the illusion of movement, an image is displayed on the computer [[computer display|screen]] then quickly replaced by a new image that is similar to the previous image, but shifted slightly. This technique is identical to the illusion of movement in [[television]] and [[film|motion pictures]].  == Concepts and principles == Images are typically produced by [[optics|optical]] devices;such as [[camera]]s, [[mirror]]s, [[Lens (optics)|lenses]], [[telescope]]s, [[microscope]]s, etc. and natural objects and phenomena, such as the [[human eye]] or water surfaces.  A [[digital image]] is a representation of a two-dimensional [[image]] in binary format as a sequence of ones and zeros. Digital images include both [[vector graphics|vector]] images and [[raster graphics|raster]] images, but raster images are more commonly used.  === Pixel === [[Image:Pixel-example.png|thumb|right|In the enlarged portion of the image individual pixels are rendered as squares and can be easily seen.]] In digital imaging, a [[pixel]] (or picture element<ref>{{Cite book   | author=Rudolf F. Graf | year=1999 | publisher=Newnes   | title=Modern Dictionary of Electronics | location=Oxford   | isbn=0-7506-4331-5|page=569|url=http://books.google.com/<!--   -->books?id=o2I1JWPpdusC&pg=PA569&dq=pixel intitle:<!--   -->%22Modern Dictionary of Electronics%22 inauthor:graf&<!--   -->lr=&as_brr=0&ei=5ygASM3qHoSgiwH45-GIDA&<!--   -->sig=7tg-LuGdu6Njypaawi2bbkeq8pw}}</ref>) is a single point in a [[raster image]]. Pixels are normally arranged in a regular 2-dimensional grid, and are often represented using dots or squares. Each pixel is a [[sample (signal)|sample]] of an original image, where more samples typically provide a more accurate representation of the original. The [[Intensity (physics)|intensity]] of each pixel is variable; in color systems, each pixel has typically three components such as [[RGB color model|red, green, and blue]].  === Graphics === [[Graphics]] are [[visual]] presentations on some surface, such as a wall, [[canvas]], computer screen, paper, or stone to [[brand]], inform, illustrate, or entertain. Examples are [[photograph]]s, [[drawings]], [[line art]], [[graphics|graphs]], [[diagrams]], [[typography]], [[number]]s, [[symbols]], [[geometric]] designs, [[maps]], [[engineering drawings]], or other [[image]]s. Graphics often combine [[character (computer)|text]], [[illustration]], and [[color]]. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style.  === Rendering === Rendering is the process of generating an image from a [[3D model|model]] (or models in what collectively could be called a ''scene'' file), by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, [[texture mapping|texture]], [[lighting]], and [[shading]] information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a [[digital image]] or [[raster graphics]] image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term "rendering" may be by analogy with an "artist's rendering" of a scene. Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the [[graphics pipeline]] along a rendering device, such as a [[Graphics processing unit|GPU]]. A GPU is a purpose-built device able to assist a [[Central processing unit|CPU]] in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the [[rendering equation]]. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output.  ;3D projection :[[3D projection]] is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread, especially in computer graphics, engineering and drafting.  ;Ray tracing :[[Ray tracing (graphics)|Ray tracing]] is a technique for generating an [[digital image|image]] by tracing the path of [[light]] through [[pixel]]s in an [[image plane]]. The technique is capable of producing a very high degree of [[photorealism]]; usually higher than that of typical [[scanline rendering]] methods, but at a greater [[computation time|computational cost]].  ;Shading [[Image:Shading1.jpg|thumb|right|Example of shading.]] :[[Shading]] refers to [[wikt:depicting|depicting]] depth in [[3D model]]s or illustrations by varying levels of [[darkness]]. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including [[Hatching|cross hatching]] where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that [[shader]]s are applied.  ;Texture mapping :[[Texture mapping]] is a method for adding detail, surface texture, or colour to a [[computer-generated imagery|computer-generated graphic]] or [[3D model]]. Its application to 3D graphics was pioneered by Dr [[Edwin Catmull]] in 1974. A texture map is applied (mapped) to the surface of a shape, or polygon. This process is akin to applying patterned paper to a plain white box. Multitexturing is the use of more than one texture at a time on a polygon.<ref>Blythe, David. ''[http://www.opengl.org/resources/code/samples/sig99/advanced99/notes/notes.html Advanced Graphics Programming Techniques Using OpenGL].'' Siggraph 1999. (see: [http://www.opengl.org/resources/code/samples/sig99/advanced99/notes/node60.html Multitexture])</ref> [[Procedural texture]]s (created from adjusting parameters of an underlying algorithm that produces an output texture), and [[bitmap|bitmap textures]] (created in an [[image editing]] application or imported from a [[digital camera]]) are, generally speaking, common methods of implementing texture definition on 3D models in computer graphics software, while intended placement of textures onto a model's surface often requires a technique known as [[UV mapping]] (arbitrary, manual layout of texture coordinates) for [[polygon mesh|polygon surfaces]], while [[NURBS|NURBS surfaces]] have their own intrinsic [[parameterization]] used as texture coordinates.  ;Anti-aliasing :Rendering resolution-independent entities (such as 3D models) for viewing on a raster (pixel-based) device such as a [[Lcd|LCD display]] or [[Cathode ray tube|CRT television]] inevitably causes [[aliasing|aliasing artifacts]] mostly along geometric edges and the boundaries of texture details; these artifacts are informally called "[[jaggies]]". Anti-aliasing methods rectify such problems, resulting in imagery more pleasing to the viewer, but can be somewhat computationally expensive. Various anti-aliasing algorithms (such as [[supersampling]]) are able to be employed, then customized for the most efficient rendering performance versus quality of the resultant imagery; a graphics artist should consider this trade-off if anti-aliasing methods are to be used. A pre-anti-aliased [[bitmap|bitmap texture]] being displayed on a screen (or screen location) at a resolution different than the resolution of the texture itself (such as a textured model in the distance from the virtual camera) will exhibit aliasing artifacts, while any [[Procedural texture|procedurally defined texture]] will always show aliasing artifacts as they are resolution-independent; techniques such as [[mipmapping]] and [[texture filtering]] help to solve texture-related aliasing problems.  === Volume rendering === [[Image:CTWristImage.png|thumb|Volume rendered [[Computed tomography|CT]] scan of a forearm with different colour schemes for muscle, fat, bone, and blood.]] [[Volume rendering]] is a technique used to display a [[3D projection|2D projection]] of a 3D discretely [[Sampling (signal processing)|sampled]] [[data set]]. A typical 3D data set is a group of 2D slice images acquired by a [[computed axial tomography|CT]] or [[magnetic resonance imaging|MRI]] scanner.  Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image [[pixel]]s in a regular pattern. This is an example of a regular volumetric grid, with each volume element, or [[voxel]] represented by a single value that is obtained by sampling the immediate area surrounding the voxel.  === 3D modeling === [[3D modeling]] is the process of developing a mathematical, [[wire frame model|wireframe]] representation of any three-dimensional object, called a "3D model", via specialized software. Models may be created automatically or manually; the manual modeling process of preparing geometric data for 3D computer graphics is similar to [[plastic arts]] such as [[sculpting]]. 3D models may be created using multiple approaches: use of [[Nonuniform rational B-spline|NURBS]] curves to generate accurate and smooth surface patches, [[Polygonal modeling|polygonal mesh modeling]] (manipulation of faceted geometry), or polygonal mesh [[Subdivision surface|subdivision]] (advanced tessellation of polygons, resulting in smooth surfaces similar to NURBS models). A 3D model can be displayed as a two-dimensional image through a process called ''[[3D rendering]]'', used in a [[computer]] [[simulation]] of physical phenomena, or animated directly for other purposes. The model can also be physically created using [[3D Printing]] devices.  == Pioneers in graphic design == <!-- this is just a first selection. --> ;Charles Csuri :[[Charles Csuri]] is a pioneer in computer animation and digital fine art and created the first computer art in 1964. Csuri was recognized by ''[[Smithsonian (magazine)|Smithsonian]]'' as the father of digital art and computer animation, and as a pioneer of computer animation by the [[Museum of Modern Art]] (MoMA) and [[Association for Computing Machinery]]-[[SIGGRAPH]].  ;Donald P. Greenberg :[[Donald P. Greenberg]] is a leading innovator in computer graphics. Greenberg has authored hundreds of articles and served as a teacher and mentor to many prominent computer graphic artists, animators, and researchers such as [[Robert L. Cook]], [[Marc Levoy]], and [[Wayne Lytle]]. Many of his former students have won Academy Awards for technical achievements and several have won the [[SIGGRAPH]] Achievement Award. Greenberg was the founding director of the NSF Center for Computer Graphics and Scientific Visualization.  ;Aaron Marcus :[[Aaron Marcus]] is one of the first graphic designer in the world to work with computer graphics. He has written over 250 articles and written/co-written six books. He has published, lectured, tutored, and consulted internationally for more than 40 years and has been an invited keynote/plenary speaker at conferences of ACM/SIGCHI, ACM/SIGGRAPH, Usability Professionals Association (UPA). He was was named an AIGA Fellow in 2007 and was elected in 2008 to the [[CHI Academy]]. He is the founder of [[Aaron Marcus and Associates, Inc.]], a pioneering, world-renowned design firm specializing in user-interface/user-experience development applications.  ;A. Michael Noll :[[A. Michael Noll|Noll]] was one of the first researchers to use a [[digital]] [[computer]] to create artistic patterns and to formalize the use of random processes in the creation of [[visual arts]]. He began creating digital computer art in 1962, making him one of the earliest digital computer artists. In 1965, Noll along with Frieder Nake and Georg Nees were the first to publicly exhibit their computer art. During April 1965, the Howard Wise Gallery exhibited Noll's computer art along with random-dot patterns by [[Bela Julesz]].  [[Image:utah teapot simple 2.png|thumb|A modern render of the [[Utah teapot]], an iconic model in 3D computer graphics created by [[Martin Newell (computer scientist)|Martin Newell]], 1975.]] ;Other pioneers * [[Jim Blinn]] * [[Arambilet]] * [[Benoît B. Mandelbrot]] * [[Henri Gouraud (computer scientist)|Henri Gouraud]] * [[Bui Tuong Phong]] * [[Pierre Bézier]] * [[Paul de Casteljau]] * [[Daniel J. Sandin]] * [[Alvy Ray Smith]] * [[Ton Roosendaal]] * [[Ivan Sutherland]] * [[Steve Russell]] * [[Ed Catmull]] * [[Fred Parke]]  == Study of computer graphics == The [[Computer graphics (computer science)|study of computer graphics]] is a sub-field of [[computer science]] which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and [[image processing]].  As an [[academic]] discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the ''mathematical'' and ''computational'' foundations of image generation and processing rather than purely [[aesthetic]] issues. Computer graphics is often differentiated from the field of [[visualization (graphic)|visualization]], although the two fields have many similarities.  == Applications == {{Portal|Computer graphics|Computer Science}} Computer graphics may be used in the following areas: * [[Computational biology]] * [[Computational physics]] * [[Computer-aided design]] * [[Computer simulation]] * [[Digital art]] * [[Education]] * [[Graphic design]] * [[Infographics]] * [[Information visualization]] * [[Rational drug design]] * [[Scientific visualization]] * [[Video Games]] * [[Virtual reality]] * [[Web design]]  == References == {{Reflist}}  == Further reading == <!-- This section just gives a first impression of the many books in this field. Please do not just add another book here. --> * David Rogers (1998). ''Procedural Elements for Computer Graphics''. McGraw-Hill. * James D. Foley, Andries Van Dam, Steven K. Feiner and John F. Hughes (1995). ''Computer Graphics: Principles and Practice''. Addison-Wesley * Donald Hearn and M. Pauline Baker (1994). ''Computer Graphics''. Prentice-Hall. * Francis S. Hill (2001). ''Computer Graphics''. Prentice Hall. * John Lewell (1985). ''Computer Graphics: A Survey of Current Techniques and Applications''. Van Nostrand Reinhold. * Jeffrey J. McConnell (2006). ''Computer Graphics: Theory Into Practice''. Jones & Bartlett Publishers. * R. D. Parslow, R. W. Prowse, Richard Elliot Green (1969). ''Computer Graphics: Techniques and Applications''. * Peter Shirley and others. (2005). ''Fundamentals of computer graphics''. A.K. Peters, Ltd. * M. Slater, A. Steed, Y. Chrysantho (2002). ''Computer graphics and virtual environments: from realism to real-time''. Addison-Wesley  ==External links== {{Commons category|Computer graphics}} {{Wiktionary|computer graphics}} *[http://accad.osu.edu/~waynec/history/lessons.html A Critical History of Computer Graphics and Animation] *[http://hem.passagen.se/des/hocg/hocg_1960.htm ''History of Computer Graphics'' series of articles] *[http://www.mixamo.com/c/resources ''CG Resources and Tools'' resource finder for 2d and 3d CG] *[http://www.cg101.com ''CG101 A Computer Graphics Industry Reference'' (2nd Edition)]  {{visualization}} {{Technology}}  {{DEFAULTSORT:Computer Graphics}} [[Category:Computer graphics|*]]  {{Link FA|de}} [[ar:رسوميات حاسوبية]] [[az:Kompüter qrafikası]] [[bn:কম্পিউটার গ্রাফিক্‌স]] [[bg:Компютърна графика]] [[bs:Računarska grafika]] [[ca:Infografia]] [[cs:Počítačová grafika]] [[da:Computergrafik]] [[de:Computergrafik]] [[el:Γραφικά υπολογιστών]] [[es:Computación gráfica]] [[eo:Komputila grafiko]] [[fa:گرافیک رایانه‌ای]] [[fr:Infographie]] [[ko:컴퓨터 그래픽스]] [[hi:कंप्यूटर ग्राफिक्स]] [[hr:Računalna grafika]] [[id:Grafika komputer]] [[it:Computer grafica]] [[he:גרפיקה ממוחשבת]] [[lt:Kompiuterinė grafika]] [[hu:Számítógépes grafika]] [[nl:Computergraphics]] [[ja:コンピュータグラフィックス]] [[no:Datagrafikk]] [[pl:Grafika komputerowa]] [[pt:Computação gráfica]] [[ro:Grafică computerizată]] [[ru:Компьютерная графика]] [[scn:Computer Grafica]] [[simple:Computer graphics]] [[sk:Počítačová grafika]] [[sr:Рачунарска графика]] [[fi:Tietokonegrafiikka]] [[sv:Datorgrafik]] [[ta:கணினி வரைகலை]] [[tk:Kompýuter grafikasy]] [[uk:Комп'ютерна графіка]] [[ur:شمارندی تخطیط]] [[vi:Đồ họa máy tính]] [[zh:计算机图形]]
{{Unreferenced stub|auto=yes|date=December 2009}} '''Computer industry''' is a collective term used to describe the whole range of businesses involved in developing [[computer software]], designing [[computer hardware]] and [[computer networking]] infrastructures, the manufacture of [[computer]] components and the provision of [[information technology]] services.  ==See also== *[[Consumer electronics]] *[[Independent hardware vendor]] *[[Independent software vendor]] *[[Information technology]] (IT) *[[Original equipment manufacturer]] (OEM) *[[Software developer]] *[[Software industry]] *[[Hardware industry]] {{DEFAULTSORT:Computer Industry}} [[Category:Computer industry| ]]    {{Industry-stub}}  [[ar:صناعة الحاسوب]] [[ca:Indústria informàtica]] [[es:Industria de la computación]] [[ko:컴퓨터 산업]] [[zh:資訊業]]
{{Redirect|Computer networks|the periodical|Computer Networks (journal)}} {{redirect|Datacom|other uses|Datacom (disambiguation)}} {{Network Science}} A '''computer network''', or simply a '''network''', is a collection of [[computers]] and other [[Networking hardware|hardware]] components interconnected by communication channels that allow sharing of resources and information.<ref>{{citation |url=http://www.atis.org/glossary/definition.aspx?id=6555 |title=Computer network definition |accessdate=2011-11-12}}</ref> Where at least one process in one device is able to send/receive data to/from at least one process residing in a remote device, then the two devices are said to be in a network. Simply, more than one computer interconnected through a communication medium for information interchange is called a computer network.  Networks may be classified according to a wide variety of characteristics, such as the medium used to transport the data, [[communications protocol]] used, scale, [[Network topology|topology]], and organizational scope.  Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for [[computer network programming|network programming]]. Well-known communications protocols include [[Ethernet]], a hardware and [[link layer]] standard that is ubiquitous in [[local area network]]s, and the [[Internet protocol suite]], which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, as well as host-to-host data transfer, and application-specific data transmission formats.  Computer networking is sometimes considered a sub-discipline of [[electrical engineering]], [[telecommunications]], [[computer science]], [[information technology]] or [[computer engineering]], since it relies upon the theoretical and practical application of these disciplines.  ==History== [[File:Distributed Processing.jpg|thumb|Distributed processing]] {{Expand section|date=December 2010}} Before the advent of computer networks that were based upon some type of telecommunications system, communication between calculation machines and early computers was performed by human users by carrying instructions between them. Many of the social behaviors seen in today's Internet were demonstrably present in the 19th century and arguably in even earlier networks using visual signals.  * In September 1940, [[George Stibitz]] used a Teletype machine to send instructions for a problem set from his Model at [[Dartmouth College]] to his [[Complex Number Calculator]] in New York and received results back by the same means. Linking output systems like teletypewriters to computers was an interest at the [[Advanced Research Projects Agency]] (ARPA) when, in 1962, [[J.C.R. Licklider]] was hired and developed a working group he called the "[[Intergalactic Computer Network]]", a precursor to the [[ARPANET]]. * Early networks of communicating computers included the military radar system [[Semi-Automatic Ground Environment]] (SAGE), started in the late 1950s. * The commercial airline reservation system [[semi-automatic business research environment]] (SABRE) went online with two connected mainframes in 1960.<ref> {{cite book | title = On the way to the web: the secret history of the internet and its founders | author = Michael A. Banks | publisher = Apress | year = 2008 | isbn = 978-1-4302-0869-3 | page = 1 | url = http://books.google.com/books?id=P9wbSjO9WMMC&pg=PA1 }}</ref><ref> {{cite book | title = History of the Internet: a chronology, 1843 to the present | author = Christos J. P. Moschovitis | publisher = ABC-CLIO | year = 1998 | isbn = 978-1-57607-118-2 | page = 36 | url = http://books.google.com/?id=Hu5SAAAAMAAJ&dq=intitle%3A%22history of the internet%22 sage sabre&q=sage sabre%27s#search_anchor }}</ref> * In 1964, researchers at Dartmouth developed the [[Dartmouth Time Sharing System]] for distributed users of large computer systems. The same year, at [[Massachusetts Institute of Technology]], a research group supported by [[General Electric]] and [[Bell Labs]] used a computer to route and manage telephone connections. * Throughout the 1960s [[Leonard Kleinrock]], [[Paul Baran]] and [[Donald Davies]] independently conceptualized and developed network systems which used [[packet (information technology)|packets]] that could be used in a network between computer systems. * 1965 [[Thomas Marill (scientist)|Thomas Marill]] and [[Lawrence G. Roberts]] created the first [[wide area network]] (WAN). This was an immediate precursor to the [[ARPANET]], of which Roberts became program manager. * The first widely used [[telephone switch]] that used true computer control was introduced by [[Western Electric]] in 1965. * In 1969 the [[University of California at Los Angeles]], the [[Stanford Research Institute]], [[University of California at Santa Barbara]], and the [[University of Utah]] were connected as the beginning of the [[ARPANET]] network using 50 kbit/s circuits.<ref>{{cite web |title=Internet Began 35 Years Ago at UCLA with First Message Ever Sent Between Two Computers |url=http://www.engineer.ucla.edu/stories/2004/Internet35.htm |author=Chris Sutton|publisher=''[[UCLA]]''|archiveurl=http://web.archive.org/web/20080308120314/http://www.engineer.ucla.edu/stories/2004/Internet35.htm |archivedate=March 8, 2008}}</ref> * Commercial services using [[X.25]] were deployed in 1972, and later used as an underlying infrastructure for expanding [[TCP/IP]] networks.  Today, computer networks are the core of modern communication. All modern aspects of the [[public switched telephone network]] (PSTN) are computer-controlled, and telephony increasingly runs over the Internet Protocol, although not necessarily the public Internet. The scope of communication has increased significantly in the past decade, and this boom in communications would not have been possible without the progressively advancing computer network. Computer networks, and the technologies needed to connect and communicate through and between them, continue to drive computer hardware, software, and peripherals industries. This expansion is mirrored by growth in the numbers and types of users of networks, from the researcher to the home user.  ==Properties== Computer networks:  ; Facilitate communications : Using a network, people can communicate efficiently and easily via email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. ; Permit sharing of files, data, and other types of information: In a network environment, authorized users may access data and information stored on other computers on the network. The capability of providing access to data and information on shared storage devices is an important feature of many networks. ; Share network and computing resources: In a networked environment, each computer on a network may access and use resources provided by devices on the network, such as printing a document on a shared network printer. [[Distributed computing]] uses computing resources across a network to accomplish tasks.  ; May be insecure: A computer network may be used by [[Hacker (computer security)|computer hackers]] to deploy [[computer virus]]es or [[computer worm]]s on devices connected to the network, or to prevent these devices from normally accessing the network ([[Denial-of-service attack|denial of service]]). ; May interfere with other technologies: [[Power line communication]] strongly disturbs certain forms of radio communication, e.g., amateur radio.<ref>{{citation |url=http://www.arrl.org/broadband-over-powerline-bpl |publisher=The National Association for Amateur Radio |title=Broadband Over Powerline |accessdate=2011-11-12}}</ref> It may also interfere with [[last mile]] access technologies such as [[ADSL]] and [[VDSL]].<ref>{{cite web|title=The Likelihood and Extent of Radio Frequency Interference from In-Home PLT Devices|url=http://stakeholders.ofcom.org.uk/binaries/research/technology-research/pltreport.pdf|publisher=Ofcom|accessdate=18 June 2011}}</ref> ; May be difficult to set up: A complex computer network may be difficult to set up. It may also be very costly to set up an effective computer network in a large organization or company.  ==Communication media== Computer networks can be classified according to the hardware and associated software technology that is used to interconnect the individual devices in the network, such as [[Cable|electrical cable]] ([[HomePNA]], [[power line communication]], [[G.hn]]), [[optical fiber]], and [[radio waves]] ([[wireless LAN]]). In the [[OSI model]], these are located at levels 1 and 2.  A well-known ''family'' of communication media is collectively known as [[Ethernet]]. It is defined by [[IEEE 802]] and utilizes various standards and media that enable communication between devices. Wireless LAN technology is designed to connect devices without wiring. These devices use [[radio waves]] or [[IrDA|infrared]] signals as a transmission medium.  ===Wired technologies===  The order of the following wired technologies is, roughly, from slowest to fastest transmission speed.  *''[[Twisted pair]] wire'' is the most widely used medium for telecommunication. Twisted-pair cabling consist of copper wires that are twisted into pairs. Ordinary telephone wires consist of two insulated copper wires twisted into pairs. Computer networking cabling (wired [[Ethernet]] as defined by [[IEEE 802.3]]) consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce [[crosstalk (electronics)|crosstalk]] and [[electromagnetic induction]]. The transmission speed ranges from 2 million bits per second to 10 billion bits per second. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios.  *''[[Coaxial cable]]'' is widely used for cable television systems, office buildings, and other work-sites for local area networks. The cables consist of copper or aluminum wire surrounded by an insulating layer (typically a flexible material with a high dielectric constant), which itself is surrounded by a conductive layer. The insulation helps minimize interference and distortion. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.  *[[ITU-T]] [[G.hn]] technology uses existing [[home wiring]] ([[Ethernet over coax|coaxial cable]], phone lines and [[Power line communication|power lines]]) to create a high-speed (up to 1 Gigabit/s) local area network.  *An [[optical fiber]] is a glass fiber. It uses pulses of light to transmit data. Some advantages of optical fibers over metal wires are less transmission loss, immunity from electromagnetic radiation, and very fast transmission speed, up to trillions of bits per second. One can use different colors of lights to increase the number of messages being sent over a fiber optic cable.  ===Wireless technologies=== *''Terrestrial [[microwave]]''&nbsp;– Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low-gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately {{convert|48|km|mi|abbr=on}} apart.  *''Communications [[satellite]]s''&nbsp;– The satellites communicate via  microwave radio waves, which are not deflected by the Earth's atmosphere. The satellites are stationed in space, typically in geosynchronous orbit {{convert|35,400|km|mi|abbr=on}} above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.  *''Cellular and PCS systems'' use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area has a low-power transmitter or radio relay antenna device to relay calls from one area to the next area.  *''Radio and spread spectrum technologies''&nbsp;– Wireless local area network use a high-frequency radio technology similar to digital cellular and a low-frequency radio technology. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. [[IEEE 802.11]] defines a common flavor of open-standards wireless radio-wave technology.  *[[Infrared communication]] can transmit signals for small distances, typically no more than 10 meters. In most cases, [[line-of-sight propagation]] is used, which limits the physical positioning of communicating devices.  *A [[global area network]] (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial [[wireless LAN]]s.<ref>{{cite web |url=http://grouper.ieee.org/groups/802/20/ |title=Mobile Broadband Wireless connections (MBWA) |accessdate=2011-11-12}}</ref>  ===Exotic technologies=== There have been various attempts at transporting data over more or less exotic media:  * [[IP over Avian Carriers]] was a humorous April fool's [[Request for Comments]], issued as '''RFC 1149'''. It was implemented in real life in 2001.<ref>[http://www.blug.linux.no/rfc1149 Bergen Linux User Group's CPIP Implementation]</ref>  * Extending the Internet to interplanetary dimensions via radio waves.<ref>{{citation |url=http://www.ipnsig.org/reports/ISART9-2000.pdf |title=Interplanetary Internet |publisher=Third Annual International Symposium on Advanced Radio Technologies |author=A. Hooke |date=September 2000 |accessdate=2011-11-12}}</ref>  Both cases have a large [[round-trip delay time]], which prevents useful communication.  ==Communications protocols and network programming== [[File:Internet map 1024.jpg|thumb|[[Internet]] map. The Internet is a global system of interconnected computer networks that use the [[Internet Protocol Suite|standard Internet Protocol Suite]] (TCP/IP) to serve billions of users worldwide.]] {{main|Communications protocol}} A communications protocol is a set of rules for exchanging information over a network. It is typically a [[protocol stack]] (also see the [[OSI model]]), which is a "stack" of protocols, in which each protocol uses the protocol below it. An important example of a protocol stack is [[HTTP]] running over [[transmission control protocol|TCP]] over [[Internet protocol|IP]] over [[IEEE 802.11]] (TCP and IP are members of the [[Internet Protocol Suite]], and IEEE 802.11 is a member of the [[Ethernet]] protocol suite). This stack is used between the [[wireless router]] and the home user's personal computer when the user is surfing the web.  Communication protocols have various properties, such as whether they are [[Connection-oriented communication|connection-oriented]] or [[Connectionless communication|connectionless]], whether they use [[circuit mode]] or [[packet switching]], or whether they use hierarchical or flat addressing.  There are many communication protocols, a few of which are described below.  ===Ethernet=== {{main|Ethernet}} Ethernet is a family of connectionless protocols used in LANs, described by a set of standards together called [[IEEE 802]] published by the [[Institute of Electrical and Electronics Engineers]]. It has a flat addressing scheme and is mostly situated at levels 1 and 2 of the [[OSI model]]. For home users today, the most well-known member of this protocol family is [[IEEE 802.11]], otherwise known as [[Wireless LAN]] (WLAN). However, the complete protocol suite deals with a multitude of networking aspects not only for home use, but especially when the technology is deployed to support a diverse range of business needs. [[Media access control|MAC]] [[Bridging (networking)|bridging]] ([[IEEE 802.1D]]) deals with the routing of Ethernet packets using a [[Spanning Tree Protocol]], [[IEEE 802.1Q]] describes [[Virtual LAN|VLANs]], and [[IEEE 802.1X]] defines a port-based [[Network Access Control]] protocol, which forms the basis for the authentication mechanisms used in VLANs, but it is also found in WLANs&nbsp;– it is what the home user sees when the user has to enter a "wireless access key".  ===Internet Protocol Suite=== The [[Internet Protocol Suite]], often also called TCP/IP, is the foundation of all modern internetworking. It offers connection-less as well as connection-oriented services over an inherently unreliable network traversed by datagram transmission at the [[Internet protocol]] (IP) level. At its core, the protocol suite defines the addressing, identification, and routing specification in form of the traditional [[IPv4|Internet Protocol Version 4]] (IPv4) and IPv6, the next generation of the protocol with a much enlarged addressing capability.  ===SONET/SDH=== {{main|Synchronous optical networking}} Synchronous Optical Networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized [[multiplexing]] protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, [[Circuit switching|circuit-switched]] voice encoded in [[Pulse code modulation|PCM]](Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting [[Asynchronous Transfer Mode]] (ATM) frames.  ===Asynchronous Transfer Mode=== {{main|Asynchronous transfer mode}} Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks.  It uses asynchronous [[time-division multiplexing]] and encodes data into small, fixed-sized [[cell relay|cells]]. This differs from other protocols such as the [[Internet Protocol Suite]] or [[Ethernet]] that use variable sized packets or [[Frame Relay|frames]]. ATM has similarity with both [[Circuit switching|circuit]] and [[Packet switching|packet]] switched networking.  This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, [[Latency (engineering)|low-latency]] content such as voice and video. ATM uses a [[connection-oriented]] model in which a [[virtual circuit]] must be established between two endpoints before the actual data exchange begins.  While the role of ATM is diminishing in favor of [[Next generation network|next-generation networks]], it still plays a role in the [[last mile]], which is the connection between an [[Internet service provider]] and the home user. For an interesting write-up of the technologies involved, including the deep stacking of communications protocols used, see.<ref>{{cite web|last=Martin|first=Thomas|title=Design Principles for DSL-Based Access Solutions|url=http://www.gsi.dit.upm.es/~legf/Varios/XDSL_MARTI.PDF|accessdate=18 June 2011}}</ref>  ===Network programming===  {{main|Computer network programming}} {{main|Network socket}}  [[Computer network programming]] involves writing computer programs that communicate with each other across a computer network. Different programs must be written for the [[client (computing)|client]] process, which initiates the communication, and for the [[server (computing)|server]] process, which waits for the communication to be initiated. Both endpoints of the communication flow are implemented as [[network sockets]]; hence network programming is basically socket programming.  ==Scale== {{Area networks}} Networks are often classified by their physical or organizational extent or their purpose. Usage, trust level, and access rights differ between these types of networks.  ===Personal area network=== A [[personal area network]] (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.<ref>{{cite web |url=http://searchmobilecomputing.techtarget.com/sDefinition/0,,sid40_gci546288,00.html |title=personal area network (PAN) |accessdate=January 29, 2011}}</ref> A wired PAN is usually constructed with USB and Firewire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.  ===Local area network=== A [[local area network]] (LAN) is a network that connects computers and devices in a limited geographical area such as home, school, computer laboratory, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Current wired LANs are most likely to be based on [[Ethernet]] technology, although new standards like [[ITU-T]] [[G.hn]] also provide a way to create a wired LAN using existing home wires (coaxial cables, phone lines and power lines).<ref>{{citation |url=http://www.itu.int/ITU-T/newslog/New Global Standard For Fully Networked Home.aspx |title=New global standard for fully networked home |publisher=ITU-T |accessdate=2011-11-12 |date=2008-12-12}}</ref>  [[File:NETWORK-Library-LAN.png|thumb | Typical library network, in a branching tree topology and controlled access to resources]]  A sample LAN is depicted in the accompanying diagram. All interconnected devices must understand the network layer (layer 3), because they are handling multiple subnets (the different colors). Those inside the library, which have only 10/100 Mbit/s Ethernet connections to the user device and a Gigabit Ethernet connection to the central router, could be called "layer 3 switches" because they only have Ethernet interfaces and must understand [[Internet Protocol|IP]]. It would be more correct to call them access routers, where the router at the top is a distribution router that connects to the Internet and academic networks' customer access routers.  The defining characteristics of LANs, in contrast to WANs (Wide Area Networks), include their higher data transfer rates, smaller geographic range, and no need for leased telecommunication lines. Current Ethernet or other [[IEEE 802.3]] LAN technologies operate at data transfer rates up to 10 Gbit/s. [[IEEE]] has projects investigating the standardization of 40 and 100 Gbit/s.<ref>{{citation |url=http://www.ieee802.org/3/ba/ |title=IEEE P802.3ba 40Gb/s and 100Gb/s Ethernet Task Force |accessdate=2011-11-12}}</ref> LANs can be connected to Wide area network by using routers.  ===Home area network=== A [[home area network]] (HAN) is a residential LAN which is used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or [[Digital Subscriber Line]] (DSL) provider.  ===Storage area network===  A [[storage area network]] (SAN) is a dedicated network that provides access to consolidated, block level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium sized business environments.  ===Campus area network=== A [[campus area network]] (CAN) is a computer network made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, [[Category 5 cable|Cat5]] cabling etc.) are almost entirely owned (by the campus tenant / owner: an enterprise, university, government etc.).  In the case of a university campus-based campus network, the network is likely to link a variety of campus buildings including, for example, academic colleges or departments, the university library, and student residence halls.  ===Backbone network=== A [[backbone network]] is part of a computer network infrastructure that interconnects various pieces of network, providing a path for the exchange of information between different LANs or subnetworks. A backbone can tie together diverse networks in the same building, in different buildings in a campus environment, or over wide areas. Normally, the backbone's capacity is greater than that of the networks connected to it.  A large corporation which has many locations may have a backbone network that ties all of these locations together, for example, if a server cluster needs to be accessed by different departments of a company which are located at different geographical locations. The equipment which ties these departments together constitute the network backbone. [[Network performance management]] including [[network congestion]] are critical parameters taken into account when designing a network backbone.  A specific case of a backbone network is the [[Internet backbone]], which is the set of wide-area network connections and [[core router]]s that interconnect all networks connected to the [[Internet]].  ===Metropolitan area network=== A [[Metropolitan area network]] (MAN) is a large computer network that usually spans a city or a large campus. [[File:EPN Frame-Relay and Dial-up Network.svg|thumb|upright=1.5| Sample EPN made of [[Frame relay]] WAN connections and dialup remote access.]]  [[File:Virtual Private Network overview.svg|thumb|upright=1.5|Sample VPN used to interconnect 3 offices and remote users]]  ===Wide area network=== A [[wide area network]] (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances, using a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often uses transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the [[OSI model|OSI reference model]]: the [[physical layer]], the [[data link layer]], and the [[network layer]].  ===Enterprise private network=== An [[enterprise private network]] is a network built by an enterprise to interconnect various company sites, e.g., production sites, head offices, remote offices, shops, in order to share computer resources.  ===Virtual private network=== A [[virtual private network]] (VPN) is a computer network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.  VPN may have best-effort performance, or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point.  ===Virtual Network=== Not to be confused with a [[Virtual Private Network]], a Virtual Network defines data traffic flows between [[virtual machines]] within a [[hypervisor]] in a virtual computing environment.  Virtual Networks may employ [[virtual security switch]]es, [[virtual router]]s, [[virtual firewall]]s and other virtual networking devices to direct and secure data traffic.  ===Internetwork=== An [[internetwork]] is the connection of multiple computer networks via a common routing technology using routers. The [[Internet]] is an aggregation of many connected internetworks spanning the [[Earth]].  ==Organizational scope== Networks are typically managed by organizations which own them. According to the owner's point of view, networks are seen as intranets or extranets. A special case of network is the [[Internet]], which has no single owner but a distinct status when seen by an organizational entity&nbsp;– that of permitting virtually unlimited global connectivity for a great multitude of purposes.  ===Intranets and extranets=== Intranets and extranets are parts or extensions of a computer network, usually a LAN.  An [[intranet]] is a set of networks, using the [[Internet Protocol]] and IP-based tools such as web browsers and file transfer applications, that is under the control of a single administrative entity. That administrative entity closes the intranet to all but specific, authorized users. Most commonly, an intranet is the internal network of an organization. A large intranet will typically have at least one web server to provide users with organizational information.  An [[extranet]] is a network that is limited in scope to a single organization or entity and also has limited connections to the networks of one or more other usually, but not necessarily, trusted organizations or entities—a company's customers may be given access to some part of its intranet—while at the same time the customers may not be considered ''trusted'' from a security standpoint. Technically, an extranet may also be categorized as a CAN, MAN, WAN, or other type of network, although an extranet cannot consist of a single LAN; it must have at least one connection with an external network.  ===Internet=== The Internet is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the [[Internet Protocol Suite]]. It is the successor of the [[ARPANET|Advanced Research Projects Agency Network]] (ARPANET) developed by [[Defense Advanced Research Projects Agency|DARPA]] of the [[United States Department of Defense]]. The Internet is also the communications backbone underlying the [[World Wide Web]] (WWW).  Participants in the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system ([[IP address]]es) administered by the [[Internet Assigned Numbers Authority]] and [[Regional Internet Registry|address registries]]. Service providers and large enterprises exchange information about the [[routing|reachability]] of their address spaces through the [[Border Gateway Protocol]] (BGP), forming a redundant worldwide mesh of transmission paths.  ==Network topology== ===Common layouts=== A [[network topology]] is the layout of the interconnections of the nodes of a computer network. Common layouts are: * A [[bus network]]: all nodes are connected to a common medium along this medium. This was the layout used in the original [[Ethernet]], called [[10BASE5]] and [[10BASE2]]. * A [[star network]]: all nodes are connected to a special central node. This is the typical layout found in a [[Wireless LAN]], where each wireless client connects to the central [[Wireless access point]]. * A [[ring network]]: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The [[Fiber Distributed Data Interface]] (FDDI) made use of such a topology. * A [[mesh network]]: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other. * A fully connected network: each node is connected to every other node in the network.  Note that the physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with [[FDDI]], the network topology is a ring (actually two counter-rotating rings), but the physical topology is a star, because all neighboring connections are routed via a central physical location.  ===Overlay network=== An [[overlay network]] is a virtual computer network that is built on top of another network. Nodes in the overlay are connected by virtual or logical links, each of which corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. [[File:Network Overlay.svg|thumb|upright=1.5| A sample overlay network: IP over SONET over Optical]] For example, many [[peer-to-peer]] networks are overlay networks because they are organized as nodes of a virtual system of links run on top of the Internet.  The Internet was initially built as an overlay on the [[telephone network]].<ref>{{citation |author1=D. Andersen |author2=H. Balakrishnan |author3=M. Kaashoek |author4=[[Robert Tappan Morris|R. Morris]] |url=http://nms.lcs.mit.edu/papers/ron-sosp2001.html |title=Resilient Overlay Networks] |date=10-2001 |publisher=[[Association for Computing Machinery]] |accessdate=2011-11-12}}</ref>  The most striking example of an overlay network, however, is the Internet itself: At the IP layer, each node can reach any other by a direct connection to the desired IP address, thereby creating a fully connected network; the underlying network, however, is composed of a mesh-like interconnect of subnetworks of varying topologies (and, in fact, technologies). [[Address Resolution Protocol|Address resolution]] and [[routing]] are the means which allows the mapping of the fully connected IP overlay network to the underlying ones.  Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using [[modem]]s, before any data network existed.  Another example of an overlay network is a [[distributed hash table]], which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a [[associative array|map]]) indexed by keys.  Overlay networks have also been proposed as a way to improve Internet routing, such as through [[quality of service]] guarantees to achieve higher-quality [[streaming media]]. Previous proposals such as [[IntServ]], [[DiffServ]], and [[IP Multicast]] have not seen wide acceptance largely because they require modification of all [[Router (computing)|router]]s in the network.{{Citation needed|date=August 2010}}  On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from [[Internet service provider]]s.  The overlay has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes a message traverses before reaching its destination.  For example, [[Akamai Technologies]] manages an overlay network that provides reliable, efficient content delivery (a kind of [[multicast]]).  Academic research includes [http://esm.cs.cmu.edu/ end system multicast] and overcast for multicast; RON ([[resilient overlay network]]) for resilient routing; and OverQoS for quality of service guarantees, among others.  ==Basic hardware components== {{main|Networking hardware}} Apart from the physical communications media themselves as described above, networks comprise additional basic hardware building blocks interconnecting their terminals, such as [[Network interface controller|network interface cards (NICs)]], [[Ethernet hub|hubs]], [[Network bridge|bridges]], [[Network switch|switches]], and [[Router (computing)|router]]s.  ===Network interface cards=== A [[network card]], network adapter, or NIC (network interface card) is a piece of [[computer hardware]] designed to allow computers to physically access a networking medium. It provides a low-level addressing system through the use of [[MAC address]]es.  Each [[Ethernet]] network interface has a unique MAC address which is usually stored in a small memory device on the card, allowing any device to connect to the network without creating an address conflict. Ethernet MAC addresses are composed of six [[Octet (computing)|octets]]. Uniqueness is maintained by the [[Institute of Electrical and Electronics Engineers|IEEE]], which manages the Ethernet address space by assigning 3-octet prefixes to equipment manufacturers. The [http://standards.ieee.org/regauth/oui/oui.txt list of prefixes] is publicly available. Each manufacturer is then obliged to both use only their assigned prefix(es) and to uniquely set the 3-octet suffix of every Ethernet interface they produce.  ===Repeaters and hubs=== A [[repeater]] is an [[Electronics|electronic]] device that receives a [[signal (information theory)|signal]], cleans it of unnecessary noise, regenerates it, and [[retransmission (data networks)|retransmit]]s it at a higher power level, or to the other side of an obstruction, so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. A repeater with multiple ports is known as a [[Network hub|hub]]. Repeaters work on the Physical Layer of the OSI model. Repeaters require a small amount of time to regenerate the signal. This can cause a [[propagation delay]] which can affect network communication when there are several repeaters in a row. Many network architectures limit the number of repeaters that can be used in a row (e.g. Ethernet's [[5-4-3 rule]]).  Today, repeaters and hubs have been made mostly obsolete by switches (see below).  ===Bridges=== A [[network bridge]] connects multiple [[network segment]]s at the [[data link layer]] (layer 2) of the [[OSI model]]. Bridges broadcast to all ports except the port on which the broadcast was received. However, bridges do not promiscuously copy traffic to all ports, as hubs do, but learn which [[MAC Address|MAC addresses]] are reachable through specific ports. Once the bridge associates a port and an address, it will send traffic for that address to that port only.  Bridges learn the association of ports and addresses by examining the source address of frames that it sees on various ports. Once a frame arrives through a port, its source address is stored and the bridge assumes that MAC address is associated with that port. The first time that a previously unknown destination address is seen, the bridge will forward the frame to all ports other than the one on which the frame arrived.  Bridges come in three basic types: *Local bridges: Directly connect LANs * Remote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers. * Wireless bridges: Can be used to join LANs or connect remote stations to LANs.  ===Switches=== A [[network switch]] is a device that forwards and filters [[OSI layer 2]] [[datagrams]] (chunks of data communication) between ports (connected cables) based on the MAC addresses in the packets.<ref> {{cite web |url=http://www.webopedia.com/TERM/s/switch.html |title=Define switch. |publisher=WWW.Wikipedia.com |accessdate=April 8, 2008 }}</ref> A switch is distinct from a hub in that it only forwards the frames to the ports involved in the communication rather than all ports connected. A switch breaks the collision domain but represents itself as a broadcast domain. Switches make forwarding decisions of frames on the basis of MAC addresses. A switch normally has numerous ports, facilitating a star topology for devices, and cascading additional switches.<ref> {{cite web |url=http://networkbits.net/lan-components/local-area-network-lan-basic-components/ |title=Basic Components of a Local Area Network (LAN) |publisher=NetworkBits.net |accessdate=April 8, 2008 }}</ref> Some switches are capable of routing based on Layer 3 addressing or additional logical levels; these are called multi-layer switches. The term ''switch'' is used loosely in marketing to encompass devices including routers and bridges, as well as devices that may distribute traffic on load or by application content (e.g., a Web [[Uniform Resource Locator|URL]] identifier).  ===Routers=== A [[Router (computing)|router]] is an internetworking device that forwards [[Packet (information technology)|packets]] between networks by processing information found in the datagram or packet (Internet protocol information from [[Osi_model#Layer 3: Network layer|Layer 3 of the OSI Model]]).  In many situations, this information is processed in conjunction with the routing table (also known as forwarding table).  Routers use routing tables to determine what interface to forward packets (this can include the "null" also known as the "black hole" interface because data can go into it, however, no further processing is done for said data).  ===Firewalls=== A [[Firewall (computing)|firewall]] is an important aspect of a network with respect to security. It typically rejects access requests from unsafe sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in 'cyber' attacks for the purpose of stealing/corrupting data, planting viruses, etc.'''  ==Network performance==  {{main|network performance}}  '''Network performance''' refers to the [[service quality]] of a telecommunications product as seen by the customer. It should not be seen merely as an attempt to get "more through" the network.  The following list gives examples of Network Performance measures for a circuit-switched network and one type of [[packet-switched network]], viz. ATM:  *Circuit-switched networks: In [[circuit switched]] networks, network performance is synonymous with the [[grade of service]]. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads.<ref>{{citation |url=http://www.com.dtu.dk/teletraffic/handbook/telenook.pdf |archiveurl=http://web.archive.org/web/20070111015452/http://oldwww.com.dtu.dk/teletraffic/handbook/telenook.pdf |archivedate=2007-01-11 |publisher=ITU-T Study Group 2 |title=Teletraffic Engineering Handbook}}</ref> Other types of performance measures can include noise, echo and so on.  *ATM: In an [[Asynchronous Transfer Mode]] (ATM) network, performance can be measured by line rate, [[quality of service]] (QoS), data throughput, connect time, stability, technology, modulation technique and modem enhancements.<ref>[http://www.telecommagazine.com Telecommunications Magazine Online], Americas January 2003, Issue Highlights, Online Exclusive: Broadband Access Maximum Performance, Retrieved on February 13, 2005.</ref>  There are many different ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modelled instead of measured; one example of this is using state transition diagrams to model queuing performance in a circuit-switched network. These diagrams allow the network planner to analyze how the network will perform in each state, ensuring that the network will be optimally designed.<ref>{{cite web|url=http://cne.gmu.edu/modules/os_perf/std.t.html |title = State Transition Diagrams |accessdate =July 13, 2003}}</ref>  ==Network security==  {{main|network security}}  In the field of networking, the area of '''network security'''<ref>{{cite journal | doi = 10.1007/978-3-540-30176-9_41 | last = Simmonds | first = A | coauthors = Sandilands, P; van Ekert, L|title = An Ontology for Network Security Attacks | journal = Lecture Notes in Computer Science | volume = 3285 | pages = 317–323 | year = 2004 | series = Lecture Notes in Computer Science | isbn = 978-3-540-23659-7 }}</ref> consists of the provisions and [[policies]] adopted by the [[network administrator]] to prevent and monitor [[unauthorized]] access, misuse, modification, or denial of the computer network and network-accessible resources. Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority.  Network Security covers a variety of computer networks, both public and private that are used in everyday jobs conducting transactions and communications among businesses, government agencies and individuals.  ==Network resilience==  {{main|resilience (network)}}  In computer networking: “'''Resilience''' is the ability to provide and maintain an acceptable level of [[Service (systems architecture)|service]] in the face of  [[Fault (technology)|faults]] and challenges to normal operation.”<ref>{{cite web |url=http://wiki.ittc.ku.edu/resilinets_wiki/index.php/Definitions#Resilience |publisher=ResiliNets Research Initiative |title=Definitions: Resilience |accessdate=2011-11-12}}</ref>  ==Views of networks==  Users and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running.  A [[community-of-interest network|community of interest]] has less of a connection of being in a local area, and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via [[peer-to-peer]] technologies.  Network administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., [[Router (computing)|router]]s, [[Network bridge|bridges]] and [[Application-level gateway|application layer gateways]]) that interconnect the physical media. Logical networks, called, in the TCP/IP architecture, [[subnetwork|subnets]], map onto one or more physical media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using [[Virtual LAN| virtual LAN (VLAN)]] technology.  Both users and administrators will be aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an [[intranet]] is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).<ref name="RFC2547">RFC 2547</ref>  Intranets do not have to be connected to the Internet, but generally have a limited connection.  An [[extranet]] is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).<ref name="RFC2547"/>  Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by [[Internet Service Providers]] (ISP). From an engineering viewpoint, the [[Internet]] is the set of subnets, and aggregates of subnets, which share the registered [[IP address]] space and exchange information about the reachability of those IP addresses using the [[Border Gateway Protocol]]. Typically, the [[human-readable]] names of servers are translated to IP addresses, transparently to users, via the directory function of the [[Domain Name System]] (DNS).  Over the Internet, there can be [[Business-to-business| business-to-business (B2B)]], [[Business-to-consumer| business-to-consumer (B2C)]] and [[Consumer-to-consumer electronic commerce|consumer-to-consumer (C2C)]] communications. Especially when money or sensitive information is exchanged, the communications are apt to be '''secured''' by some form of [[communications security]] mechanism.  Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure [[Virtual Private Network]] (VPN) technology.  ==See also== *[[Comparison of network diagram software]] *[[Network topology]] {{portal|Computer networking|Computer Science|Computing}} {{-}}  ==References== {{Reflist|30em}} {{FS1037C}}  ==Further reading== * Shelly, Gary, et al. "Discovering Computers" 2003 Edition * Cisco Systems, Inc., (2003, March 14). CCNA: network media types. Retrieved from [http://www.ciscopress.com/articles/article.asp?p=31276&rll=1 ciscopress.com] * Wendell Odom,Rus Healy, Denise Donohue. (2010) CCIE Routing and Switching. Indianapolis, IN: Cisco Press * Kurose James F and Keith W. Ross : Computer Networking: A Top-Down Approach Featuring the Internet, Pearson Education 2005. * [[Andrew S. Tanenbaum]], ''Computer Networks'', Fourth Edition, Pearson Education 2006 (ISBN 0-13-349945-6). * [[William Stallings]], ''Computer Networking with Internet Protocols and Technology'', Pearson Education 2004. * [[List of important publications in computer science#Computer networks|Important publications in computer networks]] * [[Vinton G. Cerf]] [http://www.cs.washington.edu/homes/lazowska/cra/networks.html "Software: Global Infrastructure for the 21st Century"] * Meyers, Mike, "Mike Meyers' Certification Passport: Network " ISBN 0-07-225348-7" * Odom, Wendall, "CCNA Certification Guide" * Network Communication Architecture and Protocols: OSI Network Architecture 7 Layers Model  ==External links== * [http://www.netfilter.org/documentation/HOWTO/networking-concepts-HOWTO.html Easy Network Concepts] (Linux kernel specific) * [http://nsgn.net/osi_reference_model/ Computer Networks and Protocol] (Research document, 2006) * [http://compnetworking.about.com/od/basicnetworkingconcepts/l/blglossary.htm Computer Networking Glossary] * {{dmoz|Computers/Software/Networking/|Networking}}  {{Telecommunications}} {{Operating System}} {{Technology}}  {{DEFAULTSORT:Computer Network}} [[Category:Computer networks| ]] [[Category:Computer networking| ]] [[Category:Telecommunications engineering]]  [[af:Rekenaarnetwerk]] [[am:የኮምፒዩተር አውታር]] [[ar:شبكة حاسوب]] [[as:কম্পিউটাৰ নেটৱৰ্ক]] [[az:Kompyuter şəbəkəsi]] [[bn:কম্পিউটার নেটওয়ার্ক]] [[zh-min-nan:Tiān-náu bāng-lō͘]] [[be:Камп'ютарная сетка]] [[be-x-old:Кампутарная сетка]] [[bg:Компютърна мрежа]] [[bs:Računarske mreže]] [[br:Rouedad stlennegel]] [[ca:Xarxa informàtica]] [[cs:Počítačová síť]] [[da:Datanet]] [[de:Rechnernetz]] [[et:Arvutivõrk]] [[el:Δίκτυο υπολογιστών]] [[es:Red de computadoras]] [[eo:Komputila reto]] [[eu:Konputagailu-sare]] [[fa:شبکه رایانه‌ای]] [[fr:Réseau informatique]] [[ga:Líonra ríomhaireachta]] [[gl:Rede de computadores]] [[ko:컴퓨터 네트워크]] [[hy:Համակարգչային ցանցեր]] [[hi:नेटवर्क]] [[hr:Računalne mreže]] [[id:Jaringan komputer]] [[ia:Rete de computatores]] [[is:Tölvunet]] [[it:Rete di calcolatori]] [[he:תקשורת נתונים]] [[ka:გამოთვლითი ქსელი]] [[kk:Компьютер желі]] [[sw:Mtandao wa kompyuta]] [[ku:Tor (komputer)]] [[ky:ТАРМАК (КОМПЬЮТЕР ТАРМАГЫ)]] [[lv:Datortīkls]] [[lb:Computernetzwierk]] [[lt:Kompiuterių tinklas]] [[li:Computernètwerk]] [[hu:Számítógép-hálózat]] [[mk:Сметачка мрежа]] [[ml:കമ്പ്യൂട്ടർ നെറ്റ്‌വർക്ക്]] [[xmf:გჷშაკოროცხუაშური რშვილი]] [[ms:Rangkaian komputer]] [[mn:Компьютерийн сүлжээ]] [[my:ကွန်ပျူတာ ကွန်ယက်]] [[nl:Computernetwerk]] [[ja:コンピュータネットワーク]] [[no:Datanett]] [[nn:Datanett]] [[mhr:Компьютер вапш]] [[ps:سولګریز جال]] [[nds:Reeknernettwark]] [[pl:Sieć komputerowa]] [[pt:Rede de computadores]] [[ro:Rețea de calculatoare]] [[qu:Antañiqiq llika]] [[ru:Компьютерная сеть]] [[sq:Rrjeti kompjuterik]] [[si:පරිගණක ජාල]] [[simple:Computer network]] [[sk:Počítačová sieť]] [[sl:Omreževanje]] [[ckb:تۆڕی کۆمپیوتەری]] [[sr:Računarska mreža]] [[sh:Računalne mreže]] [[fi:Tietokoneverkko]] [[sv:Datornätverk]] [[tl:Network ng kompyuter]] [[ta:கணினி வலையமைப்பு]] [[th:เครือข่ายคอมพิวเตอร์]] [[tr:Bilgisayar ağı]] [[uk:Комунікаційна мережа]] [[ur:شمارندی جالکار]] [[vi:Mạng máy tính]] [[yi:קאמפיוטער נעצווערק]] [[zh:计算机网络]]
{{About|computer model within a scientific context|artistic usage|3d modeling|simulating a computer on a computer|emulator}} {{More footnotes|date=May 2008}}  [[File:Typhoon Mawar 2005 computer simulation thumbnail.gif|400px|thumb|A 48 hour computer simulation of [[Typhoon Mawar]] using the [[Weather Research and Forecasting model]] ]]  A '''computer simulation''', a '''computer model''', or a '''computational model''' is a [[computer program]], or network of computers, that attempts to [[simulation|simulate]] an abstract [[model (abstract)|model]] of a particular system. Computer simulations have become a useful part of [[mathematical model]]ing of many natural systems in [[physics]] ([[computational physics]]), [[astrophysics]], [[chemistry]] and [[biology]], human systems in [[economics]], [[psychology]], [[social science]], and [[engineering]]. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new [[technology]], and to estimate the performance of systems too complex for [[analytical solution]]s.<ref>{{Cite book   | last =Strogatz   | first =Steven   | contribution =The End of Insight   | year =2007   | title =What is your dangerous idea?   | editor-last =Brockman   | editor-first =John   | publisher =HarperCollins   | issn =9780061214950   | postscript =<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}} }}</ref>  Computer simulations vary from computer programs that run a few minutes, to network-based groups of computers running for hours, to ongoing simulations that run for days. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation, of one force invading another, involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around [[Kuwait]], using multiple supercomputers in the [[United States Department of Defense|DoD]] High Performance Computer Modernization Program<ref name="JPLsim">" [http://www.jpl.nasa.gov/releases/97/military.html "Researchers stage largest Military Simulation ever"], [[Jet Propulsion Laboratory]], [[Caltech]], December 1997, </ref> Other examples include a 1-billion-atom model of material deformation [http://domino.watson.ibm.com/comm/pr.nsf/pages/news.20020429_fracture_simulation.html (2002)]; a 2.64-million-atom model of the complex maker of protein in all organisms, a [[ribosome]], in 2005;<ref name="LANLsim">    "Largest computational biology simulation mimics life's    most essential nanomachine" (news), News Release,    Nancy Ambrosiano, [[Los Alamos National Laboratory]],    Los Alamos, NM, October 2005, webpage:    [http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/7428 LANL-Fuse-story7428].</ref> and the [[Blue Brain]] project at [[EPFL]] (Switzerland), begun in May 2005, to create the first computer simulation of the entire human brain, right down to the molecular level.<ref name="Brainsim">[http://www.newscientist.com/article/dn7470.html "Mission to build a simulated brain begins"], project of the institute at the [[École Polytechnique Fédérale de Lausanne]] (EPFL), Switzerland, ''[[New Scientist]]'', June 2005. </ref>  ==Simulation versus model==  A computer model refers to the algorithms and equations used to capture the behavior of the system being modeled. However, a computer simulation refers to the actual running of the program which contains these equations or algorithms. Simulation, therefore, refers to an instance where you ran a model. In other words, you wouldn't "build a simulation" you would "build a model", but you could either "run a model" or "run a simulation". Model and simulation are often used interchangeably and the difference between them is trivial.  ==History== Computer simulation developed hand-in-hand with the rapid growth of the computer, following its first large-scale deployment during the [[Manhattan Project]] in [[World War II]] to model the process of [[nuclear weapon|nuclear detonation]]. It was a simulation of 12 [[hard spheres]] using a [[Monte Carlo method|Monte Carlo algorithm]]. Computer simulation is often used as an adjunct to, or substitute for, modeling systems for which simple [[closed-form solution|closed form analytic solutions]] are not possible. There are many types of computer simulations; the common feature they all share is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states of the model would be prohibitive or impossible.  ==Data preparation== The external data requirements of simulations and models vary widely. For some, the input might be just a few numbers (for example, simulation of a waveform of AC electricity on a wire), while others might require terabytes of information (such as weather and climate models).  Input sources also vary widely: * Sensors and other physical devices connected to the model; * Control surfaces used to direct the progress of the simulation in some way; * Current or Historical data entered by hand; * Values extracted as by-product from other processes; * Values output for the purpose by other simulations, models, or processes.  Lastly, the time at which data is available varies: * "invariant" data is often built into the model code, either because the value is truly invariant (e.g. the value of π) or because the designers consider the value to be invariant for all cases of interest; * data can be entered into the simulation when it starts up, for example by reading one or more files, or by reading data from a [[preprocessor (CAE)|preprocessor]]; * data can be provided during the simulation run, for example by a sensor network;  Because of this variety, and that many common elements exist between diverse simulation systems, there are a large number of specialized simulation languages. The best-known of these may be [[Simula]] (sometimes Simula-67, after the year 1967 when it was proposed). There are now [[Simulation language|many others]].  Systems that accept data from external sources must be very careful in knowing what they are receiving. While it is easy for computers to read in values from text or binary files, what is much harder is knowing what the [[accuracy]] (compared to [[Resolution#Measurement resolution|measurement resolution]] and [[Accuracy and precision|precision]]) of the values is. Often it is expressed as "error bars", a minimum and maximum deviation from the value seen within which the true value (is expected to) lie. Because digital computer mathematics is not perfect, rounding and truncation errors will multiply this error up, and it is therefore useful to perform an "error analysis"<ref name=Taylor>{{cite book  |title=An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements |author=John Robert Taylor |url=http://books.google.com/books?id=giFQcZub80oC&pg=PA128 |pages=128–129 |isbn=0-935702-75-X |year=1999 |publisher=University Science Books}}</ref> to check that values output by the simulation are still usefully accurate.  Even small errors in the original data can accumulate into substantial error later in the simulation. While all computer analysis is subject to the "GIGO" (garbage in, garbage out) restriction, this is especially true of digital simulation. Indeed, it was the observation of this inherent, cumulative error, for digital systems that is the origin of [[chaos theory]].  ==Types== Computer models can be classified according to several independent pairs of attributes, including: *  [[stochastic process|Stochastic]] or [[Deterministic algorithm|deterministic]] (and as a special case of deterministic, chaotic) - see External links below for examples of stochastic vs. deterministic simulations *  Steady-state or dynamic *  [[Continuous function|Continuous]] or [[discrete mathematics|discrete]] (and as an important special case of discrete, [[Discrete event simulation|discrete event]] or DE models) *  Local or [[distributed computing|distributed]].  Another way of categorizing models is to look at the underlying data structures. For time-stepped simulations, there are two main classes: * Simulations which store their data in regular grids and require only next-neighbor access are called [[stencil codes]]. Many CFD applications belong to this category. * If the underlying graph is not a regular grid, the model may belong to the [[meshfree method]] class.  Equations define the relationships between elements of the modeled system and attempt to find a state in which the system is in equilibrium. Such models are often used in simulating physical systems, as a simpler modeling case before dynamic simulation is attempted. *Dynamic simulations model changes in a system in response to (usually changing) input signals. *''[[stochastic process|Stochastic]]'' models use ''[[random number generator]]s'' to model chance or random events; *A ''[[discrete event simulation]]'' (DES) manages events in time. Most computer, logic-test and fault-tree simulations are of this type. In this type of simulation, the simulator maintains a queue of events sorted by the simulated time they should occur. The simulator reads the queue and triggers new events as each event is processed. It is not important to execute the simulation in real time. It's often more important to be able to access the data produced by the simulation, to discover logic defects in the design, or the sequence of events. *A ''continuous dynamic simulation'' performs numerical solution of [[Differential algebraic equation|differential-algebraic equations]] or [[differential equations]] (either [[partial differential equation|partial]] or [[ordinary differential equation|ordinary]]). Periodically, the simulation program solves all the equations, and uses the numbers to change the state and output of the simulation. Applications include flight simulators, [[construction and management simulation games]], [[chemical process modeling]], and simulations of [[electrical circuit]]s. Originally, these kinds of simulations were actually implemented on [[analog computer]]s, where the differential equations could be represented directly by various electrical components such as [[op-amp]]s. By the late 1980s, however, most "analog" simulations were run on conventional [[digital computer]]s that [[emulate]] the behavior of an analog computer. *A special type of discrete simulation that does not rely on a model with an underlying equation, but can nonetheless be represented formally, is ''agent-based simulation''. In agent-based simulation, the individual entities (such as molecules, cells, trees or consumers) in the model are represented directly (rather than by their density or concentration) and possess an internal ''state'' and set of behaviors or ''rules'' that determine how the agent's state is updated from one time-step to the next. *[[Distributed computing|Distributed]] models run on a network of interconnected computers, possibly through the [[Internet]]. Simulations dispersed across multiple host computers like this are often referred to as "distributed simulations". There are several standards for distributed simulation, including [[Aggregate Level Simulation Protocol]] (ALSP), [[Distributed Interactive Simulation]] (DIS), the [[High Level Architecture (simulation)]] (HLA) and the [[Test and Training Enabling Architecture]] (TENA).  == CGI computer simulation == Formerly, the output data from a computer simulation was sometimes presented in a table, or a matrix, showing how data were affected by numerous changes in the simulation parameters. The use of the matrix format was related to traditional use of the matrix concept in [[mathematical model]]s; however, psychologists and others noted that humans could quickly perceive trends by looking at graphs or even moving-images or motion-pictures generated from the data, as displayed by [[computer generated imagery|computer-generated-imagery]] (CGI) animation. Although observers couldn't necessarily read out numbers, or spout math formulas, from observing a moving weather chart, they might be able to predict events (and "see that rain was headed their way"), much faster than scanning tables of rain-cloud [[coordinate]]s. Such intense graphical displays, which transcended the World of numbers and formulae, sometimes also led to output that lacked a coordinate grid or omitted timestamps, as if straying too far from numeric data displays. Today, [[weather forecasting]] models tend to balance the view of moving rain/snow clouds against a map that uses numeric coordinates and numeric timestamps of events.  Similarly, CGI computer simulations of [[CAT scan]]s can simulate how a [[brain cancer|tumor]] might shrink or change, during an extended period of medical treatment, presenting the passage of time as a spinning view of the visible human head, as the tumor changes.  Other applications of CGI computer simulations are being developed to graphically display large amounts of data, in motion, as changes occur during a simulation run.  ==Computer simulation in science== [[File:Molecular simulation process.svg|400px|thumb|Process of building a computer model, and the interplay between experiment, simulation, and theory.]] [[File:Osmosis computer simulation.jpg|250px|thumb|Computer simulation of the process of [[osmosis]] ]]  Generic examples of types of computer simulations in science, which are derived from an underlying mathematical description: * a numerical simulation of [[differential equation]]s that cannot be solved analytically, theories that involve continuous systems such as phenomena in [[physical cosmology]], [[fluid dynamics]] (e.g. [[climate model]]s, [[roadway noise]] models, [[roadway air dispersion model]]s), [[continuum mechanics]] and [[chemical kinetics]] fall into this category. * a [[stochastic]] simulation, typically used for discrete systems where events occur [[probabilistic]]ally, and which cannot be described directly with differential equations (this is a ''discrete'' simulation in the above sense). Phenomena in this category include [[genetic drift]], [[biochemistry|biochemical]] or [[gene regulatory network]]s with small numbers of molecules. (see also: [[Monte Carlo method]]).  Specific examples of computer simulations follow: * statistical simulations based upon an agglomeration of a large number of input profiles, such as the forecasting of equilibrium [[temperature]] of [[receiving waters]], allowing the gamut of [[meteorological]] data to be input for a specific locale. This technique was developed for [[thermal pollution]] forecasting . * agent based simulation has been used effectively in [[ecology]], where it is often called ''individual based modeling'' and has been used in situations for which individual variability in the agents cannot be neglected, such as [[population dynamics]] of [[salmon]] and [[trout]] (most purely mathematical models assume all trout behave identically). * time stepped dynamic model. In hydrology there are several such [[hydrology transport model]]s such as the [[SWMM]] and [[DSSAM Model]]s developed by the [[United States Environmental Protection Agency|U.S. Environmental Protection Agency]] for river water quality forecasting. * computer simulations have also been used to formally model theories of human cognition and performance, e.g. [[ACT-R]] * computer simulation using [[molecular modeling]] for [[drug discovery]] * computer simulation for studying the selective sensitivity of bonds by mechanochemistry during grinding of organic molecules.<ref>Mizukami, Koichi ; Saito, Fumio ; Baron, Michel. [http://pem.utbm.fr/materiaux_2002/file/pdf/AF01078.PDF Study on grinding of pharmaceutical products with an aid of computer simulation]</ref> * [[Computational fluid dynamics]] simulations are used to simulate the behaviour of flowing air, water and other fluids. There are one-, two- and three- dimensional models used. A one dimensional model might simulate the effects of [[water hammer]] in a pipe. A two-dimensional model might be used to simulate the drag forces on the cross-section of an aeroplane wing. A three-dimensional simulation might  estimate the heating and cooling requirements of a large building. * An understanding of statistical thermodynamic molecular theory is fundamental to the appreciation of molecular solutions. Development of the [[Potential Distribution Theorem]] (PDT) allows one to simplify this complex subject to down-to-earth presentations of molecular theory.  Notable, and sometimes controversial, computer simulations used in science include: [[Donella Meadows|Donella Meadows']] [[World3]] used in the ''[[Limits to Growth]]'', [[James Lovelock|James Lovelock's]] [[Daisyworld]] and Thomas Ray's [[Tierra (computer simulation)|Tierra]].  ===Simulation environments for physics and engineering===  [[Graphical environment]]s to design simulations have been developed. Special care was taken to handle events (situations in which the simulation equations are not valid and have to be changed). The open project [[Open Source Physics]] was started to develop reusable libraries for simulations in [[Java (programming language)|Java]], together with [[EJS|Easy Java Simulations]], a complete graphical environment that generates code based on these libraries.  ==Computer simulation in practical contexts== Computer simulations are used in a wide variety of practical contexts, such as: * analysis of [[air pollutant]] dispersion using [[atmospheric dispersion modeling]] * design of complex systems such as [[aircraft]] and also [[logistics]] systems. * design of [[Noise barrier]]s to effect roadway [[noise mitigation]] * [[flight simulator]]s to train pilots * [[Atmospheric model|weather forecasting]] * Simulation of other computers is [[Emulator|emulation]]. * forecasting of prices on financial markets (for example [[Adaptive Modeler]]) * behavior of structures (such as buildings and industrial parts) under stress and other conditions * design of industrial processes, such as chemical processing plants * [[Strategic Management]] and [[Organizational Studies]] * [[Reservoir simulation]] for the petroleum engineering to model the subsurface reservoir * Process Engineering Simulation tools. * [[Robotics suite|Robot simulators]] for the design of robots and robot control algorithms * [[UrbanSim|Urban Simulation Models]] that simulate dynamic patterns of urban development and responses to urban land use and transportation policies. See a more detailed article on [[Urban Environment Simulation]]. * [[Traffic engineering (transportation)|Traffic engineering]] to plan or redesign parts of the street network from single junctions over cities to a national highway network, for transportation system planning, design and operations. See a more detailed article on [[Traffic Simulation|Simulation in Transportation]]. * modeling car crashes to test safety mechanisms in new vehicle models  The reliability and the trust people put in computer simulations depends on the [[validity]] of the simulation [[model (abstract)|model]], therefore [[verification and validation]] are of crucial importance in the development of computer simulations. Another important aspect of computer simulations is that of reproducibility of the results, meaning that a simulation model should not provide a different answer for each execution. Although this might seem obvious, this is a special point of attention in [[stochastic simulation]]s, where random numbers should actually be semi-random numbers. An exception to reproducibility are human in the loop simulations such as flight simulations and [[computer games]]. Here a human is part of the simulation and thus influences the outcome in a way that is hard, if not impossible, to reproduce exactly.  [[Vehicle]] manufacturers make use of computer simulation to test safety features in new designs. By building a copy of the car in a physics simulation environment, they can save the hundreds of thousands of dollars that would otherwise be required to build a unique prototype and test it. Engineers can step through the simulation milliseconds at a time to determine the exact stresses being put upon each section of the prototype.<ref>Baase, Sara. A Gift of Fire: Social, Legal, and Ethical Issues for Computing and the Internet. 3. Upper Saddle River: Prentice Hall, 2007. Pages 363-364. ISBN 0-13-600848-8.</ref>  [[Computer graphics]] can be used to display the results of a computer simulation. [[Animations]] can be used to experience a simulation in real-time e.g. in [[Training Simulation|training simulations]]. In some cases animations may also be useful in faster than real-time or even slower than real-time modes. For example, faster than real-time animations can be useful in visualizing the buildup of queues in the simulation of humans evacuating a building. Furthermore, simulation results are often aggregated into static images using various ways of [[scientific visualization]].  In debugging, simulating a program execution under test (rather than executing natively) can detect far more errors than the hardware itself can detect and, at the same time, log useful debugging information such as instruction trace, memory alterations and instruction counts. This technique can also detect [[buffer overflow]] and similar "hard to detect" errors as well as produce performance information and [[Performance tuning|tuning]] data.  ==Pitfalls==  Although sometimes ignored in computer simulations, it is very important to perform [[sensitivity analysis]] to ensure that the accuracy of the results are properly understood. For example, the probabilistic risk analysis of factors determining the success of an oilfield exploration program involves combining samples from a variety of statistical distributions using the [[Monte Carlo method]]. If, for instance, one of the key parameters (e.g. the net ratio of oil-bearing strata) is known to only one significant figure, then the result of the simulation might not be more precise than one significant figure, although it might (misleadingly) be presented as having four significant figures.  '''Model Calibration Techniques'''  The following three steps should be used to produce accurate simulation models: calibration, verification, and validation. Computer simulations are good at portraying and comparing theoretical scenarios but in order to accurately model actual case studies, it has to match what is actually happening today. A base model should be created and calibrated so that it matches the area being studied. The calibrated model should then be verified to ensure that the model is operating as expected based on the inputs. Once the model has been verified, the final step is to validate the model by comparing the outputs to historical data from the study area. This can be done by using statistical techniques and ensuring an adequate R-squared value. Unless these techniques are employed, the simulation model created will produce inaccurate results and not be a useful prediction tool.  Model calibration is achieved by adjusting any available parameters in order to adjust how the model operates and simulates the process. For example in traffic simulation, typical parameters include look-ahead distance, car-following sensitivity, discharge headway, and start-up lost time. These parameters influence driver behaviors such as when and how long it takes a driver to change lanes, how much distance a driver leaves between itself and the car in front of it, and how quickly it starts to accelerate through an intersection. Adjusting these parameters has a direct effect on the amount of traffic volume that can traverse through the modeled roadway network by making the drivers more or less aggressive. These are examples of calibration parameters that can be fine-tuned to match up with characteristics observed in the field at the study location. Most traffic models will have typical default values but they may need to be adjusted to better match the driver behavior at the location being studied.  Model verification is achieved by obtaining output data from the model and comparing it to what is expected from the input data. For example in traffic simulation, traffic volume can be verified to ensure that actual volume throughput in the model is reasonably close to traffic volumes input into the model. Ten percent is a typical threshold used in traffic simulation to determine if output volumes are reasonably close to input volumes. Simulation models handle model inputs in different ways so traffic that enters the network, for example, may or may not reach its desired destination. Additionally, traffic that wants to enter the network may not be able to, if any congestion exists. This is why model verification is a very important part of the modeling process.  The final step is to validate the model by comparing the results with what’s expected based on historical data from the study area. Ideally, the model should produce similar results to what has happened historically. This is typically verified by nothing more than quoting the R2 statistic from the fit. This statistic measures the fraction of variability that is accounted for by the model. A high R2 value does not necessarily mean the model fits the data well. Another tool used to validate models is graphical residual analysis. If model output values are drastically different than historical values, it probably means there’s an error in the model. This is an important step to verify before using the model as a base to produce additional models for different scenarios to ensure each one is accurate. If the outputs do not reasonably match historic values during the validation process, the model should be reviewed and updated to produce results more in line with expectations. It is an iterative process that helps to produce more realistic models.  Validating traffic simulation models requires comparing traffic estimated by the model to observed traffic on the roadway and transit systems. Initial comparisons are for trip interchanges between quadrants, sectors, or other large areas of interest. The next step is to compare traffic estimated by the models to traffic counts, including transit ridership, crossing contrived barriers in the study area. These are typically called screenlines, cutlines, and cordon lines and may be imaginary or actual physical barriers. Cordon lines surround particular areas such as the central business district or other major activity centers. Transit ridership estimates are commonly validated by comparing them to actual patronage crossing cordon lines around the central business district.  Three sources of error can cause weak correlation during calibration: input error, model error, and parameter error. In general, input error and parameter error can be adjusted easily by the user. Model error however is caused by the methodology used in the model and may not be as easy to fix. Simulation models are typically built using several different modeling theories that can produce conflicting results. Some models are more generalized while others are more detailed. If model error occurs as a result of this, in may be necessary to adjust the model methodology to make results more consistent.  In order to produce good models that can be used to produce realistic results, these are the necessary steps that need to be taken in order to ensure that simulation models are functioning properly. Simulation models can be used as a tool to verify engineering theories but are only valid, if calibrated properly. Once satisfactory estimates of the parameters for all models have been obtained, the models must be checked to assure that they adequately perform the functions for which they are intended. The validation process establishes the credibility of the model by demonstrating its ability to replicate actual traffic patterns. The importance of model validation underscores the need for careful planning, thoroughness and accuracy of the input data collection program that has this purpose. Efforts should be made to ensure collected data is consistent with expected values. For example in traffic analysis, it is typically common for a traffic engineer to perform a site visit to verify traffic counts and become familiar with traffic patterns in the area. The resulting models and forecasts will be no better than the data used for model estimation and validation.  ==See also== * [[Virtual prototyping]] * [[Stencil codes]] * [[Meshfree methods]] * [[Web-based simulation]] * [[Emulator]] * [[Procedural animation]] * [[In silico]]  ==References== {{More footnotes|date=May 2008}} {{Reflist}}  ==Notes== {{Refbegin}} * R. Frigg and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science]. Entry in the '' [[Stanford Encyclopedia of Philosophy]]''. * A.K. Hartmann, [http://www.worldscibooks.com/physics/6988.html Practical Guide to Computer Simulations], Singapore: [[World Scientific]], 2009 * S. Hartmann, [http://philsci-archive.pitt.edu/archive/00002412/ The World as a Process: Simulations in the Natural and Social Sciences], in: R. Hegselmann et al. (eds.), ''Modelling and Simulation in the Social Sciences from the Philosophy of Science Point of View'', Theory and Decision Library. Dordrecht: [[Kluwer]] 1996, 77-100. * P. Humphreys, ''Extending Ourselves: Computational Science, Empiricism, and Scientific Method''. Oxford: [[Oxford University Press]], 2004. * James J. Nutaro, ''Building Software for Simulation: Theory and Algorithms, with Applications in C  ''. Wiley, 2010. {{Refend}}  ==External links== {{Commons category}} * [http://sma.epfl.ch/~hassan/meshAdaptation.html Example of computer simulation : Aerodynamic computations for supersonic aircrafts]  <!--Categories--> {{DEFAULTSORT:Computer Simulation}} [[Category:Computational science]] [[Category:Scientific modeling]] [[Category:Simulation software| ]] [[Category:Virtual reality]]  <!--Interwikies-->  [[ar:محاكاة بالحاسوب]] [[bg:Компютърна симулация]] [[ca:Simulació d'ordinador]] [[cs:Počítačová simulace]] [[de:Computersimulation]] [[es:Simulación por computadora]] [[fr:Simulation informatique]] [[hi:कम्प्यूटरी सिमुलेशन]] [[kk:Машиналық ұқсаттыру]] [[nl:Computersimulatie]] [[no:Datasimulering]] [[pl:Symulacja komputerowa]] [[pt:Modelagem computacional]] [[ru:Компьютерное моделирование]] [[simple:Computer model]] [[sl:Računalniška simulacija]] [[sr:Рачунарска симулација]] [[fi:Tietokonesimulointi]] [[uk:Цифрове моделювання]] [[zh:计算机模拟]]
'''Computer vision''' is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, ''e.g.'', in the forms of decisions.<ref name="Shapiro-Stockman-2001"/><ref name="Morris-2004"/><ref name="Jahne-Haussecker-2000"/> A theme in the development of this field has been to duplicate the abilities of human vision by electronically perceiving and understanding an image.<ref name="Sonka-Hlavac-Boyle-2008"/> This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.<ref name="Forsyth-Ponce-2003"/>  Computer vision has also been described as the enterprise of automating and integrating a wide range of processes and representations for vision perception.<ref name="Ballard-Brown-1982"/>  Applications range from tasks such as industrial [[machine vision]] systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them.  The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields.   Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications.  As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.  As a technological discipline, computer vision seeks to apply its theories and models to the construction of computer vision systems. Examples of applications of computer vision include systems for: * Controlling processes, ''e.g.'', an [[industrial robots|industrial robot]]; * Navigation, ''e.g.'', by an [[autonomous vehicle]] or mobile robot; * Detecting events, ''e.g.'', for visual surveillance or [[people counter|people counting]]; * Organizing information, ''e.g.'', for indexing databases of images and image sequences; * Modeling objects or environments, ''e.g.'', medical image analysis or topographical modeling; * Interaction, ''e.g.'', as the input to a device for [[computer-human interaction]], and * Automatic inspection, ''e.g.'', in manufacturing applications.  Sub-domains of computer vision include scene reconstruction, event detection, [[video tracking]], [[object recognition]], learning, indexing, [[motion estimation]], and [[image restoration]].  In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.  ==Related fields== [[File:CVoverview2.svg||thumb|350px|Relation between computer vision and various other fields{{or|date=September 2011}}]] Areas of [[artificial intelligence]] deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot. Artificial intelligence and computer vision share other topics such as [[pattern recognition]] and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.  [[Physics]] is another field that is closely related to computer vision. Most computer vision systems rely on [[image sensors]], which detect electromagnetic radiation which is typically in the form of either visible or infra-red light. The sensors are designed using [[solid-state physics]]. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of [[optics]] which are a core part of most imaging systems.   Sophisticated [[image sensors]] even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.  A third field which plays an important role is [[neurobiology]], specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how "real" vision systems operate in order to solve certain vision related tasks. These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (''e.g.'' [[Neural network|neural net]] based image and feature analysis and classification)  have their background in biology.  Some strands of computer vision research are closely related to the study of [[biological vision]] – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information.  The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.  Yet another field related to computer vision is [[signal processing]]. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.  Beside the above mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on [[statistics]], [[Optimization (mathematics)|optimization]] or [[geometry]]. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.  The fields most closely related to computer vision are [[image processing]], [[image analysis]] and [[machine vision]]. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are more or less identical, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.  Computer vision is, in some ways, the inverse of [[computer graphics]]. While computer graphics produces image data from 3D models, computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, ''e.g.'', as explored in [[augmented reality]].  The following characterizations appear relevant but should not be taken as universally accepted: * [[Image processing]] and [[image analysis]] tend to focus on 2D images, how to transform one image to another, ''e.g.'', by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither require assumptions nor produce interpretations about the image content. * Computer vision includes 3D analysis from 2D images.  This analyzes the 3D scene projected onto one or several images, ''e.g.'', how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. * [[Machine vision]] is the process of applying a range of technologies & methods to provide imaging-based automatic inspection, process control  and robot guidance<ref name="NASAarticle"/> in industrial  applications.<ref name="TextbookP1"/>  Machine vision tends to focus on applications, mainly in manufacturing, ''e.g.'', vision based autonomous robots and systems for vision based inspection or measurement. This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasised by means of efficient implementations in hardware and software. It also implies that the external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. * There is also a field called [[imaging science|imaging]] which primarily focus on the process of producing images, but sometimes also deals with processing and analysis of images. For example, [[medical imaging]] includes substantial work on the analysis of image data in medical applications. * Finally, [[pattern recognition]] is a field which uses various methods to extract information from signals in general, mainly based on statistical approaches. A significant part of this field is devoted to applying these methods to image data.  ==Applications for computer vision== [[File:DARPA Visual Media Reasoning Concept Video.ogv|thumb|[[DARPA]]'s Visual Media Reasoning concept video]] One of the most prominent application fields is medical computer vision or medical image processing. This area is characterized by the extraction of information from image data for the purpose of making a medical diagnosis of a patient. Generally, image data is in the form of [[microscopy|microscopy images]], [[X-ray|X-ray images]], [[angiography|angiography images]], [[ultrasonography|ultrasonic images]], and [[tomography|tomography images]]. An example of information which can be extracted from such image data is detection of [[tumour]]s, [[arteriosclerosis]] or other malign changes. It can also be measurements of organ dimensions, blood flow, etc. This application area also supports medical research by providing new information, ''e.g.'', about the structure of the brain, or about the quality of medical treatments.  A second application area in computer vision is in industry, sometimes called [[machine vision]], where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called [[optical sorting]].  Military applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and [[missile guidance]]. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.  [[Image:NASA Mars Rover.jpg|right|200px|thumbnail|Artist's Concept of Rover on Mars, an example of an unmanned land-based vehicle. Notice the [[stereo cameras]] mounted on top of the Rover.]] One of the newer application areas is autonomous vehicles, which include [[submersible]]s, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles ([[Unmanned aerial vehicle|UAV]]). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment ([[Simultaneous localization and mapping|SLAM]]) and for detecting obstacles. It can also be used for detecting certain task specific events, ''e.g.'', a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for [[Driverless car|autonomous driving of cars]], but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, ''e.g.'', NASA's [[Mars Exploration Rover]] and ESA's [[ExoMars]] Rover.  Other application areas include: * Support of [[visual effects]] creation for cinema and broadcast, ''e.g.'', [[camera tracking]] (matchmoving). * [[Surveillance]].  ==Typical tasks of computer vision== Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.  ===Recognition=== The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. This task can normally be solved [[wikt:robust|robustly]] and without effort by a human, but is still not satisfactorily solved in computer vision for the general case{{spaced ndash}}arbitrary objects in arbitrary situations. The existing methods for dealing with this problem can at best solve it only for specific objects, such as simple geometric objects (''e.g.'', polyhedra), human faces, printed or hand-written characters, or vehicles, and in specific situations, typically described in terms of well-defined illumination, background, and [[Pose (computer vision)|pose]] of the object relative to the camera.  Different varieties of the recognition problem are described in the literature: * '''[[Object recognition]]'''{{spaced ndash}}one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene.  [[Google Goggles]] provides a stand-alone program illustration of this function. * '''Identification'''{{spaced ndash}}an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, or identification of a specific vehicle. * '''Detection'''{{spaced ndash}}the image data are scanned for a specific condition. Examples include detection of possible abnormal cells or tissues in medical images or detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.  Several specialized tasks based on recognition exist, such as: * '''[[Content-based image retrieval]]'''{{spaced ndash}}finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative a target image (give me all images similar to image X), or in terms of high-level search criteria given as text input (give me all images which contains many houses, are taken during winter, and have no cars in them). * '''[[Pose (computer vision)|Pose estimation]]'''{{spaced ndash}}estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an [[assembly line]] situation or picking parts from a bin. * '''[[Optical character recognition]]''' (OCR){{spaced ndash}}identifying [[Character (computing)|characters]] in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or [[Search index|indexing]] (''e.g.'' [[ASCII]]). * '''2D Code reading''' Reading of 2D codes such as [[Data Matrix|data matrix]] and [[QR code|QR]] codes. * '''[[Facial recognition system|Facial recognition]]'''  === Motion analysis === Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are: * '''[[Egomotion]]'''{{spaced ndash}}determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera. * '''[[video tracking|Tracking]]'''{{spaced ndash}}following the movements of a (usually) smaller set of interest points or objects (''e.g.'', vehicles or humans) in the image sequence. * '''[[Optical flow]]'''{{spaced ndash}}to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result both of how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.  ===Scene reconstruction=== Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a [[computer model|3D model]] of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model.  ===Image restoration=== The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look like, a model which distinguishes them from the noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches. An example in this field is the [[inpainting]].  ==Computer vision system methods== The organization of a computer vision system is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems. * '''Image acquisition'''{{spaced ndash}}A digital image is produced by one or several [[image sensor]]s, which, besides various types of light-sensitive cameras, include [[rangefinder|range sensors]], tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images), but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or [[Magnetic resonance imaging|nuclear magnetic resonance]].<ref name="Davies-2005"/> * '''Pre-processing'''{{spaced ndash}}Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to assure that it satisfies certain assumptions implied by the method. Examples are ** Re-sampling in order to assure that the image coordinate system is correct. ** Noise reduction in order to assure that sensor noise does not introduce false information. ** Contrast enhancement to assure that relevant information can be detected. ** [[Scale-space]] representation to enhance image structures at locally appropriate scales. * '''Feature extraction'''{{spaced ndash}}Image features at various levels of complexity are extracted from the image data.<ref name="Davies-2005"/> Typical examples of such features are ** Lines, [[edge detection|edges]] and [[ridge detection|ridges]]. ** Localized [[interest point detection|interest points]] such as [[corner detection|corners]], [[blob detection|blobs]] or points. :More complex features may be related to texture, shape or motion. * '''Detection/segmentation'''{{spaced ndash}}At some point in the processing a decision is made about which image points or regions of the image are relevant for further processing.<ref name="Davies-2005"/> Examples are ** Selection of a specific set of interest points ** Segmentation of one or multiple image regions which contain a specific object of interest. * '''High-level processing'''{{spaced ndash}}At this step the input is typically a small set of data, for example a set of points or an image region which is assumed to contain a specific object.<ref name="Davies-2005"/>  The remaining processing deals with, for example: ** Verification that the data satisfy model-based and application specific assumptions. ** Estimation of application specific parameters, such as object pose or object size. ** [[Image recognition]]{{spaced ndash}}classifying a detected object into different categories. ** [[Image registration]]{{spaced ndash}}comparing and combining two different views of the same object. * '''Decision making''' Making the final decision required for the application,<ref name="Davies-2005"/> for example: ** Pass/fail on automatic inspection applications ** Match / no-match in recognition applications ** Flag for further human review in medical, military, security and recognition applications  ==See also== * [[AI effect]] * [[Applications of artificial intelligence]] * [[Machine vision glossary]]  ; Lists * [[List of computer vision topics]] * [[List of emerging technologies]] * [[Outline of artificial intelligence]]  ==References== {{reflist| refs= <ref name="Shapiro-Stockman-2001"> {{cite book | author=Linda G. Shapiro and George C. Stockman | title=Computer Vision | publisher=Prentice Hall | year=2001 | isbn=0-13-030796-3}}</ref> <ref name="Jahne-Haussecker-2000"> {{cite book | author=Bernd Jähne and Horst Haußecker | title=Computer Vision and Applications, A Guide for Students and Practitioners | publisher=Academic Press | year=2000 | isbn=0-13-085198-1}}</ref> <ref name="Morris-2004"> {{cite book | author=Tim Morris | title=Computer Vision and Image Processing | publisher=Palgrave Macmillan | year=2004 | isbn=0-333-99451-5}}</ref> <ref name="Sonka-Hlavac-Boyle-2008"> {{cite book | author=Milan Sonka, Vaclav Hlavac and Roger Boyle | title=Image Processing, Analysis, and Machine Vision | publisher=Thomson | year=2008 | isbn=0-495-08252-X}}</ref> <ref name="Forsyth-Ponce-2003"> {{cite book | author=David A. Forsyth and Jean Ponce | title=Computer Vision, A Modern Approach | publisher=Prentice Hall | year=2003 | isbn=0-13-085198-1}}</ref> <ref name="Davies-2005"> {{cite book | author=E. Roy Davies | title=Machine Vision: Theory, Algorithms, Practicalities | publisher=Morgan Kaufmann | year=2005 | isbn=0-12-206093-8 }}</ref> <ref name = "NASAarticle"> {{cite journal| author=Turek, Fred| journal=NASA Tech Briefs magazine | volume= 35 | issue= 6 | date= June 2011 | title=Machine Vision Fundamentals, How to Make Robots See}} pages 60–62</ref> <ref name="TextbookP1"> {{cite book | author= Steger, Carsten, Markus Ulrich, and Christian Wiedemann | title= Machine Vision Algorithms and Applications | publisher=[[Wiley-VCH]] | location=Weinheim | year=2008 | isbn=978-3-527-40734-7 | url=http://books.google.com/books?id=bvSgjky9lBYC&lpg=PP1&pg=PA1#v=onepage&q&f=false | accessdate=2010-11-05| page=1}}</ref> <ref name="Ballard-Brown-1982"> {{cite book | author = Dana H. Ballard and Christopher M. Brown | title = Computer Vision | publisher = Prentice Hall | year = 1982 | url = http://homepages.inf.ed.ac.uk/rbf/BOOKS/BANDB/bandb.htm | isbn=0-13-165316-4}}</ref> }}  ==Further reading== {{Further reading cleanup|date=September 2011}} * {{cite book | author=David Marr | title=Vision | publisher=W. H. Freeman and Company | year=1982 | isbn=0-7167-1284-9}} * {{cite book | author=Azriel Rosenfeld and Avinash Kak | title=Digital Picture Processing | publisher=Academic Press | year=1982 | isbn=0-12-597301-2 }} * {{cite book | author=Berthold Klaus Paul Horn | title=Robot Vision | publisher=MIT Press| year=1986 | isbn=0-262-08159-8}} * {{cite book | author=Olivier Faugeras | title=Three-Dimensional Computer Vision, A Geometric Viewpoint | publisher=MIT Press | year=1993 | isbn=0-262-06158-9}} * {{cite book | author=Tony Lindeberg | title= Scale-Space Theory in Computer Vision | url = http://www.nada.kth.se/~tony/book.html | publisher= Springer | year=1994 | isbn=0-7923-9418-6}} * {{cite book | title = Vision as Process | author = James L. Crowley and Henrik I. Christensen (Eds.) | publisher = Springer-Verlag | year = 1995 | isbn = 3-540-58143-X and ISBN 0-387-58143-X}} * {{cite book | author=Gösta H. Granlund and Hans Knutsson | title=Signal Processing for Computer Vision | publisher=Kluwer Academic Publisher | year=1995 | isbn=0-7923-9530-1}} * {{cite book | author=Reinhard Klette, Karsten Schluens and Andreas Koschan | title=Computer Vision – Three-Dimensional Data from Images  | url=http://www.cs.auckland.ac.nz/~rklette/Books/SpringerCV98/Springer98.html | publisher=Springer, Singapore | year=1998 | isbn=981-3083-71-9}} * {{cite book | author=Emanuele Trucco and Alessandro Verri | title=Introductory Techniques for 3-D Computer Vision| publisher= Prentice Hall | year=1998| isbn=0-13-261108-2 }} * {{cite book | author=Bernd Jähne | title=Digital Image Processing | publisher=Springer | year=2002 | isbn=3-540-67754-2}} * {{cite book | author=Richard Hartley and Andrew Zisserman | title=Multiple View Geometry in Computer Vision | publisher=Cambridge University Press| year=2003 | isbn=0-521-54051-8}} * {{cite book | author=Gérard Medioni and Sing Bing Kang | title=Emerging Topics in Computer Vision | publisher=Prentice Hall | year=2004 | isbn=0-13-101366-1 }} * {{cite book | author=R. Fisher, K Dawson-Howe, A. Fitzgibbon, C. Robertson, E. Trucco | title=Dictionary of Computer Vision and Image Processing | publisher=John Wiley | year=2005 | isbn=0-470-01526-8}} * {{cite book | author=Nikos Paragios and Yunmei Chen and Olivier Faugeras| title= Handbook of Mathematical Models in Computer Vision | url= http://www.mas.ecp.fr/vision/Personnel/nikos/paragios-chen-faugeras/| publisher=Springer | year=2005 | isbn=0-387-26371-3}} * {{cite book | author = Wilhelm Burger and Mark J. Burge | title = Digital Image Processing: An Algorithmic Approach Using Java | publisher = [[Springer Science Business Media|Springer]] | year = 2007 | url = http://www.imagingbook.com/ | isbn=1-84628-379-5 and ISBN 3-540-30940-3 }} * {{cite book | author = Pedram Azad, Tilo Gockel, Rüdiger Dillmann | title = Computer Vision – Principles and Practice | publisher = Elektor International Media BV | year = 2008 | url = http://ivt.sourceforge.net/book.html | isbn=0-905705-71-8 }}  ==External links== {{external links|date=September 2011}} * [http://www.thecomputervision.com/ The Computer Vision] TheComputerVision is a community for the computer vision researchers and professionals to share, discuss, connect, and collaborate. * [http://www.computervisiononline.com/ Computer Vision Online] A good source for source codes, software packages, datasets, etc. related to computer vision * [http://iris.usc.edu/Vision-Notes/bibliography/contents.html Keith Price's Annotated Computer Vision Bibliography] and the Official Mirror Site [http://www.visionbib.com/bibliography/contents.html Keith Price's Annotated Computer Vision Bibliography] * [http://iris.usc.edu/Information/Iris-Conferences.html USC Iris computer vision conference list] * [http://homepages.inf.ed.ac.uk/rbf/CVonline/ CVonline] Bob Fisher's Compendium of Computer Vision * [http://www.iprg.co.in  IPRG] Image Processing - Online Open Research Group {{Computer vision footer}} {{Mixed reality}} {{Image_Processing Software}}  {{DEFAULTSORT:Computer Vision}} [[Category:Artificial intelligence]] [[Category:Computer vision|*Computer vision]] [[Category:Packaging machinery]]  [[ar:رؤية حاسوبية]] [[bg:Компютърно зрение]] [[bs:Računarski vid]] [[ca:Visió artificial]] [[cs:Počítačové vidění]] [[el:Μηχανική όραση]] [[es:Visión artificial]] [[fa:بینایی رایانه‌ای]] [[fr:Vision par ordinateur]] [[ko:컴퓨터 비전]] [[hr:Računalni vid]] [[it:Visione artificiale]] [[he:ראייה ממוחשבת]] [[ja:コンピュータビジョン]] [[pt:Visão computacional]] [[ru:Компьютерное зрение]] [[sq:Vizioni kompjuterik]] [[simple:Computer vision]] [[sl:Računalniški vid]] [[sv:Datorseende]] [[th:คอมพิวเตอร์วิทัศน์]] [[ur:شمارندی بصارت]] [[zh:计算机视觉]]
In a  general sense a '''control unit''' (CU) is a central (or sometimes distributed but clearly distinguishable<!--and delimited-->) part of a mechanism that controls its operation, for example in a computer or a [[motor vehicle]].  == Application in Computer Design ==  {{Multiple issues|section=y|refimprove = June 2008|cleanup = June 2008|expert=Computing|date=November 2008}} ===Definition===  The control unit coordinates the components of a computer system. It fetches the code of all of the instructions in the programme. It directs the operation of the other units by providing timing and control signals. All computer resources are managed by the CU. It directs the flow of data between the Central Processing Unit (CPU) and the other devices.{{dubious|date=April 2012}}{{cn|date=April 2012}}  The control unit was historically <!-- reluctant to write "first" for "historically" without further research which, sorry, I'm not bothering about right now --> defined as one distinct part of the 1946 reference model of [[Von Neumann architecture]]. In modern computer designs, the control unit is typically an internal part of the [[Central processing unit|CPU]]<!-- or other device that directs its operation--> with its overall role and operation unchanged.  The control unit is the circuitry that controls the flow of data through the processor, and coordinates the activities of the other units within it. In a way, it is the "brain within the brain", as it controls what happens inside the processor, which in turn controls the rest of the computer. The examples of devices that require a control unit are CPUs and graphics processing units (GPUs). The modern information age would not be possible without complex control unit designs. The control unit receives external instructions or commands which it converts into a sequence of control signals that the control unit applies to the data path to implement a sequence of [[register-transfer level]] operations.  === Hardwired Control ===  Hardwired control units are implemented through use of [[sequential logic]] units, featuring a finite number of gates that can generate specific results based on the instructions that were used to invoke those responses. Hardwired  control units are generally faster than microprogrammed designs.  Their design uses a fixed architecture &mdash; it requires changes in the wiring if the [[instruction set]] is modified or changed. This architecture is preferred in [[Reduced instruction set computing|reduced instruction  set computer]]s (RISC) as they use a simpler instruction set.  The hardwired approach has become less popular as computers have evolved as at one time, control units for CPUs were ad-hoc logic, and they were difficult to design.  === Microprogram Control Unit === {{main|Microcode}}  The idea of microprogramming was introduced by [[Maurice Wilkes]] in 1951 as an intermediate level to execute computer program instructions. Microprograms were organized as a sequence of ''microinstructions'' and stored in special control memory. The algorithm for the microprogram control unit is usually specified by [[flowchart]] description.<ref>{{Cite book  | last1 = Barkalov | first1 = Alexander | title = Logic synthesis for FSM based control units / Alexander Barkalov and Larysa Titarenko | year = 2009 | publisher = Springer | location = Berlin | isbn = 978-3-642-04308-6 | pages =  }}</ref> The main advantage of the microprogram control unit is the simplicity of its structure. Outputs of the controller are organized in microinstructions and  they can be easily replaced.<ref>{{Cite book  | last1 = Wiśniewski | first1 = Remigiusz | title = Synthesis of compositional microprogram control units for programmable devices   | year = 2009 | publisher = University of Zielona Góra | location = Zielona Góra | isbn = 978-83-7481-293-1 | pages = 153 }}</ref>  === Functions of the Control Unit ===  The control unit implements the [[Instruction set|architecture]] of the CPU. It performs the tasks of fetching, decoding, managing execution and then storing results. It may manage the translation of instructions to micro-instructions and manage scheduling the micro-instructions between the various execution units. On some processors the control unit may be further broken down into other units, such as a scheduling unit to handle scheduling and a retirement unit to deal with results coming from the pipeline.  == See also == ===Computer design=== *[[CPU design]] *[[Computer architecture]] *[[Richard's Controller]] *[[Controller (computing)|Controller]]  ===Automotive design=== * [[Electronic control unit]]  ==References== {{reflist}}  {{CPU technologies}}  {{DEFAULTSORT:Control Unit}} [[Category:Computing terminology]] [[Category:Digital electronics]]  [[ca:Unitat de control]] [[cs:Řadič]] [[de:Steuerwerk]] [[es:Unidad de control]] [[eu:Kontrol unitate]] [[fa:واحد کنترل]] [[fr:Unité de contrôle]] [[id:Unit Kendali]] [[he:יחידת בקרה]] [[mk:Управувачка единица]] [[ja:制御装置]] [[pt:Unidade de controle]] [[ru:Управляющий автомат]] [[sv:Styrenhet]] [[zh:控制单元 (计算机)]]
[[Image:Hash table 3 1 1 0 1 0 0 SP.svg|thumb|315px|right|a hash table]] In [[computer science]], a '''data structure''' is a particular way of storing and organizing [[data (computing)|data]] in a computer so that it can be used [[algorithmic efficiency|efficiently]].<ref>Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://www.itl.nist.gov/div897/sqg/dads/HTML/datastructur.html Online version] Accessed May 21, 2009.</ref><ref>Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] accessed on May 21, 2009.</ref>  Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, [[B-tree]]s are particularly well-suited for implementation of databases, while [[compiler]] implementations usually use [[hash table]]s to look up identifiers.  Data structures provide a means to manage huge amounts of data efficiently, such as large [[database]]s and [[web indexing|internet indexing services]]. Usually, efficient data structures are a key to designing efficient [[algorithms]]. Some formal design methods and [[programming language]]s emphasize data structures, rather than algorithms, as the key organizing factor in software design.  ==Overview== * An [[array data structure]] stores a number of elements of the same type in a specific order. They are accessed using an integer to specify which element is required (although the elements may be of almost any type). Arrays may be fixed-length or expandable. * [[Record (computer science)|Record]] (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called ''fields'' or ''members''. * A [[hash table|hash]] or dictionary or [[map (computer science)|map]] is a more flexible variation on a record, in which [[name-value pair]]s can be added and deleted freely. * [[Union (computer science)|Union]]. A union type definition will specify which of a number of permitted primitive types may be stored in its instances, e.g. "float or long integer". Contrast with a [[record (computer science)|record]], which could be defined to contain a float ''and'' an integer; whereas, in a union, there is only one value at a time. * A [[tagged union]] (also called a [[Variant type|variant]], variant record, discriminated union, or disjoint union) contains an additional field indicating its current type, for enhanced type safety. * A [[set (abstract data type)|set]] is an [[abstract data structure]] that can store specific values, without any particular [[sequence|order]], and no repeated values. Values themselves are not retrieved from sets, rather one tests a value for membership to obtain a boolean "in" or "not in". * An [[object (computer science)|object]] contains a number of data fields, like a record, and also a number of program code fragments for accessing or modifying them. Data structures not containing code, like those above, are called [[plain old data structure]].  Many others are possible, but they tend to be further variations and compounds of the above.  ==Basic principles== Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by an address—a bit string that can be itself stored in memory and manipulated by the program. Thus the [[record (computer science)|record]] and [[Array data structure|array]] data structures are based on computing the addresses of data items with [[arithmetic operations]]; while the [[linked data structure]]s are based on storing addresses of data items within the structure itself. Many data structures use both principles, sometimes combined in non-trivial ways (as in [[XOR linked list|XOR linking]])  The implementation of a data structure usually requires writing a set of [[subroutine|procedures]] that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an [[abstract data type]], a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).  ==Language support== Most [[assembly language]]s and some low-level languages, such as [[BCPL]](Basic Combined Programming Language), lack support for data structures. Many [[high-level programming languages]], and some higher-level assembly languages, such as [[MASM]], on the other hand, have special syntax or other built-in support for certain data structures, such as vectors (one-dimensional [[array data type|arrays]]) in the [[C (programming language)|C]] language or multi-dimensional arrays in [[Pascal (programming language)|Pascal]].  Most programming languages feature some sorts of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the [[C  ]] [[Standard Template Library]], the [[Java Collections Framework]], and [[Microsoft]]'s [[.NET Framework]].  Modern languages also generally support [[modular programming]], the separation between the [[interface (computing)|interface]] of a library module and its implementation. Some provide [[opaque data type]]s that allow clients to hide implementation details. [[Object-oriented programming language]]s, such as [[C  ]], [[Java (programming language)|Java]] and [[.NET Framework]] may use [[classes (computer science)|classes]] for this purpose.  Many known data structures have [[concurrent data structure|concurrent]] versions that allow multiple computing threads to access the data structure simultaneously.  ==See also== {{Wikipedia books|Data structures}} * [[List of data structures]] * [[Plain old data structure]] * [[Concurrent data structure]] * [[Data model]] * [[Dynamization]] * [[Linked data structure]] * [[Persistent data structure]]  ==References== {{Reflist}}  ==Further reading== * Peter Brass, ''Advanced Data Structures'', [[Cambridge University Press]], 2008. * [[Donald Knuth]], ''[[The Art of Computer Programming]]'', vol. 1. [[Addison-Wesley]], 3rd edition, 1997. * Dinesh Mehta and [[Sartaj Sahni]] ''Handbook of Data Structures and Applications'', [[Chapman and Hall]]/[[CRC Press]], 2007. * [[Niklaus Wirth]], ''Algorithms and Data Structures'', [[Prentice Hall]], 1985.  ==External links== {{Sister project links|wikt=data structure|commons=Category:Data structures|b=Data Structures|v=Topic:Data structures|n=no}} * [http://academicearth.org/courses/data-structures UC Berkeley video course on data structures] * [http://nist.gov/dads/ Descriptions] from the [[Dictionary of Algorithms and Data Structures]] * [http://www.cse.unr.edu/~bebis/CS308/ CSE.unr.edu] * [http://www.cs.auckland.ac.nz/software/AlgAnim/ds_ToC.html Data structures course with animations] * [http://courses.cs.vt.edu/~csonline/DataStructures/Lessons/index.html Data structure tutorials with animations] * [http://msdn.microsoft.com/en-us/library/aa289148(VS.71).aspx An Examination of Data Structures from .NET perspective] * [http://people.cs.vt.edu/~shaffer/Book/C  3e20110915.pdf Schaffer, C. ''Data Structures and Algorithm Analysis'']  {{Data structures}} {{Data types}} {{Data model}}  {{DEFAULTSORT:Data Structure}} [[Category:Data structures| ]]  [[ar:بنية بيانات]] [[ast:Estructura de datos]] [[bn:উপাত্ত সংগঠন]] [[be:Структура даных]] [[bg:Структура от данни]] [[bs:Struktura podataka]] [[ca:Estructura de dades]] [[da:Datastruktur]] [[de:Datenstruktur]] [[et:Andmestruktuur]] [[el:Δομή δεδομένων]] [[es:Estructura de datos]] [[fa:ساختمان داده‌ها]] [[fr:Structure de données]] [[ko:자료 구조]] [[hr:Podatkovna struktura]] [[id:Struktur data]] [[is:Gagnagrind]] [[it:Struttura dati]] [[he:מבנה נתונים]] [[kk:Мәліметтер құрылымы]] [[lv:Datu struktūras]] [[hu:Adatszerkezet]] [[ml:ഡാറ്റാ സ്ട്രക്‌ച്ചർ]] [[ms:Struktur data]] [[nl:Datastructuur]] [[ja:データ構造]] [[no:Datastruktur]] [[pl:Struktura danych]] [[pt:Estrutura de dados]] [[ro:Structură de date]] [[ru:Структура данных]] [[sq:Struktura e të dhënave]] [[simple:Data structure]] [[sk:Údajová štruktúra]] [[sl:Podatkovna struktura]] [[ckb:پێکھاتەدراوە]] [[sr:Структура података]] [[fi:Tietorakenne]] [[sv:Datastruktur]] [[th:โครงสร้างข้อมูล]] [[tr:Veri yapısı]] [[uk:Структура даних]] [[vi:Cấu trúc dữ liệu]] [[zh:数据结构]]
{{Multiple issues| {{refimprove|date=December 2011}} {{condense|date=November 2011}} {{toolong|date=August 2012}} {{longintro|date=August 2012}} }} A '''database''' is an organized collection of [[data]], today typically in digital form. The data are typically organized to model relevant aspects of reality (for example, the availability of rooms in hotels), in a way that supports processes requiring this information (for example, finding a hotel with vacancies).  The term ''database'' is correctly applied to the data and their supporting [[data structures]], and not to the [[database management system]] (DBMS). The database data collection with DBMS is called a [[database system]].  The term ''database system'' implies that the data is managed to some level of quality (measured in terms of [[Accuracy#In_information_systems|accuracy]], availability, usability, and resilience) and this in turn often implies the use of a general-purpose database management system (DBMS).<ref name=Ullman>[[Jeffrey Ullman]] and Jennifer widom 1997: ''First course in database systems'', Prentice-Hall Inc., Simon & Schuster, Page 1, ISBN 0-13-861337-0.</ref> A general-purpose DBMS is typically a complex [[Computer software|software]] system that meets many usage requirements, and the databases that it maintains are often large and complex. The utilization of databases is now so widespread that virtually every technology and product relies on databases and DBMSs for its development and commercialization, or even may have such software embedded in it. Also, organizations and companies, from small to large, depend heavily on databases for their operations.  Well known DBMSs include [[Oracle Database|Oracle]], [[IBM DB2]], [[Microsoft SQL Server]], [[Microsoft Access]], [[PostgreSQL]], [[MySQL]], and [[SQLite]]. A database is not generally [[Software portability|portable]] across different DBMS, but different DBMSs can [[Interoperation|inter-operate]] to some degree by using [[Technical standard|standard]]s like [[SQL]] and [[ODBC]] together to support a single application. A DBMS also needs to provide effective [[Run time (program lifecycle phase)|run-time]] execution to properly support (e.g., in terms of [[IT Performance Management|performance]], [[availability]], and [[security]]) as many [[end-user]]s as needed.  A way to classify databases involves the type of their contents, for example: [[Bibliographic database|bibliographic]], document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance.  The term ''database'' may be narrowed to specify particular aspects of organized collection of data and may refer to the logical database, to the physical database as data content in [[computer data storage]] or to many other database sub-definitions.  ==History== ===Database concept=== The database concept has evolved since the 1960s to ease increasing difficulties in designing, building, and maintaining complex [[information system]]s (typically with many concurrent end-users, and with a large amount of diverse data). It has evolved together with [[database management systems]] which enable the effective handling of databases. Though the terms database and DBMS define different entities, they are inseparable: a database's properties are determined by its supporting DBMS. The [[Oxford English dictionary]] cites{{citation needed|at lest here citation needed|date=November 2011}} a 1962 technical report as the first to use the term "data-base." With the progress in technology in the areas of [[processors]], [[computer memory]], [[computer storage]] and [[computer networks]], the sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitudes. For decades it has been unlikely that a complex information system can be built effectively without a proper database supported by a DBMS. The utilization of databases is now spread to such a wide degree that virtually every technology and product relies on databases and DBMSs for its development and commercialization, or even may have such embedded in it. Also, organizations and companies, from small to large, heavily depend on databases for their operations.  No widely accepted exact definition exists for DBMS. However, a system needs to provide considerable functionality to qualify as a DBMS. Accordingly its supported data collection needs to meet respective usability requirements (broadly defined by [[Database#Major database usage requirements|the requirements below]]) to qualify as a database. Thus, a database and its supporting DBMS are defined here by a set of general requirements listed below. Virtually all existing mature DBMS products meet these requirements to a great extent, while less mature either meet them or converge to meet them.  ===Evolution of database and DBMS technology=== :See also ''[[Database management system#History]]''  The introduction of the term ''database'' coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing.  In the earliest database systems, efficiency was perhaps the primary concern, but it was already recognized that there were other important objectives. One of the key aims was to make the data independent of the logic of application programs, so that the same data could be made available to different applications.  The first generation of database systems were ''[[Navigational database|navigational]]'',<ref>{{citation | author =  C. W. Bachmann | title = The Programmer as Navigator}}</ref> applications typically accessed data by following pointers from one record to another. The two main data models at this time were the [[Hierarchical database model|hierarchical model]], epitomized by IBM's IMS system, and the [[Codasyl]] model ([[Network model]]), implemented in a number of products such as [[IDMS]].  The [[relational model]], first proposed in 1970 by [[Edgar F. Codd]], departed from this tradition by insisting that applications should search for data by content, rather than by following links. This was considered necessary to allow the content of the database to evolve without constant rewriting of applications. Relational systems placed heavy demands on processing resources, and it was not until the mid 1980s that computing hardware became powerful enough to allow them to be widely deployed. By the early 1990s, however, relational systems were dominant for all large-scale data processing applications, and they remain dominant today (2012) except in niche areas. The dominant database language is the standard SQL for the Relational model, which has influenced database languages also for other data models.  Because the relational model emphasizes search rather than navigation, it does not make relationships between different entities explicit in the form of pointers, but represents them rather using ''primary keys'' and ''foreign keys''. While this is a good basis for a query language, it is less well suited as a modeling language. For this reason a different model, the [[entity-relationship model]] which emerged shortly later (1976), gained popularity for [[database design]].  In the period since the 1970s database technology has kept pace with the increasing resources becoming available from the computing platform: notably the rapid increase in the capacity and speed (and reduction in price) of disk storage, and the increasing capacity of main memory. This has enabled ever larger databases and higher throughputs to be achieved.  The rigidity of the relational model, in which all data is held in tables with a fixed structure of rows and columns, has increasingly been seen as a limitation when handling information that is richer or more varied in structure than the traditional 'ledger-book' data of corporate information systems: for example, document databases, engineering databases, multimedia databases, or databases used in the molecular sciences. Various attempts have been made to address this problem, many of them gathering under banners such as ''post-relational'' or [[NoSQL]]. Two developments of note are the [[object database]] and the [[XML database]]. The vendors of relational databases have fought off competition from these newer models by extending the capabilities of their own products to support a wider variety of data types.  ====General-purpose DBMS====  A DBMS has evolved into a complex software system and its development typically requires thousands of person-years of development effort.{{citation needed|date=December 2011}} Some general-purpose DBMSs,  like Oracle, Microsoft SQL Server, and IBM DB2, have been undergoing upgrades for thirty years or more. General-purpose DBMSs aim to satisfy as many applications as possible, which typically makes them even more complex than special-purpose databases. However, the fact that they can be used "off the shelf", as well as their amortized cost over many applications and instances, makes them an attractive alternative (Vs. one-time development) whenever they meet an application's requirements.  Though attractive in many cases, a general-purpose DBMS is not always the optimal solution: When certain applications are pervasive with many operating instances, each with many users, a general-purpose DBMS may introduce unnecessary overhead and too large "footprint" (too large amount of unnecessary, unutilized software code). Such applications usually justify dedicated development. Typical examples are [[email]] systems, though they need to possess certain DBMS properties: email systems are built in a way that optimizes email messages handling and managing, and do not need significant portions of a general-purpose DBMS functionality.  =====Types of people involved=====  Three types of people are involved with a general-purpose DBMS: #'''DBMS developers''' - These are the people that design and build the DBMS product, and the only ones who touch its code. They are typically the employees of a DBMS vendor (e.g., [[Oracle Corporation|Oracle]], [[IBM]], [[Microsoft]], [[Sybase]]), or, in the case of [[Open source]] DBMSs (e.g., MySQL), volunteers or people supported by interested companies and organizations. They are typically skilled [[Systems programming|systems programmers]]. DBMS development is a complicated task, and some of the popular DBMSs have been under development and enhancement (also to follow progress in technology) for decades. #'''[[Computer programming|Application developers]]''' and '''[[database administrator]]s''' - These are the people that design and build a database-based application that uses the DBMS. The latter group members design the needed database and maintain it. The first group members write the needed application programs which the application comprises. Both are well familiar with the DBMS product and use its user interfaces (as well as usually other tools) for their work. Sometimes the application itself is packaged and sold as a separate product, which may include the DBMS inside (see [[embedded database]]; subject to proper DBMS licensing), or sold separately as an add-on to the DBMS. #'''Application's end-users''' (e.g., accountants, insurance people, medical doctors, etc.) - These people know the application and its end-user interfaces, but need not know nor understand the underlying DBMS. Thus, though they are the intended and main beneficiaries of a DBMS, they are only indirectly involved with it.  ====Database machines and appliances==== {{Main|Database machine}}  In the 1970s and 1980s attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at lower cost. Examples were IBM [[System/38]], the early offering of [[Teradata]], and the [[Britton Lee, Inc.]] database machine. Another approach to hardware support for database management was [[International Computers Limited|ICL]]'s [[Content Addressable File Store|CAFS]] accelerator, a hardware disk controller with programmable search capabilities. In the long term these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like [[Netezza]] and [[Oracle]] ([[Exadata]]).  ===Database research=== Database research has been an active and diverse area, with many specializations, carried out since the early days of dealing with the database concept in the 1960s. It has strong ties with database technology and DBMS products. Database research has taken place at research and development groups of companies (e.g., notably at [[IBM Research]], who contributed technologies and ideas virtually to any DBMS existing today), [[research institute]]s, and [[academia]]. Research has been done both through [[Database theory|theory]] and [[prototype]]s. The interaction between research and database related product development has been very productive to the database area, and many related key concepts and technologies emerged from it. Notable are the Relational and the Entity-relationship [[Data model|models]], the [[Database transaction|atomic transaction]] concept and related [[Concurrency control]] techniques, Query languages and [[Query optimization]] methods, [[RAID]], and more. Research has provided deep [[insight]] to virtually all aspects of databases, though not always has been pragmatic, effective (and cannot and should not always be: research is exploratory in nature, and not always leads to accepted or useful ideas). Ultimately market forces and real needs determine the selection of problem solutions and related technologies, also among those proposed by research. However, occasionally, not the best and most [[Elegance|elegant]] solution wins (e.g., SQL). Along their history DBMSs and respective databases, to a great extent, have been the outcome of such research, while real product requirements and challenges triggered database research directions and sub-areas.  The database research area has several notable dedicated [[academic journal]]s (e.g., [[ACM Transactions on Database Systems]]-TODS, [[Data and Knowledge Engineering]]-DKE, and more) and annual [[Academic conference|conference]]s (e.g., [[Association for Computing Machinery|ACM]] [[SIGMOD]], ACM [[Symposium on Principles of Database Systems|PODS]], [[VLDB]], [[IEEE]] ICDE, and more), as well as an active and quite heterogeneous (subject-wise) research community all over the world.  ==Database type examples==  The following are examples of various database types. Some of them are not main-stream types, but most of them have received special attention (e.g., in research) due to end-user requirements. Some exist as specialized DBMS products, and some have their functionality types incorporated in existing general-purpose DBMSs.  *'''Active database''' {{Main|Active database}}  ::An ''active database'' is a database that includes an event-driven architecture which can respond to conditions both inside and outside the database.  Possible uses include security monitoring, alerting, statistics gathering and authorization.  ::Most modern relational databases include active database features in the form of [[database trigger]].  *'''Cloud database''' {{Main|Cloud database}} ::A ''Cloud database'' is a database that relies on [[Cloud computing|cloud technology]]. Both the database and most of its DBMS reside remotely, "in the cloud," while its applications are both developed by programmers and later maintained and utilized by (application's) end-users through a [[web browser]] and [[Open API]]s. More and more such database products are emerging, both of new vendors and by virtually all established database vendors.  *'''Data warehouse''' {{Main|Data warehouse}}  ::Data warehouses archive data from operational databases and often from external sources such as market research firms. Often operational data undergoes transformation on its way into the warehouse, getting summarized, anonymized, reclassified, etc. The warehouse becomes the central source of data for use by managers and other end-users who may not have access to operational data. For example, sales data might be aggregated to weekly totals and converted from internal product codes to use [[Universal Product Code|UPC]]s so that it can be compared with [[ACNielsen]] data. Some basic and essential components of data warehousing include retrieving, analyzing, and [[Data mining|mining]] data, transforming,loading and managing data so as to make it available for further use.  ::Operations in a data warehouse are typically concerned with bulk data manipulation, and as such, it is unusual and inefficient to target individual rows for update, insert or delete.  Bulk native loaders for input data and bulk SQL passes for aggregation are the norm.  *'''Distributed database''' {{Main|Distributed database}}  ::The definition of a ''distributed database'' is broad, and may be utilized in different meanings. In general it typically refers to a modular DBMS architecture that allows distinct DBMS instances to cooperate as a single DBMS over processes, computers, and sites, while managing a single database distributed itself over multiple computers, and different sites.  ::Examples are databases of local work-groups and departments at regional offices, branch offices, manufacturing plants and other work sites. These databases can include both segments shared by multiple sites, and segments specific to one site and used only locally in that site.  *'''Document-oriented database''' {{Main|Document-oriented database}}  ::A document-oriented database is a computer program designed for storing, retrieving, and managing document-oriented, or semi structured data, information. Document-oriented databases are one of the main categories of so-called NoSQL databases and the popularity of the term "document-oriented database" (or "document store") has grown with the use of the term NoSQL itself.  ::Utilized to conveniently store, manage, edit and retrieve documents.  *'''Embedded database''' {{Main|Embedded database}} ::An ''embedded database'' system is a DBMS which is tightly integrated with an [[application software]] that requires access to stored data in a way that the DBMS is “hidden” from the application’s end-user and requires little or no ongoing maintenance. It is actually a broad technology category that includes DBMSs with differing properties and target markets. The term "embedded database" can be confusing because only a small subset of embedded database products is used in [[Real-time computing|real-time]] [[embedded systems]] such as [[Telephone switch#Digital switches|telecommunications switches]] and [[consumer electronics]] devices.<ref>Graves, Steve.  [http://www.embedded-computing.com/articles/id/?2020 "COTS Databases For Embedded Systems"], ''Embedded Computing Design'' magazine, January, 2007.  Retrieved on August 13, 2008.</ref>  *'''End-user database'''  ::These databases consist of data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. Some of them are much simpler than full fledged DBMSs, with more elementary DBMS functionality (e.g., not supporting multiple concurrent end-users on a same database), with basic programming interfaces, and a relatively small "foot-print" (not much code to run as in "regular" general-purpose databases). However, also available general-purpose DBMSs can often be used for such purpose, if they provide basic user-interfaces for straightforward database applications (limited query and data display; no real programming needed), while still enjoying the database qualities and protections that these DBMSs can provide.  *'''Federated database and multi-database''' {{Main|Federated database system|Heterogeneous Database System}}  ::A ''federated database'' is an integrated database that comprises several distinct databases, each with its own DBMS. It is handled as a single database by a [[federated database system|federated database management system]] (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (which makes it a [[Heterogeneous Database System|heterogeneous database]]), and provides them with an integrated conceptual view. The constituent databases are interconnected via [[computer network]], and may be geographically decentralized.  ::Sometime the term ''multi-database'' is used as a synonym to federated database, though it may refer to a less integrated (e.g., without an FDBMS and a managed integrated schema) group of databases that cooperate in a single application. In this case typically [[Middleware (distributed applications)|middleware]] for distribution is used which typically includes an atomic commit protocol (ACP), e.g., the [[two-phase commit protocol]], to allow [[Distributed transaction|distributed (global) transactions]] (vs. local transactions confined to a single DBMS) across the participating databases.  *'''Graph database''' {{Main|Graph database}}  ::A ''graph database'' is a kind of NoSQL database that uses [[Graph (data structure)|graph structures]] with nodes, edges, and properties to represent and store information. General graph databases that can store any graph are distinct from specialized graph databases such as [[triplestore]]s and [[network database model|network databases]].  *'''Hypermedia databases'''  ::The [[World Wide Web]] can be thought of as a database, albeit one spread across millions of independent computing systems. Web browsers "process" this data one page at a time, while [[web crawlers]] and other software provide the equivalent of database indexes to support search and other activities.  *'''Hypertext database''' {{Main|Hypertext}}  ::In a ''Hypertext database'', any word or a piece of text representing an object, e.g., another piece of text, an article, a picture, or a film, can be linked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example they are useful for organizing online [[encyclopedia]]s, where users can conveniently jump in the texts, in a controlled way, by using [[hyperlink]]s.  *'''In-memory database''' {{Main|In-memory database}}  ::An ''in-memory database'' (IMDB; also ''main memory database'' or '''MMDB''') is a database that primarily resides in [[main memory]], but typically backed-up by non-volatile computer data storage. Main memory databases are faster than disk databases. Accessing data in memory reduces the I/O reading activity when, for example, querying the data.  In applications where response time is critical, such as telecommunications network equipment, main memory databases are often used.<ref>{{cite news| url=http://findarticles.com/p/articles/mi_m0EIN/is_2002_June_24/ai_87694370 | work=Business Wire | title=TeleCommunication Systems Signs up as a Reseller of TimesTen; Mobile Operators and Carriers Gain Real-Time Platform for Location-Based Services | date=2002-06-24}}</ref>  *'''Knowledge base''' {{Main|Knowledge base}}  ::A '''knowledge base''' (abbreviated '''KB''', '''kb''' or Δ<ref>Argumentation in Artificial Intelligence by Iyad Rahwan, Guillermo R. Simari</ref><ref>{{cite web | title = OWL DL Semantics | url = http://www.obitko.com/tutorials/ontologies-semantic-web/owl-dl-semantics.html | accessdate = 10 December 2010}}</ref>) is a special kind of database for [[knowledge management]], providing the means for the computerized collection, organization, and [[Information retrieval|retrieval]] of [[knowledge]]. Also a collection of data representing problems with their solutions and related experiences.  *'''Operational database'''  ::These databases store detailed data about the operations of an organization. They are typically organized by subject matter, process relatively high volumes of updates using [[transaction (database)|transactions]]. Essentially every major organization on earth uses such databases. Examples include [[Customer relationship management|customer databases]] that record contact, credit, and demographic information about a business' customers, personnel databases that hold information such as salary, benefits, skills data about employees, Enterprise resource planning that record details about product components, parts inventory, and financial databases that keep track of the organization's money, accounting and financial dealings.  *'''Parallel database''' {{Main|Parallel database}}  ::A '''parallel database''', run by a parallel DBMS, seeks to improve performance through [[Parallel computing|parallelization]] for tasks such as loading data, building indexes and evaluating queries. Parallel databases improve processing and [[input/output]] speeds by using multiple [[central processing unit]]s (CPUs) (including [[multi-core processor]]s) and [[Computer data storage|storage]] in parallel. In parallel processing, many operations are performed simultaneously, as opposed to serial, sequential processing, where operations are performed with no time overlap.  ::The major parallel DBMS architectures (which are induced by the underlying [[Computer hardware|hardware]] architecture are: ::* '''[[Shared memory#In hardware|Shared memory architecture]]''', where multiple processors share the main memory space, as well as other data storage. ::* '''Shared disk architecture''', where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage. ::* '''[[Shared nothing architecture]]''', where each processing unit has its own main memory and other storage.  *'''Real-time database''' {{Main|Real time database}} If a DBMS system responses users' request in a given time period, it can be regarded as a real time database.  *'''Spatial database''' {{Main|Spatial database}} A spatial database can store the data with multidimensional features. The queries on such data include location based queries, like "where is the closest hotel in my area".  *'''Temporal database''' {{Main|Temporal database}}  A temporal database is a database with built-in time aspects, for example a temporal data model and a temporal version of Structured Query Language (SQL). More specifically the temporal aspects usually include valid-time and transaction-time.   *'''Unstructured-data database''' {{Main|Unstructured data}} ::An unstructured-data database is intended to store in a manageable and protected way diverse objects that do not fit naturally and conveniently in common databases. It may include email messages, documents, journals, multimedia objects etc. The name may be misleading since some objects can be highly structured. However, the entire possible object collection does not fit into a predefined structured framework. Most established DBMSs now support unstructured data in various ways, and new dedicated DBMSs are emerging.  ==Major database usage requirements==  The major purpose of a database is to provide the information system (in its broadest sense) that utilizes it with the information the system needs according to its own requirements. A certain broad set of requirements refines this general goal. These database requirements translate to requirements for the respective DBMS, to allow conveniently building a proper database for the given application. If this goal is met by a DBMS, then the designers and builders of the specific database can concentrate on the application's aspects, and not deal with building and maintaining the underlying DBMS. Also, since a DBMS is complex and expensive to build and maintain, it is not economical to build such a new tool (DBMS) for every application. Rather it is desired to provide a flexible tool for handling databases for as many as possible given applications, i.e., a general-purpose DBMS.  ===Functional requirements===  Certain general functional requirements need to be met in conjunction with a database. They describe what is needed to be defined in a database for any specific application.  ====Defining the structure of data: Data modeling and Data definition languages====  The database needs to be based on a data model that is sufficiently rich to describe in the database all the needed respective application's aspects. A data definition language exists to describe the databases within the data model. Such language is typically data model specific.  ====Manipulating the data: Data manipulation languages and Query languages====  A database data model needs support by a sufficiently rich data manipulation language to allow all database manipulations and information generation (from the data) as needed by the respective application. Such language is typically data model specific.  ====Protecting the data: Setting database security types and levels====  The DB needs built-in security means to protect its content (and users) from dangers of unauthorized users (either [[human]]s or [[Computer program|programs]]). Protection is also provided from types of unintentional breach. Security types and levels should be defined by the database owners.  ====Describing processes that use the data: Workflow and Business process modeling==== {{Main|Workflow|Business process modeling}}  Manipulating database data often involves processes of several interdependent steps, at different times (e.g., when different people's interactions are involved; e.g., generating an insurance policy). Data manipulation languages are typically intended to describe what is needed in a single such step. Dealing with multiple steps typically requires writing quite complex programs. Most applications are programmed using common [[programming language]]s and software development tools. However the area of process description has evolved in the frameworks of ''[[workflow]]'' and ''[[Business process modeling|business processes]]'' with supporting languages and software packages which considerably simplify the tasks. Traditionally these frameworks have been out of the scope of common DBMSs, but utilization of them has become common-place, and often they are provided as add-on's to DBMSs.  ===Operational requirements===  Operational requirements are needed to be met by a database in order to effectively support an application when operational. Though it typically may be expected that operational requirements are automatically met by a DBMS, in fact it is not so in most of the cases: To be met substantial work of design and tuning is typically needed by database administrators. This is typically done by specific instructions/operations through special database user interfaces and tools, and thus may be viewed as secondary functional requirements (which are not less important than the primary).  ====Availability====  A DB should maintain needed levels of availability, i.e., the DB needs to be available in a way that a user's action does not need to wait beyond a certain time range before starting executing upon the DB. Availability also relates to failure and recovery from it (see [[Database#Recovery from failure and disaster|Recovery from failure and disaster]] below): Upon failure and during recovery normal availability changes, and special measures are needed to satisfy availability requirements.  ====Performance====  Users' actions upon the DB should be executed within needed time ranges.  ====Isolation between users====  When multiple users access the database concurrently the actions of a user should be uninterrupted and unaffected by actions of other users. These concurrent actions should maintain the DB's consistency (i.e., keep the DB from corruption).  ====Recovery from failure and disaster==== {{Main|Data recovery|Disaster recovery}} All computer systems, including DBMSs, are prone to failures for many reasons (both software and hardware related). Failures typically corrupt the DB, typically to the extent that it is impossible to repair it without special measures. The DBMS should provide automatic [[Data recovery|recovery]] from failure procedures that repair the DB and return it to a well defined state.  ====Backup and restore==== {{Main|Backup}} Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this a '''backup''' operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When this state is needed, i.e., when it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are utilized to '''restore''' that state.  ====Data independence==== {{Main|Data independence}}  Data independence pertains to a database's [[Systems Development Life Cycle|life cycle]] (see [[Database#Database building, maintaining, and tuning|Database building, maintaining, and tuning]] below). It strongly impacts the convenience and cost of maintaining an application and its database, and has been the major motivation for the emergence and success of the Relational model, as well as the convergence to a common database architecture. In general the term "data independence" means that changes in the database's structure do not require changes in its application's computer programs, and that changes in the database at a certain architectural level (see below) do not affect the database's levels above. Data independence is achieved to a great extent in contemporary DBMS, but it is not completely attainable, and achieved at different degrees for different types of database structural changes.  ==Major database functional areas==  The functional areas are domains and subjects that have evolved in order to provide proper answers and solutions to the functional requirements above.  ===Data models=== {{Main|Data model|Database model}}  A data model is an abstract structure that provides the means to effectively describe specific data structures needed to model an application. As such a data model needs sufficient expressive power to capture the needed aspects of applications. These applications are often typical to commercial companies and other organizations (like manufacturing, human-resources, stock, banking, etc.). For effective utilization and handling it is desired that a data model is relatively simple and intuitive. This may be in conflict with high expressive power needed to deal with certain complex applications. Thus any popular general-purpose data model usually well balances between being intuitive and relatively simple, and very complex with high expressive power. The application's semantics is usually not explicitly expressed in the model, but rather implicit (and detailed by documentation external to the model) and hinted to by data item types' names (e.g., "part-number") and their connections (as expressed by generic data structure types provided by each specific model).  ====Early data models==== These models were popular in the 1960s, 1970s, but nowadays can be found primarily in old [[legacy system]]s. They are characterized primarily by being [[Navigational database|navigational]] with strong connections between their logical and physical representations, and deficiencies in [[data independence]].  =====Hierarchical model===== {{Main|Hierarchical database model}}  In the Hierarchical model different record types (representing real-world entities) are embedded in a predefined hierarchical ([[Tree (data structure)|tree]]-like) structure. This hierarchy is used as the physical order of records in storage. Record access is done by navigating through the data structure using [[pointer (computer programming)|pointer]]s combined with sequential accessing.  This model has been supported primarily by the IBM [[Information Management System|IMS]] DBMS, one of the earliest DBMSs. Various limitations of the model have been compensated at later IMS versions by additional logical hierarchies imposed on the base physical hierarchy.  =====Network model===== {{Main|Network model (database)}}  In this model a hierarchical relationship between two record types (representing real-world entities) is established by the ''set'' construct. A set consists of circular [[linked list]]s where one record type, the set owner or parent, appears once in each circle, and a second record type, the subordinate or child, may appear multiple times in each circle. In this way a hierarchy may be established between any two record types, e.g., type A is the owner of B. At the same time another set may be defined where B is the owner of A. Thus all the sets comprise a general [[directed graph]] (ownership defines a direction), or ''network'' construct. Access to records is either sequential (usually in each record type) or by navigation in the circular linked lists.  This model is more general and powerful than the hierarchical, and has been the most popular before being replaced by the Relational model. It has been [[standardization|standardized]] by [[CODASYL]]. Popular DBMS products that utilized it were [[Cincom Systems]]' Total and [[Cullinet]]'s [[IDMS]].  =====Inverted file model===== {{Main|Inverted index}}  An ''inverted file'' or ''[[inverted index]]'' of a first file, by a field in this file (the inversion field), is a second file in which this field is the key. A record in the second file includes a key and pointers to records in the first file where the inversion field has the value of the key. This is also the logical structure of contemporary [[Index (database)|database indexes]]. The related ''Inverted file data model'' utilizes inverted files of primary database files to efficiently directly access needed records in these files.  Notable for using this data model is the [[ADABAS]] DBMS of [[Software AG]], introduced in 1970. ADABAS has gained considerable customer base and exists and supported until today. In the 1980s it has adopted the Relational model and SQL in addition to its original tools and languages.  ====Relational model==== {{Main|Relational model}} The [[relational model]] is a simple model that provides flexibility. It organizes data based on two-dimensional arrays known as [[Relation (database)|relations]], or tables as related to databases. These relations consist of a heading and a set of zero or more [[Tuple#Relational_model|tuples]] in arbitrary order. The heading is an unordered set of zero or more attributes, or columns of the table. The tuples are a set of unique attributes mapped to values, or the rows of data in the table. Data can be associated across multiple tables with a key.  A key is a single, or set of multiple, attribute(s) that is common to both tables. The most common language associated with the relational model is the Structured Query Language ([[SQL]]), though it differs in some places.  ====Entity-relationship model==== {{Main|Entity-relationship model}} {{Expand section|date=June 2011}}  ====Object model==== {{Main|Object model|Object database}} {{Expand section|date=June 2011}} In {{as of | 2009 | alt = recent years}}, the [[object-oriented]] paradigm has been applied in areas such as engineering and spatial databases, telecommunications and in various scientific domains. The conglomeration of object oriented programming and database technology led to this new kind of database. These databases attempt to bring the database world and the application-programming world closer together, in particular by ensuring that the database uses the same [[type system]] as the application program. This aims to avoid the overhead (sometimes referred to as the ''[[Object-Relational impedance mismatch|impedance mismatch]]'') of converting information between its representation in the database (for example as rows in tables) and its representation in the application program (typically as objects). At the same time, object databases attempt to introduce key ideas of object programming, such as [[encapsulation (computer science)|encapsulation]] and [[polymorphism (computer science)|polymorphism]], into the world of databases.  A variety of these ways have been tried{{By whom|date=October 2009}} for storing objects in a database. Some products have approached the problem from the application-programming side, by making the objects manipulated by the program [[Persistence (computer science)|persistent]]. This also typically requires the addition of some kind of query language, since conventional programming languages do not provide language-level functionality for finding objects based on their information content. Others{{Which?|date=November 2009}} have attacked the problem from the database end, by defining an object-oriented data model for the database, and defining a database programming language that allows full programming capabilities as well as traditional query facilities.  ====Object relational model==== {{Main|Object-relational database}} {{Expand section|date=June 2011}}  ====XML as a database data model==== {{Main|XML database|XML}} {{Expand section|date=June 2011}}  ====Other database models==== {{Expand section|date=June 2011}} Products offering a more general data model than the relational model are sometimes classified as ''post-relational''.<ref name="CONR">''Introducing databases'' by Stephen Chu, in Conrick, M. (2006)  ''Health informatics: transforming healthcare with technology'', Thomson, ISBN 0-17-012731-1, p. 69.</ref> Alternate terms include "hybrid database", "Object-enhanced RDBMS" and others. The data model in such products incorporates [[relation (database)|relations]] but is not constrained by [[E.F. Codd]]'s Information Principle, which requires that{{quote| all information in the database must be cast explicitly in terms of values in relations and in no other way<ref>{{cite journal |journal=Intelligent Enterprise |url=http://intelligent-enterprise.informationweek.com/db_area/archives/1999/990106/online1.jhtml;jsessionid=Y2UNK1QFKXMBTQE1GHRSKH4ATMY32JVN |date=June 1, 1999 |volume=2 |issue=8 |title=When's an extension not an extension? |last=Date |first=C. J. |authorlink=Christopher J. Date}}</ref>}}  Some of these extensions to the relational model integrate concepts from technologies that pre-date the relational model. For example, they allow representation of a directed graph with [[tree data structure|trees]] on the nodes. The German company ''sones'' implements this concept in its [[GraphDB]].  Some post-relational products extend relational systems with non-relational features. Others arrived in much the same place by adding relational features to pre-relational systems. Paradoxically, this allows products that are historically pre-relational, such as [[Pick operating system|PICK]] and [[MUMPS]], to make a plausible claim to be post-relational.  The resource space model (RSM) is a non-relational data model based on multi-dimensional classification.<ref>{{cite book |last= Zhuge |first=H. |title=The Web Resource Space Model |publisher=Springer |year=2008 |isbn=978-0-387-72771-4 |series=Web Information Systems Engineering and Internet Technologies Book Series |volume=4}}</ref>  ===Database languages=== {{Main|Data definition language|Data manipulation language|Query language}}  Database languages are dedicated programming languages, tailored and utilized to *define a database (i.e., its specific data types and the relationships among them), *manipulate its content (e.g., insert new data occurrences, and update or delete existing ones), and *query it (i.e., request information: compute and retrieve any information based on its data). Database languages are data-model-specific, i.e., each language assumes and is based on a certain structure of the data (which typically differs among different data models). They typically have commands to instruct execution of the desired operations in the database. Each such command is equivalent to a complex expression (program) in a regular programming language, and thus programming in dedicated (database) languages simplifies the task of handling databases considerably. An expressions in a database language is automatically transformed (by a [[compiler]] or interpreter, as regular programming languages) to a proper computer program that runs while accessing the database and providing the needed results. The following are notable examples:  ====SQL for the Relational model==== {{Main|SQL}} A major Relational model language supported by all the relational DBMSs and a standard.  SQL was one of the first commercial languages for the relational model.  Despite not adhering to [[Codd's 12 rules|the relational model as described by Codd]], it has become the most widely used database language.<ref name="SQL-Fundamentals">{{cite web  | last = Chapple  | first = Mike  | title = SQL Fundamentals  | work = Databases  | publisher = About.com  | url = http://databases.about.com/od/sql/a/sqlfundamentals.htm  | accessdate = 2009-01-28 }}</ref><ref name="IBM-sql">{{cite web | title = Structured Query Language (SQL) | publisher = International Business Machines | url = http://publib.boulder.ibm.com/infocenter/db2luw/v9/index.jsp?topic=com.ibm.db2.udb.admin.doc/doc/c0004100.htm | date = October 27, 2006 | accessdate = 2007-06-10 }}</ref> Though often described as, and to a great extent is a [[Declarative programming|declarative language]], SQL also includes [[Procedural programming|procedural]] elements. SQL became a [[Technical standard|standard]] of the [[American National Standards Institute]] (ANSI) in 1986, and of the [[International Organization for Standards]] (ISO) in 1987. Since then the standard has been enhanced several times with added features. However, issues of SQL code portability between major RDBMS products still exist due to lack of full compliance with, or different interpretations of the standard. Among the reasons mentioned are the large size, and incomplete specification of the standard, as well as [[vendor lock-in]].  ====OQL for the Object model==== {{Main|OQL}} An [[Object database|object model]] language standard (by the [[Object Data Management Group]]) that has influenced the design of some of the newer query languages like [[JDOQL]] and [[EJB QL]], though they cannot be considered as different flavors of OQL.  ====XQuery for the XML model==== {{Main|XQuery|XML|SQL/XML}} [[XQuery]] is an [[XML]] based database language (also named [[XQL]]). [[SQL/XML]] combines [[XQuery]] and [[XML]] with [[SQL]].<ref name="wagner">{{Citation  |title     = SQL/XML:2006 - Evaluierung der Standardkonformität ausgewählter Datenbanksysteme  |chapter   = 1. Auflage  |author    = Wagner, Michael  |year      = 2010  |publisher = Diplomica Verlag  |isbn      = 3-8366-9609-6 }}</ref>  ===Database architecture===  Database architecture (to be distinguished from DBMS architecture; [[Database#DBMS architecture: major DBMS components|see below]]) may be viewed, to some extent, as an extension of [[data modeling]]. It is used to conveniently answer requirements of different end-users from a same database, as well as for other benefits. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but not other many details about employees, that are the interest of the [[human resources]] department. Thus different departments need different ''views'' of the company's database, that both include the employees' payments, possibly in a different level of detail (and presented in different visual forms). To meet such requirement effectively database architecture consists of three levels: ''external'', ''conceptual'' and ''internal''. Clearly separating the three levels was a major feature of the relational database model implementations that dominate 21st century databases.<ref name=date31>{{harvnb|Date|1990|pages=31–32}}</ref>  *The '''external level''' defines how each end-user type understands the organization of its respective relevant data in the database, i.e., the different needed end-user views. A single database can have any number of views at the external level. *The '''conceptual level''' unifies the various external views into a coherent whole, global view.<ref name=date31/> It provides the common-denominator of all the external views. It comprises all the end-user needed generic data, i.e., all the data from which any view may be derived/computed. It is provided in the simplest possible way of such generic data, and comprises the back-bone of the database. It is out of the scope of the various database end-users, and serves database application developers and defined by database administrators that build the database. *The '''Internal level''' (or ''Physical level'') is as a matter of fact part of the database implementation inside a DBMS (see Implementation section below). It is concerned with cost, performance, scalability and other operational matters. It deals with storage layout of the conceptual level, provides supporting storage-structures like [[Index (database)|indexes]], to enhance performance, and occasionally stores data of individual views ([[materialized view]]s), computed from generic data, if performance justification exists for such redundancy. It balances all the external views' performance requirements, possibly conflicting, in attempt to optimize the overall database usage by all its end-uses according to the database goals and priorities.  All the three levels are maintained and updated according to changing needs by database administrators who often also participate in the database design.  The above three-level database architecture also relates to and being motivated by the concept of ''[[data independence]]'' which has been described for long time as a desired database property and was one of the major initial driving forces of the Relational model. In the context of the above architecture it means that changes made at a certain level do not affect definitions and software developed with higher level interfaces, and are being incorporated at the higher level automatically. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which saves substantial change work that would be needed otherwise.  In summary, the conceptual is a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it is uncomplicated by details of how the data is stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation (see Implementation section below), requires a different level of detail and uses its own data structure types, typically different in nature from the structures of the external and conceptual levels which are exposed to DBMS users (e.g., the data models above): While the external and conceptual levels are focused on and serve DBMS users, the concern of the internal level is effective implementation details.  ===Database security=== {{Main|Database security}}  Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).  The following are major areas of database security (among many others).  ====Access control==== {{Main|Access control}}  Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information).  Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.  ====Data security==== {{Main|Data security|Encryption}}  The definition of data security varies and may overlap with other database security aspects. Broadly it deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see [[Physical security]]), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see [[Data encryption]]).  ====Database audit==== {{Main|Database audit}}  Database audit primarily involves monitoring that no security breach, in all aspects, has happened. If security breach is discovered then all possible corrective actions are taken.  ===Database design=== {{Main|Database design}}  Database design is done before building it to meet needs of end-users within a given application/information-system that the database is intended to support. The database design defines the needed data and data structures that such a database comprises. A design is typically carried out according to the common three architectural levels of a database (see Database architecture above). First, the conceptual level is designed, which defines the over-all picture/view of the database, and reflects all the real-world elements (entities) the database intends to model, as well as the relationships among them. On top of it the external level, various views of the database, are designed according to (possibly completely different) needs of specific end-user types. More external views can be added later. External views requirements may modify the design of the conceptual level (i.e., add/remove entities and relationships), but usually a well designed conceptual level for an application well supports most of the needed external views. The conceptual view also determines the internal level (which primarily deals with data layout in storage) to a great extent. External views requirement may add supporting storage structures, like materialized views and indexes, for enhanced performance. Typically the internal layer is optimized for top performance, in an average way that takes into account performance requirements (possibly conflicting) of different external views according to their relative importance. While the conceptual and external levels design can usually be done independently of any DBMS (DBMS-independent design software packages exist, possibly with interfaces to some specific popular DBMSs), the internal level design highly relies on the capabilities and internal data structure of the specific DBMS utilized (see the Implementation section below).  A common way to carry out conceptual level design is to use the entity-relationship model (ERM) (both the basic one, and with possible enhancement that it has gone over), since it provides a straightforward, intuitive perception of an application's elements and semantics. An alternative approach, which preceded the ERM, is using the Relational model and dependencies (mathematical relationships) among data to [[Database normalization|normalize]] the database, i.e., to define the ("optimal") relations (data record or tupple types) in the database. Though a large body of research exists for this method it is more complex, less intuitive, and not more effective than the ERM method. Thus normalization is less utilized in practice than the ERM method.  The ERM may be less subtle than normalization in several aspects, but it captures the main needed dependencies which are induced by [[Unique key|keys]]/identifiers of entities and relationships. Also the ERM inherently includes the important inclusion dependencies (i.e., an entity instance that does not exist (has not been explicitly inserted) cannot appear in a relationship with other entities) which usually have been ignored in normalization.<ref name=Makowsky1986>Johann A. Makowsky, [[Victor M. Markowitz]] and Nimrod Rotics, 1986: [http://www.springerlink.com/content/p67756164r127m18/  "Entity-relationship consistency for relational schemas"] ''Proceedings of the 1986 Conference on Database Theory'' (ICDT '86), Lecture Notes in Computer Science, 1986, Volume 243/1986, pp. 306-322, Springer, {{doi|10.1007/3-540-17187-8_43}}</ref> In addition the ERM allows entity type generalization (the [[Is-a]] relationship) and implied property (attribute) [[Inheritance (object-oriented programming)|inheritance]] (similarly to the that found in the object model).  Another aspect of database design is its security. It involves both defining [[access control]] to database objects (e.g., Entities, Views) as well as defining security levels and methods for the data itself (See Database security above).  ====Entities and relationships==== {{Main|Entity-relationship model}}  The most common database design methods are based on the entity relationship model (ERM, or ER model). This model views the world in a simplistic but very powerful way: It consists of "Entities" and the "Relationships" among them. Accordingly a database consists of ''entity'' and ''relationship'' types, each with defined ''attributes'' (field types) that model concrete entities and relationships. Modeling a database in this way typically yields an effective one with desired properties (as in some ''normal forms''; see normalization below). Such models can be translated to any other data model required by any specific DBMS for building an effective database.  ====Database normalization==== {{Main|Database normalization}}  In the design of a [[relational database]], the process of organizing database relations to minimize redundancy is called ''normalization''. The goal is to produce well-structured relations so that additions, deletions, and modifications of a field can be made in just one relation (table) without worrying about appearance and update of the same field in other relations. The process is [[algorithm]]ic and based on dependencies (mathematical relations) that exist among relations' field types. The process result is bringing the database relations into a certain [[Database normalization#Normal forms|"normal form"]]. Several normal forms exist with different properties.  ===Database building, maintaining, and tuning=== {{Main|Database tuning}}  After designing a database for an application arrives the stage of building the database. Typically an appropriate [[Database#General-purpose DBMS|general-purpose DBMS]] can be selected to be utilized for this purpose. A DBMS provides the needed [[user interface]]s to be utilized by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).  When the database is ready (all its data structures and other needed components are defined) it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases the database becomes operational while empty from application's data, and data are accumulated along its operation.  After completing building the database and making it operational arrives the database maintenance stage: Various database parameters may need changes and [[Database tuning|tuning]] for better performance, application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.  ===Miscellaneous areas=== ====Database migration between DBMSs==== :See also ''[[Data migration#Database migration|Database migration]]'' in ''[[Data migration]]''  A database built with one DBMS is not [[Software portability|portable]] to another DBMS (i.e., the other DBMS cannot run it). However, in some situations it is desirable to move, migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different [[Total cost of ownership|total costs of ownership]]-TCO), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMS. Typically a DBMS vendor provides tools to help importing databases from other popular DBMSs.  ==Implementation: Database management systems== or '''How database usage requirements are met''' {{Main|Database management system}} A ''database management system'' (DBMS) is a system that allows to build and maintain databases, as well as to utilize their data and retrieve information from it. A DBMS defines the database type that it supports, as well as its functionality and operational capabilities. A DBMS provides the internal processes for external applications built on them. The end-users of some such specific application are usually exposed only to that application and do not directly interact with the DBMS. Thus end-users enjoy the effects of the underlying DBMS, but its internals are completely invisible to end-users. Database designers and database administrators interact with the DBMS through dedicated interfaces to build and maintain the applications' databases, and thus need some more knowledge and understanding about how DBMSs operate and the DBMSs' external interfaces and tuning parameters.  A DBMS consists of [[software]] that operates databases, providing storage, access, security, backup and other facilities to meet needed requirements. DBMSs can be categorized according to the [[database model]](s) that they support, such as relational or XML, the type(s) of computer they support, such as a server cluster or a mobile phone, the [[query language]](s) that access the database, such as [[SQL]] or [[XQuery]], performance trade-offs, such as maximum scale or maximum speed or others. Some DBMSs cover more than one entry in these categories, e.g., supporting multiple query languages. Database software typically support the [[Open Database Connectivity]] (ODBC) standard which allows the database to integrate (to some extent) with other databases.  The development of a mature general-purpose DBMS typically takes several years and many man-years. Developers of DBMS typically update their product to follow and take advantage of progress in computer and storage technologies. Several DBMS products like Oracle and IBM DB2 have been in on-going development since the 1970s-1980s. Since DBMSs comprise a significant [[Economy|economical]] [[market]], computer and storage vendors often take into account DBMS requirements in their own development plans.  ===DBMS architecture: major DBMS components===  DBMS [[Software architecture|architecture]] specifies its components (including descriptions of their functions) and their interfaces. DBMS architecture is distinct from database architecture. The following are major DBMS components:  *'''DBMS external [[Interface (computing)|interface]]s''' - They are the means to communicate with the DBMS (both ways, to and from the DBMS) to perform all the operations needed for the DBMS. These can be operations on a database, or operations to operate and manage the DBMS. For example: ::- Direct database operations: defining data types, assigning security levels, updating data, querying the database, etc. ::- Operations related to DBMS operation and management: backup and restore, database recovery, security monitoring, database storage allocation and database layout configuration monitoring, performance monitoring and tuning, etc. :An external interface can be either a ''[[user interface]]'' (e.g., typically for a database administrator), or an ''[[application programming interface]]'' (API) used for communication between an application program and the DBMS. *'''Database language engines''' (or '''processors''') - Most operations upon databases are performed through expression in Database languages (see above). Languages exist for data definition, data manipulation and queries (e.g., SQL), as well as for specifying various aspects of security, and more. Language expressions are fed into a DBMS through proper interfaces. A language engine processes the language expressions (by a compiler or language interpreter) to extract the intended database operations from the expression in a way that they can be executed by the DBMS. *'''[[Query optimizer]]''' - Performs [[query optimization]] on every query to choose for it the most efficient ''[[query plan]]'' (a partial order (tree) of operations) to be executed to compute the query result. *'''[[Database engine]]''' - Performs the received database operations on the database objects, typically at their higher-level representation. *'''Storage engine''' - translates the operations to low-level operations on the storage [[bit]]s. In some references the Storage engine is viewed as part of the Database engine. *'''[[Database transaction|Transaction]] engine''' - for correctness and reliability purposes most DBMS internal operations are performed encapsulated in transactions (see below). Transactions can also be specified externally to the DBMS to encapsulate a group of operations. The transaction engine tracks all the transactions and manages their execution according to the transaction rules (e.g., proper concurrency control, and proper ''commit'' or ''abort'' for each). *'''DBMS management and operation component''' - Comprises many components that deal with all the DBMS management and operational aspects like performance monitoring and tuning, backup and restore, recovery from failure, security management and monitoring, database storage allocation and database storage layout monitoring, etc.  ===Database storage=== {{Main|Computer data storage}}  Database storage is the container of the physical materialization of a database. It comprises the ''Internal'' (physical) ''level'' in the database architecture. It also contains all the information needed (e.g., [[metadata]], "data about the data", and internal [[data structure]]s) to reconstruct the ''Conceptual level'' and ''External level'' from the Internal level when needed. It is not part of the DBMS but rather manipulated by the DBMS (by its Storage engine; see above) to manage the database that resides in it. Though typically accessed by a DBMS through the underlying [[Operating system]] (and often utilizing the operating systems' [[File system]]s as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).  In principle the database storage can be viewed as a [[linear]] [[address space]], where every bit of data has its unique address in this address space. Practically only a very small percentage of addresses is kept as initial reference points (which also requires storage), and most of the database data is accessed by indirection using displacement calculations (distance in bits from the reference points) and data structures which define access paths (using pointers) to all needed data in effective manner, optimized for the needed data access operations.  ====Data==== =====Coding the data and Error-correcting codes===== {{Main|Code|Character encoding|Error detection and correction|Cyclic redundancy check}} *Data is [[Encoding|encoded]] by assigning a bit pattern to each language [[alphabet]] character, [[Numerical digit|digit]], other numerical patterns, and [[multimedia]] object. Many standards exist for encoding (e.g., [[ASCII]], [[JPEG]], [[MPEG-4]]). *By adding bits to each encoded unit, the redundancy allows both to detect errors in coded data and to correct them based on mathematical algorithms. Errors occur regularly in low probabilities due to [[random]] bit value flipping, or "physical bit fatigue," loss of the physical bit in storage its ability to maintain distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g., due to random [[radiation]]) is typically corrected upon detection. A bit, or a group of malfunctioning physical bits (not always the specific defective bit is known; group definition depends on specific storage device) is typically automatically fenced-out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The [[Cyclic redundancy check]] (CRC) method is typically used in storage for error detection.  =====Data compression===== {{Main|Data compression}} Data compression methods allow in many cases to represent a string of bits by a shorter bit string ("compress") and reconstruct the original string ("decompress") when needed. This allows to utilize substantially less storage (tens of percents) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data in a database compressed or not.  Data compression is typically controlled through the DBMS's data definition interface, but in some cases may be a [[Default (computer science)|default]] and automatic.  =====Data encryption===== {{Main|Cryptography}} For security reasons certain types of data (e.g., credit-card information) may be kept [[encryption|encrypted]] in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots (taken either via unforeseen vulnerabilities in a DBMS, or more likely, by bypassing it).  Data encryption is typically controlled through the DBMS's data definition interface, but in some cases may be a default and automatic.  ====Data storage types====  This collection of bits describes both the contained database data and its related metadata (i.e., data that describes the contained data and allows computer programs to manipulate the database data correctly). The size of a database can nowadays be tens of [[Terabyte]]s, where a [[byte]] is eight bits. The physical materialization of a bit can employ various existing technologies, while new and improved technologies are constantly under development. Common examples are: *''Magnetic medium'' (e.g., in [[Magnetic disk]]) - Orientation of [[magnetic field]] in magnetic regions on a surface of material (two orientation directions, for 0 and 1). *''[[Dynamic random-access memory]]'' (DRAM) - State of a miniature [[electronic circuit]] consisting of few [[transistor]]s (among millions nowadays) in an [[integrated circuit]] (two states for 0 and 1). These two examples are respectively for two major storage types: *''Nonvolatile storage'' can maintain its bit states (0s and 1s) without electrical power supply, or when power supply is interrupted; *''[[Volatile storage]]'' loses its bit values when power supply is interrupted (i.e., its content is erased).  Sophisticated storage units, which can, in fact, be effective dedicated parallel computers that support a large amount of nonvolatile storage, typically must include also components with volatile storage. Some such units employ [[Battery (electricity)|batteries]] that can provide power for several hours in case of external power interruption (e.g., see the [[EMC Symmetrix]]) and thus maintain the content of the volatile storage parts intact. Just before such a device's batteries lose their power the device typically automatically backs-up its volatile content portion (into nonvolatile) and shuts off to protect its data.  Databases are usually too expensive (in terms of importance and needed investment in resources, e.g., time, money, to build them) to be lost by a power interruption. Thus at any point in time most of their content resides in nonvolatile storage. Even if for operational reason very large portions of them reside in volatile storage (e.g., tens of [[Gigabyte]]s in volatile memory, for in-memory databases), most of this is backed-up in nonvolatile storage. A relatively small portion of this, which temporarily may not have nonvolatile backup, can be reconstructed by proper automatic database recovery procedures after volatile storage content loss.  More examples of storage types: *Volatile storage can be found in processors, computer memory (e.g., DRAM), etc. *Non-volatile storage types include [[Read-only memory|ROM]], [[EPROM]], [[Hard disk drive]]s, [[Flash memory]] and [[Flash drive|drive]]s{{disambiguation needed|date=February 2012}}, [[Storage array]]s, etc.  =====Storage metrics===== {{Expand section|date=July 2011}} Databases always use several types of storage when operational (and implied several when idle). Different types may significantly differ in their properties, and the optimal mix of storage types is determined by the types and quantities of operations that each storage type needs to perform, as well as considerations like physical space and energy consumption and dissipation (which may become critical for a large database). Storage types can be categorized by the following attributes: *Volatile/Nonvolatile. *Cost of the medium (e.g., per Megabyte), Cost to operate (cost of energy consumed per unit time). *Access speed (e.g., bytes per second). *Granularity&nbsp;— from fine to coarse (e.g., size in bytes of access operation). *Reliability (the probability of spontaneous bit value change under various conditions). *Maximal possible number of writes (of any specific bit or specific group of bits; could be constrained by the technology used (e.g., "write once" or "write twice"), or due to "physical bit fatigue," loss of ability to distinguish between the 0, 1 states due to many state changes (e.g., in Flash memory)). *Power needed to operate (Energy per time; energy per byte accessed), Energy efficiency, Heat to dissipate. *Packaging density (e.g., realistic number of bytes per volume unit)  =====Protecting storage device content: Device mirroring (replication) and RAID===== {{Main|Disk mirroring|RAID}} :See also [[Storage replication|Disk storage replication]]  While a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices: * '''Device [[Disk mirroring|mirroring]] (replication)''' - A common solution to the problem is constantly maintaining an identical copy of device content on another device (typically of a same type). The downside is that this doubles the storage, and both devices (copies) need to be updated simultaneously with some overhead and possibly some delays. The upside is possible concurrent read of a same data group by two independent processes, which increases performance. When one of the replicated devices is detected to be defective, the other copy is still operational, and is being utilized to generate a new copy on another device (usually available operational in a pool of stand-by devices for this purpose). * '''Redundant array of independent disks''' ('''[[RAID]]''') - This method generalizes the device mirroring above by allowing one device in a group of N devices to fail and be replaced with content restored (Device mirroring is RAID with N=2). RAID groups of N=5 or N=6 are common. N>2 saves storage, when comparing with N=2, at the cost of more processing during both regular operation (with often reduced performance) and defective device replacement.  Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in a same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle also recovery from disasters (see disaster recovery above).  ====Database storage layout====  Database bits are laid-out in storage in data-structures and grouping that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties. Typically the storage itself is design to meet requirements of various areas that extensively utilize storage, including databases. A DBMS in operation always simultaneously utilizes several storage types (e.g., memory, and external storage), with respective layout methods.  =====Database storage hierarchy=====  A database, while in operation, resides simultaneously in several types of storage. By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in [[CPU cache|processor's caches]]. These data are being read from/written to memory, typically through a computer [[Bus (computing)|bus]] (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., [[fibre channel]], [[iSCSI]]). A [[Disk array|storage array]], a common external storage unit, typically has storage hierarchy of it own, from a fast cache, typically consisting of (volatile and fast) [[DRAM]], which is connected (again via standard interfaces) to drives, possibly with different speeds, like [[flash drive]]s{{disambiguation needed|date=February 2012}} and magnetic [[disk drive]]s (non-volatile). The drives may be connected to [[magnetic tape]]s, on which typically the least active parts of a large database may reside, or database backup generations.  Typically a correlation exists currently between storage speed and price, while the faster storage is typically volatile.  =====Data structures===== {{Main|Database storage structures}} {{Expand section|date=June 2011}}  A data structure is an abstract construct that embeds data in a well defined manner. An efficient data structure allows to manipulate the data in efficient ways. The data manipulation may include data insertion, deletion, updating and retrieval in various modes. A certain data structure type may be very effective in certain operations, and very ineffective in others. A data structure type is selected upon DBMS development to best meet the operations needed for the types of data it contains. Type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in (e.g., speed of access, minimal size of storage chunk accessed, etc.). In some DBMSs database administrators have the flexibility to select among options of data structures to contain user data for performance reasons. Sometimes the data structures have selectable parameters to tune the database performance.  Databases may store data in many data structure types.<ref name="Physical Database Design">{{harvnb|Lightstone|Teorey|Nadeau|2007}}</ref> Common examples are the following:  * ordered/unordered [[flat file database|flat files]] * [[hash table]]s * [[B  tree]]s * [[ISAM]] * [[heap (data structure)|heaps]]  =====Application data and DBMS data=====  A typical DBMS cannot store the data of the application it serves alone. In order to handle the application data the DBMS need to store this data in data structures that comprise specific data by themselves. In addition the DBMS needs its own data structures and many types of bookkeeping data like indexes and [[Database log|log]]s. The DBMS data is an integral part of the database and may comprise a substantial portion of it.  =====Database indexing===== {{Main|Index (database)}}  [[Index (database)|Indexing]] is a  technique for improving database performance. The many types of indexes share the common property that they reduce the need to examine every entry when running a query. In large databases, this can reduce query time/cost by orders of magnitude. The simplest form of index is a sorted list of values that can be searched using a [[binary search]] with an adjacent reference to the location of the entry, analogous to the index in the back of a book. The same data can have multiple indexes (an employee database could be indexed by last name and hire date.)  Indexes affect performance, but not results. Database designers can add or remove indexes without changing application logic, reducing maintenance costs as the database grows and database usage evolves.  Given a particular query, the DBMS' query optimizer is responsible for devising the most efficient strategy for finding matching data.  Indexes can speed up data access, but they consume space in the database, and must be updated each time the data is altered. Indexes therefore can speed data access but slow data maintenance. These two properties determine whether a given index is worth the cost.  =====Database data clustering=====  In many cases substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity, being ''clustered''. This usually allows to retrieve needed related objects from storage in minimum number of input operations (each sometimes substantially time consuming). Even for in-memory databases clustering provides performance advantage due to common utilization of large caches for input-output operations in memory, with similar resulting behavior.  For example it may be beneficial to cluster a record of an ''item'' in stock with all its respective ''order'' records. The decision of whether to cluster certain objects or not depends on the objects' utilization statistics, object sizes, caches sizes, storage types, etc. In a relational database clustering the two respective relations "Items" and "Orders" results in saving the expensive execution of a [[Join (relational algebra)|Join]] operation between the two relations whenever such a join is needed in a query (the join result is already ready in storage by the clustering, available to be utilized).  =====Database materialized views===== {{Main|Materialized view}}  Often storage redundancy is employed to increase performance. A common example is storing ''[[materialized view]]s'', which are frequently needed ''External views''. Storing such external views saves expensive computing of them each time they are needed.  =====Database and database object replication===== {{Main|Database replication}} :See also ''[[Database#Replication|Replication]]'' below  Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases the entire database is replicated.  ===Database transactions=== {{Main|Database transaction}} As with every software system, a DBMS that operates in a faulty computing environment is prone to failures of many kinds. A failure can corrupt the respective database unless special measures are taken to prevent this. A DBMS achieves certain levels of fault tolerance by encapsulating operations within transactions. The concept of a ''database transaction'' (or ''atomic transaction'') has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and ''recovery'' from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).  ====ACID rules==== {{Main|ACID}} Every database transaction obeys the following rules: *'''[[Atomicity (database systems)|Atomicity]]''' - Either the effects of all or none of its operations remain ("all or nothing" semantics) when a transaction is completed (''committed'' or ''aborted'' respectively). In other words, to the outside world a committed transaction appears (by its effects on the database) to be indivisible, atomic, and an aborted transaction does not leave effects on the database at all, as if never existed. *'''[[Database Consistency (computer science)|Consistency]]''' - Every transaction must leave the database in a consistent (correct) state, i.e., maintain the predetermined integrity rules of the database (constraints upon and among the database's objects). A transaction must transform a database from one consistent state to another consistent state (however, it is the responsibility of the transaction's programmer to make sure that the transaction itself is correct, i.e., performs correctly what it intends to perform (from the application's point of view) while the predefined integrity rules are enforced by the DBMS). Thus since a database can be normally changed only by transactions, all the database's states are consistent. An aborted transaction does not change the database state it has started from, as if it never existed (atomicity above). *'''[[Isolation (database systems)|Isolation]]''' - Transactions cannot interfere with each other (as an end result of their executions). Moreover, usually (depending on concurrency control method) the effects of an incomplete transaction are not even visible to another transaction. Providing isolation is the main goal of concurrency control. *'''[[Durability (computer science)|Durability]]''' - Effects of successful (committed) transactions must persist through [[Crash (computing)|crash]]es (typically by recording the transaction's effects and its commit event in a [[non-volatile memory]]).  ====Isolation, concurrency control, and locking==== {{Main|Concurrency control|Isolation (database systems)|Two-phase locking}} '''Isolation''' provides the ability for multiple users to operate on the database at the same time without corrupting the data.  *'''[[Concurrency control]]''' comprises the underlying mechanisms in a DBMS which handle isolation and guarantee related correctness. It is heavily utilized by the Database and Storage engines (see above) both to guarantee the correct execution of concurrent transactions, and (different mechanisms) the correctness of other DBMS processes. The transaction-related mechanisms typically constrain the database data access operations' timing ([[Schedule (computer science)|transaction schedules]]) to certain orders characterized as the [[Serializability]] and [[Recoverability|Recoverabiliry]] schedule properties. Constraining database access operation execution typically means reduced performance (rates of execution), and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints. Often, when possible without harming correctness, the serializability property is compromised for better performance. However, recoverability cannot be compromised, since such typically results in a quick database integrity violation. *'''[[Two-phase locking|Locking]]''' is the most common transaction concurrency control method in DBMSs, used to provide both serializability and recoverability for correctness. In order to access a database object a transaction first needs to acquire a lock for this object. Depending on the access operation type (e.g., reading or writing an object) and on the lock type, acquiring the lock may be blocked and postponed, if another transaction is holding a lock for that object.  ===Query optimization=== {{Main|Query optimization|Query optimizer}}  A query is a request for information from a database. It can be as simple as "finding the address of a person with SS# 123-45-6789," or more complex like "finding the average salary of all the employed married men in California between the ages 30 to 39, that earn less than their wives." Queries results are generated by accessing relevant database data and manipulating it in a way that yields the requested information. Since database structures are complex, in most cases, and especially for not-very-simple queries, the needed data for a query can be collected from a database by accessing it in different ways, through different data-structures, and in different orders. Each different way typically requires different processing time. Processing times of a same query may have large variance, from a fraction of a second to hours, depending on the way selected. The purpose of '''query optimization''', which is an automated process, is to find the way to process a given query in minimum time. The large possible variance in time justifies performing query optimization, though finding the exact optimal way to execute a query, among all possibilities, is typically very complex, time consuming by itself, may be too costly, and often practically impossible. Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives  to provide in a reasonable time a "good enough" plan which typically does not deviate much from the best possible result.  ===DBMS support for the development and maintenance of a database and its application=== {{Expand section|date=May 2011}} A DBMS typically intends to provide convenient environment to develop and later maintain an application built around its respective database type. A DBMS either provides such tools, or allows integration with such external tools. Examples for tools relate to database design, application programming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), storage allocation and database layout monitoring, storage migration, etc.  ==See also== {{Wikipedia books|Databases}} * [[Comparison of database tools]] * [[Data hierarchy]] * [[Data store]] * [[Database testing]]  ==References== {{Reflist|colwidth=30em}}  ==Further reading== {{refbegin}} * Ling Liu and Tamer M. Özsu (Eds.) (2009).  "[http://www.springer.com/computer/database management & information retrieval/book/978-0-387-49616-0 Encyclopedia of Database Systems], 4100 p.&nbsp;60 illus. ISBN 978-0-387-49616-0.  * Beynon-Davies, P. (2004). Database Systems. 3rd Edition. Palgrave, Houndmills, Basingstoke. * Connolly, Thomas and Carolyn Begg. ''Database Systems.'' New York: Harlow, 2002. * {{cite book|last=Date |first=C. J. |authorlink=Christopher J. Date |title=An Introduction to Database Systems, Fifth Edition |publisher=Addison Wesley |year= 2003 |isbn=0-201-51381-1 |ref=harv}} * Gray, J. and Reuter, A. ''Transaction Processing: Concepts and Techniques'', 1st edition,  Morgan Kaufmann Publishers, 1992. * Kroenke, David M. and David J. Auer. ''Database Concepts.'' 3rd ed. New York: Prentice, 2007. * {{cite book|last1=Lightstone |first1=S. |first2=T. |last2=Teorey |first3=T. |last3=Nadeau |title=Physical Database Design: the database professional's guide to exploiting indexes, views, storage, and more |publisher=Morgan Kaufmann Press |year=2007 |isbn=0-12-369389-6 |ref=harv}} * Teorey, T.; Lightstone, S. and Nadeau, T. ''Database Modeling & Design: Logical Design'', 4th edition, Morgan Kaufmann Press, 2005. ISBN 0-12-685352-5 {{refend}}  ==External links== {{Sister project links|wikt=database|commons=Category:Database|v=Topic:Databases}} *{{dmoz|Computers/Data_Formats/Database}}  {{Database}} {{Databases}} {{Database models}} {{Data warehouse}}  {{DEFAULTSORT:Database}} [[Category:Databases| ]] [[Category:Database management systems]] [[Category:Database theory]]  [[af:Databasis]] [[ar:قاعدة بيانات]] [[an:Base de datos]] [[az:Verilənlər bazası]] [[bn:ডেটাবেজ]] [[be:База дадзеных]] [[be-x-old:База зьвестак]] [[bg:База данни]] [[bar:Datenbank]] [[bs:Baza podataka]] [[br:Stlennvon]] [[ca:Base de dades]] [[cs:Databáze]] [[da:Database]] [[de:Datenbank]] [[et:Andmebaas]] [[el:Βάση δεδομένων]] [[es:Base de datos]] [[eo:Datumbazo]] [[eu:Datu-base]] [[fa:پایگاه داده‌ها]] [[fr:Base de données]] [[ga:Bunachar sonraí]] [[gl:Base de datos]] [[ko:데이터베이스]] [[hy:Տվյալների բազա]] [[hi:डेटाबेस]] [[hr:Baza podataka]] [[id:Basis data]] [[ia:Base de datos]] [[is:Gagnagrunnur]] [[it:Database]] [[he:בסיס נתונים]] [[jv:Basis data]] [[ka:მონაცემთა ბაზა]] [[kk:Мәліметтер базасы]] [[ku:Danegeh]] [[ky:Берилиштер базасы, берилишбаза]] [[lv:Datubāze]] [[lt:Duomenų bazė]] [[hu:Adatbázis]] [[ml:ഡാറ്റാബേസ്]] [[arz:قاعدة بيانات]] [[ms:Pangkalan data]] [[my:ဒေတာဘေ့စ်]] [[nl:Database]] [[ja:データベース]] [[no:Database]] [[mhr:Ыҥпалыпого]] [[uz:Ma'lumotlar Bazasi]] [[ps:توكبنسټ]] [[pl:Baza danych]] [[pt:Banco de dados]] [[ro:Bază de date]] [[ru:База данных]] [[sq:Baza e të dhënave]] [[scn:Databbasi]] [[si:දත්ත සංචිතය]] [[simple:Database]] [[sk:Databáza]] [[sl:Podatkovna baza]] [[ckb:بنکەدراوە]] [[sr:База података]] [[sh:Baza podataka]] [[fi:Tietokanta]] [[sv:Databas]] [[tl:Database]] [[ta:தரவுத்தளம்]] [[th:ฐานข้อมูล]] [[tr:Veri tabanı]] [[uk:База даних]] [[ur:قاعدۂ معطیات]] [[vi:Cơ sở dữ liệu]] [[wa:Båze di dnêyes]] [[war:Database]] [[zh:数据库]]
[[File:Dombis 1687.jpg|thumb|right|200px|''Irrationnal Geometrics'' digital art installation 2008 by [[Pascal Dombis]]]] [[File:BOtv2002.jpg|thumb|200px|[[Joseph Nechvatal]] ''birth Of the viractual'' 2001 computer-robotic assisted acrylic on canvas]] [[File:Mandelbulb078a.JPG|thumb|right|200px|A close up of the details in a [[Mandelbulb]], a three-dimensional analog of the [[Mandelbrot set]]. An example of [[fractal art]]]] '''Digital art''' is a general term for a range of artistic works and practices that use [[digital technology]] as an essential part of the creative and/or presentation process. Since the 1970s, various names have been used to describe the process including [[computer art]] and [[multimedia art]], and digital art is itself placed under the larger umbrella term [[new media art]].<ref>[[Christiane Paul (curator)|Christiane Paul]] (2006). ''Digital Art'', pp 7–8. Thames & Hudson.</ref><ref>Lieser, Wolf. ''Digital Art''. Langenscheidt: h.f. ullmann. 2009, pp. 13–15</ref>  The impact of digital technology has transformed  activities such as [[painting]], [[drawing]], [[sculpture]] and music/[[sound art]], while new forms, such as [[net art]], digital [[installation art]], and [[virtual reality]], have become recognized artistic practices.<ref>[[Donald Kuspit]] [http://www.artnet.com/magazineus/features/kuspit/kuspit8-5-05.asp ''The Matrix of Sensations'']  ''VI: Digital Artists and the New Creative Renaissance''</ref> More generally the term digital artist is used to describe an artist who makes use of [[digital technology|digital technologies]] in the production of art. In an expanded sense, "digital art" is a term applied to [[contemporary art]] that uses the methods of mass production or digital media.<ref>[[Charlie Gere]] ''Art, Time and Technology: Histories of the Disappearing Body'' (Berg, 2005). ISBN 978-1-84520-135-7 This text concerns artistic and theoretical responses to the increasing speed of technological development and operation, especially in terms of so-called ‘real-time’ digital technologies. It draws on the ideas of [[Jacques Derrida]], [[Bernard Stiegler]], [[Jean-François Lyotard]] and [[André Leroi-Gourhan]], and looks at the work of [[Samuel Morse]], [[Vincent van Gogh]] and [[Malevich]], among others.</ref>  ==Digital production techniques in visual media== [[Image:DaVinci MonaLisa1b.jpg|thumb|right|200px|[[Lillian Schwartz]]'s ''Comparison of [[Leonardo da Vinci|Leonardo]]'s self portrait and the [[Mona Lisa]]'' based on Schwartz's ''Mona Leo''. An example of a [[collage]] of digitally manipulated [[photograph]]s]] The techniques of digital art are used extensively by the mainstream [[mass media|media]] in advertisements, and by film-makers to produce [[special effects]]. [[Desktop publishing]] has had a huge impact on the publishing world, although that is more related to [[graphic design]]. Both digital and traditional artists use many sources of electronic information and programs to create their work.<ref>[[Frank Popper]], ''Art of the Electronic Age'', Thames & Hudson, 1997.</ref> Given the parallels between visual and musical arts, it is possible that general acceptance of the value of digital art will progress in much the same way as the increased acceptance of electronically produced music over the last three decades.<ref>[[Charlie Gere]], (2002) Digital Culture, Reaktion.</ref>  Digital art can be purely computer-generated (such as [[fractal]]s and [[algorithmic art]]) or taken from other sources, such as a [[image scanner|scanned]] [[photograph]] or an image drawn using [[vector graphics]] software using a [[computer mouse|mouse]] or [[graphics tablet]].<ref>[[Christiane Paul (curator)|Christiane Paul]] (2006). ''Digital Art'', pp. 27–67. Thames & Hudson.</ref> Though technically the term may be applied to art done using other media or processes and merely scanned in, it is usually reserved for art that has been non-[[Trivial (mathematics)|trivia]]lly modified by a computing process (such as a [[computer program]], [[microcontroller]] or any electronic system capable of interpreting an input to create an output); digitized text data and raw [[Sound recording and reproduction|audio]] and [[video]] recordings are not usually considered digital art in themselves, but can be part of the larger project of [[computer art]] and [[information art]].<ref>Wands, Bruce (2006). ''Art of the Digital Age'', pp. 10–11. Thames & Hudson.</ref> Artworks are considered [[digital painting]] when created in similar fashion to non-digital [[painting]]s but using [[software]] on a computer platform and digitally outputting the resulting image as painted on [[canvas]].<ref>Paul, Christiane (2006. ''Digital Art'', pp. 54–60. Thames & Hudson.</ref>  [[Andy Warhol]] created digital art using a [[Commodore Amiga]] where the computer was publicly introduced at the [[Lincoln Center]], New York in July 1985. An image of [[Debbie Harry]] was captured in monochrome from a video camera and digitized into a graphics program called ProPaint. Warhol manipulated the image adding colour by using flood fills.<ref>'{{cite web |url=http://arstechnica.com/hardware/news/2007/10/amiga-history-4-commodore-years.ars/4 |title=A history of the Amiga, part 4: Enter Commodore |date=October 21, 2007 |last=Reimer |first=Jeremy |work=Arstechnica.com |accessdate=June 10, 2011}}</ref><ref>[http://www.youtube.com/watch?v=3oqUd8utr14 [[Andy Warhol]] makes a digital painting of [[Debbie Harry]] at the Commodore [[Amiga]] product launch press conference in 1985.]</ref>  ==Computer-generated visual media== {{see also|Computer art}} [[File:3d sculpted creature.jpg|thumb|right|200px|[[Digital sculpting]] can create [[photorealism|photorealistic]] [[3d model]]s used in still imagery.]] [[File:Terragen 2.jpg|thumb|right|200px|A [[Procedural generation|procedurally generated]] [[photorealism|photorealistic]] landscape created with [[Terragen]]. Terragen has been used in creating [[Computer-generated imagery|CGI]] for movies.]] There are two main paradigms in computer generated imagery.{{Citation needed|date=August 2009}} The simplest is [[2D computer graphics]] which reflect how you might draw using a pencil and a piece of paper. In this case, however, the image is on the computer screen and the instrument you draw with might be a tablet stylus or a mouse. What is generated on your screen might appear to be drawn with a pencil, pen or paintbrush. The second kind is [[3D&nbsp;computer graphics]], where the screen becomes a window into a [[virtual environment]], where you arrange objects to be "photographed" by the computer. Typically a 2D computer graphics use [[raster graphics]] as their primary means of source data representations, whereas 3D computer graphics use [[vector graphics]] in the creation of [[immersive virtual reality]] installations. A possible third paradigm is to generate art in 2D or 3D entirely through the execution of algorithms coded into computer programs and could be considered the native art form of the computer. That is, it cannot be produced without the computer. [[Fractal art]], [[Datamoshing]], [[algorithmic art]] and real-time [[generative art]] are examples.  ==Computer generated 3D still imagery== {{Main|3D graphics}} [[File:Virtual girl in leggings.png|thumb|right|200px|A virtual girl art generated by a computer ([[software]])]] 3D graphics are created via the process of designing [[image]]ry from [[geometry|geometric]] shapes, [[polygon]]s or [[Nonuniform rational B-spline|NURBS]] curves<ref>Wands, Bruce (2006). ''Art of the Digital Age'', pp. 15–16. Thames & Hudson.</ref> to create three-dimensional objects and scenes for use in various media such as film, [[television]], print, [[rapid prototyping]], games/simulations and special visual effects.  There are many [[software]] programs for doing this. The technology can enable [[collaboration]], lending itself to sharing and augmenting by a creative effort similar to the [[open source]] movement, and the [[creative commons]] in which users can collaborate in a project to create unique pieces of [[art]].  ==Computer generated animated imagery== {{Main|Computer-generated imagery}}{{see also|Computer animation}} Computer-generated animations are [[animations]] created with a [[computer]], from digital models created by the 3d artists or [[Procedural generation|procedurally generated]]. The term is usually applied to works created entirely with a computer. Movies make heavy use of computer-generated graphics; they are called [[computer-generated imagery]] (CGI) in the film industry. In the 1990s, and early 2000s CGI advanced enough so that for the first time it was possible to create realistic 3D computer [[animation]], although films had been using extensive computer images since the mid-70s. A number of modern films have been noted for their heavy use of photo realistic CGI.<ref>[[Lev Manovich]] (2001) ''The Language of New Media'' Cambridge, Massachusetts: The MIT Press.</ref>  ==Digital installation art== {{see also|interactive art}} [[Image:CAVE Crayoland.jpg|thumb|right|200px|The [[Cave Automatic Virtual Environment]] at the [[University of Illinois]], [[Chicago]]]] Digital installation art constitutes a broad field of activity and incorporates many forms. Some resemble video installations, particularly large scale works involving [[video projection|projections]] and [[video capture|live video capture]]. By using projection techniques that enhance an audiences impression of sensory envelopment, many digital installations attempt to create immersive environments. Others go even further and attempt to facilitate a complete immersion in [[virtual reality|virtual realms]]. This type of installation is generally [[site specific]], [[scalable]], and without fixed [[dimensionality]], meaning it can be reconfigured to accommodate different presentation spaces.<ref>Paul, Christiane (2006). ''Digital Art'', pp 71. Thames & Hudson.</ref>   [[Noah Wardrip-Fruin]]'s "Screen" (2003) is an example of digital installation art which makes use of a  [[Cave Automatic Virtual Environment]] to create an interactive experience.<ref>http://www.noahwf.com/screen/index.html</ref>  == Subtypes == {{div col|3}} * [[Art game]] * [[Computer art scene]] * [[Computer music]] * [[Cyberarts]] * [[Digital illustration]] * [[Digital imaging]] * [[Digital painting]] * [[Digital photography]] * [[Digital poetry]] * [[Dynamic Painting]] * [[Electronic music]] * [[Evolutionary art]] * [[Fractal art]] * [[Generative art]] * [[Generative music]] * [[Immersion (virtual reality)]] * [[Interactive art]] * [[Motion graphics]] * [[Music visualization]] * [[Photo manipulation]] * [[Pixel art]] * [[Software art]] * [[Systems art]] * [[texture map|Textures]] * [[Tradigital art]] * [[Via Art]] {{div col end}}   == Related Organizations and Conferences == * [[Artmedia]] * [[Austin Museum of Digital Art]] * [[Computer Arts Society]]  == See also == * [[Algorithmic art]] * [[Computer art]] * [[Computer graphics]] * [[Electronic Art]] * [[Generative art]] * [[New Media Art]] * [[Virtual art]]  ==References== {{reflist|2}}  ==External links== [http://www.artdigitalmagazine.com Art Digital Magazine (AD MAG)]  [http://www.digaran.org Dig@ran: European Festival on digital arts]  {{DEFAULTSORT:Digital Art}} [[Category:Art media]] [[Category:Computer art]] [[Category:Digital art|*]] [[Category:New media]] [[Category:Electronic music]] [[Category:Art genres]] [[Category:Painting techniques]] [[Category:Conceptual art]] [[Category:Postmodern art]] [[Category:Contemporary art]]  [[ar:فن رقمي]] [[cs:Digitální umění]] [[de:Digitale Kunst]] [[es:Arte digital]] [[fa:هنر دیجیتال]] [[fr:Art numérique]] [[ko:디지털 아트]] [[hi:डिजिटल कला]] [[it:Arte digitale]] [[he:אמנות דיגיטלית]] [[nl:Digitale kunst]] [[ja:デジタルアート]] [[pt:Arte digital]] [[ru:Компьютерное искусство]] [[sr:Дигитална уметност]] [[sv:Digital konst]] [[tr:Dijital sanat]] [[uk:Комп'ютерне мистецтво]] [[zh:数字艺术]]
The '''digital humanities''' is an area of research, teaching, and creation concerned with the intersection of computing and the disciplines of the [[humanities]]. Developing from an earlier field called '''humanities computing,''' today digital humanities embrace a variety of topics ranging from curating online collections to data mining large cultural data sets. Digital Humanities currently incorporates both digitized and born-digital materials and combines the methodologies from the traditional humanities disciplines (such as [[history]], [[philosophy]], [[linguistics]], [[literature]], [[art]], [[archaeology]], [[music]], and [[cultural studies]]) with tools provided by [[computing]] (such as [[data visualisation]], [[information retrieval]], [[data mining]], [[statistics]], [[computational analysis]]) and [[electronic publication|digital publishing]].    ==Objectives== Digital humanities scholars use computational methods either to answer existing research questions or to challenge existing theoretical paradigms, generating new questions and pioneering new approaches. One goal is to systematically integrate computer technology into the activities of humanities scholars, <ref name=neh-odh-grant-opportunities>{{cite web|title=Grant Opportunities|url=http://www.neh.gov/ODH/GrantOpportunities/tabid/57/Default.aspx|work=National Endowment for the Humanities, Office of Digital Humanities Grant Opportunities|accessdate=25 January 2012}}</ref>  such as the use of text-analytic techniques; [[GIS]]; [[commons-based peer collaboration]]; interactive games and [[multimedia]] in the traditional [[arts]] and [[humanities]] disciplines like it is done in contemporary empirical [[social sciences]].  Another goal is to create scholarship that is more than texts and papers. This includes the integration of [[multimedia]], [[metadata]] and dynamic environments. An example of this is [[The Valley of the Shadow]] project at the [[University of Virginia]] or the [[Vectors Journal of Culture and Technology in a Dynamic Vernacular]] at [[University of Southern California]].  A growing number of researchers in digital humanities are using computational methods for the analysis of large cultural data sets. Examples of such projects were highlighted by the Humanities High Performance Computing competition sponsored by the Office of Digital Humanities in 2008,<ref>{{cite news|title=Grant Announcement for Humanities High Performance Computing Program|date= December 1, 2008 |accessdate= May 1, 2012|first=Brett|last=Bobley|work=National Endowment for the Humanities|url=http://www.neh.gov/divisions/odh/grant-news/grant-announcement-humanities-high-performance-computing-program}}</ref> and also by the Digging Into Data challenge organized in 2009<ref>{{cite news|title=Awardees of 2009 Digging into Data Challenge|date= 2009 |accessdate= May 1, 2012|first=|last=|work=Digging into Data|url=http://www.diggingintodata.org/Home/AwardRecipients2009/tabid/175/Default.aspx}}</ref> and 2011<ref>{{cite news|title=NEH Announces Winners of 2011 Digging Into Data Challenge|date= January 3, 2012 |accessdate= May 1, 2012|first=|last=|work=National Endowment for the Humanities|url=http://www.neh.gov/news/press-release/2012-01-03}}</ref> by NEH in collaboration with NSF,<ref name=cohen-embrace>{{Cite news| issn = 0362-4331| last = Cohen| first = Patricia| title = Humanities Scholars Embrace Digital Technology| work = The New York Times| location = New York| accessdate = 2012-06-07| date = 2010-11-16| url = http://www.nytimes.com/2010/11/17/arts/17digital.html?pagewanted=all}}</ref> and in partnership with [[Joint Information Systems Committee|JISC]] in the UK, and [[Social Sciences and Humanities Research Council|SSHRC]] in Canada.<ref>{{cite news|title=Computationally Intensive Research in the Humanities and Social Sciences: A Report on the Experiences of First Respondents to the Digging Into Data Challenge| work=Council on Library and Information Resources|first= Christa|last= Williford| first2= Charles|last2= Henry|date=June 2012|ISBN= 978-1-932326-40-6|url=http://www.clir.org/pubs/reports/pub151}}</ref>  At present, formal academic recognition of digital work in the humanities remains somewhat problematic, although there are signs that this might be changing.{{Citation needed|date=May 2012}} Some universities do offer programs related to the field.  ==Environments and tools== Digital humanities is also involved in the creation of software, providing "environments and tools for producing, curating, and interacting with knowledge that is 'born digital' and lives in various digital contexts."<ref>{{Cite news | last = Presner | first = Todd | title = Digital Humanities 2.0: A Report on Knowledge | work = Connexions | accessdate = 2012-06-09 | date = 2010 | url = http://cnx.org/content/m34246/latest/ }}</ref> Many such projects share a "commitment to [[open standards]] and [[open source]]."<ref>{{Cite book| publisher = Ashgate| isbn = 9781409410683| pages = 11 - 26| page=14|editors = Marilyn Deegan and Willard McCarty (eds.)| last = Bradley | first = John | title = Collaborative Research in the Digital Humanities| chapter = No job for techies: Technical contributions to research in digital humanities| location = Farnham and Burlington| date = 2012}}</ref>  ==History== Digital humanities descends from the field of humanities computing, of computationally enabled "formal representations of the human record,"<ref name=unsworth-humanities-computing>{{Cite journal | volume = 4| last = Unsworth| first = John| title = What is Humanities Computing and What is not?| journal = Jahrbuch für Computerphilologie| accessdate = 2012-05-31| date = 2002-11-08| url = http://computerphilologie.tu-darmstadt.de/jg02/unsworth.html}}</ref> whose origins reach back to the late 1940s in the pioneering work of [[Roberto Busa]].<ref name=svensson>{{Cite journal| issn = 1938-4122| volume = 3| issue = 3| last = Svensson| first = Patrik| title = Humanities Computing as Digital Humanities| journal = Digital Humanities Quarterly| accessdate = 2012-05-30| date = 2009| url = http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html}}</ref><ref name=hockney>{{Cite book| publisher = Blackwell| isbn = 1405103213| editors = Susan Schreibman, Ray Siemens, John Unsworth (eds.)| last = Hockney| first = Susan| title = Companion to Digital Humanities| chapter = The History of Humanities Computing| location = Oxford| series = Blackwell Companions to Literature and Culture| date = 2004| url=http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-2-1&toc.depth=1&toc.id=ss1-2-1&brand=9781405103213_brand}}</ref>  The [[Text Encoding Initiative]], born from the desire to create a standard encoding scheme for humanities electronic texts, is the outstanding achievement of early humanities computing. The project was launched in 1987 and published the first full version of the ''TEI Guidelines'' in May 1994.<ref name=hockney />   In the nineties, major digital text and image archives emerged at centers of humanities computing in the U.S. (e.g. the ''Women Writers Project'',<ref>{{Cite| publisher = Brown University| coauthors = | title = Women Writers Project| accessdate = 2012-06-16| date =| url =http://www.wwp.brown.edu/}}</ref> the ''Rossetti Archive'',<ref>{{Cite| publisher = Institute for Advanced Technology in the Humanities, University of Virginia| coauthors = Jerome J. McGann (ed.)| title = Rossetti Archive| accessdate = 2012-06-16| date =| url = http://www.rossettiarchive.org/}}</ref> and the ''William Blake Archive''<ref>{{Cite| publisher = | coauthors = Morris Eaves, Robert Essick, and Joseph Viscomi (eds.)| title = The William Blake Archive| accessdate = 2012-06-16| date =| url = http://www.blakearchive.org/}}</ref>), which demonstrated the sophistication and robustness of text-encoding for literature.<ref>{{Cite journal| issn = 0093-1896| volume = 31| issue = 1| pages = 49-84| last = Liu| first = Alan| title = Transcendental Data: Toward a Cultural History and Aesthetics of the New Encoded Discourse| journal = Critical Inquiry| accessdate = 2012-06-16| date = 2004| url = http://www.jstor.org/stable/10.1086/427302}}</ref>  The terminological change from "humanities computing" to "digital humanities" has been attributed to [[John Unsworth]] and [[Ray Siemens]] who, as editors of the monograph ''A Companion to Digital Humanities'' (2001), tried to prevent the field from being viewed as "mere digitization."<ref name="fitzpatrick">{{Cite news| last = Fitzpatrick| first = Kathleen| title = The humanities, done digitally| work = The Chronicle of Higher Education| accessdate = 2011-07-10| date = 2011-05-08| url = http://chronicle.com/article/The-Humanities-Done-Digitally/127382/}}</ref> Consequently, the hybrid term has created an overlap between fields like rhetoric and composition, which use "the methods of contemporary humanities in studying digital objects,"<ref name="fitzpatrick"/> and digital humanities, which uses "digital technology in studying traditional humanities objects".<ref name="fitzpatrick" /> The use of computational systems and the study of computational media within the arts and humanities more generally has been termed the 'computational turn'.<ref name="berry">{{Cite news| last = Berry| first = David| title = The Computational Turn: Thinking About the Digital Humanities| work = Culture Machine| accessdate = 2012-01-31| date = 2011-06-01| url = http://culturemachine.net/index.php/cm/article/viewDownloadInterstitial/440/470}}</ref>  In 2006 the [[National Endowment for the Humanities]] (NEH), the federal granting agency for scholarships in the humanities, launched the Digital Humanities Initiative (renamed Office of Digital Humanities in 2008), which made widespread adoption of the term "digital humanities" all but irreversible in the United States.<ref name=kirschenbaum-dept />  Digital humanities emerged from its former niche status and became "big news"<ref name=kirschenbaum-dept>{{Cite news| volume =| issue = 150| last = Kirschenbaum| first = Matthew G. | title = What is Digital Humanities and What's it Doing in English Departments?| work = ADE Bulletin| accessdate = | date = 2010| url = http://mkirschenbaum.files.wordpress.com/2011/01/kirschenbaum_ade150.pdf}}</ref> at the 2009 [[Modern Language Association |MLA convention]] in Philadelphia, where digital humanists made "some of the liveliest and most visible contributions"<ref>{{Cite news| issn = 0009-5982| last = Howard| first = Jennifer| title = The MLA Convention in Translation| work = The Chronicle of Higher Education | accessdate = 2012-05-31| date = 2009-12-31| url = http://chronicle.com/article/The-MLA-Convention-in/63379/}}</ref> and had their field hailed as "the first 'next big thing' in a long time."<ref name=pannapacker-mla>{{Cite web| last = Pannapacker| first = William| title = The MLA and the Digital Humanities| work = Brainstorm| format = The Chronicle of Higher Education| accessdate = 2012-05-30| date = 2009-12-28| url = http://chronicle.com/blogPost/The-MLAthe-Digital/19468/}}</ref>  ==Organizations and Institutions==  The field of digital humanities is served by several organisations: [[The Association for Literary and Linguistic Computing]] (ALLC), the Association for Computers and the Humanities (ACH), and the Society for Digital Humanities/Société pour l'étude des médias interactifs (SDH/SEMI), which are joined under the umbrella organisation of the [[Alliance of Digital Humanities Organizations]] (ADHO). The alliance funds a number of projects such as the [[Digital Humanities Quarterly]], supports  the [[Text Encoding Initiative]], the organisation and sponsoring of workshops and conferences, as well as the funding of small projects, awards and bursaries.<ref>{{Cite journal| doi = 10.1093/llc/fqr002| volume = 26| issue = 1| pages = 3–4| last = Vanhoutte| first = Edward| title = Editorial| journal = Literary and Linguistic Computing| accessdate = 2011-07-11| date = 2011-04-01| url = http://llc.oxfordjournals.org/content/26/1/3.short}}</ref>  ADHO also oversees a joint annual conference, which began as the ACH/ALLC (or ALLC/ACH) conference, and is now known as the [[Digital Humanities conference]].  CenterNet is an international network of of about 100 digital humanities centers in 19 countries, working together to benefit digital humanities and related fields.<ref>{{cite web|url=http://digitalhumanities.org/centernet/about/|work=CenterNet|title=About|accessdate=June 16,2012}}</ref><ref name=caraco>{{Cite journal| volume = 57| issue = 2| last = Caraco| first = Benjamin| title = Les digital humanities et les bibliothèques | journal = Le Bulletin des Bibliothèques de France| accessdate = 12 April 2012| date = 1 January 2012| url = http://bbf.enssib.fr/consulter/bbf-2012-02-0069-002#appelnote-11}}</ref>  ==Criticism== Many conventional humanities scholars dismiss digital humanities as "whimsical."<ref name=cohen-embrace /> The literary theorist [[Stanley Fish]] claims that the digital humanities pursue a revolutionary agenda and thereby undermine the conventional standards of "pre-eminence, authority and disciplinary power."<ref name=fish-mortality>{{Cite news | last = Fish| first = Stanley | title = The Digital Humanities and the Transcending of Mortality | work = The New York Times | location = New York | accessdate = 2012-05-30| date = 2012-01-09 | url = http://opinionator.blogs.nytimes.com/2012/01/09/the-digital-humanities-and-the-transcending-of-mortality/}}</ref>  ==See also==  ===Centers===  *[[Center for History and New Media | Roy Rosenzweig Center for History and New Media]] *[[Department of Digital Humanities|Department of Digital Humanities at King's College London]] *[[Digital Humanities Observatory]] *[[Humanities Advanced Technology and Information Institute]] *[[Institute for Advanced Technology in the Humanities]] *[[Maryland Institute for Technology in the Humanities]] *[[UCL Centre for Digital Humanities]]  ===Journals===  *[[Literary and Linguistic Computing]] *[[Digital Studies]] *[[Digital Medievalist]] *[[Digital Humanities Quarterly]] *[[Southern Spaces]] *[http://journalofdigitalhumanities.org Journal of Digital Humanities]  ===Meetings===  *[[Digital Humanities conference]] *[[THATCamp]]  ===Miscellaneous===  *[[Computers and writing]] *[[Computational archaeology]] *[[Cybertext]] *[[Cultural analytics]] *[[Digital Classicist]] *[[Digital library]] *[[Digital Medievalist]] *[[Digital history]] *[[Electronic Cultural Atlas Initiative]] *[[Electronic literature]] *[[EpiDoc]] *[[Humanistic informatics]] *[[Multimedia literacy]] *[[New media]] *[[Systems theory]] *[[Stylometry]] *[[Text Encoding Initiative]] *[[Text mining]] *[[Topic model|Topic Modeling]] *[[Transliteracy]]  ==References== {{reflist |2}}  ==Bibliography== #Berry, D. M., ed. (2012) ''[http://www.palgrave.com/products/title.aspx?pid=493310 Understanding Digital Humanities]'', Basingstoke: Palgrave Macmillan.  #Busa, Roberto. (1980). ‘The Annals of Humanities Computing: The Index Thomisticus’, in Computers and the Humanities vol. 14, pp.83-90.  #Computers and the Humanities (1966-2004) #Celentano A., Cortesi A., Mastandrea P. (2004), Informatica Umanistica: una disciplina di confine, Mondo Digitale, vol. 4, pp. 44-55. #Condron Frances, Michael Fraser, and Stuart Sutherland, eds. (2001), ''[http://users.ox.ac.uk/~ctitext2/resguide2000/contents.shtml Oxford University Computing Services Guide to Digital Resources for the Humanities]'', West Virginia University Press.  #Fitzpatrick, Kathleen (2011). ''[http://nyupress.org/books/book-details.aspx?bookId=4998 Planned Obsolescence: Publishing, Technology, and the Future of the Academy]''. New York; NYU Press.  #Gold, Matthew K., ed. (2012), ''[http://www.upress.umn.edu/book-division/books/debates-in-the-digital-humanities Debates In the Digital Humanities]''. Minneapolis: University of Minnesota Press.  #Hancock, B. & Giarlo, M.J. (2001). ''[http://dx.doi.org/10.1108/07378830110405139 Moving to XML: Latin texts XML conversion project at the Center for Electronic Texts in the Humanities]''. Library Hi Tech, 19(3), 257-264. #Hockey, Susan. (2001), ''Electronic Text in the Humanities: Principles and Practice'', Oxford: Oxford University Press. # Honing, Henkjan (2008). The role of ICT in music research: A bridge too far? ''International Journal of Humanities and Arts Computing'', 1 (1), 67-75.  #Inman James, Cheryl reed, & Peter Sands, eds. (2003), ''Electronic Collaboration in the Humanities: Issues and Options'', Mahwah, NJ: Lawrence Erlbaum. #Kenna, Stephanie and Seamus Ross, eds. (1995), ''Networking in the humanities: Proceedings of the Second Conference on Scholarship and Technology in the Humanities held at Elvetham Hall, Hampshire, UK 13-16 April 1994''. London: Bowker-Saur. #Kirschenbaum, Matthew (2008). ''[http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=11336 Mechanisms: New Media and the Forensic Imagination]''. Cambridge, Mass.: MIT Press.  #McCarty, Willard (2005), ''Humanities Computing'', Basingstoke: Palgrave Macmillan. #Moretti, Franco (2007), ''[http://www.versobooks.com/books/261-graphs-maps-trees Graphs, Maps, Trees: Abstract Models for Literary History]''. New York: Verso.  #Mullings, Christine, Stephanie Kenna, Marilyn Deegan, and Seamus Ross, eds. (1996), ''New Technologies for the Humanities'' London: Bowker-Saur. #Newell, William H., ed. (1998), ''Interdisciplinarity: Essays from the Literature.'' New York: College Entrance Examination Board. #Nowviskie, Bethany, ed. (2011). ''[http://mediacommons.futureofthebook.org/alt-ac/ Alt-Academy: Alternative Academic Careers for Humanities Scholars]''. New York: MediaCommons.  #Ramsay, Steve. (2011). ''[http://www.press.uillinois.edu/books/catalog/75tms2pw9780252036415.html Reading Machines: Toward an Algorithmic Criticism]''. Urbana: University of Illinois Press.  #Schreibman Susan, Siemens Ray, and Unsworth John eds. (2004). ''[http://www.digitalhumanities.org/companion/ A Companion To Digital Humanities]'' Blackwell Publishers.  #Selfridge-Field, Eleanor (ed). (1997) Beyond MIDI: The Handbook of Musical Codes. Cambridge, MA: The MIT Press. #Unsworth, John, (2005). ''[http://jefferson.village.virginia.edu/~jmu2m/Kings.5-00/primitives.html Scholarly Primitives: What methods do humanities researchers have in common, and how might our tools reflect this?]''  ==External links==  *[http://www.digitalhumanities.org/ The Alliance of Digital Humanities Organizations] *[http://digitalhumanities.org/centernet/ CenterNet] *[http://tapor.ualberta.ca/taporwiki/index.php/Day_in_the_Life_of_the_Digital_Humanities A Day in the Life of the Digital Humanities] *[http://www.ias.umn.edu/media/DigitalHumanities.php Interview with Matthew Gold, author of Debating the Digital Humanities] *[http://lab.softwarestudies.com/2012/03/computational-humanities-vs-digital.html Lev Manovich, Computational Humanities vs. Digital Humanities]  [[Category:Digital humanities| ]]  [[cy:Dyniaethau Digidol]] [[de:Digital Humanities]] [[fr:Humanités numériques]] [[it:Informatica umanistica]] [[ja:デジタル・ヒューマニティーズ]] [[sl:Digitalna humanistika]] [[sh:Дигиталне_хуманистичке_науке]]
{{For|the mathematics journal|Discrete Mathematics (journal)}} [[File:6n-graf.svg|thumb|250px|[[Graph (mathematics)|Graphs]] like this are among the objects studied by discrete mathematics, for their interesting [[graph property|mathematical properties]], their usefulness as models of real-world problems, and their importance in developing computer [[algorithm]]s.]]  '''Discrete mathematics''' is the study of [[Mathematics|mathematical]] [[Mathematical structure|structures]] that are fundamentally [[discrete space|discrete]] rather than [[Continuous function|continuous]].  In contrast to [[real number]]s that have the property of varying "smoothly", the objects studied in discrete mathematics – such as [[integer]]s,  [[Graph (mathematics)|graphs]], and statements in [[Mathematical logic|logic]]<ref>Richard Johnsonbaugh, ''Discrete Mathematics'', Prentice Hall, 2008.</ref> – do not vary smoothly in this way, but have distinct, separated values.<ref>{{MathWorld |title=Discrete mathematics |urlname=DiscreteMathematics}}</ref> Discrete mathematics therefore excludes topics in "continuous mathematics" such as [[calculus]] and [[Mathematical analysis|analysis]].  Discrete objects can often be [[enumeration|enumerated]] by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with [[countable set]]s<ref>[[Norman L. Biggs]], ''Discrete mathematics'', Oxford University Press, 2002.</ref> (sets that have the same cardinality as subsets of the natural numbers, including rational numbers but not real numbers). However, there is no exact, universally agreed, definition of the term "discrete mathematics."<ref>Brian Hopkins, ''Resources for Teaching Discrete Mathematics'', Mathematical Association of America, 2008.</ref> Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.  The set of objects studied in discrete mathematics can be finite or infinite. The term '''finite mathematics''' is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.  Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of [[digital computers]] which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, [[programming language]]s, [[cryptography]], [[automated theorem proving]], and [[software development]]. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in [[operations research]].  Although the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.  ==Grand challenges, past and present== [[File:Four Colour Map Example.svg|thumb|180px|right|Much research in [[graph theory]] was motivated by attempts to prove that all maps, like this one, could be [[graph coloring|colored]] with [[four color theorem|only four colors]]. [[Kenneth Appel]] and [[Wolfgang Haken]] finally proved this in 1976.<ref name="4colors">{{Cite book |last=Wilson |first=Robin |authorlink=Robin Wilson (mathematician) |title=Four Colors Suffice |place=London |publisher=Penguin Books |year=2002 |isbn=0-691-11533-8}}</ref>]]  The history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field.  In graph theory, much research was motivated  by attempts to prove the [[four color theorem]], first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).<ref name="4colors" />  In [[Mathematical logic|logic]], the [[Hilbert's second problem|second problem]] on [[David Hilbert]]'s list of open [[Hilbert's problems|problems]] presented in 1900 was to prove that the [[axioms]] of [[arithmetic]] are [[consistent]]. [[Gödel's second incompleteness theorem]], proved in 1931, showed that this was not possible – at least not within arithmetic itself.  [[Hilbert's tenth problem]] was to determine whether a given polynomial [[Diophantine equation]] with integer coefficients has an integer solution.  In 1970, [[Yuri Matiyasevich]] proved that this [[Matiyasevich's theorem|could not be done]].  The need to [[Cryptanalysis|break]] German codes in [[World War II]] led to advances in [[cryptography]] and [[theoretical computer science]], with the [[Colossus computer|first programmable digital electronic computer]] being developed at England's [[Bletchley Park]].  At the same time, military requirements motivated advances in [[operations research]]. The [[Cold War]] meant that cryptography remained important, with fundamental advances such as [[public-key cryptography]] being developed in the following decades.  Operations research remained important as a tool in business and project management, with the [[critical path method]] being developed in the 1950s. The [[telecommunication]] industry has also motivated advances in discrete mathematics, particularly in graph theory and [[information theory]].  [[Formal verification]] of statements in logic has been necessary for [[software development]] of [[safety-critical system]]s, and advances in [[automated theorem proving]] have been driven by this need.  [[Computational geometry]] has been an important part of the [[Computer graphics (computer science)|computer graphics]] incorporated into modern [[video game]]s and [[computer-aided design]] tools.  Several fields of discrete mathematics, particularly theoretical computer science, graph theory, and [[combinatorics]], are important in addressing the challenging [[bioinformatics]] problems associated with understanding the [[Phylogenetic tree|tree of life]].<ref>Trevor R. Hodkinson and John A. N. Parnell, ''[http://books.google.com/books?id=7GKkbJ4yOKAC&pg=PA97 Reconstructing the Tree of Life: Taxonomy and systematics of species rich taxa]'', CRC Press, 2007, ISBN 0-8493-9579-8, p. 97.</ref>  Currently, one of the most famous open problems in theoretical computer science is the [[P = NP problem]], which involves the relationship between the [[complexity class]]es [[P (complexity)|P]] and [[NP (complexity)|NP]]. The [[Clay Mathematics Institute]] has offered a $1 million [[USD]] prize for the first correct proof, along with prizes for [[Millennium Prize Problems|six other mathematical problems]].<ref name="CMI Millennium Prize Problems">{{cite web|title=Millennium Prize Problems|url=http://www.claymath.org/millennium/|date=2000-05-24|accessdate=2008-01-12}}</ref>  ==Topics in discrete mathematics== ===Theoretical computer science=== {{Main|Theoretical computer science}} [[File:Sorting quicksort anim.gif|thumb|210px|[[Computational complexity theory|Complexity]] studies the time taken by [[algorithm]]s, such as this [[Quicksort|sorting routine]].]]<!-- image only seems to work at multiples of 70px --> Theoretical computer science includes areas of discrete mathematics relevant to computing.  It draws heavily on [[graph theory]] and [[Mathematical logic|logic]].  Included within theoretical computer science is the study of algorithms for computing mathematical results.  [[Computability]] studies what can be computed in principle, and has close ties to logic, while complexity studies the time taken by computations.   [[Automata theory]] and [[formal language]] theory are closely related to computability. [[Petri net]]s and [[process algebra]]s are used to model computer systems, and methods from discrete mathematics are used in analyzing [[VLSI]] electronic circuits. [[Computational geometry]] applies algorithms to geometrical problems, while [[computer image analysis]] applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.  ===Information theory=== {{Main|Information theory}} [[File:WikipediaBinary.svg|thumb|150px|The [[ASCII]] codes for the word "Wikipedia", given here in [[Binary numeral system|binary]], provide a way of representing the word in [[information theory]], as well as for information-processing [[algorithm]]s.]] Information theory involves the quantification of [[information]].  Closely related is [[coding theory]] which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: [[analog signal]]s, [[analog coding]], [[analog encryption]].  ===Logic=== {{Main|Mathematical logic}} Logic is the study of the principles of valid reasoning and [[inference]], as well as of [[consistency]], [[soundness]], and [[completeness]].  For example, in most systems of logic (but not in [[intuitionistic logic]]) [[Peirce's law]] (((''P''→''Q'')→''P'')→''P'') is a theorem. For classical logic, it can be easily verified with a [[truth table]]. The study of [[mathematical proof]] is particularly important in logic, and has applications to [[automated theorem proving]] and [[formal verification]] of software.  [[Well-formed formula|Logical formulas]] are discrete structures, as are [[Proof theory|proofs]], which form finite [[tree structure|trees]]<ref>Sjerp Troelstra, Helmut Schwichtenberg, ''[http://books.google.com/books?id=x9x6F_4mUPgC&pg=PA186 Basic Proof Theory]'', Cambridge University Press, 2000, ISBN 0-521-77911-1, p. 186.</ref> or, more generally, [[directed acyclic graph]] structures<ref>Samuel R. Buss, ''[http://books.google.com/books?id=MfTMDeCq7ukC&pg=PA13 Handbook of Proof Theory]'' (Volume 137 of ''Studies in logic and the foundations of mathematics''), Elsevier, 1998. ISBN 0-444-89840-9, p 13.</ref><ref>Stephan Schulz, "Learning Search Control Knowledge for Equational Theorem Proving," in ''[http://books.google.com/books?id=27A2XJPYwIkC&pg=PA325 KI 2001: Advances in Artificial Intelligence : Joint German/Austrian Conference on AI, Vienna, Austria, September 19-21, 2001 : Proceedings]'' (Volume 2174 of ''Lecture notes in Artificial Intelligence''), Franz Baader, Gerhard Brewka, and Thomas Eiter, eds., Springer, 2001, ISBN 3-540-42612-4, p. 325.</ref> (with each [[Rule of inference|inference step]] combining one or more [[premise]] branches to give a single conclusion). The [[truth value]]s of logical formulas usually form a finite set, generally restricted to two values: ''true'' and ''false'', but logic can also be continuous-valued, e.g., [[fuzzy logic]]. Concepts such as infinite proof trees or infinite derivation trees have also been studied,<ref>{{cite journal | id = {{citeseerx|10.1.1.111.1105}} | title = Cyclic proofs of program termination in separation logic | first1 = J. | last1 = Brotherston | first2 = R. | last2 = Bornat | first3 = C. | last3 = Calcagno | journal = ACM SIGPLAN Notices | volume = 43 | issue = 1 | month = January | year = 2008 }}</ref> e.g. [[infinitary logic]].  ===Set theory=== {{Main|Set theory}} Set theory is the branch of mathematics that studies [[set (mathematics)|sets]], which are collections of objects, such as {blue, white, red} or the (infinite) set of all [[prime number]]s.  [[Partially ordered set]]s and sets with other [[Relation (mathematics)|relations]] have applications in several areas.  In discrete mathematics, [[countable set]]s (including [[finite set]]s) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by [[Georg Cantor]]'s work distinguishing between different kinds of [[infinite set]], motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in [[descriptive set theory]] makes extensive use of traditional continuous mathematics.  ===Combinatorics=== {{Main|Combinatorics}} Combinatorics studies the way in which discrete structures can be combined or arranged. [[Enumerative combinatorics]] concentrates on counting the number of certain combinatorial objects - e.g. the [[twelvefold way]] provides a unified framework for counting [[permutations]], [[combinations]] and [[Partition of a set|partitions]]. [[Analytic combinatorics]] concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from [[complex analysis]] and [[probability theory]]. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and [[generating functions]] to describe the results, analytic combinatorics aims at obtaining [[Asymptotic analysis|asymptotic formulae]]. Design theory is a study of [[combinatorial design]]s, which are collections of subsets with certain [[Set intersection|intersection]] properties. [[Partition theory]] studies various enumeration and asymptotic problems related to [[integer partition]]s, and is closely related to [[q-series]], [[special functions]] and [[orthogonal polynomials]]. Originally a part of [[number theory]] and [[analysis]], partition theory is now considered a part of combinatorics or an independent field. [[Order theory]] is the study of [[partially ordered sets]], both finite and infinite.  ===Graph theory=== {{Main|Graph theory}} [[File:TruncatedTetrahedron.gif|thumb|right|200px|[[Graph theory]] has close links to [[group theory]]. This [[truncated tetrahedron]] graph is related to the [[alternating group]] ''A''<sub>4</sub>.]] Graph theory, the study of [[Graph (mathematics)|graphs]] and [[network theory|network]]s, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right.<ref>[http://jhupbooks.press.jhu.edu/ecom/MasterServlet/GetItemDetailsHandler?iN=9780801866890&qty=1&viewMode=1&loggedIN=false&JavaScript=y Graphs on Surfaces], [[Bojan Mohar]] and [[Carsten Thomassen]], Johns Hopkins University press, 2001</ref> Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of [[topology]], e.g. [[knot theory]]. [[Algebraic graph theory]] has close links with group theory. There are also [[continuous graph]]s, however for the most part research in graph theory falls within the domain of discrete mathematics.  ===Probability=== {{Main|Discrete probability theory}} Discrete probability theory deals with events that occur in countable [[sample spaces]]. For example, count observations such as the numbers of birds in flocks comprise only natural number values {0, 1, 2, ...}. On the other hand, continuous observations such as the weights of birds comprise real number values and would typically be modeled by a continuous probability distribution such as the [[normal distribution|normal]]. Discrete probability distributions can be used to approximate continuous ones and vice versa. For highly constrained situations such as throwing [[dice]] or experiments with [[decks of cards]], calculating the probability of events is basically [[enumerative combinatorics]].  ===Number theory=== [[File:Ulam 1.png|thumb|200px|right|The [[Ulam spiral]] of numbers, with black pixels showing [[prime number]]s. This diagram hints at patterns in the [[Prime number#Distribution|distribution]] of prime numbers.]] {{Main|Number theory}} Number theory is concerned with the properties of numbers in general, particularly [[integer]]s. It has applications to [[cryptography]], [[cryptanalysis]], and [[cryptology]], particularly with regard to [[modular arithmetic]], [[diophantine equations]], linear and quadratic congruences, prime numbers and [[primality test]]ing.  Other discrete aspects of number theory include [[geometry of numbers]].  In [[analytic number theory]], techniques from continuous mathematics are also used. Topics that go beyond discrete objects include [[transcendental number]]s, [[diophantine approximation]], [[p-adic analysis]] and [[function field of an algebraic variety|function fields]].  ===Algebra=== {{Main|Abstract algebra}} [[Algebraic structure]]s occur as both discrete examples and continuous examples. Discrete algebras include: [[boolean algebra (logic)|boolean algebra]] used in [[logic gate]]s and programming; [[relational algebra]] used in [[databases]]; discrete and finite versions of [[group (mathematics)|group]]s, [[ring (mathematics)|rings]] and [[field (mathematics)|field]]s  are important in [[algebraic coding theory]]; discrete [[semigroup]]s and [[monoid]]s appear in the theory of [[formal languages]].  ===Calculus of finite differences, discrete calculus or discrete analysis=== {{Main|finite difference}} A [[function (mathematics)|function]] defined on an interval of the [[integer]]s is usually called a [[sequence]]. A sequence could be a finite sequence from a data source or an infinite sequence from a [[discrete dynamical system]]. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a [[recurrence relation]] or [[difference equation]]. Difference equations are similar to a [[differential equation]]s, but replace [[derivative|differentiation]] by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance where there are [[integral transforms]] in [[harmonic analysis]] for studying continuous functions or analog signals, there are [[discrete transform]]s for discrete functions or digital signals. As well as the [[discrete metric]] there are more general discrete or [[finite metric space]]s and [[finite topological space]]s.  ===Geometry=== [[File:SimplexRangeSearching.png|right|thumb|150px|[[Computational geometry]] applies computer [[algorithm]]s to representations of [[geometry|geometrical]] objects.]] {{Main|discrete geometry|computational geometry}} [[Discrete geometry]] and combinatorial geometry are about combinatorial properties of ''discrete collections'' of geometrical objects. A long-standing topic in discrete geometry is [[tesselation|tiling of the plane]].  Computational geometry applies algorithms to geometrical problems.  === Topology === Although [[topology]] is the field of mathematics that formalizes and generalizes the intuitive notion of "continuous deformation" of objects, it gives rise to many discrete topics; this can be attributed in part to the focus on [[topological invariant]]s, which themselves usually take discrete values. See [[combinatorial topology]], [[topological graph theory]], [[topological combinatorics]], [[computational topology]], [[discrete topological space]], [[finite topological space]], [[topology (chemistry)]].  ===Operations research=== {{Main|Operations research}} [[File:Pert chart colored.svg|right|thumb|150px|[[Program Evaluation and Review Technique|PERT]] charts like this provide a business management technique based on [[graph theory]].]] Operations research provides techniques for solving practical problems in business and other fields — problems such as allocating resources to maximize profit, or scheduling project activities to minimize risk.  Operations research techniques include [[linear programming]] and other areas of [[optimization (mathematics)|optimization]], [[queuing theory]], [[scheduling algorithm|scheduling theory]], [[network theory]]. Operations research also includes continuous topics such as [[continuous-time Markov process]], continuous-time [[Martingale (probability theory)|martingales]], [[process optimization]], and continuous and hybrid [[control theory]].  ===Game theory, decision theory, utility theory, social choice theory=== {{Payoff matrix | Name = Payoff matrix for the [[Prisoner's dilemma]], a common example in [[game theory]]. One player chooses a row, the other a column; the resulting pair gives their payoffs                 | 2L = Cooperate  | 2R = Defect     | 1U = Cooperate  | UL = -1, -1       | UR = -10, 0       | 1D = Defect     | DL = 0, -10       | DR = -5, -5      }} [[Decision theory]] is concerned with identifying the values, uncertainties  and other issues relevant in a given decision, its rationality, and the resulting optimal decision.  [[Utility theory]] is about measures of the relative [[economic]] satisfaction from, or desirability of, consumption of various goods and services.  [[Social choice theory]] is about [[voting]]. A more puzzle-based approach to voting is [[ballot theory]].  [[Game theory]] deals with situations where success depends on the choices of others, which makes choosing the best course of action more complex. There are even continuous games, see [[differential game]]. Topics include [[auction theory]] and [[fair division]].  ===Discretization=== {{Main|Discretization}}  Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. [[Numerical analysis]] provides an important example.  ===Discrete analogues of continuous mathematics=== There are many concepts in continuous mathematics which have discrete versions, such as [[discrete calculus]], [[discrete probability distribution]]s, [[discrete Fourier transform]]s, [[discrete geometry]], [[discrete logarithm]]s, [[discrete differential geometry]], [[discrete exterior calculus]], [[discrete Morse theory]], [[difference equation]]s, [[discrete dynamical system]]s, and [[Shapley–Folkman lemma#Probability and measure theory|discrete vector&nbsp;measures]].   In [[applied mathematics]], [[discrete modelling]] is the discrete analogue of [[continuous modelling]].  In discrete modelling, discrete formulae are fit to [[data]]. A common method in this form of modelling is to use [[recurrence relation]]s.  ===Hybrid discrete and continuous mathematics=== The [[time scale calculus]] is a unification of the theory of [[difference equations]] with that of [[differential equations]], which has applications to fields requiring simultaneous modelling of discrete and continuous  ==See also== {{Portal|Discrete mathematics}} * [[Outline of discrete mathematics]] * [[CyberChase]], a show that teaches Discrete Mathematics to children  ==References== {{Reflist}}  ==Further reading== {{Wikibooks|Discrete Mathematics}} * [[Norman L. Biggs]], ''Discrete Mathematics'' 2nd ed. Oxford University Press. ISBN 0-19-850717-8, and [http://www.oup.co.uk/isbn/0-19-850717-8 companion web site including questions together with solutions]. * [[Ronald Graham]], [[Donald Knuth|Donald E. Knuth]], [[Oren Patashnik]], ''[[Concrete Mathematics]]'' * [[Donald E. Knuth]], ''[[The Art of Computer Programming]]'' ISBN 978-0-321-75104-1. * [[Kenneth H. Rosen]], ''Handbook of Discrete and Combinatorial Mathematics'' CRC Press.  ISBN 0-8493-0149-1. * [[Richard Johnsonbaugh]], ''Discrete Mathematics'' 6th ed. Macmillan. ISBN 0-13-045803-1, and [http://wps.prenhall.com/esm_johnsonbau_discrtmath_6/ companion web site]. * [[John Dwyer (mathematician)|John Dwyer]] & Suzy Jagger, ''Discrete Mathematics for Business & Computing'', 1st ed. 2010 ISBN 978-1-907934-00-1. * [[Kenneth H. Rosen]], ''Discrete Mathematics and Its Applications'' 6th ed. McGraw Hill. ISBN 0-07-288008-2, and [http://highered.mcgraw-hill.com/sites/0072880082/information_center_view0/ companion web site]. * [[Ralph Grimaldi|Ralph P. Grimaldi]], ''Discrete and Combinatorial Mathematics: An Applied Introduction'' 5th ed. Addison Wesley. ISBN 0-201-72634-3 * [[Susanna S. Epp]], ''Discrete Mathematics with Applications'' Brooks Cole. ISBN 978-0-495-39132-6 * [[Jiří Matoušek (mathematician)|Jiří Matoušek]] & [[Jaroslav Nešetřil]], ''Invitation to Discrete Mathematics'', OUP, ISBN 978-0-19-850208-1. * [http://archives.math.utk.edu/topics/discreteMath.html Mathematics Archives], Discrete Mathematics links to syllabi, tutorials, programs, etc. * [[Andrew Clive Simpson|Andrew Simpson]], ''Discrete Mathematics by Example'' McGraw Hill. ISBN 0-07-709840-4 {{Mathematics-footer}}  {{DEFAULTSORT:Discrete Mathematics}} [[Category:Discrete mathematics| ]]  [[af:Diskrete wiskunde]] [[ar:رياضيات متقطعة]] [[an:Matematica discreta]] [[az:Diskret riyaziyyat]] [[bn:বিচ্ছিন্ন গণিত]] [[be:Дыскрэтная матэматыка]] [[be-x-old:Дыскрэтная матэматыка]] [[bg:Дискретна математика]] [[bs:Diskretna matematika]] [[ca:Matemàtica discreta]] [[cs:Diskrétní matematika]] [[da:Diskret matematik]] [[de:Diskrete Mathematik]] [[et:Diskreetne matemaatika]] [[el:Διακριτά μαθηματικά]] [[es:Matemáticas discretas]] [[eo:Diskreta matematiko]] [[fa:ریاضیات گسسته]] [[fr:Mathématiques discrètes]] [[ko:이산수학]] [[hy:Դիսկրետ մաթեմատիկա]] [[hi:विविक्त गणित]] [[hr:Diskretna matematika]] [[id:Matematika diskret]] [[it:Matematica discreta]] [[he:מתמטיקה בדידה]] [[ka:დისკრეტული მათემატიკა]] [[kk:Дискреттік математика]] [[lt:Diskrečioji matematika]] [[hu:Diszkrét matematika]] [[mr:सांगणिक गणित]] [[ms:Matematik diskret]] [[nl:Discrete wiskunde]] [[ja:離散数学]] [[no:Diskret matematikk]] [[nn:Diskret matematikk]] [[pms:Matemàtica discreta]] [[pl:Matematyka dyskretna]] [[pt:Matemática discreta]] [[ru:Дискретная математика]] [[sq:Matematika diskrete]] [[simple:Discrete mathematics]] [[sk:Diskrétna matematika]] [[sl:Diskretna matematika]] [[sr:Дискретна математика]] [[sh:Diskretna matematika]] [[fi:Diskreetti matematiikka]] [[sv:Diskret matematik]] [[tl:Diskretong matematika]] [[ta:இலக்கமியல் கணிதம்]] [[th:วิยุตคณิต]] [[tg:Математикаи дискретӣ]] [[tr:Ayrık matematik]] [[uk:Дискретна математика]] [[ur:متفرد ریاضیات]] [[vi:Toán học rời rạc]] [[zh:离散数学]]
{{expert|date=November 2010}} {{Unreferenced stub|auto=yes|date=December 2009}} '''Document Processing''' involves the conversion of typed and handwritten text on paper-based & electronic documents (e.g., scanned image of a document) into electronic information utilising one of, or a combination of, [[Intelligent Character Recognition]] (ICR), [[Optical Character Recognition]] (OCR) and experienced [[Data Entry Clerks]].  ==See also== *[[Outsourced document processing]]  {{DEFAULTSORT:Document Processing}} [[Category:Artificial intelligence applications]]   {{Tech-stub}}
{{Merge from|Edtech|discuss=Talk:Educational technology#Merger proposal (from Edtech) |date=July 2012}}  {{Merge from|Impact of technology on the educational system|date=September 2011}} {{Educational research}} '''Educational technology''' is the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources."<ref>Richey, R.C. (2008). Reflections on the 2008 AECT Definitions of the Field. TechTrends. 52(1) 24-25</ref> The term educational technology is often associated with, and encompasses, [[instructional theory]] and [[Learning theory (education)|learning theory]]. While  '''[[instructional technology]]''' is "the theory and practice of design, development, utilization, management, and evaluation of processes and resources for learning," according to the Association for Educational Communications and Technology (AECT) Definitions and Terminology Committee,<ref>{{cite book | title = E-Learning in the 21st Century: A Framework for Research and Practice | author = D. Randy Garrison and Terry Anderson | isbn = 0-415-26346-8 | year = 2003 | publisher = Routledge | url = http://books.google.com/books?id=UZOG5KEoiCQC&pg=PA33&dq=define-instructional-technology&lr=&as_brr=0&ei=ClahR5qoMY_-sQPyx-2bCg&sig=P3kU_P8ZfHHGAvedqg3rF2UG7gc }}</ref> educational technology includes other systems used in the process of developing human capability. Educational technology includes, but is not limited to, software, hardware, as well as Internet applications, such as wiki's and blogs, and activities.  But there is still debate on what these terms mean.<ref>Lowenthal, P. R., & Wilson, B. G. (2010). Labels do matter! A critique of AECT’s redefinition of the field. ''TechTrends, 54''(1), 38-46. {{doi|10.1007/s11528-009-0362-y}}</ref>  Technology of education is most simply and comfortably defined as an array of tools that might prove helpful in advancing student learning and may be measured in how and why individuals behave.   Educational Technology relies on a broad definition of the word "[[technology]]."  Technology can refer to material objects of use to humanity, such as machines or hardware, but it can also encompass broader themes, including systems, methods of organization, and techniques. Some modern tools include but are not limited to overhead projectors, laptop computers, and calculators. Newer tools such as "smartphones" and games (both online and offline) are beginning to draw serious attention for their learning potential. Media psychology is the field of study that applies theories in human behavior to educational technology.  Consider the ''Handbook of Human Performance Technology''.<ref>''Handbook of Human Performance Technology'' (Eds. Harold Stolovich, Erica Keeps, James Pershing) (3rd ed, 2006)</ref> The word technology for the sister fields of Educational and [[Human Performance Technology]] means "applied science." In other words, any valid and reliable process or procedure that is derived from basic research using the "scientific method" is considered a "technology." Educational or Human Performance Technology may be based purely on algorithmic or heuristic processes, but neither necessarily implies physical technology. The word technology comes from the Greek "[[techne]]" which means craft or art. Another word, "technique," with the same origin, also may be used when considering the field Educational Technology. So Educational Technology may be extended to include the techniques of the educator.{{Citation needed|date=April 2008}}  A classic example of an Educational Psychology text is Bloom's 1956 book, ''Taxonomy of Educational Objectives''.<ref>Bloom B. S. (1956). ''Taxonomy of Educational Objectives, Handbook I: The Cognitive Domain''. New York: David McKay Co Inc.</ref> [[Bloom's Taxonomy]] is helpful when designing learning activities to keep in mind what is expected of—and what are the learning goals for—learners. However, Bloom's work does not explicitly deal with educational technology ''per se'' and is more concerned with pedagogical strategies.  According to some, an Educational Technologist is someone who transforms basic educational and psychological research into an evidence-based applied science (or a technology) of learning or instruction. Educational Technologists typically have a graduate degree (Master's, Doctorate, Ph.D., or D.Phil.) in a field related to educational psychology, educational media, experimental psychology, cognitive psychology or, more purely, in the fields of Educational, Instructional or Human Performance Technology or [[Instruction design|Instructional Systems Design]]. But few of those listed below as theorists would ever use the term "educational technologist" as a term to describe themselves, preferring terms such as "educator."{{Citation needed|date=April 2008}} The transformation of educational technology from a cottage industry to a profession is discussed by Shurville, Browne, and Whitaker.  ==History== Educational technology in a way could be traced back to the emergence of very early tools, e.g., paintings on cave walls. But usually its history starts with educational film (1900s) or Sidney Pressey's mechanical teaching machines in the 1920s.  The first large scale usage of new technologies can be traced to US WWII training of soldiers through training films and other mediated materials. Today, presentation-based technology, based on the idea that people can learn through aural and visual reception, exists in many forms, e.g., streaming audio and video, or PowerPoint presentations with voice-over. Another interesting invention of the 1940s was hypertext, i.e., V. Bush's memex.  The 1950s led to two major, still popular designs. Skinners work led to "[[programmed instruction]]" focusing on the formulation of behavioral objectives, breaking instructional content into small units and rewarding correct responses early and often. Advocating a mastery approach to learning based on his taxonomy of intellectual behaviors, Bloom endorsed instructional techniques that varied both instruction and time according to learner requirements. Models based on these designs were usually referred to as computer-based training" (CBT), Computer-aided instruction or computer-assisted instruction (CAI) in the 1970s through the 1990s. In a more simplified form they correspond to today's "e-contents" that often form the core of "[[e-learning]]" set-ups, sometimes also referred to as web-based training (WBT) or e-instruction. The course designer divides learning contents into smaller chunks of text augmented with graphics and multimedia presentation. Frequent Multiple Choice questions with immediate feedback are added for self-assessment and guidance. Such e-contents can rely on standards defined by IMS, ADL/[[SCORM]] and IEEE.  The 1980s and 1990s produced a variety of schools that can be put under the umbrella of the label Computer-based learning (CBL). Frequently based on constructivist and cognitivist learning theories, these environments focused on teaching both abstract and domain-specific problem solving. Preferred technologies were micro-worlds (computer environments where learners could explore and build), simulations (computer environments where learner can play with parameters of dynamic systems) and hypertext.  Digitized communication and networking in education started in the mid 80s and became popular by the mid-90's, in particular through the World-Wide Web (WWW), eMail and Forums. There is a difference between two major forms of online learning. The earlier type, based on either Computer Based Training (CBT) or Computer-based learning (CBL), focused on the interaction between the student and computer drills plus tutorials on one hand or micro-worlds and simulations on the other. Both can be delivered today over the WWW. Today, the prevailing paradigm in the regular school system is Computer-mediated communication (CMC), where the primary form of interaction is between students and instructors, mediated by the computer. CBT/CBL usually means individualized (self-study) learning, while CMC involves teacher/tutor facilitation and requires scenarization of flexible learning activities. In addition, modern ICT provides education with tools for sustaining learning communities and associated knowledge management tasks. It also provides tools for student and curriculum management.  In addition to classroom enhancement, learning technologies also play a major role in full-time [[distance education|distance teaching]]. While most quality offers still rely on paper, videos and occasional CBT/CBL materials, there is increased use of e-tutoring through forums, instant messaging, video-conferencing etc. Courses addressed to smaller groups frequently use blended or hybrid designs that mix presence courses (usually in the beginning and at the end of a module) with distance activities and use various pedagogical styles (e.g., drill & practise, exercises, projects, etc.).  The 2000s emergence of multiple mobile and ubiquitous technologies gave a new impulse to situated learning theories favoring learning-in-context scenarios. Some literature uses the concept of integrated learning to describe [[blended learning]] scenarios that integrate both school and authentic (e.g., workplace) settings.  ==Theories and practices== Three main theoretical schools or philosophical frameworks have been present in the educational technology literature. These are [[Behaviorism]], [[Cognitivism (psychology)|Cognitivism]] and [[Constructivism (learning theory)|Constructivism]]. Each of these schools of thought are still present in today's literature but have evolved as the [[Psychology]] literature has evolved.  ===Behaviorism=== This theoretical framework was developed in the early 20th century with the animal learning experiments of [[Ivan Pavlov]], [[Edward Thorndike]], [[Edward C. Tolman]], [[Clark L. Hull]], [[B.F. Skinner]] and many others. Many psychologists used these theories to describe and experiment with human learning. While still very useful this philosophy of learning has lost favor with many educators.  ====Skinner's contributions==== [[B.F. Skinner]] wrote extensively on improvements of teaching based on his functional analysis of [[Verbal Behavior]]<ref>Skinner, B.F. The science of learning and the art of teaching. Harvard Educational Review, 1954, 24, 86-97., Teaching machines. ''Science'', 1958, 128, 969-77. and others see http://www.bfskinner.org/f/EpsteinBibliography.pdf</ref> and wrote "The Technology of Teaching",<ref>{{cite journal |journal= Proc R Soc Lond B Biol Sci |year=1965 |volume=162 |issue=989 |pages=427–43 |title= The technology of teaching |author= Skinner BF |doi=10.1098/rspb.1965.0048 |pmid=4378497}}</ref> an attempt to dispel the myths underlying contemporary education as well as promote his system he called [[programmed instruction]]. [[Ogden Lindsley]] also developed the Celeration learning system similarly based on behavior analysis but quite different from Keller's and Skinner's models.  ===Cognitivism=== [[Cognitive science]] has changed how educators view learning. Since the very early beginning of the Cognitive Revolution of the 1960s and 1970s, learning theory has undergone a great deal of change. Much of the empirical framework of Behaviorism was retained even though a new paradigm had begun. Cognitive theories look beyond behavior to explain brain-based learning. Cognitivists consider how human memory works to promote learning.  After memory theories like the [[Atkinson-Shiffrin memory model]] and Baddeley's [[Working memory]] model were established as a theoretical framework in [[Cognitive Psychology]], new cognitive frameworks of learning began to emerge during the 1970s, 1980s, and 1990s. It is important to note that Computer Science and Information Technology have had a major influence on Cognitive Science theory. The Cognitive concepts of working memory (formerly known as short term memory) and long term memory have been facilitated by research and technology from the field of Computer Science. Another major influence on the field of Cognitive Science is [[Noam Chomsky]]. Today researchers are concentrating on topics like [[Cognitive load]] and [[Information Processing]] Theory. In addition, psychology as applied to media is easily measured in studying behavior. The area of media psychology is both cognative and affective and is central to understanding educational technology.  ===Constructivism=== [[Constructivism (learning theory)|Constructivism]] is a learning theory or educational philosophy that many educators began to consider in the 1990s. One of the primary tenets of this philosophy is that learners construct their own meaning from new information, as they interact with reality or others with different perspectives.  Constructivist learning environments require students to utilize their prior knowledge and experiences to formulate new, related, and/or adaptive concepts in learning. Under this framework the role of the teacher becomes that of a facilitator, providing guidance so that learners can construct their own knowledge. Constructivist educators must make sure that the prior learning experiences are appropriate and related to the concepts being taught. Jonassen (1997) suggests "well-structured" learning environments are useful for novice learners and that "ill-structured" environments are only useful for more advanced learners. Educators utilizing technology when teaching with a constructivist perspective should choose technologies that reinforce prior learning perhaps in a problem-solving environment.  ==Instructional technique and technologies== [[Problem Based Learning]], [[Project-based Learning]], and [[Inquiry-based learning]] are [[active learning]] educational technologies used to facilitate learning. [[Technology]] which includes physical and process applied science can be incorporated into project, problem, inquiry-based learning as they all have a similar educational philosophy. All three are student centered, ideally involving real-world scenarios in which students are actively engaged in critical thinking activities. The process that students are encouraged to employ (as long as it is based on empirical research) is considered to be a technology. Classic examples of technologies used by teachers and Educational Technologists include Bloom's Taxonomy and Instructional Design.  ==Theorists== This is an area where new thinkers are coming to the forefront everyday. Many of the ideas spread from theorists, researchers, and experts through their blogs. Extensive lists of educational bloggers by area of interest are available at Steve Hargadon's "SupportBloggers" site or at the "movingforward" wiki started by Scott McLeod.<ref>See http://supportblogging.com/Links to School Bloggers and http://movingforward.wikispaces.com/Blogs</ref> Many of these blogs are recognized by their peers each year through the edublogger awards.<ref>[http://edublogawards.com/ » Welcome to the Eddies! The Edublog Awards<!-- Bot generated title -->]</ref> [[Web 2.0]] technologies have led to a huge increase in the amount of information available on this topic and the number of educators formally and informally discussing it. Most listed below have been around for more than a decade, however, and few new thinkers mentioned above are listed here.  {{Col-begin}} {{Col-2}} *[[Alan November]] *[[Seymour Papert]]<ref>[http://www.papert.org/ Professor Seymour Papert<!-- Bot generated title -->]</ref> *[[Will Richardson]] *[[John Sweller]] *[[Don Krug]] {{Col-2}} *[[Alex Jones (educational theorist)|Alex Jones]] *[[George Siemens]] *[[David A. Wiley|David Wiley]] *[[David Wilson (educational theorist)|David Wilson]] *[[Bernard Luskin]] {{Col-end}}  ==Benefits== Educational technology is intended to improve education over what it would be without technology. Some of the claimed benefits are listed below: *'''Easy-to-access course materials'''. Instructors can post the course material or important information on a course website, which means students can study at a time and location they prefer and can obtain the study material very quickly<ref name="num1">[http://www.nsba.org/sbot/toolkit/tiol.html Technology Impact on Learning<!-- Bot generated title -->]</ref> *'''Student motivation'''. Computer-based instruction can give instant feedback to students and explain correct answers. Moreover, a computer is patient and non-judgmental, which can give the student motivation to continue learning. According to James Kulik, who studies the effectiveness of computers used for instruction, students usually learn more in less time when receiving computer-based instruction and they like classes more and develop more positive attitudes toward computers in computer-based classes.<ref name="num2">[http://www.electronic-school.com/0997f3.html Technology's Impact<!-- Bot generated title -->]</ref> The American educator, [[Cassandra B. Whyte]], researched and reported about the importance of [[locus of control]] and successful academic performance and by the late 1980s, she wrote of how important computer usage and information technology would become in the higher education experience of the future.<ref>Whyte,Cassandra Bolyard. (1980). "An Integrated Counseling and Learning Assistance Center." New Directions Sourcebook. Jossey-Bass, Inc. San Francisco, California.</ref><ref>Whyte, Cassandra B. (1989). Student Affairs - The Future", Journal of College Student Development, 10, (1), 86-89.</ref> *'''Wide participation'''. Learning material can be used for long distance learning and are accessible to a wider audience<ref>[http://www.nsba.org/sbot/toolkit/tuie.html Technology Uses in Education<!-- Bot generated title -->]</ref> *'''Improved student writing'''. It is convenient for students to edit their written work on word processors, which can, in turn, improve the quality of their writing. According to some studies, the students are better at critiquing and editing written work that is exchanged over a computer network with students they know<ref name="num1"/> *'''Subjects made easier to learn'''. Many different types of educational software are designed and developed to help children or teenagers to learn specific subjects. Examples include pre-school software, computer simulators, and graphics software<ref name="num2"/> *A structure that is more amenable to measurement and improvement of outcomes. With proper structuring it can become easier to monitor and maintain student work while also quickly gauging modifications to the instruction necessary to enhance student learning. *'''Differentiated Instruction.'''  Educational technology provides the means to focus on active student participation and to present differentiated questioning strategies.  It broadens individualized instruction and promotes the development of personalized learning plans.  Students are encouraged to use multimedia components and to incorporate the knowledge they gained in creative ways.<ref>Smith, Grace and Stephanie Throne.  Differentiating Instruction with Technology in the K-5 Classrooms.  International Society for Technology in Education.  2004</ref>  ==Criticism== Although technology in the classroom does have many benefits, there are clear drawbacks as well.  Lack of proper training, limited access to sufficient quantities of a technology, and the extra time required for many implementations of technology are just a few of the reasons that technology is often not used extensively in the classroom. To understand educational technology one must also understand theories in human behavior as behavior is affected by technology. Media Psychology is the study of media, technology and how and why individuals, groups and societies behave the way they do. The first Ph.D program with a concentration in media psychology was started in 2002 at Fielding Graduate University by Bernard Luskin. The Media Psychology division of APA, division 46 has a focus on media psychology. Media and the family is another emerging area affected by rapidly changing educational technology.  Similar to learning a new task or trade, special training is vital to ensuring the effective integration of classroom technology.  Since technology is not the end goal of education, but rather a means by which it can be accomplished, educators must have a good grasp of the technology being used and its advantages over more traditional methods.  If there is a lack in either of these areas, technology will be seen as a hindrance and not a benefit to the goals of teaching.  Another difficulty is introduced when access to a sufficient quantity of a resource is limited.  This is often seen when the quantity of computers or digital cameras for classroom use is not enough to meet the needs of an entire classroom.  It also occurs in less noticed forms such as limited access for technology exploration because of the high cost of technology and the fear of damages.  In other cases, the inconvenience of resource placement is a hindrance, such as having to transport a classroom to a computer lab instead of having in-classroom computer access by means of technology such as laptop carts.  Technology implementation can also be time consuming.  There may be an initial setup or training time cost inherent in the use of certain technologies.   Even with these tasks accomplished, technology failure may occur during the activity and as a result teachers must have an alternative lesson ready.  Another major issue arises because of the evolving nature of technology.  New resources have to be designed and distributed whenever the technological platform has been changed.  Finding quality materials to support classroom objectives after such changes is often difficult even after they exist in sufficient quantity and teachers must design these resources on their own.  Experimental evidence suggests that these criticisms may have limited basis.  See, for example, the work done by Sugata Mitra.<ref>http://www.ascilite.org.au/ajet/ajet21/mitra.html</ref>  A recent presentation summarizes the research and Dr. Mitra's current research initiative.<ref>http://www.ted.com/talks/sugata_mitra_the_child_driven_education.html#</ref><ref name="cordes">Cordes, Colleen & Miller, Edward. (1999),[http://drupal6.allianceforchildhood.org/fools_gold "Fool's Gold: A Critical Look at Computers in Childhood"]</ref>  ==Educational technology and the humanities== Research from the Alberta Initiative for School Improvement (AISI)<ref>[http://education.alberta.ca/media/616853/techprojectsreview.pdf AISI Technology Projects Research Review]</ref> indicates that [[Inquiry-based learning|inquiry]] and [[Project-based learning|project-based]] approaches, combined with a focus on curriculum, effectively supports the infusion of educational technologies into the learning and teaching process.  == Technology in the classroom == There are various types of technologies currently used in traditional classrooms. Among these are:  *'''Computer in the classroom:''' Having a computer in the classroom is an asset to any teacher. With a computer in the classroom, teachers are able to demonstrate a new lesson, present new material, illustrate how to use new programs, and show new websites.<ref> [http://www.thejournal.com/articles/15769 Using Technology to Enhance the Classroom Environment]. THE Journal, 01 January 2002 </ref>  *'''Class website:''' An easy way to display your student's work is to create a web page designed for your class.  Once a web page is designed, teachers can post homework assignments, student work, famous quotes, trivia games, and so much more. In today's society, children know how to use the computer and navigate their way through a website, so why not give them one where they can be a published author. Just be careful as most districts maintain strong policies to manage official websites for a school or classroom. Also, most school districts provide teacher webpages that can easily be viewed through the school district's website.  *'''Class blogs and wikis:''' There are a variety of Web 2.0 tools that are currently being implemented in the classroom. Blogs allow for students to maintain a running dialogue, such as a journal,thoughts, ideas, and assignments that also provide for student comment and reflection. Wikis are more group focused to allow multiple members of the group to edit a single document and create a truly collaborative and carefully edited finished product.  *'''Wireless classroom microphones:''' Noisy classrooms are a daily occurrence, and with the help of microphones, students are able to hear their teachers more clearly. Children learn better when they hear the teacher clearly. The benefit for teachers is that they no longer lose their voices at the end of the day.  *'''Mobile devices:''' Mobile devices such as [[Clicker (classroom)|clickers]] or [[smartphone]] can be used to enhance the experience in the classroom by providing the possibility for professors to get feedback.<ref>{{cite web | last = Tremblay | first = Eric|title = Educating the Mobile Generation – using personal cell phones as audience response systems in post-secondary science teaching. Journal of Computers in Mathematics and Science Teaching, 2010, 29(2), 217-227. Chesapeake, VA: AACE.| url=http://editlib.org/p/32314 |accessdate = 2010-11-05}}</ref>  See also [[MLearning]].  *'''Interactive Whiteboards:''' An interactive whiteboard that provides touch control of computer applications.  These enhance the experience in the classroom by showing anything that can be on a computer screen.  This not only aids in visual learning, but it is interactive so the students can draw, write, or manipulate images on the interactive whiteboard.  *'''Online media:''' Streamed video websites can be utilized to enhance a classroom lesson (e.g. United Streaming, Teacher Tube, etc.)  *'''Digital Games:''' The field of educational games and serious games has been growing significantly over the last few years. The digital games are being provided as tools for the classroom and have a lot of positive feedback including higher motivation for students.<ref>{{cite web|last=Biocchi|first=Michael|title=Games in the Classroom|url=http://educationtech.ca/2011/03/24/games-in-the-classroom/|work=Gaming in the Classroom|accessdate=24 March 2011}}</ref>   There are many other tools being utilized depending on the local school board and funds available. These may include: digital cameras, video cameras, interactive whiteboard tools, document cameras, or LCD projectors.  *'''Podcasts:''' Podcasting is a relatively new invention that allows anybody to publish files to the Internet where individuals can subscribe and receive new files from people by a subscription. The primary benefit of podcasting for educators is quite simple. It enables teachers to reach students through a medium that is both "cool" and a part of their daily lives. For a technology that only requires a computer, microphone and internet connection, podcasting has the capacity of advancing a student’s education beyond the classroom. When students listen to the podcasts of other students as well as their own, they can quickly demonstrate their capacities to identify and define "quality." This can be a great tool for learning and developing literacy inside and outside the classroom. Podcasting can help sharpen students’ vocabulary, writing, editing, public speaking, and presentation skills. Students will also learn skills that will be valuable in the working world, such as communication, time management, and problem-solving.  Although podcasts are a new phenomenon in classrooms, especially on college campuses, studies have shown the differences in effectiveness between a live lecture versus podcast are minor in terms of the education of the student. <ref>{{cite journal|last=Reeves|first=Thomas C.|title=The Impact of Media and Technology in Schools|date=February 12, 1998|accessdate=3 October 2011}}</ref>    ==Societies== Learned societies concerned with educational technology include: * [[Association for the Advancement of Computing in Education (AACE)]] * [[Association for Educational Communications and Technology]] * [[Association for Learning Technology]] * [[International Society for Performance Improvement]] * [[International Society for Technology in Education|International Society for Technology in Education - (ISTE)]] * [http://www.ea-tel.eu/ European Association of Technology-Enhanced Learning]  ==See also== {{Col-begin}} {{Col-2}} * [[ADDIE Model]] * [[Assistive technology]] * [[Computerized adaptive testing]] * [[Impact of technology on the educational system]] * [[Information and communication technologies in education]] * [[Information mapping]] * [[Intelligent tutoring system]] {{Col-2}} * [[Matching Person & Technology Model]] * [[Mind map]] * [[MyEdu]] * [[Technological Pedagogical Content Knowledge]] * [[Usability testing]] {{Col-end}}  ==References== {{Reflist}}  ==Further reading== {{Wiktionary}} *Bednar, M. R., & Sweeder, J. J. (2005). Defining and applying idea technologies: A systematic, conceptual framework for teachers. ''Computers in the Schools, 22''(3/4). * {{cite book | first=Alan | last=Januszewski | year=2001 | title=Educational Technology: The Development of a Concept | publisher=Libraries Unlimited | isbn=1-56308-749-9}} *Jonassen, D. (1997). Instructional design models for well-structured and ill-structured problem-solving learning outcomes. ''Educational Technology Research & Development'', 45, 65–94. * {{cite book | first=D H| last= Jonassen | year=2006| title= Modeling with Technology: Mindtools for Conceptual Change| publisher= OH: Merrill/Prentice-Hall}} * [http://www.cogtech.usc.edu/publications/kirschner_Sweller_Clark.pdf Kirschner, P. A., Sweller, J., and Clark, R. E. (2006) Why minimal guidance during instruction does not work: an analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching. Educational Psychologist 41 (2) 75-86] * {{cite book | first=K L | last=Kumar | year=1997 | title=Educational Technology: A Practical Textbook for Students, Teachers, Professionals and Trainers | publisher=New Age International | location=New Delhi | isbn=81-224-0833-8}} * [http://coe.sdsu.edu/eet/ Encyclopedia of Educational Technology], a comprehensive resource of articles about Educational Technology, published by the Department of Educational Technology, [[San Diego State University]] * [http://www.open.ac.uk/pbpl/resources/details/detail.php?itemId=49d62232adc62 Looking Back to Look Ahead - Learning Through and From the Human Spirit] * [http://quest.eeaonline.org/india/test/images/india/Image/File/output/Geetha%20Keynote.pdf Geetha Narayanan's Keynote address at Symposium on Education and Technology in Schools in 2008, Bangalore] * Lipsitz, Lawrence, (Editor); Reisner, Trudi, [http://books.google.com/books?id=pFzxLUAnSS8C&printsec=frontcover ''The Computer and Education''], Englewood Cliffs, NJ : Educational Technology Publications, January 1973. Articles selected from ''Educational Technology'' magazine. * L Low & M O'Connell, [https://olt.qut.edu.au/udf/OLT2006/gen/static/papers/Low_OLT2006_paper.pdf Learner-Centric Design of Digital Mobile Learning], Queensland University of Technology, 2006. *Professor Brian J. Ford, ''[http://www.brianjford.com/a-05-ZENO.HTM Absolute Zeno]'', Laboratory News p 16, January 2006. * McKenzie, Jamie (2006). [http://fno.org/dec05/writing.html "Inspired Writing and Thinking"] * McKenzie, Jamie (2007). [http://fno.org/nov07/nativism.html "Digital Nativism, Digital Delusions, and Digital Deprivation"] * McKenzie, Jamie (2008). [http://fno.org/may08/digital.html "What Digital Age?"] * Mishra, P. & Koehler, M.J. (2006). [http://www.tcrecord.org/content.asp?contentid=12516 Technological pedagogical content knowledge: A framework for integrating technology in teacher knowledge]. Teachers College Record, 108(6), 1017-1054. * Monahan, Torin (2005). ''Globalization, Technological Change, and Public Education''. New York: Routledge: ISBN 0-415-95103-8. * {{cite book | last=Randolph | first=J. J. | year=2007| title=Multidisciplinary Methods in Educational Technology Research and Development | publisher=Hameenlinna, Finland: HAMK |  accessdate=2008-06-10 | isbn=978-951-784-453-6 | url=http://justus.randolph.name/methods }} * {{cite book | first=S K | last= Soni | year=2004 | title=An Information Resource on Educational Technology for: Technical & Vocational Education and Training (TVET)  | publisher= Sarup & Sons Publishers | location = New Delhi | isbn=81-7625-506-8}} * Scherer, M.J.  (2004).  ''Connecting to Learn:  Educational and Assistive Technology for People with Disabilities''.  Washington, DC:  American Psychological Association (APA) Books: ISBN 1-55798-982-6. * Shurville, S., Browne, H. and Whitaker, M. (2008). [http://www.ascilite.org.au/conferences/melbourne08/procs/shurville.pdf "Employing Educational Technologists: A Call for Evidenced Change"]. In Hello! Where are you in the landscape of educational technology? Proceedings ascilite Melbourne 2008. * {{cite book | first=B.F.| last=Skinner| year=1968 | title= The technology of teaching| publisher=New York: Appleton-Century-Crofts | id=Library of Congress Card Number 68-12340 E 81290}} * [[Patrick Suppes|Suppes, Patrick]], [http://suppes-corpus.stanford.edu/article.html?id=67 "The Uses of Computers in Education"], Scientific American, v215 n3 p206-20 Sep 1966   {{Technology}} {{Education}}  {{DEFAULTSORT:Educational Technology}} [[Category:Educational technology| ]] [[Category:Educational psychology]]  [[ar:تكنولوجيا التعليم]] [[bg:Образователна технология]] [[el:Εκπαιδευτική τεχνολογία]] [[fa:فناوری آموزشی]] [[ko:교육공학]] [[hi:शैक्षिक प्रौद्योगिकी]] [[id:Teknologi pendidikan]] [[ms:Teknologi pendidikan]] [[nl:Leermiddel]] [[ja:教育工学]] [[no:Utdanningsteknologi]] [[pt:Tecnologia educacional]] [[ru:Образовательные технологии]] [[fi:Opetusteknologia]] [[te:విద్యా సాంకేతికత]] [[th:เทคโนโลยีการศึกษา]] [[tr:Eğitim teknolojisi]]
{{Redirect|Encrypt|the film|Encrypt (film)}} {{About|algorithms for encryption and decryption|an overview of cryptographic technology in general|Cryptography}}  In [[cryptography]],   '''encryption''' is the process of transforming [[information]] (referred to as [[plaintext]]) using an [[algorithm]] (called a [[cipher]]) to make it unreadable to anyone except those possessing special knowledge, usually referred to as a [[key (cryptography)|key]].  The result of the process is '''encrypted''' information (in cryptography, referred to as [[ciphertext]]). {{anchor|decryption}}The reverse process, i.e., to make the encrypted information readable again, is referred to as '''decryption''' (i.e., to make it unencrypted).{{cn|date=April 2012}}  In many contexts, the word '''encryption''' may also implicitly refer to the reverse process, '''decryption''' e.g. “[[encryption software|software for encryption]]” can typically also perform decryption.{{citation needed|date=December 2011}}  Encryption has long been used by militaries and governments to facilitate secret communication. It is now commonly used in protecting information within many kinds of civilian systems. For example, the [[Computer Security Institute]] reported that in 2007, 71% of companies surveyed utilized encryption for some of their data in transit, and 53% utilized encryption for some of their data in storage.<ref>Robert Richardson, 2008 CSI Computer Crime and Security Survey at 19.  Online at [http://i.cmpnet.com/v2.gocsi.com/pdf/CSIsurvey2008.pdf i.cmpnet.com]</ref> Encryption can be used to protect data "at rest", such as files on [[computers]] and storage devices (e.g. [[USB flash drives]]). In recent years there have been numerous reports of confidential data such as customers' personal records being exposed through loss or theft of laptops or backup drives. Encrypting such files at rest helps protect them should physical security measures fail. [[Digital rights management]] systems which prevent unauthorized use or reproduction of copyrighted material and protect software against [[reverse engineering]] (see also [[copy protection]]) are another somewhat different example of using encryption on data at rest.{{cn|date=April 2012}}  Encryption is also used to protect data in transit, for example data being transferred via [[computer network|networks]] (e.g. the [[Internet]], [[e-commerce]]), [[mobile telephone]]s, [[wireless microphone]]s, [[wireless intercom]] systems, [[Bluetooth]] devices and bank [[automatic teller machine]]s. There have been numerous reports of data in transit being intercepted in recent years.<ref>Fiber Optic Networks Vulnerable to Attack, Information Security Magazine, November 15, 2006, Sandra Kay Miller</ref> Encrypting data in transit also helps to secure it as it is often difficult to physically secure all access to networks.{{cn|date=April 2012}}  Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a [[message authentication code]] (MAC) or a [[digital signature]]. Standards and [[cryptographic software]] and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single slip-up in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See, e.g., [[traffic analysis]], [[TEMPEST]], or [[Trojan horse (computing)|Trojan horse]].{{cn|date=April 2012}}  One of the earliest [[public key encryption]] applications was called [[Pretty Good Privacy]] (PGP). It was written in 1991 by [[Phil Zimmermann]] and was purchased by [[Symantec]] in 2010.<ref>{{cite web|url=http://www.computerworld.com/s/article/9176121/Symantec_buys_encryption_specialist_PGP_for_300M |title=Symantec buys encryption specialist PGP for $300M |publisher=Computerworld |date=2010-04-29 |accessdate=2010-04-29}}</ref>  Digital signature and encryption must be applied at message creation time (i.e. on the same device it has been composed) to avoid tampering. Otherwise any node between the sender and the encryption agent could potentially tamper it.{{cn|date=April 2012}}  == See also == {{Portal|Cryptography}} * [[Brute-force attack]] * [[Cold boot attack]] * [[Cyberspace Electronic Security Act]] (in the US) * [[Disk encryption]] * [[Key management]]  ==Notes== {{refimprove|date=December 2011}} {{Reflist}}  ==References== * Helen Fouché Gaines, “Cryptanalysis”, 1939, Dover. ISBN 0-486-20097-3 * [[David Kahn (writer)|David Kahn]], ''The Codebreakers - The Story of Secret Writing'' (ISBN 0-684-83130-9) (1967) * [[Abraham Sinkov]], ''Elementary Cryptanalysis: A Mathematical Approach'', Mathematical Association of America, 1966. ISBN 0-88385-622-0  ==External links== {{Wiktionary|encryption}} *[http://www.securitydocs.com/Encryption SecurityDocs Resource for encryption whitepapers]  {{Cryptography navbox}}  [[Category:Cryptography]]  [[af:Versleuteling]] [[ar:تعمية]] [[bg:Шифроване]] [[bs:Enkripcija]] [[ca:Xifratge]] [[da:Kryptering]] [[de:Verschlüsselung]] [[es:Cifrado]] [[et:Šifreerimine]] [[eo:Ĉifrado]] [[fa:رمزگذاری]] [[fr:Chiffrement]] [[ko:암호화]] [[id:Enkripsi]] [[is:Dulkóðun]] [[it:Cifrario]] [[mk:Encryption]] [[ms:Penyulitan]] [[nl:Encryptie]] [[ja:暗号]] [[pt:Encriptação]] [[ru:Шифрование]] [[simple:Encryption]] [[sr:Enkripcija]] [[fi:Salauksen purku]] [[sv:Kryptering]] [[ta:மறையாக்கம்]] [[th:การเข้ารหัส]] [[uk:Шифрування]] [[ur:صفریت]] [[vi:Mã hóa]] [[zh:加密]]
{{About|a technical term in mathematics and computer science|related studies about natural languages|Grammar framework|formal modes of speech in natural languages|Register (sociolinguistics)}} In [[mathematics]], [[computer science]], and [[linguistics]], a '''formal language''' is a [[set (mathematics)|set]] of [[string (computer science)|strings]] of [[symbol (formal)|symbols]].  The [[Alphabet (computer science)|alphabet]] of a formal language is the set of symbols, letters, or tokens from which the strings of the language may be formed; frequently it is required to be  [[finite set|finite]]. The strings formed from this alphabet are called words, and the words that belong to a particular formal language are sometimes called ''well-formed words'' or ''[[well-formed formula]]s''. A formal language is often defined by means of a [[formal grammar]] such as a [[regular grammar]] or [[context-free grammar]], also called its [[formation rule]].  The field of '''formal language theory''' studies the purely [[syntax|syntactical]] aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of [[natural language]]s. In computer science, formal languages are often used as the basis for defining [[programming language]]s and other systems in which the words of the language are associated with particular meanings or [[semantics]]. In [[computational complexity theory]], [[decision problem]]s are typically defined as formal languages, and [[complexity class]]es are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In [[logic]] and the [[foundations of mathematics]], formal languages are used to represent the syntax of [[axiomatic system]]s, and [[Formalism (mathematics)|mathematical formalism]] is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.  == History == {{Expand section|date=April 2011}} The first formal language is thought be the one used by [[Gottlob Frege]] in his ''[[Begriffsschrift]]'' (1879), literally meaning "concept writing", and which Frege described as a "formal language of pure thought."<ref name="Herken1995">{{cite book|editor=Rolf Herken|title=The universal Turing machine: a half-century survey|url=http://books.google.com/books?id=YafIDVd1Z68C&pg=PA290|year=1995|publisher=Springer|isbn=978-3-211-82637-9|page=290|chapter=Influences of Mathematical Logic on Computer Science|author=Martin Davis}}</ref>  [[Axel Thue]]'s early [[Semi-Thue system]] which can be used for rewriting strings was influential on [[formal grammar]]s.  ==Words over an alphabet==  An '''alphabet''', in the context of formal languages, can be any [[set (mathematics)|set]], although it often makes sense to use an [[alphabet]] in the usual sense of the word, or more generally a [[character set]] such as [[ASCII]]. Alphabets can also be infinite; e.g. [[first-order logic]] is often expressed using an alphabet which, besides symbols such as ∧, ¬, ∀ and parentheses, contains infinitely many elements ''x''<sub>0</sub>,&nbsp;''x''<sub>1</sub>,&nbsp;''x''<sub>2</sub>,&nbsp;… that play the role of variables. The elements of an alphabet are called its '''letters'''.  A '''word''' over an alphabet can be any finite sequence, or [[string (computer science)|string]], of letters. The set of all words over an alphabet Σ is usually denoted by Σ<sup>*</sup> (using the [[Kleene star]]). For any alphabet there is only one word of length 0, the ''empty word'', which is often denoted by e, ε or λ. By [[concatenation]] one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.  In some applications, especially in [[logic]], the alphabet is also known as the ''vocabulary'' and words are known as ''formulas'' or ''sentences''; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.  ==Definition== A '''formal language''' ''L'' over an alphabet Σ is a [[subset]] of Σ<sup>*</sup>, that is, a set of [[#Words over an alphabet|words]] over that alphabet.  In computer science and mathematics, which do not usually deal with [[natural language]]s, the adjective "formal" is often omitted as redundant.  While formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept "formal language" is only as above: a (possibly infinite) set of finite-length strings, no more nor less.  In practice, there are many languages that can be described by rules, such as [[regular language]]s or [[context-free language]]s.  The notion of a [[formal grammar]] may be closer to the intuitive concept of a "language," one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.  ==Examples==  The following rules describe a formal language&nbsp;{{mvar|L}} over the alphabet Σ&nbsp;=&nbsp;{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  , = }: * Every nonempty string that does not contain " " or "=" and does not start with "0" is in&nbsp;{{mvar|L}}. * The string "0" is in&nbsp;{{mvar|L}}. * A string containing "=" is in&nbsp;{{mvar|L}} if and only if there is exactly one "=", and it separates two valid strings of&nbsp;{{mvar|L}}. * A string containing " " but not "=" is in&nbsp;{{mvar|L}} if and only if every " " in the string separates two valid strings of&nbsp;{{mvar|L}}. * No string is in&nbsp;{{mvar|L}} other than those implied by the previous rules. Under these rules, the string "23 4=555" is in&nbsp;{{mvar|L}}, but the string "=234= " is not. This formal language expresses [[natural number]]s, well-formed addition statements, and well-formed addition equalities, but it expresses only what they look like (their [[syntax]]), not what they mean ([[semantics]]). For instance, nowhere in these rules is there any indication that "0" means the number zero, or that " " means addition.  === Constructions ===<!-- [[empty language]] redirects here --> For finite languages one can explicitly enumerate all well-formed words. For example, we can describe a language&nbsp;{{mvar|L}} as just {{mvar|L}}&nbsp;=&nbsp;{"a", "b", "ab", "cba"}. The [[degeneracy (mathematics)|degenerate]] case of this construction is the '''empty language''', which contains no words at all (<span class="nounderlines">{{mvar|L}}&nbsp;=&nbsp;[[∅]]</span>).  However, even over a finite (non-empty) alphabet such as Σ&nbsp;=&nbsp;{a,&nbsp;b} there are infinitely many words: "a", "abb", "ababba", "aaababbbbaab",&nbsp;…. Therefore formal languages are typically infinite, and describing an infinite formal language is not as simple as writing ''L''&nbsp;=&nbsp;{"a", "b", "ab", "cba"}. Here are some examples of formal languages: * {{mvar|L}} = Σ<sup>*</sup>, the set of ''all'' words over Σ; * {{mvar|L}} = {"a"}<sup>*</sup> = {"a"<sup>''n''</sup>}, where ''n'' ranges over the natural numbers and "a"<sup>''n''</sup><!-- I know it is ugly, but it is consistent with the notation in other examples. Should we use through the article, say, angle brackets ‹a›? --> means "a" repeated ''n'' times (this is the set of words consisting only of the symbol "a"); * the set of syntactically correct programs in a given programming language (the syntax of which is usually defined by a [[context-free grammar]]); * the set of inputs upon which a certain [[Turing machine]] halts; or * the set of maximal strings of [[alphanumeric]] [[ASCII]] characters on this line, (i.e., the set {"the", "set", "of", "maximal", "strings", "alphanumeric", "ASCII", "characters", "on", "this", "line", "i", "e"}).  == Language-specification formalisms ==  Formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as * those strings generated by some [[formal grammar]]; * those strings described or matched by a particular [[regular expression]]; * those strings accepted by some [[Automata theory|automaton]], such as a [[Turing machine]] or [[Finite state machine|finite state automaton]]; * those strings for which some [[decision problem|decision procedure]] (an [[algorithm]] that asks a sequence of related YES/NO questions) produces the answer YES.  Typical questions asked about such formalisms include:  * What is their expressive power? (Can formalism ''X'' describe every language that formalism ''Y'' can describe? Can it describe other languages?) * What is their recognizability? (How difficult is it to decide whether a given word belongs to a language described by formalism ''X''?) * What is their comparability? (How difficult is it to decide whether two languages, one described in formalism ''X'' and one in formalism ''Y'', or in ''X'' again, are actually the same language?).  Surprisingly often, the answer to these decision problems is "it cannot be done at all", or "it is extremely expensive" (with a characterization of how expensive). Therefore, formal language theory is a major application area of [[Computability theory (computer science)|computability theory]] and [[computational complexity theory|complexity theory]]. Formal languages may be classified in the [[Chomsky hierarchy]] based on the expressive power of their generative grammar as well as the complexity of their recognizing [[automata theory|automaton]]. [[Context-free grammar]]s and [[regular grammar]]s provide a good compromise between expressivity and ease of [[parsing]], and are widely used in practical applications.  == Operations on languages ==  Certain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.  Examples: suppose ''L''<sub>1</sub> and ''L''<sub>2</sub> are languages over some common alphabet. * The ''[[concatenation]]'' ''L''<sub>1</sub>''L''<sub>2</sub> consists of all strings of the form ''vw'' where ''v'' is a string from ''L''<sub>1</sub> and ''w'' is a string from ''L''<sub>2</sub>. * The ''intersection'' ''L''<sub>1</sub>&nbsp;∩&nbsp;''L''<sub>2</sub> of ''L''<sub>1</sub> and ''L''<sub>2</sub> consists of all strings which are contained in both languages * The ''complement'' ¬''L'' of a language with respect to a given alphabet consists of all strings over the alphabet that are not in the language. * The [[Kleene star]]: the language consisting of all words that are concatenations of 0 or more words in the original language; * ''Reversal'': ** Let ''e'' be the empty word, then ''e<sup>R</sup>''&nbsp;=&nbsp;''e'', and ** for each non-empty word ''w''&nbsp;=&nbsp;''x''<sub>1</sub>…''x<sub>n</sub>'' over some alphabet, let ''w<sup>R</sup>''&nbsp;=&nbsp;''x''<sub>n</sub>…''x<sub>1</sub>'', ** then for a formal language ''L'', ''L<sup>R</sup>&nbsp;=&nbsp;{''w<sup>R</sup>&nbsp;|&nbsp;''w'' ∈ ''L''}. * [[String homomorphism]]  Such [[string operations]] are used to investigate [[Closure (mathematics)|closure properties]] of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the [[context-free language]]s are known to be closed under union, concatenation, and intersection with [[regular language]]s, but not closed under intersection or complement. The theory of [[cone (formal languages)|trios]] and [[abstract family of languages|abstract families of languages]] studies the most common closure properties of language families in their own right.<ref>{{harvtxt|Hopcroft|Ullman|1979}}, Chapter 11: Closure properties of families of languages.</ref>  :{| class="wikitable" |  align="top"|Closure properties of language families (<math>L_1</math> Op <math>L_2</math> where both <math>L_1</math> and <math>L_2</math> are in the language family given by the column). After Hopcroft and Ullman. |- ! Operation ! ! Regular ! [[Deterministic context-free language|DCFL]] ! [[Context free language|CFL]] ! [[Indexed language|IND]] ! [[Context sensitive language|CSL]] ! [[Recursive language|recursive]] ! [[Recursively enumerable language|RE]] |- |[[Union (set theory)|Union]] | <math>\{w | w \in L_1 \lor w \in L_2\} </math> | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |[[Intersection (set theory)|Intersection]] | <math>\{w | w \in L_1 \land w \in L_2\}</math> | {{Yes}} | {{No}} | {{No}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} |- |[[Complement (set theory)|Complement]] | <math>\{w | w \not\in L_1\}</math> | {{Yes}} | {{Yes}} | {{No}} | {{No}} | {{Yes}} | {{Yes}} | {{No}} |- |[[Concatenation]] | <math>L_1\cdot L_2 = \{w\cdot z | w \in L_1 \land z \in L_2\}</math> | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Kleene star | <math>L_1^{*} = \{\epsilon\} \cup \{w \cdot z | w \in L_1 \land z \in L_1^{*}\}</math> | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Homomorphism | | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{No}} | {{No}} | {{Yes}} |- |e-free Homomorphism | | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Substitution | | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{No}} | {{Yes}} |- |Inverse Homomorphism | | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Reverse | <math>\{w^R | w \in L\} </math> | {{Yes}} | {{No}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Intersection with a [[regular language]] | <math>\{w | w \in L_1 \land w \in R\}, R \text{ regular}</math> | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} | {{Yes}} <!-- |- |Min | | {{Yes}} | {{Yes}} | {{No}} | {{?}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Max | | {{Yes}} | {{Yes}} | {{No}} | {{?}} | {{No}} | {{No}} | {{No}} |- |Init | | {{Yes}} | {{No}} | {{Yes}} | {{?}} | {{No}} | {{No}} | {{Yes}} |- |Cycle | | {{Yes}} | {{No}} | {{Yes}} | {{?}} | {{Yes}} | {{Yes}} | {{Yes}} |- |Shuffle | | {{Yes}} | {{?}} | {{Yes}} | {{?}} | {{?}} | {{?}} | {{Yes}} |- |Perfect Shuffle | | {{Yes}} | {{?}} | {{?}} | {{?}} | {{?}} | {{?}} | {{Yes}} --> |}  == Applications == === Programming languages === {{Main|Syntax (programming languages)|Compiler compiler}}  A compiler usually has two distinct components. A [[lexical analyzer]], generated by a tool like [[lex programming tool|<code>lex</code>]], identifies the tokens of the programming language grammar, e.g. [[identifier]]s or [[Keyword (computer programming)|keyword]]s, which are themselves expressed in a simpler formal language, usually by means of [[regular expressions]]. At the most basic conceptual level, a [[parser]], usually generated by a [[parser generator]] like <code>[[yacc]]</code>, attempts to decide if the source program is valid, that is if it belongs to the programming language for which the compiler was built. Of course, compilers do more than just parse the source code—they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an [[abstract syntax tree]], which is used by subsequent stages of the compiler to eventually generate an [[executable]] containing [[machine code]] that runs directly on the hardware, or some [[intermediate code]] that requires a [[virtual machine]] to execute.  === Formal theories, systems and proofs === [[File:Formal languages.png|thumb|300px|right|This diagram shows the [[Syntax (logic)|syntactic]] divisions within a [[formal system]]. [[string (computer science)|Strings of symbols]] may be broadly divided into nonsense and [[well-formed formula]]s.  The set of well-formed formulas is divided into [[theorem]]s and non-theorems.]]  {{Main|Theory (mathematical logic)|Formal system}}  In [[mathematical logic]], a ''formal theory'' is a set of [[sentence (mathematical logic)|sentence]]s expressed in a formal language.  A ''formal system'' (also called a ''logical calculus'', or a ''logical system'') consists of a formal language together with a [[deductive apparatus]] (also called a ''deductive system''). The deductive apparatus may consist of a set of [[transformation rule]]s which may be interpreted as valid rules of inference or a set of [[axiom]]s, or have both. A formal system is used to [[Proof theory|derive]] one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems <math>\mathcal{FS}</math> and <math>\mathcal{FS'}</math> may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).  A ''formal proof'' or ''derivation'' is a finite sequence of well-formed formulas (which may be interpreted as [[proposition]]s) each of which is an axiom or follows from the preceding formulas in the sequence by a [[rule of inference]]. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.  ====Interpretations and models==== {{main|Formal semantics (logic)||Interpretation (logic)|Model theory}}  Formal languages are entirely syntactic in nature but may be given [[semantics]] that give meaning to the elements of the language. For instance, in mathematical [[logic]], the set of possible formulas of a particular logic is a formal language, and an [[interpretation (logic)|interpretation]] assigns a meaning to each of the formulas—usually, a [[truth value]].  The study of interpretations of formal languages is called [[Formal semantics (logic)|formal semantics]].  In mathematical logic, this is often done in terms of [[model theory]].  In model theory, the terms that occur in a formula are interpreted as [[Structure (mathematical logic)|mathematical structures]], and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a ''model'' for a formula is an interpretation of terms such that the formula becomes true.  == See also == * [[Combinatorics on words]] * [[Grammar framework]] * [[Formal method]] * [[Mathematical notation]]  == References == ===Notes=== {{Reflist}}  ===General references=== {{Refbegin}} * A. G. Hamilton, ''Logic for Mathematicians'', Cambridge University Press, 1978, ISBN 0-521-21838-1. * [[Seymour Ginsburg]], ''Algebraic and automata theoretic properties of formal languages'', North-Holland, 1975, ISBN 0-7204-2506-9. * Michael A. Harrison, ''Introduction to Formal Language Theory'', Addison-Wesley, 1978. * [[John Hopcroft|John E. Hopcroft]] and [[Jeffrey Ullman|Jeffrey D. Ullman]], ''[[Introduction to Automata Theory, Languages, and Computation]]'', Addison-Wesley Publishing, Reading Massachusetts, 1979. ISBN 81-7808-347-7. * {{Citation|last=Rautenberg|first=Wolfgang|authorlink=Wolfgang Rautenberg|doi=10.1007/978-1-4419-1221-3|title=A Concise Introduction to Mathematical Logic|url=http://www.springerlink.com/content/978-1-4419-1220-6/|publisher=[[Springer Science Business Media]]|location=[[New York City|New York]]|edition=3rd|isbn=978-1-4419-1220-6|year=2010}}. * [[Grzegorz Rozenberg]], [[Arto Salomaa]], ''Handbook of Formal Languages: Volume I-III'', Springer, 1997, ISBN 3-540-61486-9. * Patrick Suppes, ''Introduction to Logic'', D. Van Nostrand, 1957, ISBN 0-442-08072-7. {{Refend}}  == External links ==  * {{planetmath reference|id=1681|title=Alphabet}} * {{planetmath reference|id=1767|title=Language}} * [[University of Maryland, Baltimore|University of Maryland]], [http://www.csee.umbc.edu/help/theory/lang_def.shtml Formal Language Definitions] * James Power, [http://www.cs.nuim.ie/~jpower/Courses/parsing/ "Notes on Formal Language Theory and Parsing"], 29 November 2002.  * Drafts of some chapters in the "Handbook of Formal Language Theory", Vol. 1-3, G. Rozenberg and A. Salomaa (eds.), Springer Verlag, (1997):t ** Alexandru Mateescu and Arto Salomaa, [http://www.cs.cmu.edu/~lkontor/noam/Mateescu-Salomaa.pdf "Preface" in Vol.1, pp. v-viii, and "Formal Languages: An Introduction and a Synopsis", Chapter 1 in Vol. 1, pp.1-39] ** Sheng Yu, [http://www.csd.uwo.ca/~syu/public/draft.ps "Regular Languages", Chapter 2 in Vol. 1] ** Jean-Michel Autebert, Jean Berstel, Luc Boasson, [http://citeseer.ist.psu.edu/248295.html "Context-Free Languages and Push-Down Automata", Chapter 3 in Vol. 1] ** Christian Choffrut and Juhani Karhumäki, [http://www.liafa.jussieu.fr/~cc/PUBLICATIONS/CKTUCS.PS.gz "Combinatorics of Words", Chapter 6 in Vol. 1] ** Tero Harju and Juhani Karhumäki, [http://users.utu.fi/harju/articles/morph.pdf "Morphisms", Chapter 7 in Vol. 1, pp. 439 - 510] ** Jean-Eric Pin, [http://www.liafa.jussieu.fr/~jep/PDF/HandBook.pdf "Syntactic semigroups", Chapter 10 in Vol. 1, pp. 679-746] ** M. Crochemore and C. Hancart, [http://www-igm.univ-mlv.fr/~mac/REC/DOC/B4.ps "Automata for matching patterns", Chapter 9 in Vol. 2] ** Dora Giammarresi, Antonio Restivo, [http://bruno.maitresdumonde.com/optinfo/Spe-MP/dmds1998/2dlang/giammaresi-restivo-paper.ps "Two-dimensional Languages", Chapter 4 in Vol. 3, pp. 215 - 267]  {{Formal languages and grammars}} {{logic}}  {{Use dmy dates|date=October 2010}}  {{DEFAULTSORT:Formal Language}} [[Category:Formal languages]] [[Category:Theoretical computer science]] [[Category:Combinatorics on words]]  [[ar:لغة شكلية]] [[bg:Формален език]] [[bs:Formalni jezik]] [[ca:Llenguatge formal]] [[cs:Formální jazyk]] [[da:Formelt sprog]] [[de:Formale Sprache]] [[el:Τυπική γλώσσα]] [[es:Lenguaje formal]] [[fa:زبان صوری]] [[fr:Langage formel]] [[ko:형식 언어]] [[hi:औपचारिक भाषा]] [[hr:Formalni jezik]] [[it:Linguaggio formale (matematica)]] [[he:שפה פורמלית]] [[lt:Formali kalba]] [[hu:Formális nyelv]] [[mk:Формален јазик]] [[nl:Formele taal]] [[ja:形式言語]] [[no:Formelt språk]] [[pl:Język formalny]] [[pt:Linguagem formal]] [[ro:Limbaje formale]] [[ru:Формальный язык]] [[simple:Formal language]] [[sk:Formálny jazyk]] [[ckb:زمانی شێوەیی]] [[sr:Формални језик]] [[sh:Formalni jezik]] [[fi:Formaali kieli]] [[sv:Formella språk]] [[tr:Biçimsel dil kuramı]] [[uk:Формальна мова]] [[vi:Ngôn ngữ hình thức]] [[zh:形式语言]]
[[File:Agendacumple en Z.jpg|thumb|upright|An example [[formal specification]] using the [[Z notation]].]]  In [[computer science]], specifically [[software engineering]], '''formal methods''' are a particular kind of [[mathematically]] based techniques for the [[formal specification|specification]], development and [[formal verification|verification]] of [[software]] and [[computer hardware|hardware]] systems.<ref name="butler">{{cite web|author=R. W. Butler|title=What is Formal Methods?|url=http://shemesh.larc.nasa.gov/fm/fm-what.html|date=2001-08-06|accessdate=2006-11-16}}</ref> The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.<ref>{{cite journal|author=C. Michael Holloway|title=Why Engineers Should Consider Formal Methods|url=http://klabs.org/richcontent/verification/holloway/nasa-97-16dasc-cmh.pdf| publisher=16th Digital Avionics Systems Conference (27–30 October 1997)|accessdate=2006-11-16}}</ref>   Formal methods are best described as the application of a fairly broad variety of [[theoretical computer science]] fundamentals, in particular [[logic in computer science|logic]] calculi, [[formal language]]s, [[automata theory]], and [[program semantics]], but also [[type systems]] and [[algebraic data types]] to problems in software and hardware specification and verification.<ref>Monin, pp.3-4</ref>  ==Taxonomy==  Formal methods can be used at a number of levels:  '''Level 0:''' [[Formal specification]] may be undertaken and then a program developed from this informally. This has been dubbed ''formal methods lite''. This may be the most cost-effective option in many cases.  '''Level 1:''' [[Formal development]] and [[formal verification]] may be used to produce a program in a more formal manner. For example, proofs of properties or [[program refinement|refinement]] from the [[formal specification|specification]] to a program may be undertaken. This may be most appropriate in high-integrity systems involving [[safety]] or [[security]].  '''Level 2:''' [[Theorem prover]]s may be used to undertake fully formal machine-checked proofs. This can be very expensive and is only practically worthwhile if the cost of mistakes is extremely high (e.g., in critical parts of microprocessor design).  Further information on this is expanded [[#Uses|below]].  As with [[Formal semantics of programming languages|programming language semantics]], styles of formal methods may be roughly classified as follows:  * [[Denotational semantics]], in which the meaning of a system is expressed in the mathematical theory of [[domain theory|domains]].  Proponents of such methods rely on the well-understood nature of domains to give meaning to the system; critics point out that not every system may be intuitively or naturally viewed as a function. * [[Operational semantics]], in which the meaning of a system is expressed as a sequence of actions of a (presumably) simpler computational model.  Proponents of such methods point to the simplicity of their models as a means to expressive clarity; critics counter that the problem of semantics has just been delayed (who defines the semantics of the simpler model?). * [[Axiomatic semantics]], in which the meaning of the system is expressed in terms of [[precondition]]s and [[postcondition]]s which are true before and after the system performs a task, respectively.  Proponents note the connection to classical [[logic]]; critics note that such semantics never really describe what a system ''does'' (merely what is true before and afterwards).  ===Lightweight formal methods===  Some practitioners believe that the formal methods community has overemphasized full formalization of a specification or design.<ref>[[Daniel Jackson (computer scientist)|Daniel Jackson]] and [[Jeannette Wing]], [http://people.csail.mit.edu/dnj/publications/ieee96-roundtable.html "Lightweight Formal Methods"], ''IEEE Computer'', April 1996</ref><ref>Vinu George and Rayford Vaughn, [http://www.stsc.hill.af.mil/crosstalk/2003/01/George.html "Application of Lightweight Formal Methods in Requirement Engineering"], ''Crosstalk: The Journal of Defense Software Engineering'', January 2003</ref> They contend that the expressiveness of the languages involved, as well as the complexity of the systems being modelled, make full formalization a difficult and expensive task. As an alternative, various ''lightweight'' formal methods, which emphasize partial specification and focused application, have been proposed. Examples of this lightweight approach to formal methods include the [[Alloy language|Alloy]] object modelling notation,<ref>Daniel Jackson, [http://people.csail.mit.edu/dnj/publications/alloy-journal.pdf "Alloy: A Lightweight Object Modelling Notation"], ''ACM Transactions on Software Engineering and Methodology (TOSEM)'', Volume 11, Issue 2 (April 2002), pp. 256-290</ref> Denney's synthesis of some aspects of the [[Z notation]] with [[use case]] driven development,<ref>Richard Denney, ''Succeeding with Use Cases: Working Smart to Deliver Quality'', Addison-Wesley Professional Publishing, 2005, ISBN 0-321-31643-6.</ref> and the CSK [[Vienna Development Method|VDM]] Tools.<ref>Sten Agerholm and Peter G. Larsen, [http://home0.inet.tele.dk/pgl/fmtrends98.pdf "A Lightweight Approach to Formal Methods"], In ''Proceedings of the International Workshop on Current Trends in Applied Formal Methods'', Boppard, Germany, Springer-Verlag, October 1998</ref>  ==Uses==  Formal methods can be applied at various points through the [[software development process|development process]].  ===Specification=== Formal methods may be used to give a description of the system to be developed, at whatever level(s) of detail desired.  This formal description can be used to guide further development activities (see following sections); additionally, it can be used to verify that the requirements for the system being developed have been completely and accurately specified.  The need for formal specification systems has been noted for years.  In the [[ALGOL 58]] report,<ref>{{cite conference | first = J.W.  | last = Backus | title = The Syntax and Semantics of the Proposed International Algebraic Language of Zürich ACM-GAMM Conference | booktitle = Proceedings of the International Conference on Information Processing | publisher = UNESCO | year = 1959 }}</ref> [[John Backus]] presented a formal notation for describing programming language syntax (later named [[Backus Normal Form]] then renamed [[Backus-Naur Form]] (BNF)<ref>[[Donald Knuth|Knuth, Donald E.]] (1964), Backus Normal Form vs Backus Naur Form. ''[[Communications of the ACM]]'', 7(12):735–736.</ref>). Backus also wrote that a formal description of the meaning of syntactically valid ALGOL programs wasn't completed in time for inclusion in the report. "Therefore the formal treatment of the semantics of legal programs will be included in a subsequent paper." It never appeared.  ===Development=== Once a formal specification has been produced, the specification may be used as a guide while the concrete system is [[Software development|developed]] during the [[Software design|design]] process (i.e., realized typically in software, but also potentially in hardware). For example:  * If the formal specification is in an operational semantics, the observed behavior of the concrete system can be compared with the behavior of the specification (which itself should be executable or simulateable). Additionally, the operational commands of the specification may be amenable to direct translation into executable code. * If the formal specification is in an axiomatic semantics, the preconditions and postconditions of the specification may become [[assertion (computing)|assertion]]s in the executable code.  ===Verification===  Once a formal specification has been developed, the specification may be used as the basis for [[mathematical proof|proving]] properties of the specification (and hopefully by inference the developed system).  ====Human-directed proof==== Sometimes, the motivation for proving the correctness of a system is not the obvious need for re-assurance of the correctness of the system, but a desire to understand the system better.  Consequently, some proofs of correctness are produced in the style of [[mathematical proof]]: handwritten (or typeset) using [[natural language]], using a level of informality common to such proofs.  A "good" proof is one which is readable and understandable by other human readers.  Critics of such approaches point out that the [[ambiguity]] inherent in natural language allows errors to be undetected in such proofs; often, subtle errors can be present in the low-level details typically overlooked by such proofs.  Additionally, the work involved in producing such a good proof requires a high level of mathematical sophistication and expertise.  ====Automated proof==== In contrast, there is increasing interest in producing proofs of correctness of such systems by automated means.  Automated techniques fall into two general categories: * [[Automated theorem proving]], in which a system attempts to produce a formal proof from scratch, given a description of the system, a set of logical axioms, and a set of inference rules. * [[Model checking]], in which a system verifies certain properties by means of an exhaustive search of all possible states that a system could enter during its execution.  Some automated theorem provers require guidance as to which properties are "interesting" enough to pursue, while others work without human intervention. Model checkers can quickly get bogged down in checking millions of uninteresting states if not given a sufficiently abstract model.  Proponents of such systems argue that the results have greater mathematical certainty than human-produced proofs, since all the tedious details have been algorithmically verified.  The training required to use such systems is also less than that required to produce good mathematical proofs by hand, making the techniques accessible to a wider variety of practitioners.  Critics note that some of those systems are like [[Oracle machine|oracle]]s: they make a pronouncement of truth, yet give no explanation of that truth.  There is also the problem of "[[Quis custodiet ipsos custodes?|verifying the verifier]]"; if the program which aids in the verification is itself unproven, there may be reason to doubt the soundness of the produced results. Some modern model checking tools produce a "proof log" detailing each step in their proof, making it possible to perform, given suitable tools, independent verification.  ==Criticisms== The field of formal methods has its critics.{{Citation needed|date=January 2012}} Handwritten proofs of correctness need significant time (and thus money) to produce, with limited utility other than assuring correctness. This makes formal methods more likely to be used in fields where it is possible to perform automated proofs using software, or in cases where the cost of a fault is high. Example: in railway engineering and [[aerospace engineering]], undetected errors may cause [[death]], so formal methods are more popular in this field than in other application areas.  ==Formal methods and notations== {{Prose|section|date=August 2009}} There are a variety of formal methods and notations available.  ;Specification languages * [[Abstract State Machines]] (ASMs) * [[ANSI/ISO C Specification Language]] (ACSL) * [[Alloy language|Alloy]] * [[B-Method]] * [[CADP]] * [[Common Algebraic Specification Language]] (CASL) * [[Process calculi]] ** [[Communicating Sequential Processes|CSP]] ** [[Language Of Temporal Ordering Specification|LOTOS]] ** [[Pi-calculus|π-calculus]] * [[Actor model]] * [[Esterel]] * [[Lustre programming language|Lustre]] * [[mCRL2]] * [[Perfect Developer]] * [[Petri nets]] * [[RAISE specification language|RAISE]] * [[SPARK (programming language)|SPARK Ada]] * [[Specification and Description Language]] * [[Temporal logic of actions]] (TLA) * [[Universal Systems Language|USL]] * [[Vienna Development Method|VDM]] ** [[VDM specification language|VDM-SL]] ** VDM   * [[Z notation]] * [[Rebeca Modeling Language]]  ;Model checkers * [[SPIN model checker|SPIN]] * [http://www.comp.nus.edu.sg/~pat/ PAT] is a powerful free model checker, simulator and refinement checker for concurrent systems and CSP extensions (e.g. shared variables, arrays, fairness). * [[MALPAS Software Static Analysis Toolset]] is an industrial strength model checker used for Formal Proof of safety critical systems  ==See also== * [[Automated theorem proving]] * [[Design by contract]] * [[:Category:Formal methods people|Formal methods people]] * [[Formal specification]] * [[Formal verification]] * [[Formal system]] * [[Model checking]] * [[Software engineering]] * [[:Category:Software engineering disasters|Software engineering disasters]] * [[Specification language]]  ==References== {{reflist}} {{FOLDOC}}  ==Further reading== {{refbegin}} * Jean François Monin and [[Michael G. Hinchey]], ''Understanding formal methods'', [[Springer-Verlag|Springer]], 2003, ISBN 1-85233-247-6. * [[Jonathan P. Bowen]] and  Michael G. Hinchey, ''Formal Methods''. In Allen B. Tucker, Jr. (ed.), ''Computer Science Handbook'', 2nd edition, Section XI, ''Software Engineering'',Chapter 106, pages 106-1 – 106-25, [[Chapman & Hall]] / [[CRC Press]], [[Association for Computing Machinery]], 2004. * Michael G. Hinchey, Jonathan P. Bowen, and Emil Vassev, ''Formal Methods''. In Philip A. Laplante (ed.), ''Encyclopedia of Software Engineering'', [[Taylor & Francis]], 2010, pages 308–320. {{refend}}  ==External links== * [http://formalmethods.wikia.com/wiki/Formal_Methods_Wiki Formal Methods Wiki] ** [http://formalmethods.wikia.com/wiki/VL List of formal methods and tools] ** [http://formalmethods.wikia.com/wiki/pubs Formal methods publications] ** [http://formalmethods.wikia.com/wiki/whos-who Who's who in formal methods] * [http://www.fmeurope.org/ Formal Methods Europe (FME)] * [http://academic.research.microsoft.com/Keyword/14916  Formal method] keyword on [[Microsoft Academic Search]] * [http://foldoc.org/formal methods Foldoc:formal methods] * [http://www.fm4industry.org Evidence on Formal Methods uses and impact on Industry] Supported by the [http://www.deploy-project.eu DEPLOY] project (EU FP7)  {{Computer science}} {{Software engineering}}  {{DEFAULTSORT:Formal Methods}} [[Category:Formal methods| ]] [[Category:Software development philosophies]] [[Category:Theoretical computer science]]  [[de:Formale Methode]] [[es:Método formal]] [[fa:روش‌های صوری]] [[fr:Méthode formelle (informatique)]] [[id:Metoda formal]] [[lt:Formalūs metodai]] [[ms:Kaedah formal]] [[nl:Formele methoden]] [[ja:形式手法]] [[pl:Metody formalne]] [[pt:Métodos formais]] [[ru:Формальные методы]] [[uk:Формальні методи]] [[vi:Các phương pháp hình thức]] [[zh:形式化方法]]
{{history of computing}}  {{Cleanup|date=March 2012}} The '''history of computer science''' began long before the modern discipline of [[computer science]] that emerged in the 20th century, and hinted at in the centuries prior. The progression, from mechanical inventions and [[mathematics|mathematical]] theories towards the modern concepts and machines, formed a major academic field and the basis of a massive worldwide industry.<ref>[http://www.cs.uwaterloo.ca/~shallit/Courses/134/history.html History of Computer Science]</ref>  ==Early history== The earliest known tool for use in computation was the [[abacus]], developed in period 2700–2300 BC in [[Sumer]]. The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their [[sexagesimal]] number system.<ref>{{Harvcolnb|Ifrah|2001|p=11}}</ref> Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today.  The [[Antikythera mechanism]] is believed to be the earliest known mechanical analog computer.<ref>[http://www.antikythera-mechanism.gr/project/general/the-project.html ''The Antikythera Mechanism Research Project''], The Antikythera Mechanism Research Project. Retrieved 2007-07-01</ref> It was designed to calculate astronomical positions. It was discovered in 1901 in the [[Antikythera]] wreck off the [[Greece|Greek]] island of Antikythera, between Kythera and Crete, and has been dated to c. 100 BC. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical [[astronomical clock]]s appeared in [[Europe]].<ref name=insearchoflosttime>In search of lost time, Jo Marchant, ''Nature'' '''444''', #7119 (November 30, 2006), pp. 534–538, {{doi|10.1038/444534a}} PMID 17136067.</ref>  Mechanical analog computing devices appeared a thousand years later in the [[Islamic Golden Age|medieval Islamic world]]. Examples of devices from this period include the [[equatorium]] by [[Abū Ishāq Ibrāhīm al-Zarqālī|Arzachel]],<ref>{{Cite web|last=Hassan |first=Ahmad Y. |authorlink=Ahmad Y Hassan |url=http://www.history-science-technology.com/Articles/articles%2071.htm |title=Transfer Of Islamic Technology To The West, Part II: Transmission Of Islamic Engineering |accessdate=2008-01-22 |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->}}</ref> the mechanical geared [[astrolabe]] by [[Abū Rayhān al-Bīrūnī]],<ref name=usc>{{Cite web|url=http://www.usc.edu/dept/MSA/introduction/woi_knowledge.html|title=Islam, Knowledge, and Science|publisher=[[University of Southern California]]|accessdate=2008-01-22}}</ref> and the [[torquetum]] by [[Jabir ibn Aflah]].<ref>{{Cite journal|first=R. P.|last=Lorch|title=The Astronomical Instruments of Jabir ibn Aflah and the Torquetum|journal=[[Centaurus (journal)|Centaurus]]|volume=20|issue=1|year=1976|pages=11–34|doi=10.1111/j.1600-0498.1976.tb00214.x|bibcode=1976Cent...20...11L}}</ref> Muslim engineers built a number of [[Automaton|automata]], including some musical automata that could be 'programmed' to play different musical patterns. These devices were developed by the [[Banū Mūsā]] brothers<ref name=Koetsier>{{cite journal |last1=Koetsier |first1=Teun |year=2001 |title=On the prehistory of programmable machines: musical automata, looms, calculators |journal=Mechanism and Machine Theory |volume=36 |issue=5 |pages=589–603 |publisher=Elsevier |doi=10.1016/S0094-114X(01)00005-2}}</ref> and [[Al-Jazari]]<ref>[http://www.shef.ac.uk/marcoms/eview/articles58/robot.html A 13th Century Programmable Robot], [[University of Sheffield]]</ref> [[Islamic mathematics|Muslim mathematicians]] also made important advances in [[cryptography]], such as the development of [[cryptanalysis]] and [[frequency analysis]] by [[Al-Kindi|Alkindus]].<ref>Simon Singh, ''The Code Book'', pp. 14-20</ref>  When [[John Napier]] discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 [[Wilhelm Schickard]] designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624. Around 1640, [[Blaise Pascal]], a leading French mathematician, constructed the first mechanical adding device<ref>[http://www.knobblycrab.co.uk/computer-history.html Short history of the computer]</ref> based on a design described by [[Ancient Greece|Greek]] mathematician [[Hero of Alexandria]].<ref>[http://lecture.eingang.org/pascaline.html History of Computing Science: The First Mechanical Calculator]</ref> Then in 1672 [[Gottfried Wilhelm Leibniz]] invented the [[Stepped Reckoner]] which he completed in 1694.<ref name="Meyer1925">{{Cite book |last=Kidwell |first=Peggy Aldritch |coauthors=Williams, Michael R. |title=The Calculating Machines: Their history and development |year=1992 |publisher=Massachusetts Institute of Technology and Tomash Publishers |location=USA |url=http://www.rechenmaschinen-illustrated.com/Martins_book/Ernst%20Martin%20-%20Rechen%20Machinen%20OCR%204.pdf}}, p.38-42, translated and edited from {{Cite book |last=Martin |first=Ernst |title=Die Rechenmaschinen und ihre Entwicklungsgeschichte |year=1925 |publisher=Pappenheim |location=Germany}}</ref>  In 1837 [[Charles Babbage]] first described his [[Analytical Engine]] which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be [[Turing complete]]. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.  Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.  ===Binary logic=== In 1703, [[Gottfried Leibnitz]] developed [[logic]] in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent ''true'' and ''false'' values or ''on'' and ''off'' states. But it took more than a century before [[George Boole]] published his [[Boolean algebra (logic)|Boolean algebra]] in 1854 with a complete system that allowed computational processes to be mathematically modeled.  By this time, the first mechanical devices driven by a binary pattern had been invented. The [[industrial revolution]] had driven forward the mechanization of many tasks, and this included [[weaving]]. [[Punched cards]] controlled [[Joseph Marie Jacquard]]'s loom in 1801, where a hole punched in the card indicated a binary ''one'' and an unpunched spot indicated a binary ''zero''. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.  ===Birth of computer science=== Before the 1920s, ''computers'' (sometimes ''computors'') were  human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Most of these computers were women, and they were known to have a degree in calculus. Some performed astronomical calculations for calendars.  After the 1920s, the expression ''computing machine'' referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the [[Church-Turing thesis]]. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.  Machines that computed with continuous values became known as the ''analog'' kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.  Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.  The phrase ''computing machine'' gradually gave away, after the late 1940s, to just ''computer'' as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.  Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described "purely mechanical." The theoretical [[Turing Machine]], created by [[Alan Turing]], is a hypothetical device theorized in order to study the properties of such hardware.  {{See also|Philosophy of physics|Philosophy of biology|Philosophy of mathematics|Philosophy of language|Philosophy of mind}}  ==Emergence of a discipline== The mathematical foundations of modern computer science began to be laid by [[Kurt Gödel]] with his [[Gödel's incompleteness theorems|incompleteness theorem]] (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a [[formal system]]. This led to work by Gödel and others to define and describe these formal systems, including concepts such as [[mu-recursive function]]s and [[lambda-definable functions]].  1936 was a key year for computer science. Alan Turing and [[Alonzo Church]] independently, and also together, introduced the formalization of an [[algorithm]], with limits on what can be computed, and a "purely mechanical" model for computing.  These topics are covered by what is now called the [[Church–Turing thesis]], a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis claims that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.  Turing also included with the thesis a description of the [[Turing machine]]. A Turing machine has an infinitely long tape and a read/write head that can move along the tape, changing the values along the way. Clearly such a machine could never be built, but nonetheless, the model can simulate the computation of any algorithm which can be performed on a modern computer.  Turing is so important to computer science that his name is also featured on the [[Turing Award]] and the [[Turing test]]. He contributed greatly to British code-breaking successes in the [[Second World War]], and continued to design computers and software through the 1940s, but committed suicide  in 1954.  At a symposium on large-scale digital machinery in Cambridge, Turing said, "We are trying to build a machine to do all kinds of different things simply by programming rather than by the addition of extra apparatus".  In 1941, [[Konrad Zuse]] developed the world's first functional program-controlled [[Turing completeness|Turing-complete]] computer, the [[Z3 (computer)|Z3]].  Zuse was also noted for the S2 computing machine, considered the first process-controlled computer.  He founded one of the earliest computer businesses in 1941, producing the [[Z4 (computer)|Z4]], which became the world's first commercial computer.  In 1946, he designed the first [[high-level programming language|high-level]] [[programming language]], [[Plankalkül]].<ref name="HZ2010-11-18">Talk given by [[Horst Zuse]] to the [[Computer Conservation Society]] at the [[Science Museum (London)]] on 18 November 2010</ref>   In 1969, Zuse suggested the concept of a [[digital physics|computation-based universe]] in his book ''Rechnender Raum'' (''[[Calculating Space]]'').  In 1948, the first practical computer that could run stored programs, based on the Turing machine model, had been built - the [[Manchester Small-Scale Experimental Machine|Manchester Baby]].  In 1950, Britain's [[National Physical Laboratory, UK|National Physical Laboratory]] completed [[Pilot ACE]], a small scale programmable computer, based on Turing's philosophy.  ===Shannon and information theory=== Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ''ad hoc'' manner, lacking any theoretical rigor.  This changed with [[Claude Shannon|Claude Elwood Shannon]]'s publication of his 1937 master's thesis, [[A Symbolic Analysis of Relay and Switching Circuits]].  While taking an undergraduate philosophy class, Shannon had been exposed to [[George Boole|Boole's]] work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems.  This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers, and his thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.  Shannon went on to found the field of [[information theory]] with his 1948 paper titled [[A Mathematical Theory of Communication]], which applied [[probability theory]] to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including [[data compression]] and [[cryptography]].  ===Wiener and Cybernetics=== From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, [[Norbert Wiener]] coined the term [[cybernetics]] from the Greek word for "steersman." He published "Cybernetics" in 1948, which influenced [[artificial intelligence]]. Wiener also compared [[computation]], computing machinery, [[computer memory|memory]] devices, and other cognitive similarities with his analysis of brain waves.  The first actual computer bug was a [[moth]]. It was stuck in between the relays on the Harvard Mark II.[http://www.history.navy.mil/photos/images/h96000/h96566kc.htm] While the invention of the term 'bug' is often but erroneously attributed to [[Grace Hopper]], a future rear admiral in the U.S. Navy, who supposedly logged the "bug" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' &mdash; along with the insect and the notation "First actual case of bug being found" (see [[software bug]] for details).  ==See also== * [[History of computing]] * [[History of computing hardware]] * [[Timeline of algorithms]] * [[List of prominent pioneers in computer science]] * [[List of computer term etymologies]], the origins of computer science words * [[Computer History Museum]]  ==Notes== {{Reflist}}  ==Sources== {{Citation  | last = Ifrah  | first = Georges  | year = 2001  | title = The Universal History of Computing: From the Abacus to the Quantum Computer  | publication-place = New York  | publisher=John Wiley & Sons  | isbn = 0-471-39671-0}}  == Further reading == * [http://www.cs.uwaterloo.ca/~shallit/Courses/134/history.html  A Very Brief History of Computer Science] * [http://www.computerhistory.org/ Computer History Museum] * [http://www.eingang.org/Lecture/ Computers: From the Past to the Present] * [http://www.history.navy.mil/photos/images/h96000/h96566kc.htm The First "Computer Bug"] at the Online Library of the Naval Historical Center, retrieved February 28, 2006 * [http://www.bitsavers.org/ Bitsavers], an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s * Gordana Dodig-Crnkovic. "[http://www.mrtc.mdh.se/publications/0337.pdf History of Computer Science]". [[Mälardalen University]]. * [[Matti Tedre]] (2006). ''[ftp://cs.joensuu.fi/pub/Dissertations/tedre.pdf The Development of Computer Science: A Sociocultural Perspective]''. Doctoral thesis for University of Joensuu.  ==External links== *[http://purl.umn.edu/107140 Oral history interview with Albert H. Bowker] at [[Charles Babbage Institute]], University of Minnesota.  Bowker discusses his role in the formation of the Stanford University computer science department, and his vision, as early as 1956, of computer science as an academic discipline. *[http://purl.umn.edu/107684 Oral history interview with Joseph F. Traub] at [[Charles Babbage Institute]], University of Minnesota.  Traub discusses why computer science has developed as a discipline at institutions including Stanford, Berkeley, University of Pennsylvania, MIT, and Carnegie-Mellon. *[http://purl.umn.edu/107334 Oral history interview with Gene H. Golub] at [[Charles Babbage Institute]], University of Minnesota.  Golub discusses his career in computer science at Stanford University. *[http://purl.umn.edu/107356 Oral history interview with John Herriot] at [[Charles Babbage Institute]], University of Minnesota.  Herriot describes the early years of computing at Stanford University, including formation of the computer science department, centering on the role of [[George Forsythe]]. *[http://purl.umn.edu/107502 Oral history interview with William F. Miller] at [[Charles Babbage Institute]], University of Minnesota.  Miller contrasts the emergence of computer science at Stanford with developments at Harvard and the University of Pennsylvania. *[http://purl.umn.edu/107291 Oral history interview with Alexandra Forsythe] at [[Charles Babbage Institute]], University of Minnesota.  Forsythe discusses the career of her husband, [[George Forsythe]], who established Stanford University's program in computer science. *[http://purl.umn.edu/107544 Oral history interview with Allen Newell] at [[Charles Babbage Institute]], University of Minnesota.  Newell discusses his entry into computer science, funding for computer science departments and research, the development of the Computer Science Department at Carnegie Mellon University, including the work of [[Alan Perlis|Alan J. Perlis]] and [[Raj Reddy]], and the growth of the computer science and artificial intelligence research communities.  Compares computer science programs at Stanford, MIT, and Carnegie Mellon. *[http://purl.umn.edu/107284 Oral history interview with Louis Fein] at [[Charles Babbage Institute]], University of Minnesota.  Fein discusses establishing computer science as an academic discipline at [[Stanford Research Institute]] (SRI) as well as contacts with the University of California—Berkeley, the University of North Carolina, Purdue, [[International Federation for Information Processing]] and other institutions. *[http://purl.umn.edu/104300 Oral history interview with W. Richards Adrion] at [[Charles Babbage Institute]], University of Minnesota.  Adrion gives a brief history of theoretical computer science in the United States and NSF's role in funding that area during the 1970s and 1980s. *[http://purl.umn.edu/107301 Oral history interview with Bernard A. Galler] at [[Charles Babbage Institute]], University of Minnesota.  Galler describes the development of computer science at the University of Michigan from the 1950s through the 1980s and discusses his own work in computer science. *[http://purl.umn.edu/92154 Michael S. Mahoney Papers] at [[Charles Babbage Institute]], University of Minnesota—Mahoney was the preeminent historian of computer science as a distinct academic discipline.  Papers contain 38 boxes of books, serials, notes, and manuscripts related to the history of computing, mathematics, and related fields. *{{sep entry|computing-history|The Modern History of Computing|B. Jack Copeland}}  {{DEFAULTSORT:History Of Computer Science}} [[Category:Articles with inconsistent citation formats]] [[Category:History of computer science| ]]  [[ar:تاريخ علم الحاسوب]] [[cs:Dějiny informatiky]] [[fa:پیشینه علوم رایانه]] [[it:Storia dell'informatica]] [[ru:История информационных технологий]]
{{Refimprove|date=December 2009}} {{Wiktionary}} '''Implementation''' is the realization of an application, or execution of a [[plan]], idea, [[scientific modelling|model]], [[design]], [[specification]], [[Standardization|standard]], [[algorithm]], or [[policy]].  ==Computer Science== In [[computer science]], an implementation is a realization of a technical specification or algorithm as a [[computer program|program]], [[software component]], or other [[computer system]] through [[programming]] and [[Software deployment|deployment]].  Many implementations may exist for a given specification or standard.  For example, [[web browser]]s contain implementations of [[World Wide Web Consortium]]-recommended specifications, and software development tools contain implementations of [[programming languages]].  ==IT Industry== In the IT Industry, implementation refers to post-sales process of guiding a client from purchase to use of the software or hardware that was purchased. This includes Requirements Analysis, Scope Analysis, Customizations, Systems Integrations, User Policies, User Training and Delivery. These steps are often overseen by a Project Manager using Project Management Methodologies set forth in the Project Management Body of Knowledge. Software Implementations involve several professionals that are relatively new to the knowledge based economy such as [[Business analysis|Business Analysts]], [[Technical analysis|Technical Analysts]], [[Solutions Architect]], and Project Managers.  ==IT Implementation and the Role of End users== System implementation generally benefits from high levels of user involvement and management support. User participation in the design and operation of information systems has several positive results. First, if users are heavily involved in systems design, they move opportunities to mold the system according to their priorities and business requirements, and more opportunities to control the outcome.  Second, they are more likely to react positively to the change process. Incorporating user knowledge and expertise leads to better solutions.  The relationship between users and information systems specialists has traditionally been a problem area for information systems implementation efforts.  Users and information systems specialists tend to have different backgrounds, interests, and priorities.  This is referred to as the '''user-designer communications gap'''.  These differences lead to divergent organizational loyalties, approaches to problem solving, and vocabularies.<ref name="Laudon2010"/> Examples of these differences or concerns are below:  ===User Concerns=== * Will the system deliver the information I need for my work? * How quickly can I access the data? * How easily can I retrieve the data? * How much clerical support will I need to enter data into the system? * How will the operation of the system fit into my daily business schedule?<ref name="Laudon2010">Laudon, K., & Laudon, J. (2010). "Management Information Systems: Managing the Digital Firm." Eleventh  Edition (11 ed.). New Jersey: Prentice Hall.</ref>  ===Designer Concerns=== * How much disk storage space will the master file consume? * How many lines of program code will it take to perform this function? * How can we cut down on CPU time when we run the system? * What are the most efficient ways of storing this data? * What database management system should we use?<ref name="Laudon2010"/>  ==Political Science== In [[political science]], implementation refers to the carrying out of [[public policy]].  [[Legislatures]] pass laws that are then carried out by [[public servants]] working in [[bureaucracy|bureaucratic agencies]].  This process consists of rule-making, rule-administration and rule-adjudication.  Factors impacting implementation include the legislative intent, the administrative capacity of the implementing bureaucracy, interest group activity and opposition, and presidential or executive support.  ==Types of implementation== *[[Direct changeover]] *[[Parallel running]] or as known as parallel *[[Pilot introduction]] or as known as pilot *[[Well-trade]]  ==References== {{Reflist}}  <!-- Commented because of bad formed  {{Reflist|refs= <ref name="Laudon2010">Laudon, K., & Laudon, J. (2010). "Management Information Systems: Managing the Digital Firm." Eleventh  Edition (11 ed.). New Jersey: Prentice Hall.</ref>-->  <!-- Note: I think the above comment can be deleted now. See the history log for the date/time (and editor ID) of the edit when this comment was added -- AND the previous edit, when the wiki markup for the first 3 footnotes was updated. [fixed -- I think].  Thank you!Khaitov -->  ==See also== *[[Interface (computer science)]] *[[Programming language implementation]] *[[Algorithm]] *[[Application software]] *[[Code]] *[[Computation]] *[[Function (computer science)|Function]] *[[Method (computer science)]] *[[process (computing)|Process]] *[[Proceeding]] *[[Procedure (disambiguation)]] *[[Scheme (disambiguation)]] *[[Solution]] *[[System]]  [[Category:Computing terminology]] [[Category:Political science terms]] [[Category:Design]]  [[ca:Implementació]] [[cs:Implementace]] [[da:Implementering]] [[de:Implementierung]] [[es:Implementación]] [[fr:Mise en œuvre]] [[ko:구현]] [[it:Implementazione]] [[nl:Implementatie]] [[ja:実装]] [[pl:Implementacja (informatyka)]] [[pt:Implementação de software]] [[ru:Правовая имплементация]] [[sq:Implementimi]] [[sv:Implementation (samhällskunskap)]] [[uk:Імплементація]]
 Automated information retrieval systems are used to reduce what has been called "[[information overload]]". Many universities and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].  == History == {{Rquote|right|But do you know that, although I have kept the diary [on a phonograph] for months past, it never once struck me how I was going to find any particular part of it in case I wanted to look it up?|[[John Seward|Dr Seward]]| [[Bram Stoker]]'s ''[[Dracula]]'',  1897}} The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering |volume=24 |issue=4 |pages=35–43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.  In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of web [[Web search engine|search engine]]s has boosted the need for very large scale retrieval systems even further.  The use of digital methods for storing and retrieving information has led to the phenomenon of [[digital obsolescence]], where a digital resource ceases to be readable because the physical media, the reader required to read the media, the hardware, or the software that runs on it, is no longer available. The information is initially easier to retrieve than if it were on paper, but is then effectively lost.  === Timeline ===  * Before the '''1900s''' *: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations. *: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium. *: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data. * '''1920s-1930s''' *: [[Emanuel Goldberg]] submits patents for his "Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents. * '''1940s–1950s''' *: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans. *:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''. *:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds. *: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems ([[Allen Kent]] ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]). *: '''1950''': The term "information retrieval" appears to have been coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; ''Theory Digital Handling Non-numerical Information'' (Zator Technical Bulletin No. 48) 5, cited in "information, n.". OED Online. December 2011. Oxford University Press.</ref> *: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref> *: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved. *: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959) *: '''1959''': Hans Peter Luhn published "Auto-encoding of documents for information retrieval." * '''1960s''': *: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell. *: '''1960''': Melvin Earl (Bill) Maron and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971–972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216–244, July 1960. *: '''1962''': *:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962. *:* Kent published ''Information Analysis and Retrieval''. *: '''1963''': *:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]]. *:* [[Joseph Becker (author)|Joseph Becker]] and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963). *: '''1964''': *:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR. *:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system. *:'''mid-1960s''': *::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system. *::* Project Intrex at MIT. *:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''. *:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs. *: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval. *:: '''1968''': *:* Gerard Salton published ''Automatic Information Organization and Retrieval''. *:* [[John W. Sammon, Jr.]]'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model. *:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system. * '''1970s''' *: '''early 1970s''': *::* First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT. *::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''. *: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis." (Information Storage and Retrieval, 7(5), pp.&nbsp;217–240, December 1971) *: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model: *::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics) *::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26) *::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11) *: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference. *: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models. * '''1980s''' *: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge. *: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing. *: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models. *: '''1985''': Blair and Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System *: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems. *:: '''1985–1993''': Key papers on and experimental systems for visualization interfaces. *:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], [[Matthew Chalmers]], [[Anselm Spoerri]] and others. *: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]]. * '''1990s''' *: '''1992''': First [[Text Retrieval Conference|TREC]] conference. *: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems. *: '''late 1990s''': Web [[Web search engine|search engines]] implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models, research, and implementation.  == Overview ==  An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[relevance|relevancy]].  An object is an entity that is represented by information in a [[database]]. User queries are matched against the database information. Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite document |first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |contribution=Information Retrieval On Mind Maps - What Could It Be Good For? |contribution-url=http://www.sciplore.org/publications_en.php |title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.  Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>  == Performance and correctness measures == {{main|Precision and recall}}  Many different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be [[ill-posed]] and there may be different shades of relevancy.  === Precision ===  Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.  :<math> \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} </math>  In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.  Note that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.  === Recall ===  Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.  :<math>\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} </math>  In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.  It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.  === Fall-out === The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:  :<math> \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} </math>  In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\mbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.  It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.  === F-measure === {{main|F-score}} The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:  :<math>F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision}   \mathrm{recall})}.\,</math>  This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.  The general formula for non-negative real <math>\beta</math> is: :<math>F_\beta = \frac{(1   \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision}   \mathrm{recall})}\,</math>.  Two other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.  The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \frac{1}{\frac{\alpha}{P}   \frac{1-\alpha}{R}}</math>.  Their relationship is <math>F_\beta = 1 - E</math> where <math>\alpha=\frac{1}{1   \beta^2}</math>.  === Average precision === <!-- [[Average precision]] redirects here --> Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite document |first=Mu |last=Zhu |contribution=Recall, Precision and Average Precision |contribution-url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref> :<math>\operatorname{AveP} = \int_0^1 p(r)dr.</math> This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents: :<math>\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)</math> where <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />  This finite sum is equivalent to: :<math> \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times rel(k))}{\mbox{number of relevant documents}} \!</math> where <math>rel(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06-11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.   Some authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}<ref name="voc2010" /><ref name="nlpbook" />: :<math>\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_\operatorname{interp}(r)</math> where <math>p_\operatorname{interp}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>: :<math>p_\operatorname{interp}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})</math>.  An alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>  Average precision is also sometimes referred to geometrically as the area under the precision-recall curve.{{Citation needed|date=August 2011}}  === R-Precision ===  Precision at '''R'''-th position in the ranking of results for a query that has '''R''' relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the '''R'''-th position.  === Mean average precision === <!-- [[Mean average precision]] redirects here --> Mean average precision for a set of queries is the mean of the average precision scores for each query. :<math> \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!</math> where ''Q'' is the number of queries.  === Discounted cumulative gain === {{main|Discounted cumulative gain}} DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.  The DCG accumulated at a particular rank position <math>p</math> is defined as:  :<math> \mathrm{DCG_{p}} = rel_{1}   \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. </math>  Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:  :<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. </math>  The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.  === Other Measures === * [[Mean reciprocal rank]] * [[Spearman's rank correlation coefficient]]  == Model types == [[Image:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]] For the information retrieval to be efficient, the documents are typically transformed into a suitable representation. There are several representations. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.  === First dimension: mathematical basis === * ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are: ** [[Standard Boolean model]] ** [[Extended Boolean model]] ** [[Fuzzy retrieval]] * ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. ** [[Vector space model]] ** [[Generalized vector space model]] ** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]] ** [[Extended Boolean model]] ** [[Latent semantic indexing]] aka [[latent semantic analysis]] * ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models. ** [[Binary Independence Model]] ** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function ** [[Uncertain inference]] ** [[Language model]]s ** [[Divergence-from-randomness model]] ** [[Latent Dirichlet allocation]] * ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just a yet another feature.  === Second dimension: properties of the model === * ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[independency]] assumption for term variables. * ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents. * ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They relay an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.) <!--  == Research Groups (in no particular order) == * [http://ciir.cs.umass.edu/ Center for Intelligent Information Retrieval at University of Massachusetts] * [http://www.lti.cs.cmu.edu/Research/projects.html#ir Information Retrieval at the Language Technologies Institute, Carnegie Mellon University] * [http://ir.dcs.gla.ac.uk Glasgow Information Retrieval Group] -->  == Major figures ==  * [[Thomas Bayes]] * [[Claude E. Shannon]] * [[Gerard Salton]] * [[Hans Peter Luhn]] * [[Cyril Cleverdon]] * [[W. Bruce Croft]] * [[Karen Spärck Jones]] * [[Calvin Mooers]] * [[C. J. van Rijsbergen]] * [[Stephen E. Robertson]] * [[Martin Porter]] * [[Amit Singhal]]  == Awards in the field ==  * [[Tony Kent Strix award]] * [[Gerard Salton Award]]  ==See also==  {{col-begin}} {{col-break}}  * [[Adversarial information retrieval]] * [[Collaborative information seeking]] * [[Controlled vocabulary]] * [[Cross-language information retrieval]] * [[Data mining]] * [[European Summer School in Information Retrieval]] * [[Human–computer information retrieval]] * [[Information extraction]] * [[Information Retrieval Facility]] * [[Information science]] * [[Information seeking]] * [[Knowledge visualization]] {{col-break}} * [[Multimedia Information Retrieval]] * [[Personal information management]] * [[Relevance (Information Retrieval)]] * [[Relevance feedback]] * [[Rocchio Classification]] * [[Index (search engine)|Search index]] * [[Social information seeking]] * [[Special Interest Group on Information Retrieval]] * [[Subject indexing]] * [[Tf-idf]] * [[XML-Retrieval]]  {{col-end}}  == References == {{reflist}}  ==External links== * [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group] * [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group] * [http://trec.nist.gov Text Retrieval Conference (TREC)] * [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)] * [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]] * [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki] * [http://ir-facility.org/ Information Retrieval Facility] * [http://www.nonrelevant.net Information Retrieval @ DUTH] * [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval (online book) by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press. 2008.  ]  {{DEFAULTSORT:Information Retrieval}} [[Category:Articles with inconsistent citation formats]] [[Category:Information retrieval| ]] [[Category:Natural language processing]]  [[ar:استرجاع المعلومات]] [[bg:Извличане на информация]] [[de:Information Retrieval]] [[el:Ανάκτηση Πληροφοριών]] [[es:Recuperación de información]] [[eu:Informazioa eskuratzea]] [[fa:بازیابی اطلاعات]] [[fr:Recherche d'information]] [[gl:Recuperación de información]] [[ko:정보 검색]] [[id:Sistem temu balik informasi]] [[it:Information retrieval]] [[ms:Dapatan kembali maklumat]] [[nl:Information retrieval]] [[ja:情報検索]] [[no:Informasjonsgjenfinning]] [[nn:Informasjonsattfinning]] [[pt:Recuperação de informação]] [[ru:Информационный поиск]] [[fi:Tiedonhaku]] [[sv:Informationsåtkomst]] [[tg:Ҷустуҷӯи информатсионӣ]] [[uk:Інформаційний пошук]] [[zh:信息檢索]]
{{Dablink|Not to be confused with [[Information science]].}}  '''Information theory''' is a branch of [[applied mathematics]], [[electrical engineering]], and [[computer science]] involving the [[quantification]] of [[information]].  Information theory was developed by [[Claude E. Shannon]] to find fundamental limits on [[signal processing]] operations such as [[data compression|compressing data]] and on reliably [[Computer data storage|storing]] and [[Telecommunication|communicating]] data. Since its inception it has broadened to find applications in many other areas, including [[statistical inference]], [[natural language processing]], [[cryptography]] generally, networks other than communication networks—as in [[neurobiology]],<ref>F. Rieke, D. Warland, R Ruyter van Steveninck, W Bialek,  Spikes: Exploring the Neural Code. The MIT press (1997).</ref> the evolution<ref>cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, ''Science'' '''294''':2310-2314</ref> and function<ref>Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas D. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, ''Gene'' '''215''':1, 111-122</ref> of molecular codes, model selection<ref>Burnham, K. P. and Anderson D. R. (2002) ''Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition'' (Springer Science, New York) ISBN 978-0-387-95364-9.</ref> in ecology, thermal physics,<ref>Jaynes, E. T. (1957) [http://bayes.wustl.edu/ Information Theory and Statistical Mechanics], ''Phys. Rev.'' '''106''':620</ref> [[quantum computing]], plagiarism detection<ref>Charles H. Bennett, Ming Li, and Bin Ma (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary Histories], ''Scientific American'' '''288''':6, 76-81</ref> and other forms of [[data analysis]].<ref> {{Cite web   | author = David R. Anderson   | title = Some background on why people in the empirical sciences may want to better understand the information-theoretic methods   | date = November 1, 2003   | url = http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf   | format = pdf   | accessdate = 2010-06-23}} </ref>  A key measure of information is known as [[Entropy (information theory)|entropy]], which is usually expressed by the average number of bits needed to store or communicate one [[Symbol (data)|symbol]] in a message. Entropy quantifies the uncertainty involved in predicting the value of a [[random variable]]. For example, specifying the outcome of a fair [[coin flip]] (two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a {{dice}} (six equally likely outcomes).  Applications of fundamental topics of information theory include [[lossless data compression]] (e.g. [[ZIP (file format)|ZIP files]]), [[lossy data compression]] (e.g. [[MP3]]s and [[JPG]]s), and [[channel capacity|channel coding]] (e.g. for [[DSL|Digital Subscriber Line (DSL)]]).  The field is at the intersection of [[mathematics]], [[statistics]], [[computer science]], [[physics]], [[neurobiology]], and [[electrical engineering]]. Its impact has been crucial to the success of the [[Voyager program|Voyager]] missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the [[Internet]], the study of [[linguistics]] and of human perception, the understanding of [[black hole]]s, and numerous other fields. Important sub-fields of information theory are [[source coding]], [[channel coding]], [[algorithmic complexity theory]], [[algorithmic information theory]], [[information-theoretic security]], and measures of information.  ==Overview== The main concepts of information theory can be grasped by considering the most widespread means of human communication: language.  Two important aspects of a concise language are as follows:  First, the most common words (e.g., "a", "the", "I") should be shorter than less common words (e.g., "roundabout", "generation", "mediocre",) so that sentences will not be too long.  Such a tradeoff in word length is analogous to [[data compression]] and is the essential aspect of [[source coding]].  Second, if part of a sentence is unheard or misheard due to noise — e.g., a passing car — the listener should still be able to glean the meaning of the underlying message.  Such robustness is as essential for an electronic communication system as it is for a language; properly building such robustness into communications is done by [[Channel capacity|channel coding]]. Source coding and channel coding are the fundamental concerns of information theory.  Note that these concerns have nothing to do with the ''importance'' of messages. For example, a platitude such as "Thank you; come again" takes about as long to say or write as the urgent plea, "Call an ambulance!" while the latter may be more important and more meaningful in many contexts. Information theory, however, does not consider message importance or meaning, as these are matters of the quality of data rather than the quantity and readability of data, the latter of which is determined solely by probabilities.  Information theory is generally considered to have been founded in 1948 by [[Claude Elwood Shannon|Claude Shannon]] in his seminal work, "[[A Mathematical Theory of Communication]]".  The central paradigm of classical information theory is the engineering problem of the transmission of information over a noisy channel. The most fundamental results of this theory are Shannon's [[source coding theorem]], which establishes that, on average, the number of ''bits'' needed to represent the result of an uncertain event is given by its [[information entropy|entropy]]; and Shannon's [[noisy-channel coding theorem]], which states that ''reliable'' communication is possible over ''noisy'' channels provided that the rate of communication is below a certain threshold, called the channel capacity. The channel capacity can be approached in practice by using appropriate encoding and decoding systems.  Information theory is closely associated with a collection of pure and applied disciplines that have been investigated and reduced to engineering practice under a variety of [[Rubric (academic)|rubrics]] throughout the world over the past half century or more: [[adaptive system]]s, [[anticipatory system]]s, [[artificial intelligence]], [[complex system]]s, [[complexity science]], [[cybernetics]], [[Informatics (academic field)|informatics]], [[machine learning]], along with [[systems science]]s of many descriptions. Information theory is a broad and deep mathematical theory, with equally broad and deep applications, amongst which is the vital field of [[coding theory]].  Coding theory is concerned with finding explicit methods, called ''codes'', of increasing the efficiency and reducing the net error rate of data communication over a noisy channel to near the limit that Shannon proved is the maximum possible for that channel. These codes can be roughly subdivided into [[data compression]] (source coding) and [[error-correction]] (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible. A third class of information theory codes are cryptographic algorithms (both [[code (cryptography)|code]]s and [[cipher]]s). Concepts, methods and results from coding theory and information theory are widely used in [[cryptography]] and [[cryptanalysis]]. ''See the article [[ban (information)]] for a historical application.''  Information theory is also used in [[information retrieval]], [[intelligence (information gathering)|intelligence gathering]], [[gambling]], [[statistics]], and even in [[musical composition]].  ==Historical background== {{Main|History of information theory}}  The landmark event that established the discipline of information theory, and brought it to immediate worldwide attention, was the publication of [[Claude E. Shannon]]'s classic paper "[[A Mathematical Theory of Communication]]" in the ''[[Bell System Technical Journal]]'' in July and October 1948.  Prior to this paper, limited information-theoretic ideas had been developed at [[Bell Labs]], all implicitly assuming events of equal probability.  [[Harry Nyquist]]'s 1924 paper, ''Certain Factors Affecting Telegraph Speed,'' contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation <math>W = K \log m</math>, where ''W'' is the speed of transmission of intelligence, ''m'' is the number of different voltage levels to choose from at each time step, and ''K'' is a constant.  [[Ralph Hartley]]'s 1928 paper, ''Transmission of Information,'' uses the word ''information'' as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as <math>H = \log S^n = n \log S</math>, where ''S'' was the number of possible symbols, and ''n'' the number of symbols in a transmission. The natural unit of information was therefore the decimal digit, much later renamed the [[ban (information)|hartley]] in his honour as a unit or scale or measure of information. [[Alan Turing]] in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war [[Cryptanalysis of the Enigma|Enigma]] ciphers.  Much of the mathematics behind information theory with events of different probabilities was developed for the field of [[thermodynamics]] by [[Ludwig Boltzmann]] and [[J. Willard Gibbs]].  Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by [[Rolf Landauer]] in the 1960s, are explored in ''[[Entropy in thermodynamics and information theory]]''.  In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that :"The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point."  With it came the ideas of * the [[information entropy]] and [[redundancy (information theory)|redundancy]] of a source, and its relevance through the [[source coding theorem]]; * the [[mutual information]], and the [[channel capacity]] of a noisy channel, including the promise of perfect loss-free communication given by the [[noisy-channel coding theorem]]; * the practical result of the [[Shannon–Hartley law]] for the channel capacity of a [[Gaussian channel]]; as well as * the [[bit]]—a new way of seeing the most fundamental unit of information.  ==Quantities of information== {{Main|Quantities of information}}  Information theory is based on [[probability theory]] and [[statistics]].  The most important quantities of information are [[Entropy (information theory)|entropy]], the information in a [[random variable]], and [[mutual information]], the amount of information in common between two random variables.  The former quantity indicates how easily message data can be [[data compression|compressed]] while the latter can be used to find the communication rate across a [[Channel (communications)|channel]].  The choice of logarithmic base in the following formulae determines the [[units of measurement|unit]] of [[information entropy]] that is used.  The most common unit of information is the [[bit]], based on the [[binary logarithm]]. Other units include the [[nat (information)|nat]], which is based on the [[natural logarithm]], and the [[deciban|hartley]], which is based on the [[common logarithm]].  In what follows, an expression of the form <math>p \log p \,</math> is considered by convention to be equal to zero whenever <math>p=0.</math>  This is justified because <math>\lim_{p \rightarrow 0 } p \log p = 0</math> for any logarithmic base.  ===Entropy=== [[Image:Binary entropy plot.svg|thumbnail|right|200px|Entropy of a [[Bernoulli trial]] as a function of success probability, often called the '''[[binary entropy function]]''', <math>H_\mbox{b}(p)</math>.  The entropy is maximized at 1 bit per trial when the two possible outcomes are equally probable, as in an unbiased coin toss.]] The '''[[Entropy (information theory)|entropy]]''', <math>H</math>, of a discrete random variable <math>X</math> is a measure of the amount of ''uncertainty'' associated with the value of <math>X</math>.  Suppose one transmits 1000 bits (0s and 1s).   If these bits are known ahead of transmission (to be a certain value with absolute probability), logic dictates that no information has been transmitted.  If, however, each is equally and independently likely to be 0 or 1, 1000 bits (in the information theoretic sense) have been transmitted.  Between these two extremes, information can be quantified as follows. If <math>\mathbb{X}</math> is the set of all messages <math>\{x_1, ..., x_n\}</math> that <math>X</math> could be, and <math>p(x)</math> is the probability of <math>x</math> given some <math>x \in \mathbb X</math>, then the entropy of <math>X</math> is defined:<ref name = Reza>{{cite book | title = An Introduction to Information Theory | author = Fazlollah M. Reza | publisher = Dover Publications, Inc., New York | year = 1961, 1994 | isbn = 0-486-68210-2 | url = http://books.google.com/books?id=RtzpRAiX6OgC&pg=PA8&dq=intitle:%22An Introduction to Information Theory%22  %22entropy of a simple source%22}}</ref>  :<math> H(X) = \mathbb{E}_{X} [I(x)] = -\sum_{x \in \mathbb{X}} p(x) \log p(x).</math>  (Here, <math>I(x)</math> is the [[self-information]], which is the entropy contribution of an individual message, and <math>\mathbb{E}_{X}</math> is the [[expected value]].) An important property of entropy is that it is maximized when all the messages in the message space are equiprobable <math>p(x)=1/n</math>,—i.e., most unpredictable—in which case <math> H(X)=\log n</math>.  The special case of information entropy for a random variable with two outcomes is the '''[[binary entropy function]]''', usually taken to the logarithmic base 2:  :<math>H_\mbox{b}(p) = - p \log_2 p - (1-p)\log_2 (1-p).\,</math>  ===Joint entropy=== The '''[[joint entropy]]''' of two discrete random variables <math>X</math> and <math>Y</math> is merely the entropy of their pairing: <math>(X, Y)</math>.  This implies that if <math>X</math> and <math>Y</math> are [[statistical independence|independent]], then their joint entropy is the sum of their individual entropies.  For example, if <math>(X,Y)</math> represents the position of a [[chess]] piece &mdash; <math>X</math> the row and <math>Y</math> the column, then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece.  :<math>H(X, Y) = \mathbb{E}_{X,Y} [-\log p(x,y)] = - \sum_{x, y} p(x, y) \log p(x, y) \,</math>  Despite similar notation, joint entropy should not be confused with '''[[cross entropy]]'''.  ===Conditional entropy (equivocation)=== The '''[[conditional entropy]]''' or '''conditional uncertainty''' of <math>X</math> given random variable <math>Y</math> (also called the '''equivocation''' of <math>X</math> about <math>Y</math>) is the average conditional entropy over <math>Y</math>:<ref name=Ash>{{cite book | title = Information Theory | author = Robert B. Ash | publisher = Dover Publications, Inc. | year = 1965, 1990 | isbn = 0-486-66521-6 | url = http://books.google.com/books?id=ngZhvUfF0UIC&pg=PA16&dq=intitle:information intitle:theory inauthor:ash conditional uncertainty}}</ref>  :<math> H(X|Y) = \mathbb E_Y [H(X|y)] = -\sum_{y \in Y} p(y) \sum_{x \in X} p(x|y) \log p(x|y) = -\sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(y)}.</math>  Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use.  A basic property of this form of conditional entropy is that:  : <math> H(X|Y) = H(X,Y) - H(Y) .\,</math>  ===Mutual information (transinformation)=== '''[[Mutual information]]''' measures the amount of information that can be obtained about one random variable by observing another.  It is important in communication where it can be used to maximize the amount of information shared between sent and received signals.  The mutual information of <math>X</math> relative to <math>Y</math> is given by:  :<math>I(X;Y) = \mathbb{E}_{X,Y} [SI(x,y)] = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)\, p(y)}</math> where <math>SI</math> (''S''pecific mutual ''I''nformation) is the [[pointwise mutual information]].  A basic property of the mutual information is that : <math>I(X;Y) = H(X) - H(X|Y).\,</math> That is, knowing ''Y'', we can save an average of <math>I(X; Y)</math> bits in encoding ''X'' compared to not knowing ''Y''.  Mutual information is [[symmetric function|symmetric]]: : <math>I(X;Y) = I(Y;X) = H(X)   H(Y) - H(X,Y).\,</math>  Mutual information can be expressed as the average [[Kullback–Leibler divergence]] (information gain) of the [[posterior probability|posterior probability distribution]] of ''X'' given the value of ''Y'' to the [[prior probability|prior distribution]] on ''X'': : <math>I(X;Y) = \mathbb E_{p(y)} [D_{\mathrm{KL}}( p(X|Y=y) \| p(X) )].</math> In other words, this is a measure of how much, on the average, the probability distribution on ''X'' will change if we are given the value of ''Y''.  This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution: : <math>I(X; Y) = D_{\mathrm{KL}}(p(X,Y) \| p(X)p(Y)).</math>  Mutual information is closely related to the [[likelihood-ratio test|log-likelihood ratio test]] in the context of contingency tables and the [[multinomial distribution]] and to [[Pearson's chi-squared test|Pearson's χ<sup>2</sup> test]]: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.  ===Kullback–Leibler divergence (information gain)=== The '''[[Kullback–Leibler divergence]]''' (or '''information divergence''', '''information gain''', or '''relative entropy''') is a way of comparing two distributions: a "true" [[probability distribution]] ''p(X)'', and an arbitrary probability distribution ''q(X)''. If we compress data in a manner that assumes ''q(X)'' is the distribution underlying some data, when, in reality, ''p(X)'' is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression.  It is thus defined  :<math>D_{\mathrm{KL}}(p(X) \| q(X)) = \sum_{x \in X} -p(x) \log {q(x)} \, - \, \left( -p(x) \log {p(x)}\right) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}.</math>  Although it is sometimes used as a 'distance metric', KL divergence is not a true [[Metric (mathematics)|metric]] since it is not symmetric and does not satisfy the [[triangle inequality]] (making it a semi-quasimetric).  ===Kullback–Leibler divergence of a prior from the truth=== Another interpretation of KL divergence is this: suppose a number ''X'' is about to be drawn randomly from a discrete set with probability distribution ''p(x)''.  If Alice knows the true distribution ''p(x)'', while Bob believes (has a prior) that the distribution is ''q(x)'', then Bob will be more [[Self-information|surprised]] than Alice, on average, upon seeing the value of ''X''.  The KL divergence is the (objective) expected value of the Bob's (subjective) [[surprisal]] minus Alice's surprisal, measured in bits if the ''log'' is in base 2.  In this way, the extent to which Bob's prior is "wrong" can be quantified in terms of how "unnecessarily surprised" it's expected to make him.  ===Other quantities=== Other important information theoretic quantities include [[Rényi entropy]] (a generalization of entropy), [[differential entropy]] (a generalization of quantities of information to continuous distributions), and the [[conditional mutual information]].  ==Coding theory==  {{Main|Coding theory}}  [[Image:CDSCRATCHES.jpg|thumb|right|A picture showing scratches on the readable surface of a CD-R.  Music and data CDs are coded using error correcting codes and thus can still be read even if they have minor scratches using [[error detection and correction]].]]  [[Coding theory]] is one of the most important and direct applications of information theory. It can be subdivided into [[data compression|source coding]] theory and [[error correction|channel coding]] theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.  * Data compression (source coding): There are two formulations for the compression problem: #[[lossless data compression]]: the data must be reconstructed exactly; #[[lossy data compression]]: allocates bits needed to reconstruct the data, within a specified fidelity level measured by a distortion function. This subset of Information theory is called [[rate–distortion theory]].  * Error-correcting codes (channel coding): While data compression removes as much [[redundancy (information theory)|redundancy]] as possible, an error correcting code adds just the right kind of redundancy (i.e., [[error correction]]) needed to transmit the data efficiently and faithfully across a noisy channel.  This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the [[broadcast channel]]) or intermediary "helpers" (the [[relay channel]]), or more general [[computer network|networks]], compression followed by transmission may no longer be optimal. [[Network information theory]] refers to these multi-agent communication models.  ===Source theory===  Any process that generates successive messages can be considered a '''[[Communication source|source]]''' of information.  A memoryless source is one in which each message is an [[Independent identically distributed random variables|independent identically distributed random variable]], whereas the properties of [[ergodic theory|ergodicity]] and [[stationary process|stationarity]] impose more general constraints.  All such sources are [[stochastic process|stochastic]].  These terms are well studied in their own right outside information theory.  ====Rate====<!-- This section is linked from [[Channel capacity]] --> Information [[Entropy rate|'''rate''']] is the average entropy per symbol.  For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is  :<math>r = \lim_{n \to \infty} H(X_n|X_{n-1},X_{n-2},X_{n-3}, \ldots);</math>  that is, the conditional entropy of a symbol given all the previous symbols generated.  For the more general case of a process that is not necessarily stationary, the ''average rate'' is  :<math>r = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots X_n);</math>  that is, the limit of the joint entropy per symbol.  For stationary sources, these two expressions give the same result.<ref>{{cite book | title = Digital Compression for Multimedia: Principles and Standards | author = Jerry D. Gibson | publisher = Morgan Kaufmann | year = 1998 | url = http://books.google.com/books?id=aqQ2Ry6spu0C&pg=PA56&dq=entropy-rate conditional#PPA57,M1 | isbn = 1-55860-369-7 }}</ref>  It is common in information theory to speak of the "rate" or "entropy" of a language.  This is appropriate, for example, when the source of information is English prose.  The rate of a source of information is related to its [[redundancy (information theory)|redundancy]] and how well it can be [[data compression|compressed]], the subject of '''source coding'''.  ===Channel capacity=== {{Main|Channel capacity}}  Communications over a channel—such as an [[ethernet]] [[cable]]—is the primary motivation of information theory.  As anyone who's ever used a telephone (mobile or landline) knows, however, such channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.  How much information can one hope to communicate over a noisy (or otherwise imperfect) channel?  Consider the communications process over a discrete channel. A simple model of the process is shown below:  [[Image:Comm Channel.svg|center|500px]]  Here ''X'' represents the space of messages transmitted, and ''Y'' the space of messages received during a unit time over our channel. Let <math>p(y|x)</math> be the [[conditional probability]] distribution function of ''Y'' given ''X''. We will consider <math>p(y|x)</math> to be an inherent fixed property of our communications channel (representing the nature of the '''[[Signal noise|noise]]''' of our channel). Then the joint distribution of ''X'' and ''Y'' is completely determined by our channel and by our choice of <math>f(x)</math>, the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the '''[[Signal (electrical engineering)|signal]]''', we can communicate over the channel. The appropriate measure for this is the [[mutual information]], and this maximum mutual information is called the '''[[channel capacity]]''' and is given by: :<math> C = \max_{f} I(X;Y).\! </math> This capacity has the following property related to communicating at information rate ''R'' (where ''R'' is usually bits per symbol).  For any information rate ''R < C'' and coding error ε > 0, for large enough ''N'', there exists a code of length ''N'' and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is ≤ ε; that is, it is always possible to transmit with arbitrarily small block error.  In addition, for any rate ''R > C'', it is impossible to transmit with arbitrarily small block error.  '''[[Channel code|Channel coding]]''' is concerned with finding such nearly optimal [[error detection and correction|codes]] that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.  ====Capacity of particular channel models====  * A continuous-time analog communications channel subject to [[Gaussian noise]] — see [[Shannon–Hartley theorem]].  * A [[binary symmetric channel]] (BSC) with crossover probability ''p'' is a binary input, binary output channel that flips the input bit with probability '' p''. The BSC has a capacity of <math>1 - H_\mbox{b}(p)</math> bits per channel use, where <math>H_\mbox{b}</math> is the [[binary entropy function]] to the base 2 logarithm:  ::[[Image:Binary symmetric channel.svg]]  * A [[binary erasure channel]] (BEC) with erasure probability '' p '' is a binary input, ternary output channel. The possible channel outputs are ''0'', ''1'', and a third symbol 'e' called an erasure. The erasure represents complete loss of information about an input bit. The capacity of the BEC is ''1 - p'' bits per channel use.  ::[[Image:Binary erasure channel.svg]]  ==Applications to other fields== ===Intelligence uses and secrecy applications===  Information theoretic concepts apply to [[cryptography]] and [[cryptanalysis]].  [[Turing]]'s information unit, the [[Ban (information)|ban]], was used in the [[Ultra]] project, breaking the German [[Enigma machine]] code and hastening the [[Victory in Europe Day|end of WWII in Europe]].  Shannon himself defined an important concept now called the [[unicity distance]]. Based on the [[redundancy (information theory)|redundancy]] of the [[plaintext]], it attempts to give a minimum amount of [[ciphertext]] necessary to ensure unique decipherability.  Information theory leads us to believe it is much more difficult to keep secrets than it might first appear.  A [[brute force attack]] can break systems based on [[public-key cryptography|asymmetric key algorithms]] or on most commonly used methods of [[symmetric-key algorithm|symmetric key algorithms]] (sometimes called secret key algorithms), such as [[block cipher]]s.  The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.  [[Information theoretic security]] refers to methods such as the [[one-time pad]] that are not vulnerable to such brute force attacks.  In such cases, the positive conditional [[mutual information]] between the [[plaintext]] and [[ciphertext]] (conditioned on the [[key (cryptography)|key]]) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications.  In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the [[Venona project]] was able to crack the one-time pads of the [[Soviet Union]] due to their improper reuse of key material.  ===Pseudorandom number generation=== [[Pseudorandom number generator]]s are widely available in computer language libraries and application programs. They are, almost universally, unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software. A class of improved random number generators is termed [[cryptographically secure pseudorandom number generator]]s, but even they require external to the software [[random seed]]s to work as intended. These can be obtained via [[Extractor (mathematics)|extractors]], if done carefully. The measure of  sufficient randomness in extractors is [[min-entropy]], a value related to Shannon entropy through [[Rényi entropy]]; Rényi entropy is also used in evaluating randomness in cryptographic systems.  Although related, the distinctions among these measures mean that a [[random variable]] with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses.  ===Seismic exploration=== One early commercial application of information theory was in the field seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and [[digital signal processing]] offer a major improvement of resolution and image clarity over previous analog methods.<ref>The Corporation and Innovation, Haggerty, Patrick, Strategic Management Journal, Vol. 2, 97-118 (1981)</ref>  ===Miscellaneous applications=== Information theory also has applications in [[Gambling and information theory|gambling and investing]], [[black hole information paradox|black holes]], [[bioinformatics]], and [[music]].  ==See also== {{Portal|Mathematics}} *[[Communication theory]] *[[List of important publications in theoretical computer science#Information theory|List of important publications]] *[[Philosophy of information]]  ===Applications=== * [[Cryptanalysis]] * [[Cryptography]] * [[Cybernetics]] * [[Entropy in thermodynamics and information theory]] * [[Gambling]] * [[Intelligence (information gathering)]] * [[reflection seismology|Seismic exploration]]  ===History=== * [[Ralph Hartley|Hartley, R.V.L.]] * [[History of information theory]] * [[Claude Elwood Shannon|Shannon, C.E.]] * [[Timeline of information theory]] * [[Hubert Yockey|Yockey, H.P.]]  ===Theory=== <div style="-moz-column-count:3; column-count:3;"> * [[Coding theory]] * [[Detection theory]] * [[Estimation theory]] * [[Fisher information]] * [[Information algebra]] * [[Information asymmetry]] * [[Information geometry]] * [[Information theory and measure theory]] * [[Kolmogorov complexity]] * [[Logic of information]] * [[Network coding]] * [[Philosophy of Information]] * [[Quantum information science]] * [[Semiotic information theory]] * [[Source coding]] </div>  ===Concepts=== <div style="-moz-column-count:3; column-count:3;"> * [[ban (information)]] * [[Channel capacity]] * [[Channel (communications)]] * [[Communication source]] * [[Conditional entropy]] * [[Covert channel]] * [[Decoder]] * [[Differential entropy]] * [[Encoder]] * [[Information entropy]] * [[Joint entropy]] * [[Kullback-Leibler divergence]] * [[Mutual information]] * [[Pointwise Mutual Information]] (PMI) * [[Receiver (information theory)]] * [[Redundancy (information theory)|Redundancy]] * [[Rényi entropy]] * [[Self-information]] * [[Unicity distance]] * [[Variety (cybernetics)|Variety]] </div>  ==References==  {{Reflist}}  ===The classic work=== * [[Claude Elwood Shannon|Shannon, C.E.]] (1948), "[[A Mathematical Theory of Communication]]", ''Bell System Technical Journal'', 27, pp.&nbsp;379–423 & 623–656, July & October, 1948. [http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf PDF.] <br />[http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html Notes and other formats.] * R.V.L. Hartley, [http://www.dotrose.com/etext/90_Miscellaneous/transmission_of_information_1928b.pdf "Transmission of Information"], ''Bell System Technical Journal'', July 1928 * [[Andrey Kolmogorov]] (1968), "Three approaches to the quantitative definition of information" in International Journal of Computer Mathematics.  ===Other journal articles=== * J. L. Kelly, Jr., [http://www.racing.saratoga.ny.us/kelly.pdf Saratoga.ny.us], "A New Interpretation of Information Rate" ''Bell System Technical Journal'', Vol. 35, July 1956, pp.&nbsp;917–26. * R. Landauer, [http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=615478 IEEE.org], "Information is Physical" ''Proc. Workshop on Physics and Computation PhysComp'92'' (IEEE Comp. Sci.Press, Los Alamitos, 1993) pp.&nbsp;1–4. * R. Landauer, [http://www.research.ibm.com/journal/rd/441/landauerii.pdf IBM.com], "Irreversibility and Heat Generation in the Computing Process" ''IBM J. Res. Develop.'' Vol. 5, No. 3, 1961  ===Textbooks on information theory=== * [[Claude E. Shannon]], Warren Weaver. ''The Mathematical Theory of Communication.'' Univ of Illinois Press, 1949. ISBN 0-252-72548-4 * [[Robert Gallager]]. ''Information Theory and Reliable Communication.'' New York: John Wiley and Sons, 1968. ISBN 0-471-29048-3 * Robert B. Ash. ''Information Theory''. New York: Interscience, 1965. ISBN 0-470-03445-9. New York: Dover 1990. ISBN 0-486-66521-6 * [[Thomas M. Cover]], Joy A. Thomas. ''Elements of information theory'', 1st Edition.  New York: Wiley-Interscience, 1991. ISBN 0-471-06259-6. :2nd Edition. New York: Wiley-Interscience, 2006. ISBN 0-471-24195-4. * [[Imre Csiszar]], Janos Korner. ''Information Theory: Coding Theorems for Discrete Memoryless Systems''  Akademiai Kiado: 2nd edition, 1997. ISBN 963-05-7440-3 * Raymond W. Yeung.  ''[http://iest2.ie.cuhk.edu.hk/~whyeung/book/ A First Course in Information Theory]'' Kluwer Academic/Plenum Publishers, 2002.  ISBN 0-306-46791-7 * David J. C. MacKay. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1 * Raymond W. Yeung.  ''[http://iest2.ie.cuhk.edu.hk/~whyeung/book2/ Information Theory and Network Coding]'' Springer 2008, 2002.  ISBN 978-0-387-79233-0 * Stanford Goldman. ''Information Theory''. New York: Prentice Hall, 1953. New York: Dover 1968 ISBN 0-486-62209-6, 2005 ISBN 0-486-44271-3 * [[Fazlollah Reza]]. ''An Introduction to Information Theory''. New York: McGraw-Hill 1961. New York: Dover 1994. ISBN 0-486-68210-2 * Masud Mansuripur. ''Introduction to Information Theory''. New York: Prentice Hall, 1987. ISBN 0-13-484668-0 * Christoph Arndt: ''Information Measures, Information and its Description in Science and Engineering'' (Springer Series: Signals and Communication Technology), 2004, ISBN 978-3-540-40855-0  ===Other books=== * Leon Brillouin, ''Science and Information Theory'', Mineola, N.Y.: Dover, [1956, 1962] 2004. ISBN 0-486-43918-6 * [[James Gleick]], ''[[The Information: A History, a Theory, a Flood]]'', New York: Pantheon, 2011. ISBN 978-0-375-42372-7 * A. I. Khinchin, ''Mathematical Foundations of Information Theory'', New York: Dover, 1957. ISBN 0-486-60434-9 * H. S. Leff and A. F. Rex, Editors, ''Maxwell's Demon: Entropy, Information, Computing'', [[Princeton University Press]], Princeton, NJ (1990). ISBN 0-691-08727-X * Tom Siegfried, ''The Bit and the Pendulum'', Wiley, 2000. ISBN 0-471-32174-5 * Charles Seife, ''Decoding The Universe'', Viking, 2006. ISBN 0-670-03441-X * Jeremy Campbell, ''Grammatical Man'', Touchstone/Simon & Schuster, 1982, ISBN 0-671-44062-4 * Henri Theil, ''Economics and Information Theory'', Rand McNally & Company - Chicago, 1967. * Escolano, Suau, Bonev, ''Information Theory in Computer Vision and Pattern Recognition'', Springer, 2009. ISBN 978-1-84882-296-2 [http://www.springer.com/computer/image processing/book/978-1-84882-296-2]  ==External links== * [http://alum.mit.edu/www/toms/paper/primer alum.mit.edu], Eprint, Schneider, T. D., "Information Theory Primer" * [http://www.nd.edu/~jnl/ee80653/tutorials/sunil.pdf ND.edu], Srinivasa, S. "A Review on Multivariate Mutual Information" * [http://jchemed.chem.wisc.edu/Journal/Issues/1999/Oct/abs1385.html Chem.wisc.edu], Journal of Chemical Education, ''Shuffled Cards, Messy Desks, and Disorderly Dorm Rooms - Examples of Entropy Increase? Nonsense!'' * [http://www.itsoc.org/index.html ITsoc.org], IEEE Information Theory Society and [http://www.itsoc.org/review.html ITsoc.org] review articles * [http://www.inference.phy.cam.ac.uk/mackay/itila/ Cam.ac.uk], On-line textbook: "Information Theory, Inference, and Learning Algorithms" by [[David MacKay (scientist)|David MacKay]] - giving an entertaining and thorough introduction to Shannon theory, including state-of-the-art methods from coding theory, such as [[arithmetic coding]], [[low-density parity-check code]]s, and [[Turbo code]]s. * [http://research.umbc.edu/~erill/Documents/Introduction_Information_Theory.pdf UMBC.edu], Eprint, Erill, I., "A gentle introduction to information content in transcription factor binding sites"  {{Cybernetics}} {{Compression Methods}} {{Mathematics-footer}}  {{DEFAULTSORT:Information Theory}} [[Category:Communication]] [[Category:Cybernetics]] [[Category:Formal sciences]] [[Category:Information Age]] [[Category:Information theory|*]]  [[af:Inligtingsteorie]] [[ar:نظرية المعلومات]] [[bn:তথ্য তত্ত্ব]] [[bg:Теория на информацията]] [[bar:Informationstheorie]] [[ca:Teoria de la informació]] [[cs:Teorie informace]] [[da:Informationsteori]] [[de:Informationstheorie]] [[et:Informatsiooniteooria]] [[el:Θεωρία Πληροφορίας]] [[es:Teoría de la información]] [[fa:نظریه اطلاعات]] [[fr:Théorie de l'information]] [[gl:Teoría da información]] [[ko:정보 이론]] [[hr:Teorija informacije]] [[io:Informo-teorio]] [[id:Teori informasi]] [[it:Teoria dell'informazione]] [[he:תורת האינפורמציה]] [[ka:ინფორმაციის თეორია]] [[lv:Informācijas teorija]] [[lt:Informacijos teorija]] [[hu:Információelmélet]] [[ms:Teori maklumat]] [[nl:Informatietheorie]] [[ja:情報理論]] [[no:Informasjonsteori]] [[nn:Informasjonsteori]] [[pl:Teoria informacji]] [[pt:Teoria da informação]] [[ru:Теория информации]] [[simple:Information theory]] [[sk:Teória informácie]] [[sl:Teorija informacij]] [[ckb:بیردۆزی زانیاری]] [[sr:Теорија информације]] [[fi:Informaatioteoria]] [[sv:Informationsteori]] [[ta:தகவல் கோட்பாடு]] [[th:ทฤษฎีข้อมูล]] [[tr:Bilgi kuramı]] [[uk:Теорія інформації]] [[ur:نظریۂ اطلاعات]] [[vi:Lý thuyết thông tin]] [[zh:信息论]]
{{pp-semi|small=yes}} {{redirect|I/O}} {{about||uses of the term input-output in economics|Input-output model}} {{refimprove|date=June 2010}}  In [[computing]], '''input/output''' or '''I/O''' is the communication between an [[information processing system]] (such as a [[computer]]) and the outside world, possibly a human or another information processing system. [[Information|Inputs]] are the signals or data received by the system, and [[output]]s are the signals or data sent from it. The term can also be used as part of an action; to "perform I/O" is to perform an [[I/O scheduling|input or output operation]]. I/O devices are used by a person (or other system) to communicate with a computer. For instance, a [[computer keyboard|keyboard]] or a [[computer mouse|mouse]] may be an input device for a computer, while [[computer monitor|monitor]]s and [[computer printer|printer]]s are considered output devices for a computer. Devices for communication between computers, such as [[modem]]s and [[network card]]s, typically serve for both input and output.  Note that the designation of a device as either input or output depends on the perspective. Mouse and keyboards take as input physical movement that the human user outputs and convert it into signals that a computer can understand. The output from these devices is input for the computer. Similarly, printers and monitors take as input signals that a computer outputs. They then convert these signals into representations that human users can see or read. For a human user the process of reading or seeing these representations is receiving input. These interactions between computers and humans is studied in a field called [[human–computer interaction]].  In computer architecture, the combination of the [[Central processing unit|CPU]] and [[main memory]] (i.e. memory that the CPU can read and write to directly, with individual [[instruction (computer science)|instructions]]) is considered the brain of a computer, and from that point of view any transfer of information from or to that combination, for example to or from a [[disk drive]], is considered I/O.  The CPU and its supporting circuitry provide [[memory-mapped I/O]] that is used in low-level [[computer programming]], such as the implementation of [[device driver]]s.  An [[External memory algorithm|I/O algorithm]] is one designed to exploit locality and perform efficiently when data reside on secondary storage, such as a disk drive.  ==Interface==  An I/O interface is required whenever the I/O device is driven by the processor. The interface must have necessary logic to interpret the device address generated by the processor. [[Handshaking]] should be implemented by the interface using appropriate commands (like BUSY, READY, and WAIT), and the processor can communicate with an I/O device through the interface. If different data formats are being exchanged, the interface must be able to convert serial data to parallel form and vice-versa. There must be provision for generating [[interrupt]]s and the corresponding type numbers for further processing by the processor if required.  A computer that uses [[memory-mapped I/O]] accesses hardware by reading and writing to specific memory locations, using the same assembly language instructions that computer would normally use to access memory.  === Higher-level implementation === Higher-level [[operating system]] and programming facilities employ separate, more abstract I/O concepts and [[primitive (computer science)|primitive]]s. For example, most operating systems provide application programs with the concept of [[computer file|file]]s. The [[C (programming language)|C]] and [[C  ]] programming languages, and operating systems in the [[Unix]] family, traditionally abstract files and devices as [[stream (computing)|stream]]s, which can be read or written, or sometimes both. The [[C standard library]] provides [[C file input/output|functions for manipulating streams]] for input and output.  In the context of the [[ALGOL 68]] programming language, the ''input'' and ''output'' facilities are collectively referred to as ''transput''.  The ''ALGOL 68'' transput library recognizes the following standard files/devices: <code>stand in</code>, <code>stand out</code>, <code>stand errors</code> and <code>stand back</code>.   An alternative to special primitive functions is the I/O [[Monad (functional programming)|monad]], which permits programs to just describe I/O, and the actions are carried out outside the program. This is notable because the {{nowrap|I/O}} functions would introduce [[side-effect (computer science)|side-effect]]s to any programming language, but this allows [[purely functional]] programming to be practical.  ==Addressing mode==  There are many ways through which data can be read or stored in the memory. Each method is an addressing mode, and has its own advantages and limitations.  There are many type of addressing modes such as direct addressing, indirect addressing, immediate addressing, index addressing, based addressing, based-index addressing, implied addressing, etc.  ===Direct addressing=== In this type of address of the data is a part of the instructions itself. When the processor interprets the instruction, it gets the memory address from where it can be read/written the required information. For example:<ref>[http://books.google.com/books?id=pbB8Z1ewwEgC&lpg=PA67&ots=pq2zZDeqdf&dq=dereference%20operator%20assembly&hl=th&pg=PA66#v=onepage&q&f=false LINUX assembly language programming]</ref>  <source lang="asm"> MOV register, [address] ; to read MOV [address], register ; to write  ; similarly IN  register, [address] ; to read as input OUT [address], register ; to write as output </source>  Here the <var>address</var> operand points to a memory location which holds the data and copies it into/from the specified <var>register</var>. A pair of brackets is a [[dereference operator]].  ===Indirect addressing===  According to the above example, the <var>address</var> can be stored in another register. Therefore, the instructions will have the register representing the address. So to fetch the data, the instruction must be interpreted appropriate register selected. The value of the register will be used for addressing appropriate memory location and then data will be read/written. This addressing method has an advantage against the direct mode that the register value is changeable so the appropriate memory location can also be dynamically selected.  == Port-mapped I/O ==  [[Port-mapped I/O]] usually requires the use of instructions which are specifically designed to perform I/O operations.  ==See also== *[[BASIC#Input and output]] *[[C file input/output]]  == References == {{Reflist}}  [[Category:Input/output| ]]  [[af:Toevoer/afvoer]] [[ar:وحدات الإدخال و الإخراج]] [[bs:Ulaz/izlaz]] [[ca:Entrada/sortida]] [[cs:Vstup/výstup]] [[da:I/O]] [[de:Eingabe und Ausgabe]] [[et:Sisend/väljund]] [[es:Entrada/salida]] [[eu:Sarrera-irteerako unitate]] [[fr:Entrées-sorties]] [[ko:입출력]] [[hr:Ulazno/izlazne jedinice]] [[id:I/O]] [[it:Input/output]] [[nl:I/O]] [[ja:入出力]] [[no:I/O]] [[pl:Urządzenie wejścia-wyjścia]] [[pt:Entrada/saída]] [[ru:Ввод/вывод]] [[simple:Input/output]] [[sl:Vhodno-izhodna enota]] [[sh:Ulaz/Izlaz]] [[fi:Siirräntä]] [[sv:I/O]] [[ta:உள்ளீடு/வெளியீடு]] [[th:อินพุต/เอาต์พุต]] [[tr:Giriş/çıkış]] [[zh:I/O]]
{{copy edit|date=July 2012}} {{technical|date=May 2012}} {{Use dmy dates|date=May 2012}}  {{Redirect|Silicon chip|the electronics magazine|Silicon Chip}}{{Redirect|Microchip||Microchip (disambiguation)}}  [[File:Microchips.jpg|right|thumb|220px|Wide angle shot of the memory microchip shown in detail below. The microchips have a transparent window, showing the integrated circuit inside. The window allows the memory contents of the chip to be erased, by exposure to strong [[ultraviolet light]] in an eraser device.]] [[File:EPROM Microchip SuperMacro.jpg|right|thumb|220px|Integrated circuit from an [[EPROM]] memory microchip showing the memory blocks, the supporting circuitry and the fine silver wires which connect the integrated circuit die to the legs of the packaging.]] An '''integrated circuit''' or '''monolithic integrated circuit''' (also referred to as '''IC''', '''chip''', or '''microchip''') is an [[electronic circuit]] manufactured by [[lithography]], or the patterned diffusion of trace [[Chemical element|elements]] into the surface of a thin [[Substrate (electronics)|substrate]] of [[semiconductor]] material. Additional materials are deposited and patterned to form interconnections between semiconductor devices.  Integrated circuits are used in virtually all electronic equipment today and have revolutionized the world of [[electronics]]. Computers, mobile phones, and other digital [[home appliances]] are now inextricable parts of the structure of modern societies, made possible by the low cost of producing integrated circuits.  ==Introduction== [[Image:Siliconchip by shapeshifter.png|right|thumb|200px|Synthetic detail of an integrated circuit through four layers of planarized copper interconnect, down to the polysilicon (pink), wells (greyish), and substrate (green)]] ICs were made possible by experimental discoveries showing that [[semiconductor device]]s could perform the functions of [[vacuum tube]]s and by mid-20th-century technology advancements in [[semiconductor fabrication|semiconductor device fabrication]]. The integration of large numbers of tiny [[transistor]]s into a small chip was an enormous improvement over the manual assembly of circuits using discrete [[electronic component]]s. The integrated circuit's [[mass production]] capability, reliability, and building-block approach to [[Integrated circuit design|circuit design]] ensured the rapid adoption of standardized Integrated Circuits in place of designs using discrete transistors.  There are two main advantages of ICs over [[discrete circuit]]s: cost and performance. Cost is low because the chips, with all their components, are printed as a unit by [[photolithography]] rather than being constructed one transistor at a time. Furthermore, much less material is used to construct a packaged IC die than to construct a discrete circuit. Performance is high because the components switch quickly and consume little power (compared to their discrete counterparts) as a result of the small size and close proximity of the components. As of 2012, typical chip areas range from a few square millimeters to around 450&nbsp;mm<sup>2</sup>, with up to 9 million [[transistor]]s per mm<sup>2</sup>.  ==Terminology== ''Integrated circuit'' originally referred to a miniaturized [[electronic circuit]] consisting of [[semiconductor device]]s, as well as [[passive component]]s bonded to a substrate or circuit board.<ref>{{cite web |title=The first integrated circuits |url=http://homepages.nildram.co.uk/~wylie/ICs/ICs.htm |author=Andrew Wylie |year=2009 |accessdate=14 March 2011}}</ref> This configuration is now commonly referred to as a [[hybrid integrated circuit]]. ''Integrated circuit'' has since come to refer to the single-piece circuit construction originally known as a ''monolithic integrated circuit''.<ref>{{cite web |title=The first monolithic integrated circuits |url=http://homepages.nildram.co.uk/~wylie/ICs/monolith.htm |quote=Nowadays when people say 'integrated circuit' they usually mean a monolithic IC, where the entire circuit is constructed in a single piece of silicon. |author=Andrew Wylie |year=2009 |accessdate=14 March 2011}}</ref>  ==Invention== Early developments of the integrated circuit go back to 1949, when the German engineer [[Werner Jacobi]] ([[Siemens AG|Siemens&nbsp;AG]]) [http://integratedcircuithelp.com/invention.htm] filed a patent for an integrated-circuit-like semiconductor amplifying device<ref name="jacobi1949">{{patent|DE|833366|W. Jacobi/SIEMENS AG: „Halbleiterverstärker“ priority filing on 14 April 1949, published on 15 May 1952.}}</ref> showing five transistors on a common substrate in a 2-stage [[amplifier]] arrangement. Jacobi disclosed small and cheap [[hearing aid]]s as typical industrial applications of his patent. A commercial use of his patent has not been reported.  The idea of the integrated circuit was conceived by a radar scientist working for the [[Royal Radar Establishment]] of the British [[Ministry of Defence (United Kingdom)|Ministry of Defence]], [[Geoffrey Dummer|Geoffrey W.A. Dummer]] (1909–2002). Dummer presented the idea to the public at the Symposium on Progress in Quality Electronic Components in [[Washington, D.C.|Washington,&nbsp;D.C.]] on 7 May 1952.<ref>[http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html "The Hapless Tale of Geoffrey Dummer"], (n.d.), (HTML), ''Electronic Product News'', accessed 8 July 2008.</ref> He gave many symposia publicly to propagate his ideas, and unsuccessfully attempted to build such a circuit in 1956.  A precursor idea to the IC was to create small ceramic squares (wafers), each one containing a single miniaturized component. Components could then be integrated and wired into a bidimensional or tridimensional compact grid. This idea, which looked very promising in 1957, was proposed to the US Army by [[Jack Kilby]], and led to the short-lived Micromodule Program (similar to 1951's [[Project Tinkertoy]]).<ref>George Rostky, (n. d.), [http://www.eetimes.com/special/special_issues/millennium/milestones/kilby.html "Micromodules: the ultimate package"], (HTML), ''EE Times'', accessed 8 July 2008.</ref> However, as the project was gaining momentum, Kilby came up with a new, revolutionary design: the&nbsp;IC.  [[Robert Noyce]] credited [[Kurt Lehovec]] of [[Sprague Electric]] for the ''principle of [[p-n junction isolation|p-n&nbsp;junction isolation]]'' caused by the action of a biased p-n&nbsp;junction (the diode) as a key concept behind the&nbsp;IC.<ref>Kurt Lehovec's patent on the isolation p-n junction: {{US patent|3029366}} granted on 10 April 1962, filed 22 April 1959. Robert Noyce credits Lehovec in his article – "Microelectronics", ''[[Scientific American]]'', September 1977, Volume 23, Number 3, pp. 63–9.</ref> [[Image:Kilby solid circuit.jpg|thumb|right|[[Jack Kilby]]'s original integrated circuit]]  Newly employed by [[Texas Instruments]], Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.<ref name="TIJackBuilt">[http://www.ti.com/corp/docs/kilbyctr/jackbuilt.shtml ''The Chip that Jack Built''], (c. 2008), (HTML), Texas Instruments, Retrieved 29 May 2008.</ref> In his patent application of 6 February 1959, Kilby described his new device as “a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated.”<ref>Winston, Brian. [http://books.google.com/books?id=gfeCXlElJTwC&pg=RA2-PA221&dq=%22wherein all the components of the electronic circuit%22&lr=&client=firefox#v=onepage&q=%22wherein%20all%20the%20components%20of%20the%20electronic%20circuit%22&f=false ''Media technology and society: a history: from the telegraph to the Internet''], (1998), Routeledge, London, ISBN 0-415-14230-X ISBN 978-0-415-14230-4, p. 221</ref> The first customer for the new invention was the [[US Air Force]]<ref>http://www.ti.com/corp/docs/company/history/timeline/defense/1960/docs/61-first_ic.htm</ref>.  Kilby won the 2000 Nobel Prize in Physics for his part of the invention of the integrated circuit.<ref>Nobel Web AB, (10 October 2000),([http://nobelprize.org/nobel_prizes/physics/laureates/2000/press.html ''The Nobel Prize in Physics 2000''], Retrieved 29 May 2008</ref> Kilby's work was named an [[List of IEEE milestones|IEEE Milestone]] in 2009.<ref>{{cite web |url=http://www.ieeeghn.org/wiki/index.php/Milestones:First_Semiconductor_Integrated_Circuit_%28IC%29,_1958 |title=Milestones:First Semiconductor Integrated Circuit (IC), 1958 |work=IEEE Global History Network |publisher=IEEE |accessdate=3 August 2011}}</ref>  Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at [[Fairchild Semiconductor]], it was made of [[silicon]], whereas Kilby's chip was made of [[germanium]].  [[Fairchild Semiconductor]] was also home of the first silicon gate IC technology with [[self-aligned gate]]s, which stands as the basis of all modern CMOS computer chips. The technology was developed by Italian physicist [[Federico Faggin]] in 1968, who later joined Intel in order to develop the very first Central Processing Unit (CPU) on one chip ([[Intel 4004]]), for which he received the [[National Medal of Technology and Innovation]] in 2010.  ==Generations== In the early days of integrated circuits, only a few transistors could be placed on a chip, as the scale used was large because of the contemporary technology, and manufacturing yields were low by today's standards. As the degree of integration was small, the design was done easily. Over time, millions, and today billions,<ref>Peter Clarke, ''Intel enters billion-transistor processor era'', [http://www.eetimes.com/news/latest/showArticle.jhtml?articleID=172301051 EE Times, 14 October 2005]</ref> of transistors could be placed on one chip, and to make a good design became a task to be planned thoroughly. This gave rise to new [[Y diagram|design methods]].  ===SSI, MSI and LSI {{Anchor|SSI, MSI and LSI|SSI|MSI|LSI}}===<!-- This section is linked from [[PDP-11]] and Computer fan-->  The first integrated circuits contained only a few transistors. Called "'''small-scale integration'''" ('''SSI'''), digital circuits containing transistors numbering in the tens provided a few logic gates for example, while early linear ICs such as the [[Plessey]] SL201 or the [[Philips]] TAA320 had as few as two transistors. The term Large Scale Integration was first used by [[IBM]] scientist [[Rolf Landauer]] when describing the theoretical concept{{Citation needed|date=March 2011}}, from there came the terms for SSI, MSI, VLSI, and ULSI.  SSI circuits were crucial to early aerospace projects, and aerospace projects helped inspire development of the technology. Both the [[Minuteman missile]] and [[Apollo program]] needed lightweight digital computers for their inertial guidance systems; the [[Apollo guidance computer]] led and motivated the integrated-circuit technology,<ref>{{cite book |title= Digital Apollo: Human and Machine in Spaceflight|last= Mindell|first= David A. |year=2008 |publisher= The MIT Press |isbn=978-0-262-13497-2}}</ref> while the Minuteman missile forced it into mass-production. The Minuteman missile program and various other Navy programs accounted for the total $4 million integrated circuit market in 1962, and by 1968, U.S. Government space and defense spending still accounted for 37% of the $312 million total production. The demand by the U.S. Government supported the nascent integrated circuit market until costs fell enough to allow firms to penetrate the industrial and eventually the consumer markets. The average price per integrated circuit dropped from $50.00 in 1962 to $2.33 in 1968.<ref>Ginzberg, E., Kuhn, J.W., Schnee, J., & Yavitz, B. (1975). Economic Impact of Large Public Programs: The NASA Experience. (pp. 57–60). Salt Lake City, U.S.: Olympus Publishing Company. ISBN 0-913420-68-9</ref> Integrated circuits began to appear in consumer products by the turn of the decade, a typical application being [[Frequency modulation|FM]] inter-carrier sound processing in television receivers.  The next step in the development of integrated circuits, taken in the late 1960s, introduced devices which contained hundreds of transistors on each chip, called "'''medium-scale integration'''" ('''MSI''').  They were attractive economically because while they cost little more to produce than SSI devices, they allowed more complex systems to be produced using smaller circuit boards, less assembly work (because of fewer separate components), and a number of other advantages.  Further development, driven by the same economic factors, led to "'''large-scale integration'''" ('''LSI''') in the mid 1970s, with tens of thousands of transistors per chip.  Integrated circuits such as 1K-bit RAMs, calculator chips, and the first microprocessors, that began to be manufactured in moderate quantities in the early 1970s, had under 4000 transistors. True LSI circuits, approaching 10,000 transistors, began to be produced around 1974, for computer main memories and second-generation microprocessors.  ===VLSI=== {{Main|Very-large-scale integration}} [[Image:80486DX2 200x.png|right|thumb|Upper interconnect layers on an [[Intel 80486]]DX2 microprocessor die]]  The final step in the development process, starting in the 1980s and continuing through the present, was "very large-scale integration" ([[VLSI]]). The development started with hundreds of thousands of transistors in the early 1980s, and continues beyond several billion transistors as of 2009.  Multiple developments were required to achieve this increased density. Manufacturers moved to smaller design rules and cleaner fabrication facilities, so that they could make chips with more transistors and maintain adequate yield. The path of process improvements was summarized by the [[International Technology Roadmap for Semiconductors]] (ITRS). [[Electronic Design Automation|Design tools]] improved enough to make it practical to finish these designs in a reasonable time. The more energy efficient [[CMOS]] replaced [[NMOS logic|NMOS]] and [[PMOS logic|PMOS]], avoiding a prohibitive increase in power consumption.  In 1986 the first one megabit [[Random Access Memory|RAM]] chips were introduced, which contained more than one million transistors. Microprocessor chips passed the million transistor mark in 1989 and the billion transistor mark in 2005.<ref>Peter Clarke, EE Times: ''Intel enters billion-transistor processor era'', 14 November 2005</ref> The trend continues largely unabated, with chips introduced in 2007 containing tens of billions of memory transistors.<ref>Antone Gonsalves, ''EE Times'', "Samsung begins production of 16-Gb flash", 30 April 2007</ref>  ===ULSI, WSI, SOC and 3D-IC=== {{unreferenced|section|date=April 2012}} To reflect further growth of the complexity, the term ''ULSI'' that stands for "ultra-large-scale integration" was proposed for chips of complexity of more than 1 million transistors.  [[Wafer-scale integration]] (WSI) is a system of building very-large integrated circuits that uses an entire silicon wafer to produce a single "super-chip". Through a combination of large size and reduced packaging, WSI could lead to dramatically reduced costs for some systems, notably massively parallel supercomputers. The name is taken from the term Very-Large-Scale Integration, the current state of the art when WSI was being developed.  A [[system-on-a-chip]] (SoC or SOC) is an integrated circuit in which all the components needed for a computer or other system are included on a single chip. The design of such a device can be complex and costly, and building disparate components on a single piece of silicon may compromise the efficiency of some elements. However, these drawbacks are offset by lower manufacturing and assembly costs and by a greatly reduced power budget: because signals among the components are kept on-die, much less power is required (see [[#Packaging|Packaging]]).  A [[three-dimensional integrated circuit]] (3D-IC) has two or more layers of active electronic components that are integrated both vertically and horizontally into a single circuit. Communication between layers uses on-die signaling, so power consumption is much lower than in equivalent separate circuits. Judicious use of short vertical wires can substantially reduce overall wire length for faster operation.  ==Advances in integrated circuits== {{refimprove|section|date=April 2012}} [[Image:153056995 5ef8b01016 o.jpg|right|thumb|200px|The [[die (integrated circuit)|die]] from an Intel [[Intel MCS-48|8742]], an 8-bit [[microcontroller]] that includes a [[CPU]] running at 12 MHz, 128 bytes of [[RAM]], 2048 bytes of [[EPROM]], and [[Input/output|I/O]] in the same chip]] Among the most advanced integrated circuits are the [[microprocessor]]s or "'''cores'''", which control everything from computers and cellular phones to digital [[microwave oven]]s. Digital [[Random access memory|memory chips]] and [[Application-specific integrated circuit|ASICs]] are examples of other families of integrated circuits that are important to the modern [[information society]]. While the cost of [[Integrated circuit design|designing]] and developing a complex integrated circuit is quite high, when spread across typically millions of production units the individual IC cost is minimized. The performance of ICs is high because the small size allows short traces which in turn allows low [[Electric power|power]] logic (such as [[CMOS]]) to be used at fast switching speeds.  ICs have consistently migrated to smaller feature sizes over the years, allowing more circuitry to be packed on each chip. This increased capacity per unit area can be used to decrease cost and/or increase functionality—see [[Moore's law]] which, in its modern interpretation, states that the number of transistors in an integrated circuit doubles every two years. In general, as the feature size shrinks, almost everything improves—the cost per unit and the switching power consumption go down, and the speed goes up. However, ICs with [[nanometer]]-scale devices are not without their problems, principal among which is leakage current (see [[subthreshold leakage]] for a discussion of this), although these problems are not insurmountable and will likely be solved or at least ameliorated by the introduction of [[high-k Dielectric|high-k dielectrics]]. Since these speed and power consumption gains are apparent to the end user, there is fierce competition among the manufacturers to use finer geometries. This process, and the expected progress over the next few years, is well described by the [[International Technology Roadmap for Semiconductors]] (ITRS).  In current research projects, integrated circuits are also developed for [[sensor]]ic applications in [[implant (medicine)|medical implants]] or other [[bioelectronics|bioelectronic]] devices. Particular sealing strategies have to be taken in such biogenic environments to avoid [[corrosion]] or [[biodegradation]] of the exposed semiconductor materials.<ref name= Graham2011>{{cite journal | author = A.H.D. Graham, J. Robbins, C.R. Bowen, J. Taylor | title = Commercialisation of CMOS Integrated Circuit Technology in Multi-Electrode Arrays for Neuroscience and Cell-Based Biosensors | journal = Sensors | volume = 11 | pages = 4943–4971 | year = 2011 | doi = 10.3390/s110504943 }}</ref> As one of the few materials well established in [[CMOS]] technology, [[titanium nitride]] (TiN) turned out as exceptionally stable and well suited for electrode applications in [[medical implants]].<ref name= Haemmerle2002>{{cite journal | author = H. Hämmerle, K. Kobuch, K. Kohler, W. Nisch, H. Sachs, M. Stelzle | title = Biostability of micro-photodiode arrays for subretinal implantation | journal = Biomat. | volume = 23 | pages = 797–804 | year = 2002 | doi = 10.1016/S0142-9612(01)00185-5 }}</ref><ref name= MBSCT2010>{{cite journal | author = M. Birkholz, K.-E. Ehwald, D. Wolansky, I. Costina, C. Baristiran-Kaynak, M. Fröhlich, H. Beyer, A. Kapp, F. Lisdat | title = Corrosion-resistant metal layers from a CMOS process for bioelectronic applications | journal = Surf. Coat. Technol. | volume = 204 | pages = 2055–2059 | year = 2010 | doi = 10.1016/j.surfcoat.2009.09.075 | url=http://www.mariobirkholz.de/SCT2010.pdf}}</ref>  ==Classification== {{unreferenced|section|date=April 2012}} [[Image:cmosic.JPG|thumb|A [[CMOS]] [[4000 series|4000]] IC in a [[Dual in-line package|DIP]]]] Integrated circuits can be classified into [[analog circuit|analog]], [[digital circuit|digital]] and [[mixed-signal integrated circuit|mixed signal]] (both analog and digital on the same chip).  Digital integrated circuits can contain anything from one to millions of [[logic gate]]s, [[flip-flop (electronics)|flip-flops]], [[multiplexer]]s, and other circuits in a few square millimeters. The small size of these circuits allows high speed, low power dissipation, and reduced [[manufacturing cost]] compared with board-level integration. These digital ICs, typically [[microprocessor]]s, [[digital signal processors|DSPs]], and micro controllers, work using binary mathematics to process "one" and "zero" signals.  Analog ICs, such as sensors, [[power network design (IC)|power management circuits]], and [[operational amplifier]]s, work by processing continuous signals. They perform functions like [[Amplifier|amplification]], [[active filter]]ing, [[demodulation]], and [[Frequency mixer|mixing]]. Analog ICs ease the burden on circuit designers by having expertly designed analog circuits available instead of designing a difficult analog circuit from scratch.  ICs can also combine analog and digital circuits on a single chip to create functions such as [[analog-to-digital converter|A/D converters]] and [[digital-to-analog converter|D/A converters]]. Such circuits offer smaller size and lower cost, but must carefully account for signal interference.  ==Manufacturing==  ===Fabrication=== {{Main|Semiconductor fabrication}}  [[Image:Silicon chip 3d.png|right|thumb|200px|Rendering of a small [[standard cell]] with three metal layers ([[dielectric]] has been removed). The sand-colored structures are metal interconnect, with the vertical pillars being contacts, typically plugs of tungsten. The reddish structures are polysilicon gates, and the solid at the bottom is the [[Monocrystalline silicon|crystalline silicon]] bulk.]]  [[File:Cmos-chip structure in 2000s (en).svg|right|thumb|Schematic structure of a CMOS chip, as built in the early 2000s. The graphic shows LDD-MISFET's on an SOI substrate with five metallization layers and solder bump for flip-chip bonding. It also shows the section for FEOL (front-end of line), BEOL (back-end of line) and first parts of back-end process.]]  The [[semiconductor]]s of the [[periodic table]] of the [[chemical element]]s were identified as the most likely materials for a ''[[solid-state (electronics)|solid-state]] [[vacuum tube]]''. Starting with [[copper(I) oxide|copper oxide]], proceeding to [[germanium]], then [[silicon]], the materials were systematically studied in the 1940s and 1950s. Today, silicon [[monocrystal]]s are the main [[Substrate (printing)|substrate]] used for ICs although some III-V compounds of the periodic table such as [[gallium arsenide]] are used for specialized applications like [[LEDs]], [[lasers]], [[solar cells]] and the highest-speed integrated circuits. It took decades to perfect methods of creating [[crystal]]s without defects in the [[crystalline structure]] of the semiconducting material.  [[Semiconductor]] ICs are fabricated in a layer process which includes these key process steps: * Imaging * Deposition * Etching  The main process steps are supplemented by doping and cleaning.  [[Monocrystalline silicon|Mono-crystal silicon]] [[wafer (electronics)|wafers]] (or for special applications, [[silicon on sapphire]] or [[gallium arsenide]] wafers) are used as the ''substrate''. [[Photolithography]] is used to mark different areas of the substrate to be [[Doping (Semiconductors)|doped]] or to have polysilicon, insulators or metal (typically aluminium) tracks deposited on them. * Integrated circuits are composed of many overlapping layers, each defined by photolithography, and normally shown in different colors. Some layers mark where various dopants are diffused into the substrate (called diffusion layers), some define where additional ions are implanted (implant layers), some define the conductors (polysilicon or metal layers), and some define the connections between the conducting layers (via or contact layers). All components are constructed from a specific combination of these layers. * In a self-aligned [[CMOS]] process, a [[transistor]] is formed wherever the gate layer (polysilicon or metal) crosses a diffusion layer. * [[capacitor|Capacitive structures]], in form very much like the parallel conducting plates of a traditional electrical capacitor, are formed according to the area of the "plates", with insulating material between the plates. Capacitors of a wide range of sizes are common on ICs. * Meandering stripes of varying lengths are sometimes used to form on-chip [[resistor]]s, though most logic circuits do not need any resistors. The ratio of the length of the resistive structure to its width, combined with its sheet resistivity, determines the resistance. * More rarely, [[inductor|inductive structures]] can be built as tiny on-chip coils, or simulated by [[gyrator]]s.  Since a CMOS device only draws current on the ''transition'' between [[boolean algebra (logic)|logic]] [[State (computer science)|states]], CMOS devices consume much less current than [[bipolar transistor|bipolar]] devices.  A [[random access memory]] is the most regular type of integrated circuit; the highest density devices are thus memories; but even a [[microprocessor]] will have memory on the chip. (See the regular array structure at the bottom of the first image.) Although the structures are intricate – with widths which have been shrinking for decades – the layers remain much thinner than the device widths. The layers of material are fabricated much like a photographic process, although light [[wave]]s in the [[visible spectrum]] cannot be used to "expose" a layer of material, as they would be too large for the features. Thus [[photon]]s of higher frequencies (typically [[ultraviolet]]) are used to create the patterns for each layer. Because each feature is so small, [[electron microscope]]s are essential tools for a [[industrial process|process]] engineer who might be [[debugging]] a fabrication process.  Each device is tested before packaging using automated test equipment (ATE), in a process known as [[wafer testing]], or wafer probing. The wafer is then cut into rectangular blocks, each of which is called a ''die''. Each good [[die (integrated circuit)|die]] (plural ''dice'', ''dies'', or ''die'') is then connected into a package using aluminium (or gold) [[bond wire]]s which are [[welding|welded]] and/or [[Thermosonic Bonding|thermosonic bonded]] to ''pads'', usually found around the edge of the die. After packaging, the devices go through final testing on the same or similar ATE used during wafer probing. [[Industrial CT scanning]] can also be used. Test cost can account for over 25% of the cost of fabrication on lower cost products, but can be negligible on low yielding, larger, and/or higher cost devices.  As of 2005, a [[Semiconductor fabrication plant|fabrication facility]] (commonly known as a ''semiconductor fab'') costs over US$1 billion to construct,<ref>For example, Intel Fab 28 cost $3.5 billion, while its neighboring Fab 18 cost $1.5 billion http://www.theinquirer.net/default.aspx?article=29958</ref> because much of the operation is automated. Today, the most advanced processes employ the following techniques: * The wafers are up to 300&nbsp;mm in diameter (wider than a common dinner plate). * Use of 32 nanometer or smaller chip manufacturing process. [[Intel]], [[IBM]], [[NEC]], and [[AMD]] are using ~32 nanometers for their [[central processing unit|CPU]] chips. IBM and AMD introduced [[immersion lithography]] for their 45&nbsp;nm processes<ref>[http://www.itjungle.com/breaking/bn121206-story03.html Breaking News-IBM, AMD Expect 45-Nanometer Chips in Mid-2008<!-- Bot generated title -->]</ref> * [[Copper interconnect]]s where copper wiring replaces aluminium for interconnects. * [[Low-K]] dielectric insulators. * [[Silicon on insulator]] (SOI) * [[Strained silicon]] in a process used by [[IBM]] known as [[strained silicon directly on insulator]] (SSDOI) * [[Multigate device]]s such as tri-gate transistors being manufactured by [[Intel]] from 2011 in their 22&nbsp;nm process.  ===Packaging=== {{Main|Integrated circuit packaging}} [[Image:RUS-IC.JPG|right|thumb|A Soviet MSI nMOS chip made in 1977, part of a four-chip calculator set designed in 1970<ref>{{cite web | url=http://www.155la3.ru/k145_3.htm#k145hk1 | title = 145 series ICs (in Russian) | accessdate=22 April 2012 }}</ref>]]  The earliest integrated circuits were packaged in ceramic flat packs, which continued to be used by the military for their reliability and small size for many years. Commercial circuit packaging quickly moved to the [[dual in-line package]] (DIP), first in ceramic and later in plastic. In the 1980s pin counts of VLSI circuits exceeded the practical limit for DIP packaging, leading to [[pin grid array]] (PGA) and [[leadless chip carrier]] (LCC) packages. [[Surface mount]] packaging appeared in the early 1980s and became popular in the late 1980s, using finer lead pitch with leads formed as either gull-wing or J-lead, as exemplified by [[small-outline integrated circuit]] – a carrier which occupies an area about 30–50% less than an equivalent [[dual in-line package|DIP]], with a typical thickness that is 70% less. This package has "gull wing" leads protruding from the two long sides and a lead spacing of 0.050&nbsp;inches.  In the late 1990s, [[PQFP|plastic quad flat pack]] (PQFP) and [[thin small-outline package]] (TSOP) packages became the most common for high pin count devices, though PGA packages are still often used for high-end [[microprocessor]]s. Intel and AMD are currently transitioning from PGA packages on high-end microprocessors to [[land grid array]] (LGA) packages.  [[Ball grid array]] (BGA) packages have existed since the 1970s. [[Flip-chip Ball Grid Array]] packages, which allow for much higher pin count than other package types, were developed in the 1990s. In an FCBGA package the die is mounted upside-down (flipped) and connects to the package balls via a package substrate that is similar to a printed-circuit board rather than by wires. FCBGA packages allow an array of input-output signals (called Area-I/O) to be distributed over the entire die rather than being confined to the die periphery.  Traces out of the die, through the package, and into the [[printed circuit board]] have very different electrical properties, compared to on-chip signals. They require special design techniques and need much more electric power than signals confined to the chip itself.  When multiple dies are put in one package, it is called SiP, for ''[[System In Package]]''. When multiple dies are combined on a small substrate, often ceramic, it's called an MCM, or [[Multi-Chip Module]]. The boundary between a big MCM and a small printed circuit board is sometimes fuzzy.  ===Chip labeling and manufacture date=== Most integrated circuits large enough to include identifying information include four common sections: the manufacturer's name or logo, the part number, a part production batch number and/or serial number, and a four-digit code that identifies when the chip was manufactured. Extremely small [[surface mount technology]] parts often bear only a number used in a manufacturer's lookup table to find the chip characteristics.  The manufacturing date is commonly represented as a two-digit year followed by a two-digit week code, such that a part bearing the code 8341 was manufactured in week 41 of 1983, or approximately in October 1983.  ==Legal protection of semiconductor chip layouts== {{Main|Integrated circuit layout design protection}} Like most of the other forms of intellectual property, IC layout designs are creations of the human mind. They are usually the result of an enormous investment, both in terms of the time of highly qualified experts, and financially. There is a continuing need for the creation of new layout-designs which reduce the dimensions of existing integrated circuits and simultaneously increase their functions. The smaller an integrated circuit, the less the material needed for its manufacture, and the smaller the space needed to accommodate it. Integrated circuits are utilized in a large range of products, including articles of everyday use, such as watches, television sets, appliances, automobiles, etc., as well as sophisticated data processing equipment.  The possibility of copying by photographing each layer of an integrated circuit and preparing [[photomasks]] for its production on the basis of the photographs obtained is the main reason for the introduction of legislation for the protection of layout-designs.  A diplomatic conference was held at Washington, D.C., in 1989, which adopted a [[Treaty on Intellectual Property in Respect of Integrated Circuits]] (IPIC Treaty).  The Treaty on Intellectual Property in respect of Integrated Circuits, also called Washington Treaty or IPIC Treaty (signed at Washington on 26 May 1989) is currently not in force, but was partially integrated into the [[TRIPS]] agreement.  National laws protecting IC layout designs have been adopted in a number of countries.  ==Other developments== In the 1980s, [[programmable logic device]]s were developed. These devices contain circuits whose logical function and connectivity can be programmed by the user, rather than being fixed by the integrated circuit manufacturer. This allows a single chip to be programmed to implement different LSI-type functions such as [[logic gate]]s, [[adder (electronics)|adders]] and [[processor register|registers]]. Current devices called [[field-programmable gate array]]s can now implement tens of thousands of LSI circuits in parallel and operate up to 1.5&nbsp;GHz (Achronix holding the speed record).  The techniques perfected by the integrated circuits industry over the last three decades have been used to create very small mechanical devices driven by electricity using a technology known as [[microelectromechanical systems]]. These devices are used in a variety of commercial and military applications. Example commercial applications include [[DLP projector]]s, [[inkjet printer]]s, and [[accelerometer]]s used to deploy automobile [[airbag]]s.  In the past, radios could not be fabricated in the same low-cost processes as microprocessors. But since 1998, a large number of radio chips have been developed using CMOS processes. Examples include Intel's DECT cordless phone, or [[Atheros]]'s 802.11 card.  Future developments seem to follow the [[multi-core]] multi-microprocessor paradigm, already used by the Intel and AMD dual-core processors. Intel recently unveiled a prototype, "not for commercial sale" chip that bears 80 microprocessors. Each core is capable of handling its own task independently of the others. This is in response to the heat-versus-speed limit that is about to be reached using existing transistor technology. This design provides a new challenge to chip programming. [[Parallel programming language]]s such as the open-source [[X10 (programming language)|X10]] programming language are designed to assist with this task.<ref>Biever, C. "Chip revolution poses problems for programmers", New Scientist (Vol 193, Number 2594)</ref>  ==Silicon labelling and graffiti== To allow identification during production most silicon chips will have a serial number in one corner. It is also common to add the manufacturer's logo. Ever since ICs were created, some chip designers have used the silicon surface area for surreptitious, non-functional images or words. These are sometimes referred to as [[Chip art|Chip Art]], ''Silicon Art'', ''Silicon Graffiti'' or ''Silicon Doodling''.  ==ICs and IC families== * The [[555 timer IC]] * The [[741 operational amplifier]] * [[7400 series]] [[Transistor-transistor logic|TTL]] logic building blocks * [[4000 series]], the [[CMOS]] counterpart to the 7400 series (see also: [[HCMOS|74HC00 series]]) * [[Intel 4004]], the world's first [[microprocessor]], which led to the famous [[Intel 8080|8080]] CPU and then the [[IBM PC]]'s [[Intel 8088|8088]], [[80286]], [[Intel i486|486]] etc. * The [[MOS Technology 6502]] and [[Zilog Z80]] microprocessors, used in many [[home computer]]s of the early 1980s * The [[Motorola 6800]] series of computer-related chips, leading to the [[68000]] and [[88000]] series (used in some [[Apple computers]] and in the 1980s Commodore [[Amiga]] series).  ==See also== {{Portal|Electronics}} ;General topics * [[Computer engineering]] * [[Electrical engineering]]  ;Related devices and terms * [[ASIC]] Application Specific Integrated Circuit * [[Clean room]] * [[Current mirror]] * [[FPGA]] Field Programmable Gate Array * [[PGA|Gate Array]]{{dn|date=June 2012}} * [[Hybrid integrated circuit]] * [[Integrated circuit design]] * [[Integrated circuit development]] * [[Ion implantation]] * [[MMIC]] * [[Master-Slice]] * [[Photonic integrated circuit]] * PLD [[Programmable Logic Device]] * [[PGA]]{{dn|date=June 2012}} Programmable Gate Array * [[Printed circuit board]] * [[Silicon photonics]] * [[Integrated circuit vacuum tube]]  ;IC device technologies * [[BCDMOS]] * [[BiCMOS]] Bipolar/CMOS mixed technology * [[Bipolar junction transistor]] * [[CMOS]] Complementary Metal-Oxid Technology * [[CSGT]] Complementary Silicon Gate Technology * [[Gallium(III) arsenide|GaAs]] Gallium-Arsenide technology * [[integrated injection logic|I²L]] Integrated Injection Logic * [[LDMOS]] * [[Logic family]] * [[Mixed-signal integrated circuit]] * [[MOSFET]] Metal-Oxid-Silicon Field Effect Transistor * [[Multi-threshold CMOS]] (MTCMOS) * [[NMOS]]{{dn|date=June 2012}} n-channel Metal-Oxid Technology * [[nSGT]] n-channel Silicon Gate Technology * [[Depletion-mode NMOS logic]] * [[PMOS]]{{dn|date=June 2012}} p-channel Metal-Oxid Technology * [[pSGT]] p-channel Silicon Gate Technology * [[SiGe]] Silicon-Germanium technology * [[SBC]]{{dn|date=June 2012}} Standard Buried Collector Technology - bipolar basic technology of the beginning  ;Other * [[Automatic test pattern generation]] * [[DatasheetArchive]] * [[Hardware description language]] * [[JTAG]] * [[LFSR]] Linear Feedback Shift-Register * [[Memristor]] * [[Microcontroller]] * [[Moore's law]] * [[RC delay]] * [[JTAG|Scan-Path]] * [[Semiconductor manufacturing]] * [[Simulation]] * [[Sound chip]] * [[SPICE]] * [[Three-dimensional integrated circuit]] * [[Zero insertion force|ZIF]]  ==References== {{Reflist|2}}  ==Further reading== * [http://www.intel.com/technology/architecture-silicon/65nm-technology/index.htm Intel 65-Nanometer Technology] * {{cite book |author= Baker, R. J. |title=CMOS: Circuit Design, Layout, and Simulation, Third Edition |publisher=Wiley-IEEE |year=2010 |isbn=978-0-470-88132-3 |oclc= }} http://CMOSedu.com/ * Hodges, D.A., Jackson H.G., and Saleh, R. (2003). ''Analysis and Design of Digital Integrated Circuits''. McGraw-Hill. ISBN 0-07-228365-3. * Rabaey, J.M., Chandrakasan, A., and Nikolic, B. (2003). ''Digital Integrated Circuits, 2nd Edition.'' ISBN 0-13-090996-3 '' * Mead, C. and Conway, L. (1980). ''Introduction to VLSI Systems''. Addison-Wesley. ISBN 0-201-04358-0. * {{cite book |author= Veendrick, H.J.M. |title=Bits on Chips |year=2011 |pages= 253|isbn=978-1-61627-947-9}} http://openlibrary.org/works/OL15759799W/Bits_on_Chips/ * [http://books.google.com/books?id=z7738Wq-j-8C&pg=PR3&dq=%22Invention Of Integrated Circuits: Untold Important Facts%22&hl=en&ei=0O_DS7PBD5KGNK64tIMO&sa=X&oi=book_result&ct=result&resnum=1&ved=0CD0Q6AEwAA#v=onepage&q&f=false ''Invention Of Integrated Circuits: Untold Important Facts''], 2009, [http://books.google.com/books?id=z7738Wq-j-8C&pg=PA523&dq=%22Dr. Arjun N. Saxena is an Emeritus Professor of the Rensselaer Polytechnic Institute%22&hl=en&ei=xPjDS66iEIOANrHgjMUO&sa=X&oi=book_result&ct=result&resnum=1&ved=0CDYQ6AEwAA#v=onepage&q=%22Dr.%20Arjun%20N.%20Saxena%20is%20an%20Emeritus%20Professor%20of%20the%20Rensselaer%20Polytechnic%20Institute%22&f=false Arjun N. Saxena], World Scientific Publishing, Singapore, ISBN 978-981-281-445-6 ISBN 981-281-445-0 * {{cite book |title=Bits on Chips |year=2011 |author= Veendrick, H.J.M. |pages= 253|isbn=978-1-61627-947-9 |oclc= }}http://openlibrary.org/works/OL15759799W/Bits_on_Chips/ {{Refend}} {{Reflist}}  ==External links== {{Commons|Integrated circuit|Integrated circuit}}  '''General''' * Krazit, Tom "[http://news.com.com/2061-10791_3-6145549.html – AMD's new 65-nanometer chips sip energy but trail Intel]," ''C-net'', 2006-12-21. Retrieved on 8 January 2007 * [http://www.classiccmp.org/rtellason/by-generic-number.html a large chart listing ICs by generic number] and [http://www.classiccmp.org/rtellason/by-mfr-number.html A larger one listing by mfr. number], both including access to most of the datasheets for the parts. * ''Practical MMIC Design'' published by Artech House ISBN 1-59693-036-5 Author S.P. Marsh * Introduction to Circuit Boards and [http://osi.parsons.edu/archive/osi2007/?q=node/15711 Integrated Circuits] 6/21/2011  '''Patents''' * {{US patent|3138743|US3,138,743}} – Miniaturized electronic circuit – [[Jack Kilby|J. S. Kilby]] * {{US patent|3138747|US3,138,747}} – Integrated semiconductor circuit device – R. F. Stewart * {{US patent|3261081|US3,261,081}} – Method of making miniaturized electronic circuits – J. S. Kilby * {{US patent|3434015|US3,434,015}} – Capacitor for miniaturized electronic circuits or the like – J. S. Kilby  '''Silicon graffiti''' * [http://www.chipworks.com/silicon_art_gallery.aspx The Chipworks silicon art gallery]  '''Integrated circuit die photographs''' * [http://diephotos.blogspot.com/ IC Die Photography] – A gallery of IC die photographs {{Digital systems}} {{DEFAULTSORT:Integrated Circuit}} [[Category:Integrated circuits| ]] [[Category:American inventions]] [[Category:Discovery and invention controversies]] [[Category:Semiconductor devices]]  [[af:Geïntegreerde stroombaan]] [[ar:دارة متكاملة]] [[bn:সমন্বিত বর্তনী]] [[be:Інтэгральная схема]] [[bg:Интегрална схема]] [[bs:Integralno kolo]] [[ca:Circuit integrat]] [[cs:Integrovaný obvod]] [[da:Integreret kredsløb]] [[de:Integrierter Schaltkreis]] [[et:Mikrokiip]] [[el:Ολοκληρωμένο κύκλωμα]] [[es:Circuito integrado]] [[eo:Integra cirkvito]] [[eu:Txip]] [[fa:تراشه]] [[fr:Circuit intégré]] [[gan:集成電路]] [[ko:집적 회로]] [[hy:Ինտեգրալ սխեմա]] [[hi:एकीकृत परिपथ]] [[hr:Integrirani krug]] [[id:Sirkuit terpadu]] [[it:Circuito integrato]] [[he:מעגל משולב]] [[kk:Программаланатын үлкен интегралдық схема]] [[ht:Sikui entegre]] [[la:Circuitus integratus]] [[lv:Integrālā shēma]] [[lt:Integrinis grandynas]] [[hu:Integrált áramkör]] [[mk:Интегрално коло]] [[ml:ഇൻറഗ്രേറ്റഡ് സർക്യൂട്ട്]] [[ms:Litar bersepadu]] [[my:အိုင်စီပတ်လမ်း]] [[nl:Geïntegreerde schakeling]] [[ja:集積回路]] [[no:Integrert krets]] [[mhr:Интеграл микросхеме]] [[pa:ਏਕੀਕ੍ਰਿਤ ਪਰਿਪਥ]] [[pl:Układ scalony]] [[pt:Circuito integrado]] [[ro:Circuit integrat]] [[ru:Интегральная схема]] [[sq:Qarqet e integruara]] [[simple:Integrated circuit]] [[sk:Integrovaný obvod]] [[sl:Integrirano vezje]] [[sr:Интегрисано коло]] [[sh:Integralno kolo]] [[fi:Mikropiiri]] [[sv:Integrerad krets]] [[th:วงจรรวม]] [[tr:Tümdevre]] [[uk:Мікросхема]] [[vi:Vi mạch]] [[war:Sirkito integrado]] [[zh-yue:集成電路]] [[zh:集成电路]]
{{computer law}} {{Globalize/US|date=February 2012}} '''Legal aspects of computing''' are related to the over-lapping areas of [[law]] and [[computing]].   The first one, historically, was '''information technology law''' (or '''IT law'''). ''(It should not be confused with the [[legal informatics|IT aspects of law itself]], albeit there is an overlap between the two, as well)''.   '''IT Law''' is a set of [[Statutory law|legal enactments]], currently in existence in several countries, which governs the [[digital]] dissemination of both ([[digitalized]]) information and [[software]] itself (see [[History of free and open-source software]]). IT Law covers mainly the [[digital]] information (including [[information security]] and [[electronic commerce]]) aspects and it has been described as "paper laws" for a "paperless environment".  '''Cyberlaw''' or '''[[#Internet Law|Internet law]]''' is a term that encapsulates the legal issues related to use of the [[Internet]]. It is less a distinct field of law than [[intellectual property]] or [[contract]] law, as it is a domain covering many areas of law and regulation.  Some leading topics include [[Internet|internet access and usage]], [[privacy]], [[freedom of expression]], and [[jurisdiction]].  == Areas of law == {{See also|Software law}} There is [[intellectual property]] in general, including [[copyright]], rules on [[fair use]], and special rules on [[copy protection]] for digital media, and circumvention of such schemes. The area of [[software patent]]s is [[Software patent debate|controversial]], and still evolving in Europe and elsewhere.<ref>Computer Law:  Drafting and Negotiating Forms and Agreements, by [[Richard Raysman]] and Peter Brown.  Law Journal Press, 1999&ndash;2008.  ISBN 978-1-58852-024-1</ref>  The related topics of [[software license]]s, [[end user license agreement]]s, [[free software license]]s and [[open-source license]]s can involve discussion of product liability, professional liability of individual developers, warranties, contract law, trade secrets and intellectual property.  In various countries, areas of the computing and communication industries are regulated &ndash; often strictly &ndash; by government bodies.  There are rules on the uses to which computers and computer networks may be put, in particular there are rules on [[Security cracking|unauthorized access]], [[data privacy]] and [[spamming]]. There are also limits on the use of [[encryption]] and of equipment which may be used to defeat copy protection schemes. The export of Hardware and Software between certain states is also controlled.  There are laws governing trade on the Internet, taxation, consumer protection, and advertising.  There are laws on [[censorship]] versus freedom of expression, rules on public access to government information, and individual access to information held on them by private bodies. There are laws on what data must be retained for law enforcement, and what may not be gathered or retained, for privacy reasons.  In certain circumstances and jurisdictions, computer communications may be used in evidence, and to establish contracts. New methods of tapping and surveillance made possible by computers have wildly differing rules on how they may be used by law enforcement bodies and as evidence in court.  Computerized voting technology, from polling machines to internet and mobile-phone voting, raise a host of legal issues.  Some states limit access to the Internet, by law as well as by technical means.  == Jurisdiction == Issues of [[jurisdiction]] and [[sovereignty]] have quickly come to the fore in the era of the [[Internet]].  Jurisdiction is an aspect of state [[sovereignty]] and it refers to judicial, legislative and administrative competence. Although jurisdiction is an aspect of sovereignty, it is not coextensive with it. The laws of a nation may have extraterritorial impact extending the jurisdiction beyond the sovereign and territorial limits of that nation. This is particularly problematic as the medium of the Internet does not explicitly recognize sovereignty and territorial limitations. There is no uniform, international jurisdictional law of universal application, and such questions are generally a matter of [[conflict of laws]], particularly private international law. An example would be where the contents of a web site are legal in one country and illegal in another. In the absence of a uniform jurisdictional code, legal practitioners are generally left with a conflict of law issue.  Another major problem of cyberlaw lies in whether to treat the Internet as if it were physical space (and thus subject to a given jurisdiction's laws) or to act as if the Internet is a world unto itself (and therefore free of such restraints). Those who favor the latter view often feel that government should leave the Internet community to self-regulate. [[John Perry Barlow]], for example, has addressed the governments of the world and stated, "Where there are real conflicts, where there are wrongs, we will identify them and address them by our means. We are forming our own Social Contract. This governance will arise according to the conditions of our world, not yours. Our world is different".<ref>{{cite web|last=Barlow|title=[[A Declaration of the Independence of Cyberspace]]|url=http://homes.eff.org/~barlow/Declaration-Final.html}}</ref> A more balanced alternative is the Declaration of Cybersecession: "Human beings possess a mind, which they are absolutely free to inhabit with no legal constraints. Human civilization is developing its own (collective) mind. All we want is to be free to inhabit it with no legal constraints. Since you make sure we cannot harm you, you have no ethical right to intrude our lives. So stop intruding!"<ref>{{cite web|url=http://editthis.info/cybersecession/An_Introduction_to_Cybersecession|title=An Introduction to Cybersecession}}</ref> Other scholars argue for more of a compromise between the two notions, such as [[Lawrence Lessig]]'s argument that "The problem for law is to work out how the norms of the two communities are to apply given that the subject to whom they apply may be in both places at once" (Lessig, Code 190).  With the internationalism of the Internet, [[jurisdiction]] is a much more tricky area than before, and courts in different countries have taken various views on whether they have jurisdiction over items published on the Internet, or business agreements entered into over the Internet. This can cover areas from contract law, trading standards and tax, through rules on [[Security cracking|unauthorized access]], [[data privacy]] and [[spamming]] to more political areas such as freedom of speech, censorship, libel or sedition.  Certainly, the frontier idea that the law does not apply in "[[Cyberspace]]" is not true. In fact, conflicting laws from different jurisdictions may apply, simultaneously, to the same event. The Internet does not tend to make geographical and jurisdictional boundaries clear, but Internet users remain in physical jurisdictions and are subject to laws independent of their presence on the Internet.<ref>Trout, B. (2007). "Cyber Law: A Legal Arsenal For Online Business", New York: World Audience, Inc.</ref> As such, a single transaction may involve the laws of at least three jurisdictions: # the laws of the state/nation in which the user resides, # the laws of the state/nation that apply where the server hosting the transaction is located, and # the laws of the state/nation which apply to the person or business with whom the transaction takes place. So a user in one of the United States conducting a transaction with another user in Britain through a server in Canada could theoretically be subject to the laws of all three countries as they relate to the transaction at hand.<ref>Emerging Technologies and the Law:  Forms and Analysis, by [[Richard Raysman]], Peter Brown, Jeffrey D. Neuburger and William E. Bandon, III.  Law Journal Press, 2002-2008.  ISBN 1-58852-107-9</ref>  In practical terms, a user of the Internet is subject to the laws of the state or nation within which he or she goes online. Thus, in the U.S., [[Jake Baker]] faced criminal charges for his e-conduct, and numerous users of peer-to-peer [[file-sharing]] software were subject to civil lawsuits for [[copyright infringement]]. This system runs into conflicts, however, when these suits are international in nature. Simply put, legal conduct in one nation may be decidedly illegal in another. In fact, even different standards concerning the [[Legal burden of proof|burden of proof]] in a civil case can cause jurisdictional problems. For example, an American celebrity, claiming to be insulted by an online American magazine, faces a difficult task of winning a lawsuit against that magazine for [[libel]]. But if the celebrity has ties, economic or otherwise, to England, he or she can sue for libel in the British court system, where the standard of "libelous speech" is far lower.  [[Internet governance]] is a live issue in international fora such as the [[International Telecommunication Union]] (ITU), and the role of the current US-based co-ordinating body, the [[ICANN|Internet Corporation for Assigned Names and Numbers]] (ICANN) was discussed in the UN-sponsored [[World Summit on the Information Society]] (WSIS) in December 2003  ==Internet Law== If there are laws that could govern the Internet, then it appears that such laws would be fundamentally different from laws that geographic nations use today. The unique structure of the [[Internet]] has raised several [[law|judicial]] concerns. There is a substantial literature and commentary that the Internet is not only "regulable," but is already subject to substantial law regulations, both public and private, by many parties and at many different levels. Since the Internet defies geographical boundaries, national laws can not apply globally and it has been suggested instead that the Internet can be self-regulated as being its own trans-national "nation".     Since the Internet law represents a legal [[paradigm shift]], it is still in the process of development.<ref>{{cite web|url=http://www.cli.org/X0025_LBFIN.html|title=Law and Borders - The Rise of Law in Cyberspace}}</ref>   In their essay "Law and Borders -- The Rise of Law in Cyberspace", [[David R. Johnson]] and [[David G. Post]] argue that it became necessary for the Internet to govern itself and instead of obeying the laws of a particular country, "Internet citizens" will obey the laws of electronic entities like service providers. Instead of identifying as a physical person, Internet citizens will be known by their usernames or email addresses (or, more recently, by their Facebook accounts).   Leaving aside the most obvious examples of [[internet censorship]] in nations like [[China]] or [[Saudi Arabia]] or [[Iran]] (that monitor content), there are four primary modes of regulation of the internet described by [[Lawrence Lessig]] in his book, [[Code and Other Laws of Cyberspace]]: #'''Law''': Standard East Coast Code, and the most self-evident of the four modes of regulation.  As the numerous statutes, evolving case law and precedents make clear, many actions on the internet are already subject to conventional legislation (both with regard to transactions conducted on the internet and images posted).  Areas like gambling, child pornography, and fraud are regulated in very similar ways online as off-line.  While one of the most controversial and unclear areas of evolving laws is the determination of what forum has subject matter jurisdiction over activity (economic and other) conducted on the internet, particularly as cross border transactions affect local jurisdictions, it is certainly clear that substantial portions of internet activity are subject to traditional regulation, and that conduct that is unlawful off-line is presumptively unlawful online, and subject to similar laws and regulations.  Scandals with major corporations led to US legislation rethinking corporate [[governance]] regulations such as the [[Sarbanes-Oxley Act]]. #'''Architecture''': West Coast Code: these mechanisms concern the parameters of how information can and cannot be transmitted across the internet.  Everything from internet filtering software (which searches for keywords or specific URLs and blocks them before they can even appear on the computer requesting them), to encryption programs, to the very basic architecture of TCP/IP protocol, falls within this category of regulation.  It is arguable that all other modes of regulation either rely on, or are significantly supported by, regulation via West Coast Code. #'''Norms''': As in all other modes of social interaction, conduct is regulated by social norms and conventions in significant ways.  While certain activities or kinds of conduct online may not be specifically prohibited by the code architecture of the internet, or expressly prohibited by applicable law, nevertheless these activities or conduct will be invisibly regulated by the inherent standards of the community, in this case the internet "users."  And just as certain patterns of conduct will cause an individual to be ostracised from our real world society, so too certain actions will be censored or self-regulated by the norms of whatever community one chooses to associate with on the internet. #'''Markets''': Closely allied with regulation by virtue of social norms, markets also regulate certain patterns of conduct on the internet.  While economic markets will have limited influence over non-commercial portions of the internet, the internet also creates a virtual marketplace for information, and such information affects everything from the comparative valuation of services to the traditional valuation of stocks.  In addition, the increase in popularity of the internet as a means for transacting all forms of commercial activity, and as a forum for advertisement, has brought the laws of supply and demand in cyberspace.  ===Net neutrality=== Another major area of interest is [[net neutrality]], which affects the regulation of the infrastructure of the Internet.  Though not obvious to most Internet users, every packet of data sent and received by every user on the Internet passes through routers and transmission infrastructure owned by a collection of private and public entities, including telecommunications companies, universities, and governments, suggesting that the Internet is not as independent as [[John Perry Barlow|Barlow]] and others would like to believe.  This is turning into one of the most critical aspects of cyberlaw and has immediate jurisdictional implications, as laws in force in one jurisdiction have the potential to have dramatic effects in other jurisdictions when host servers or telecommunications companies are affected.  === Free speech on the Internet === Article 19 of the [[Universal Declaration of Human Rights]] calls for the protection of [[free expression]] in all media.  In comparison to traditional print-based media, the accessibility and relative anonymity of cyber space has torn down traditional barriers between an individual and his or her ability to publish. Any person with an internet connection has the potential to reach an audience of millions with little-to-no distribution costs.  Yet this new form of highly accessible authorship in cyber space raises questions and perhaps magnifies legal complexities relating to the freedom and regulation of speech in cyberspace.  These complexities have taken many forms, three notable examples being the [[Jake Baker]] incident, in which the limits of obscene Internet postings were at issue, the controversial distribution of the [[DeCSS]] code, and [[Gutnick v Dow Jones]], in which libel laws were considered in the context of online publishing.  The last example was particularly significant because it epitomized the complexities inherent to applying one country's laws (nation-specific by definition) to the internet (international by nature).  In 2003, [[Jonathan Zittrain]] considered this issue in his paper, "Be Careful What You Ask For: Reconciling a Global Internet and Local Law".<ref>{{cite web|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=395300|title=Be Careful What You Ask For: Reconciling a Global Internet and Local Law|first=Jonathan|last=Zittrain|year=2003}}</ref>  In the UK the case of [[Keith-Smith v Williams]] confirmed that existing [[libel]] laws applied to internet discussions.<ref>{{cite news|url=http://www.guardian.co.uk/law/story/0,,1737445,00.html|title=Warning to chatroom users after libel award for man labelled a Nazi|first=Owen|last=Gibson|date=March 23, 2006|publisher=The Guardian}}</ref>  In terms of the [[tort]] liability of ISPs and hosts of internet forums, Section 230(c) of the [[Communications Decency Act]] may provide immunity in the United States.<ref>{{cite journal | author = Myers KS| title=Wikimmunity: Fitting the Communications Decency Act to Wikipedia  | newspaper=[[Harvard Journal of Law & Technology]]  | volume=20 | page=163 | date=Fall 2006 | ssrn=916529}}</ref>  ===Internet censorship=== {{main|Internet censorship}} In many countries, speech through cyberspace has proven to be another means of communication which has been regulated by the government.  The "Open Net Initiative",<ref>{{cite web|url=http://www.opennetinitiative.net |title=opennetinitiative.net |publisher=opennetinitiative.net |date= |accessdate=2012-01-17}}</ref> whose mission statement is "to investigate and challenge state filtration and surveillance practices" to "...generate a credible picture of these practices," has released numerous reports documenting the filtration of internet-speech in various countries.  While [[China]] has thus far proven to be the most rigorous in its attempts to filter unwanted parts of the internet from its citizens,<ref name="ONIChina">{{cite web|url=http://www.opennetinitiative.net/studies/china/|title=All Content related to China|publisher=OpenNet Initiative}}</ref> many other countries - including [[Singapore]], [[Iran]], [[Saudi Arabia]], and [[Tunisia]] - have engaged in similar practices of [[Internet censorship]].  In one of the most vivid examples of information control, the Chinese government for a short time transparently forwarded requests to the [[Google]] search engine to its own, state-controlled search engines.  These examples of filtration bring to light many underlying questions concerning the freedom of speech. For example, does the government have a legitimate role in limiting access to information?  And if so, what forms of regulation are acceptable? For example, some argue that the blocking of "[[blogspot]]" and other websites in [[India]]  failed to reconcile the conflicting interests of speech and expression on the one hand and legitimate government concerns on the other hand.<ref>[http://www.aaronkellylaw.com/internet-law/free-speech-implications-of-blocking-blog-posts-in-india/ Free Speech Implications Of Blocking Blog Posts In India], taken from [http://www.aaronkellylaw.com/ Aaron Kelly Internet Law Firm], Retrieved December 05, 2011.</ref>  == The Creation of Privacy in U.S. Internet Law == ===Warren and Brandeis=== At the close of the 19th Century, concerns about [[privacy]] captivated the general public, and led to the 1890 publication of Samuel Warren and Louis Brandeis: "The Right to Privacy".<ref>Warren & Louis Brandeis, The Right to Privacy , 4 Harv. L. Rev. 193 (1890)</ref>  The vitality of this article can be seen today, when examining the USSC decision of [[Kyllo v. United States]], 533 U.S. 27 (2001) where it is cited by the majority, those in concurrence, and even those in dissent.<ref>Solove, D.,Schwartz, P.. (2009). Privacy, Information, and Technology. (2nd Ed.). New York, NY: Aspen Publishers. ISBN 978-0-7355-7910-1.</ref>  The motivation of both authors to write such an article is heavily debated amongst scholars, however, two developments during this time give some insight to the reasons behind it. First, the sensationalistic press and the concurrent rise and use of "[[yellow journalism]]" to promote the sale of newspapers in the time following the Civil War brought privacy to the forefront of the public eye. The other reason that brought privacy to the forefront of public concern was the technological development of "[[instant photography]]". This article set the stage for all privacy legislation to follow during the 20 and 21st Centuries.  ===Reasonable Expectation of Privacy Test and emerging technology=== In 1967, the United States Supreme Court decision in Katz v United States, 389 U.S. 347 (1967) established what is known as the Reasonable Expectation of Privacy Test to determine the applicability of the Fourth Amendment in a given situation. It should be noted that the test was not noted by the majority, but instead it was articulated by the concurring opinion of Justice Harlan. Under this test, 1) a person must exhibit an "actual (subjective) expectation of privacy" and 2) "the expectation [must] be one that society is prepared to recognize as 'reasonable.'"  ===Privacy Act of 1974=== Inspired by the [[Watergate scandal]], the [[United States Congress]] enacted the Privacy Act of 1974 just four months after the resignation of then President [[Richard Nixon]]. In passing this Act, Congress found that "the privacy of an individual is directly affected by the collection, maintenance, use, and dissemination of personal information by Federal agencies" and that "the increasing use of computers and sophisticated information technology, while essential to the efficient operations of the Government, has greatly magnified the harm to individual privacy that can occur from any collection, maintenance, use, or dissemination of personal information."<br> For more information see: [[Privacy Act of 1974]]  ===Foreign Intelligence Surveillance Act of 1978=== Codified at 50 U.S.C. §§ 1801-1811, this act establishes standards and procedures for use of electronic surveillance to collect "foreign intelligence" within the United States. §1804(a)(7)(B). FISA overrides the Electronic Communications Privacy Act during investigations when foreign intelligence is "a significant purpose" of said investigation. {{usc|50|1804}}(a)(7)(B) and §1823(a)(7)(B). Another interesting result of FISA, is the creation of the Foreign Intelligence Surveillance Court (FISC). All FISA orders are reviewed by this special court of federal district judges. The FISC meets in secret, with all proceedings usually also held from both the public eye and those targets of the desired surveillance.<br> For more information see: [[Foreign Intelligence Act]]  ===(1986) Electronic Communication Privacy Act=== The ECPA represents an effort by the United States Congress to modernize federal wiretap law. The ECPA amended Title III (see: [[Omnibus Crime Control and Safe Streets Act of 1968]]) and included two new acts in response to developing computer technology and communication networks. Thus the ECPA in the domestic venue into three parts: 1) Wiretap Act, 2) Stored Communications Act, and 3) The Pen Register Act.  ::*Types of Communication :::** '''Wire Communication:'''  Any communication containing the human voice that travels at some point across a wired medium such as radio, satellite or cable. :::** '''Oral Communication:''' :::** '''Electronic Communication''' ::# '''The Wiretap Act:''' For Information See [[Wiretap Act]] ::# '''The Stored Communications Act:''' For information see [[Stored Communications Act]] ::# '''The Pen Register Act:''' For information see [[Pen Register|Pen Register Act]]  ===(1994) Driver's Privacy Protection Act=== The DPPA was passed in response to states selling motor vehicle records to private industry. These records contained personal information such as name, address, phone number, SSN, medical information, height, weight, gender, eye color, photograph and date of birth. In 1994, Congress passed the Driver's Privacy Protection (DPPA), 18 U.S.C. §§ 2721-2725, to cease this activity.<br> For more information see: [[Driver's Privacy Protection Act]]  ===(1999) Gramm-Leach-Bliley Act=== -This act authorizes widespread sharing of personal information by financial institutions such as banks, insurers, and investment companies. The GLBA permits sharing of personal information between companies joined together or affiliated as well as those companies unaffiliated. To protect privacy, the act requires a variety of agencies such as the SEC, FTC, etc. to establish "appropriate standards for the financial institutions subject to their jurisdiction" to "insure security and confidentiality of customer records and information" and "protect against unauthorized access" to this information. {{usc|15|6801}}<br> For more information see: [[Gramm-Leach-Bliley Act]]  ===(2002) Homeland Security Act=== -Passed by Congress in 2002, the Homeland Security Act, {{usc|6|222}}, consolidated 22 federal agencies into what is commonly known today as the Department of Homeland Security (DHS). The HSA, also created a Privacy Office under the DoHS. The Secretary of Homeland Security must "appoint a senior official to assume primary responsibility for privacy policy." This privacy official's responsibilities include but are not limited to: ensuring compliance with the Privacy Act of 1974, evaluating "legislative and regulatory proposals involving the collection, use, and disclosure of personal information by the Federal Government", while also preparing an annual report to Congress.<br> For more information see: [[Homeland Security Act]]  ===(2004) Intelligence Reform and Terrorism Prevention Act=== -This Act mandates that intelligence be "provided in its most shareable form" that the heads of intelligence agencies and federal departments "promote a culture of information sharing." The IRTPA also sought to establish protection of privacy and civil liberties by setting up a five-member Privacy and Civil Liberties Oversight Board. This Board offers advice to both the President of the United States and the entire executive branch of the Federal Government concerning its actions to ensure that the branch's information sharing policies are adequately protecting privacy and civil liberties.<br> For more information see: [[Intelligence Reform and Terrorism Prevention Act]]  ==Legal enactments – examples== The [[Computer Misuse Act 1990]]<ref>{{cite web|url=http://www.homeoffice.gov.uk/crime/internetcrime/compmisuse.html |title=[ARCHIVED CONTENT&#93; Internet Crime - The Computer Misuse Act 1990 |publisher=Homeoffice.gov.uk |date= |accessdate=2012-01-17}}</ref> enacted by [[Great Britain]] on 29 June 1990, and which came into force on 29 August 1990, is an example of one of the earliest of such legal enactments. This Act was enacted with an express purpose of making "provision for securing computer material against unauthorised access or modification." Certain major provisions of the Computer Misuse Act 1990 relate to: *"unauthorised access to computer materials", *"unauthorised access with intent to commit or facilitate the commission of further offences", and *"unauthorised modification of computer material."  The impact of the Computer Misuse Act 1990 has been limited and with the adoption of the Council of Europe adopts its Convention on Cyber-Crime, it has been indicated that amending legislation would be introduced in parliamentary session 2004–05 in order to rectify possible gaps in its coverage, which are many.  The CMA 1990 has many weaknesses; the most notable is its inability to cater for, or provide suitable protection against, a host of high tech attacks/crimes which have became more prevalent in the last decade.  Certain attacks such as DDOS and BOTNET attacks can not be effectively brought to justice under the CMA.  This act has been under review for a number of years.  Computer crimes such as electronic theft are usually prosecuted in the UK under the legislation that caters for traditional theft ([[Theft Act 1968]]), because the CMA is so ineffective.  A recent example of information technology law is India's [[Information Technology Act|Information Technology Act 2000]], which became effective from 17 October 2000. This Act applies to whole of India, and its provisions also apply to any offence or contravention, committed even outside the territorial jurisdiction of [[Republic of India]], by any person irrespective of his [[citizen|nationality]]. In order to attract provisions of this Act, such an offence or contravention should involve a computer, computer system, or computer network located in India. The IT Act 2000 provides an extraterritorial applicability to its provisions by virtue of section 1(2) read with section 75.  India's Information Technology Act 2000 has tried to assimilate legal principles available in several such laws (relating to information technology) enacted earlier in several other countries, as also various guidelines pertaining to Information Technology Law. The government of India appointed an Expert Committee to suggest suitable amendments into the existing IT Act, 2000. The amendments suggested by the Committee were severely criticised on various grounds. The chief among them was the dilution of criminal sanctions under the proposed amendments. These amendments, perhaps with some modifications, have been approved by the Cabinet in India on 16 October 2006 and very soon{{When|date=September 2011}} the amendments will be laid down before the Indian Parliament for suitable legislation.  The IT Act, 2000 needs an overall haul,{{Says who|date=September 2011}} keeping in mind the contemporary standards and requirements and the Indian law in this regard is lagging far behind. In the absence of proper law in place, the only recourse is to rely upon the traditional criminal law of India, i.e. Indian Penal Code, 1860 (IPC) that is highly insufficient for computer crimes in India. Alternatively, a purposive, updating and organic interpretation of the existing provisions of the IT Act, 2000 and IPC by the judiciary must be tried.{{Says who|date=September 2011}}  The IT Act, 2000 requires a purposive and updating amendment initiative as many contemporary crimes and contraventions are missing from it. Besides, there is an emergent need of introducing the concept of cyber forensics in India.  Many Asian and Middle Eastern nations use any number of combinations of code-based regulation (one of Lessig's four methods of net regulation) to block material that their governments have deemed inappropriate for their citizens to view.  [[People's Republic of China|PRC]], [[Saudi Arabia]] and [[Iran]] are three examples of nations that have achieved high degrees of success in regulating their citizens' access to the Internet.<ref name="ONIChina" /><ref>{{cite web|url=http://www.opennetinitiative.net/studies/saudi/|publisher=OpenNet Initiative|title=All Content related to Saudi Arabia}}</ref>  ===Electronic signature laws=== * [[Australia]] - ''Electronic Transactions Act 1999'' (Cth) (also note that there is State and Territory mirror legislation) * [[Costa Rica]] - Digital Signature Law 8454 (2005) * [[European Union]] - Electronic Signature Directive (1999/93/EC) * [[Mexico]] - E-Commerce Act [2000] * U.S. - [[Digital Signature And Electronic Authentication Law]] * U.S. - [[Electronic Signatures in Global and National Commerce Act]] * U.S. - [[Government Paperwork Elimination Act]] (GPEA) * U.S. - [[Uniform Commercial Code]] (UCC) * U.S. - [[Uniform Electronic Transactions Act]] - adopted by 46 states * [[United Kingdom|UK]] - s.7 [[Electronic Communications Act 2000]]  ===Information technology law=== #Florida Electronic Security Act #Illinois Electronic Commerce Security Act #Texas Penal Code - Computer Crimes Statute #Maine Criminal Code - Computer Crimes #Singapore Electronic Transactions Act #Malaysia Computer Crimes Act #Malaysia Digital Signature Act #UNCITRAL Model Law on Electronic Commerce #[[Information Technology Act|Information Technology Act 2000 of India]]  ===Information Technology Guidelines=== #ABA Digital Signature Guidelines #[[United States Office of Management and Budget]]  ==Enforcement agencies== The Information Technology Laws of various countries, and / or their criminal laws generally stipulate [[List of law enforcement agencies|enforcement agencies]], entrusted with the task of enforcing the legal provisions and requirements.  ===United States Federal Agencies=== Many [[List of United States federal agencies|United States federal agencies]] oversee the use of information technology. Their regulations are promulgated in the [[Code of Federal Regulations]] of the United States.  Over 25 U.S. federal agencies have regulations concerning the use of digital and electronic signatures.<ref>{{cite web|url=http://www.isaacbowman.com/esign-and-the-code-of-federal-regulations-cfr |title=Federal Agency Digital and Electronic Signature Regulations |publisher=Isaacbowman.com |date=2009-03-16 |accessdate=2012-01-17}}</ref>  ===India=== A live example of such an enforcement agency is Cyber Crime Police Station, Bangalore,<ref>{{cite web|url=http://www.cyberpolicebangalore.nic.in/ |title=cyberpolicebangalore.nic.in |publisher=cyberpolicebangalore.nic.in |date= |accessdate=2012-01-17}}</ref> India's first exclusive Cyber Crime enforcement agency. *Other examples of such enforcement agencies include: *[[Cyber Crime Investigation Cell]]<ref>http://www.cybercellmumbai.com/index.htm</ref> of India's [[Mumbai]] Police. *''Cyber Crime Police Station''<ref>[http://www.cidap.gov.in/cybercrimes.aspx ]{{dead link|date=January 2012}}</ref> of the state [[Andhra Pradesh|Government of Andhra Pradesh]], India. This [[Police station]] has jurisdiction over the entire state of [[Andhra Pradesh]], and functions from the [[Hyderabad, India|Hyderabad city]]. *In [[South India]], the Crime Branch of Criminal Investigation Department, [[Tamilnadu|Tamilnadu police]], India, has a Cyber Crime Cell at [[Chennai]]. *In [[East India]], Cyber Crime Cells have been set up by the [[Kolkata Police]] as well as the Criminal Investigation Department, [[West Bengal]].  ==Information Technology Lawyer== An information technology attorney is a professional who handles a variety of legal matters related to IT. The attorney gets involved in drafting, negotiating, and interpreting agreements in the areas of software licensing and maintenance, IT consulting, e-commerce, web site hosting and development, and telecommunications agreements, as well as handling dispute resolution and assisting with the client's Internet domain name portfolio. An information technology attorney works with engineering, IT, and other business units and ensures that customer information gathered by company is collected, stored and used in compliance with privacy policies and applicable laws.  Duties also include providing high quality, specialized and practical advice in business-to-business and business-to-consumer arrangements and advising on issues like IT outsourcing arrangements, software and hardware supply and implementation agreements. An information technology attorney contracts for web site developers and consultants in relation to on-line projects. Provides support and maintains confidentiality/know how agreements. Contracts for Internet service providers and data protection advice. An information technology attorney should have a JD degree or an LL.M degree with admission to the local state bar.  == Quotations == * "In Cyberspace, the [[First Amendment to the United States Constitution|First Amendment]] is a local ordinance." <br>&mdash; [[John Perry Barlow]], quoted by Mitchell Kapor in the foreword to ''The Big Dummy's Guide to the Internet'' * "National borders aren't even speed bumps on the information superhighway." <br>&mdash; Tim May, ''signature'', from 1996  ==See also== * [[Berkman Center for Internet & Society]] * ''[[Bernstein v. United States]]'' and ''[[Junger v. Daley]]'' &ndash; on free speech protection of software * [[Computer forensics]] * [[Computer crime|Сybercrime]] * [[Digital Millennium Copyright Act]] (DMCA) * [[Electronic Communications Privacy Act]] * [[Export of cryptography]] * [[:wikt:Transwiki:Glossary of legal terms in technology|Glossary of legal terms in technology]] * [[Software patent debate]] * [[Universal v. Reimerdes]] &ndash; test of DMCA * [[Ouellette v. Viacom International Inc.]] (DMCA and ADA) * [[Wassenaar Arrangement]] * [[Doe v. 2themart.com Inc.]] - First Amendment right to speak anonymously * [[United States v. Ivanov]] - Applying United States cyber-law to a foreign national operating outside the US ;Centers and groups for the study of cyberlaw and related areas * [[Berkman Center for Internet and Society]] at [[Harvard Law School]] * [[Centre for Internet and Society (India)|Centre for Internet and Society]], in Bangalore, India. * [[Institute for Information, Telecommunication and Media Law]] in Münster, Germany * [[Institute of Space and Telecommunications Law]] at [[Paris-Sud 11 University|University Paris-Sud]], Master's degree in Space Activities and Telecommunications Law * Master of New Technologies law<ref>{{cite web|url=http://www.upo.es/postgrado/detalle_curso.php?id_curso=367 |title=Centro de Estudios de Postgrado de la Universidad Pablo de Olavide |publisher=Upo.es |date= |accessdate=2012-01-17}}</ref> at [[Pablo de Olavide University]] in Seville, Spain * [[Norwegian Research Center for Computers and Law]] * [[Stanford Center for Internet and Society]], at [[Stanford Law School]]  ;Topics related to cyberlaw * [[Copyright]], especially the [[Digital Millenium Copyright Act]] in the United States, and similar laws in other countries * [[Cyber defamation law]] * [[Digital Rights Management]] * [[Intellectual property]] * [[Internet censorship]] * [[Stop Online Piracy Act]] * [[Spamming]] * ''[[The Law of Cyber-Space]]'' (book)  ;Conferences related to cyberlaw * [[State of Play (Conference series)|State of Play]], a conference series put on by the Institute for Information Law & Policy at [[New York Law School]], concerning the intersection of [[virtual world]]s, [[game]]s and the law.  ==Further reading== *''[[Code and Other Laws of Cyberspace]],'' ''[[The Future of Ideas]],'' and ''[[Free Culture (book)|Free Culture]]'' by [[Lawrence Lessig]] *''[[Cyber Rights]]'' by [[Mike Godwin]] *''E-Commerce and Internet Law: Treatise with Forms 2d edition,'' by [[Ian C. Ballon]]  ==References== {{Cleanup-link rot|date=January 2012}} {{reflist}}  == External links == *[http://cyber.law.harvard.edu/home/ Berkman Center for Internet & Society at Harvard Law School] *[http://grep.law.harvard.edu/ Greplaw is a news and discussion site for legal and computing issues.] *[http://www.eff.org The Electronic Frontier Foundation works for individuals' rights.] *[http://www.legalarchiver.org/safe.htm Text of the United States' Security And Freedom through Encryption (SAFE) Act.] *[http://www.cyberlawconsulting.com/it-act.html/ The Information Technology Amended Act 2008 of India] *[http://www.cybercrime.gov/ US Department of Justice - Cyber crimes] *[http://www.cert.org/ CERT] *[http://www.gstj.org/ Global School of Tech Juris Pioneering Cyber Law Institute] *[http://www.naavi.org/ Indian Portal on Cyber Law] *[http://www.crime-research.org/ Computer Crime Research Center] *[http://www.unesco.org/cgi-bin/webworld/portal_observatory/cgi/page.cgi?d=1&g=Enabling_Environment/Cyber-Crime_and_Misuse_of_ICT/index.shtml Cyber crime and misuse of ICT-UNESCO] *[http://www.ncfs.ucf.edu/digital_evd.html Digital Evidence] *[http://www.computerforensicsworld.com/ Computer Forensics World] *[http://ssrn.com/abstract=909781 "Model Information Technology Contract Terms and Systems Implementation Contracts in Europe" . European Newsletter, June 2006 Lestrade, K.OSt.J., SJDr, Dr. Edward] *[http://www.law.ed.ac.uk/ahrc/ AHRC Research Centre for Studies in IP and IT Law at Edinburgh Law School] *[http://perry4law.blogspot.com/2005/09/critical-analysis-of-proposed-it-act.html Critical analysis of proposed Information Technology Act, 2000 amendments] *[http://perry4law.blogspot.com/2006/10/proposed-it-act-2000-amendments-boon.html The proposed IT Act, 2000 amendments: Boon or bane] *[http://gstj.org/Cyber%20Censor.pdf Global School of Tech Juris White Paper on Censoring the Indian Cyberspace] *[http://gstj.org/Cyber%20Voyeurism.pdf Global School of Tech Juris White Paper on Cyber Voyeurism Perverts Dot Com] *[http://www.naavi.org/cl_editorial_06/edit_oct_21.htm You Be The Judge] *[http://www.mid-day.com/news/city/2001/october/16106.htm Natural Porn Killers - Jayesh and Sunil Thacker (Mid Day - October 7, 2001)] *[http://timesofindia.indiatimes.com/articleshow/1171868083.cms HC admits cyberporn complaint as PIL (Times of India, July 13, 2001)] *[http://timesofindia.indiatimes.com/articleshow/1124348984.cms Cyberporn panel set up, HC wants minors protected, Sunil Thacker invited as a special invitee (Times of India, September 30, 2001)] *[http://www.indiacensored.com indiacensored.com] *[http://www.cyberlawsindia.net Cyber Law Complete Information] *[http://www.cyberlaw.ro Articles on Romanian Cyberlaw] *[http://cyberlaw.stanford.edu/cyberlaw-clinic Stanford Law School Cyberlaw Clinic] *[http://www.techlawforum.net/category/internet-policy/ Santa Clara University School of Law Tech LawForum] *[http://www.cybertelecom.org/ Cybertelecom: Federal Internet Policy] *[http://www.internetlibrary.com Internet Library of Law and Court Decisions] *[http://www.cyberlawdb.com/main Global Cyber Law Database] *[http://www.worldlii.org/catalog/284.html WorldLII Cyberspace Law] *[http://www.cyberlawtimes.com Cyber Law World] *[http://www.crime-research.org/ Computer Crime Research Center] *[http://www.asianlaws.org/cyberlaw/library/index.htm ASCL Cyber Law Library] *[http://epublications.bond.edu.au/law_pubs/16 Borders on, or border around – the future of the Internet] *[http://www.dsci.in/images/stories/sig_-_cyberlaws.pdf White Paper on Information technology Act Amendments 2008]  {{DEFAULTSORT:Legal Aspects Of Computing}} [[Category:Computer law|*]] [[Category:Information technology|Law, Information technology]] [[Category:Computer law]] [[Category:Cyberspace]]  [[ar:قانون إنترنت]] [[de:Rechtsinformatik]] [[es:Derecho informático]] [[gl:Dereito informático]] [[ko:기술법]] [[it:Diritto informatico]] [[he:דיני מחשבים]] [[lt:Informacinių technologijų teisė]] [[ms:Undang-undang siber]] [[pl:Cyberprawo]] [[pt:Direito da informática]] [[ru:Информационное право]] [[uk:Інформаційне право]]
In electronics, '''logic synthesis''' is a process by which an abstract form of desired circuit behavior, typically [[register transfer level]] (RTL), is turned into a design implementation in terms of [[logic gates]]. Common examples of this process include synthesis of [[Hardware Description Language|HDL]]s, including [[VHDL]] and [[Verilog]]. Some tools can generate [[bitstream]]s for [[programmable logic device]]s such as [[programmable array logic|PAL]]s or [[Field-programmable gate array|FPGA]]s, while others target the creation of [[ASIC]]s.  Logic synthesis is one aspect of [[electronic design automation]].  == History of logic synthesis == The roots of logic synthesis can be traced to the treatment of logic by [[George Boole]] (1815 to 1864), in what is now termed [[Boolean algebra (logic)|Boolean algebra]]. In 1938, [[Claude Elwood Shannon|Claude Shannon]] showed that the two-valued Boolean algebra can describe the operation of switching circuits.  In the early days, '''logic design''' involved manipulating the truth table representations as [[Karnaugh map]]s. The Karnaugh map-based minimization of logic is guided by a set of rules on how entries in the maps can be combined. A human designer can typically only work with Karnaugh maps containing up to four to six variables.   The first step toward automation of [[logic minimization]] was the introduction of the [[Quine&ndash;McCluskey algorithm]] that could be implemented on a computer. This exact minimization technique presented the notion of prime implicants and minimum cost covers that would become the cornerstone of [[two-level minimization]]. Nowadays, the much more efficient [[Espresso heuristic logic minimizer]] has become the standard tool for this operation. Another area of early research was in state minimization and encoding of [[finite state machine]]s (FSMs), a task that was the bane of designers. The applications for logic synthesis lay primarily in digital computer design. Hence, [[IBM]] and [[Bell Labs]] played a pivotal role in the early automation of logic synthesis. The evolution from [[discrete logic]] components to [[programmable logic array]]s (PLAs) hastened the need for efficient two-level minimization, since minimizing terms in a two-level representation reduces the area in a PLA.   However, two-level logic circuits are of limited importance in a [[very-large-scale integration]] (VLSI) design; most designs use multiple levels of logic. As a matter of fact, almost any circuit representation in RTL or Behavioural Description is a multi-level representation. An early system that was used to design multilevel circuits was LSS from IBM. It used local transformations to simplify logic. Work on LSS and the Yorktown Silicon Compiler spurred rapid research progress in logic synthesis in the 1980s. Several universities contributed by making their research available to the public, most notably SIS from [[University of California, Berkeley]], RASP from  [[University of California, Los Angeles]] and BOLD from [[University of Colorado, Boulder]].  Within a decade, the technology migrated to commercial logic synthesis products offered by electronic design automation companies.  ==Logic elements== ''Logic design'' is a step in the standard  design cycle in which the [[functional design]] of an [[electronic circuit]] is converted into the representation which captures [[Boolean algebra (logic)|logic operations]], [[arithmetic operations]], [[control flow]], etc. A common output of this step is [[RTL description]]. Logic design is commonly followed by the [[circuit design]] step. In modern [[electronic design automation]] parts of the logical design may be automated using [[high-level synthesis]] tools based on the behavioral description of the circuit.<ref name="Sherwani1999">{{cite book|author=Naveed A. Sherwani|title=Algorithms for VLSI physical design automation|year=1999|edition=3rd|page=4|publisher=Kluwer Academic Publishers|isbn=978-0-7923-8393-2}}</ref>  [[Image:Baops.gif|right|thumb|450px|Various representations of Boolean operations]]  Logic operations usually consist of boolean AND, OR, XOR and NAND operations, and are the most basic forms of operations in an electronic circuit. Arithmetic operations are usually implemented with the use of logic operators. Circuits such as a [[binary multiplier]] or a [[binary adder]] are examples of more complex binary operations that can be implemented using basic logic operators.   == High-level synthesis or behavioral synthesis == {{Main|High-level synthesis}}  With a goal of increasing designer productivity, research efforts on the synthesis of circuits specified at the behavioral level have led to the emergence of commercial solutions in 2004,<ref name='EETimes'>EETimes: [http://archives.eetimes.com/high-level-synthesis-rollouts-enable-esl/110436.html High-level synthesis rollouts enable ESL]</ref> which are used for complex ASIC and FPGA design. These tools automatically synthesize circuits specified at C level to a register transfer level (RTL) specification, which can be used as input to a gate-level logic synthesis flow.<ref name="EETimes"/> Today, high-level synthesis, also known as ESL synthesis and behavioral synthesis, essentially refers to circuit synthesis from high level Languages like ANSI C/C   or SystemC etc., whereas Logic Synthesis refers to synthesis from structural or functional description to RTL.  == Multi-level logic minimization == {{see also|Logic optimization|Circuit minimization}} Typical practical implementations of a logic function utilize a multi-level network of logic elements.  Starting from an RTL description of a design, the synthesis tool constructs a corresponding multilevel Boolean network.   Next, this network is optimized using several technology-independent techniques before technology-dependent optimizations are performed. The typical cost function during technology-independent optimizations is total [[Propositional_formula#Literal.2C_term_and_alterm|literal]] count of the factored representation of the logic function (which correlates quite well with circuit area).  Finally, technology-dependent optimization transforms the technology-independent circuit into a network of gates in a given technology. The simple cost estimates are replaced by more concrete, implementation-driven estimates during and after technology mapping.  Mapping is constrained by factors such as the available gates (logic functions) in the technology library, the drive sizes for each gate, and the delay, power, and area characteristics of each gate.  == Commercial tool for logic synthesis == === Software tools for logic synthesis targeting ASICs === * ''[http://www.synopsys.com/Tools/Implementation/RTLSynthesis/Pages/default.aspx Design Compiler]'' by [[Synopsys]] * ''[http://www.cadence.com/products/digital_ic/rtl_compiler/index.aspx Encounter RTL Compiler]'' by [[Cadence Design Systems]] ** ''BuildGates'', an older product by [[Cadence Design Systems]], humorously named after [[Bill Gates]] *''[http://www.magma-da.com/products-solutions/digitaldesign/talusdesign.aspx TalusDesign]'' by [[Magma Design Automation]] *''[http://www.oasys-ds.com RealTime Designer]'' by Oasys Design Systems *''[http://domino.research.ibm.com/tchjr/journalindex.nsf/0b9bc46ed06cbac1852565e6006fe1a0/5588d005a20caff385256bfa0067f992?OpenDocument BooleDozer:]'' Logic synthesis tool by [[IBM]] (internal IBM EDA tool)  === Software tools for logic synthesis targeting FPGAs === * ''[[Xilinx ISE|XST]] (delivered within [[Xilinx ISE|ISE]]) by [[Xilinx]] * ''[[Altera Quartus#Quartus II|Quartus II]] integrated Synthesis by [[Altera]] * ''IspLever'' by [[Lattice Semiconductor]] * ''Encounter RTL Compiler'' by [[Cadence Design Systems]] * ''[http://www.mentor.com/products/fpga_pld/synthesis/ LeonardoSpectrum and Precision (RTL / Physical)]'' by [[Mentor Graphics]] * ''[http://www.synopsys.com/Support/Training/Pages/SynplifyProPremier.aspx Synplify (PRO / Premier)]'' by [[Synopsys]] * ''[http://www.magma-da.com/Pages/BlastFPGA.html BlastFPGA]'' by [[Magma Design Automation]]  == See also == *[[Binary decision diagram]] *[[Functional verification]]  ==Footnote== <references/>  == References == *''Electronic Design Automation For Integrated Circuits Handbook'', by Lavagno, Martin, and Scheffer, ISBN 0-8493-3096-3 A survey of the field of [[Electronic design automation]].  The above summary was derived, with permission, from Volume 2, Chapter 2, ''Logic Synthesis'' by Sunil Khatri and Narendra Shenoy. *''A Consistent Approach in Logic Synthesis for FPGA Architectures'', by Burgun Luc, Greiner Alain, and Prado Lopes Eudes, Proceedings of the international Conference on Asic (ASICON), Pekin, October 1994, pp.&nbsp;104–107.  == Further reading == * {{cite book|editor=Laung-Terng Wang, Yao-Wen Chang, Kwang-Ting Cheng|title=Electronic design automation: synthesis, verification, and test|year=2009|author=Jie-Hong (Roland) Jiang, Srinivas Devadas|chapter=Logic synthesis in a nutshell|publisher=Morgan Kaufmann|isbn=978-0-12-374364-0|id=chapter 6}} *{{cite book|author1=Gary D. Hachtel|author2=Fabio Somenzi|title=Logic synthesis and verification algorithms|year=1996|publisher=Springer|isbn=0-7923-9746-0}} also as published as [[softcover]] ISBN 0-387-31004-5 in 2006 * {{cite book|editor=Soha Hassoun, Tsutomu Sasao|title=Logic synthesis and verification|year=2002|publisher=Kluwer|isbn=978-0-7923-7606-4}}  {{DEFAULTSORT:Logic Synthesis}} [[Category:Electronic engineering]] [[Category:Electronic design]] [[Category:Digital electronics]] [[Category:Electronic design automation]]  [[de:Logiksynthese]] [[el:Λογική Σχεδίαση]] [[fr:Synthèse logique]] [[ja:論理合成]] [[pt:Síntese lógica]] [[fi:Logiikkasynteesi]] [[zh:逻辑综合]]
'''Mathematical logic''' (also known as '''symbolic logic''') is a subfield of [[mathematics]] with close connections to the [[foundations of mathematics]], [[theoretical computer science]] and [[philosophical logic]].<ref>Undergraduate texts include Boolos, Burgess, and Jeffrey [[#CITEREFBoolosBurgessJeffrey2002|(2002)]], [[Herbert Enderton|Enderton]] [[#CITEREFEnderton2001|(2001)]], and Mendelson [[#CITEREFMendelson1997|(1997)]]. A classic graduate text by Shoenfield [[#CITEREFShoenfield2001|(2001)]] first appeared in 1967.</ref> The field includes both the mathematical study of [[logic]] and the applications of formal logic to other areas of mathematics.  The unifying themes in mathematical logic include the study of the expressive power of [[formal system]]s and the deductive power of formal [[mathematical proof|proof]] systems.  Mathematical logic is often divided into the fields of [[set theory]], [[model theory]], [[recursion theory]], and [[proof theory]]. These areas share basic results on logic, particularly [[first-order logic]], and [[definable set|definability]]. In computer science (particularly in the [[ACM Computing Classification System|ACM Classification]]) mathematical logic encompasses additional topics not detailed in this article; see [[logic in computer science]] for those.  Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of [[foundations of mathematics]]. This study began in the late 19th century with the development of [[axiom]]atic frameworks for [[geometry]], [[arithmetic]], and [[analysis]]. In the early 20th century it was shaped by [[David Hilbert]]'s [[Hilbert's program|program]] to prove the consistency of foundational theories. Results of [[Kurt Gödel]], [[Gerhard Gentzen]], and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in [[reverse mathematics]]) rather than trying to find theories in which all of mathematics can be developed.  ==Subfields and scope==  The ''Handbook of Mathematical Logic'' makes a rough division of contemporary mathematical logic into four areas: #[[set theory]] #[[model theory]] #[[recursion theory]], and #[[proof theory]] and [[constructive mathematics]] (considered as parts of a single area). Each area has a distinct focus, although many techniques and results are shared between multiple areas. The border lines between these fields, and the lines between mathematical logic and other fields of mathematics, are not always sharp.  [[Gödel's incompleteness theorem]] marks not only a milestone in recursion theory and proof theory, but has also led to [[Löb's theorem]] in modal logic. The method of [[forcing (mathematics)|forcing]] is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.  The mathematical field of [[category theory]] uses many formal axiomatic methods, and includes the study of [[categorical logic]], but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including [[Saunders Mac Lane]] have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use [[topos]]es, which resemble generalized models of set theory that may employ classical or nonclassical logic.  ==History== Mathematical logic emerged in the mid-19th century as a subfield of mathematics independent of the traditional study of logic ([[#CITEREFFerreir.C3.B3s2001|Ferreirós 2001]], p.&nbsp;443). Before this emergence, logic was studied with [[rhetoric]], through the [[syllogism]], and with [[philosophy]].  The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.  === Early history === {{Further|History of logic}} Theories of logic were developed in many cultures in history, including [[Logic in China|China]], [[Logic in India|India]], [[Logic in Greece|Greece]] and the [[Logic in Islamic philosophy|Islamic world]]. In 18th century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including [[Leibniz]] and [[Johann Heinrich Lambert|Lambert]], but their labors remained isolated and little known.  === 19th century ===  <!-- symbolic logic --> In the middle of the nineteenth century, [[George Boole]] and then [[Augustus De Morgan]] presented systematic mathematical treatments of logic.  Their work, building on work by algebraists such as [[George Peacock]], extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of [[foundations of mathematics]]&nbsp;([[#CITEREFKatz1998|Katz 1998]], p.&nbsp;686).  [[Charles Sanders Peirce]] built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885. [[Gottlob Frege]] presented an independent development of logic with quantifiers in his ''[[Begriffsschrift]]'', published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until [[Bertrand Russell]] began to promote it near the turn of the century.  The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.  From 1890 to 1905, [[Ernst Schröder]] published ''Vorlesungen über die Algebra der Logik'' in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.  ==== Foundational theories ====  Concerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.  <!-- arithmetic --> In logic, the term ''arithmetic'' refers to the theory of the [[natural number]]s. [[Giuseppe Peano]] ([[#CITEREFPeano1888|1888]]) published a set of axioms for arithmetic that came to bear his name ([[Peano axioms]]), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time [[Richard Dedekind]] showed that the natural numbers are uniquely characterized by their [[mathematical induction|induction]] properties. Dedekind ([[#CITEREFDedekind1888|1888]]) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the [[successor function]] and mathematical induction.  <!-- geometry --> In the mid-19th century, flaws in Euclid's axioms for geometry became known ([[#CITEREFKatz1998|Katz 1998]], p.&nbsp;774).  In addition to the independence of the [[parallel postulate]], established by [[Nikolai Lobachevsky]] in 1826 ([[#CITEREFLobachevsky1840|Lobachevsky 1840]]), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert ([[#CITEREFHilbert1899|1899]]) developed a complete set of [[Hilbert's axioms|axioms for geometry]], building on [[Pasch's axiom|previous work]] by Pasch ([[#CITEREFPasch1882|1882]]).  The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the [[real line]].  This would prove to be a major area of research in the first half of the 20th century.  The 19th century saw great advances in the theory of [[real analysis]], including theories of convergence of functions and [[Fourier series]]. Mathematicians such as [[Karl Weierstrass]] began to construct functions that stretched intuition, such as [[Continuous, nowhere differentiable function|nowhere-differentiable continuous functions]]. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.  Weierstrass began to advocate the [[arithmetization of analysis]], which sought to axiomatize analysis using properties of the natural numbers. The modern [[(ε, δ)-definition of limit]] and [[continuous function]]s was already developed by [[Bernard Bolzano|Bolzano]] in 1817 ([[#CITEREFFelscher2000|Felscher 2000]]), but remained relatively unknown. [[Cauchy]] in 1821 defined continuity in terms of [[infinitesimal]]s (see Cours d'Analyse, page 34).  In 1858, Dedekind proposed a definition of the real numbers in terms of [[Dedekind cuts]] of rational numbers [[#CITEREFDedekind1872|(Dedekind 1872)]], a definition still employed in contemporary texts.  [[Georg Cantor]] developed the fundamental concepts of infinite set theory. His early results developed the theory of [[cardinality]] and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of [[transfinite number]]s in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the [[Cantor's diagonal argument|diagonal argument]], and used this method to prove [[Cantor's theorem]] that no set can have the same cardinality as its [[powerset]]. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 ([[#CITEREFKatz1998|Katz 1998, p.&nbsp;807]]).  === 20th century === In the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.  In 1900, [[David Hilbert|Hilbert]] posed a famous list of [[Hilbert's problems|23 problems]] for the next century. The first two of these were to resolve the [[continuum hypothesis]] and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the [[integer]]s has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's ''[[Entscheidungsproblem]]'', posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.  ==== Set theory and paradoxes ====  [[Ernst Zermelo]] ([[#CITEREFZermelo1904|1904]]) gave a proof that every set could be well-ordered, a result [[Georg Cantor]] had been unable to obtain. To achieve the proof, Zermelo introduced the [[axiom of choice]], which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof ([[#CITEREFZermelo1908a|Zermelo 1908a]]). This paper led to the general acceptance of the axiom of choice in the mathematics community.  Skepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. [[Cesare Burali-Forti]] ([[#CITEREFBurali-Forti1897|1897]]) was the first to state a paradox: the [[Burali-Forti paradox]] shows that the collection of all [[ordinal number]]s cannot form a set. Very soon thereafter, [[Bertrand Russell]] discovered [[Russell's paradox]] in 1901, and [[Jules Richard]] ([[#CITEREFRichard1905|1905]]) discovered [[Richard's paradox]].  Zermelo ([[#CITEREFZermelo1908b|1908b]]) provided the first set of axioms for set theory. These axioms, together with the additional [[axiom of replacement]] proposed by [[Abraham Fraenkel]], are now called [[Zermelo–Fraenkel set theory]] (ZF). Zermelo's axioms incorporated the principle of [[limitation of size]] to avoid Russell's paradox.  In 1910, the first volume of ''[[Principia Mathematica]]'' by Russell and [[Alfred North Whitehead]] was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of [[type theory]], which Russell and Whitehead developed in an effort to avoid the paradoxes. ''Principia Mathematica'' is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics ([[#CITEREFFerreir.C3.B3s2001|Ferreirós 2001]], p.&nbsp;445).  Fraenkel ([[#CITEREFFraenkel1922|1922]]) proved that the axiom of choice cannot be proved from the remaining axioms of Zermelo's set theory with [[urelements]]. Later work by [[Paul Cohen (mathematician)|Paul Cohen]] ([[#CITEREFCohen1966|1966]]) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of [[forcing (mathematics)|forcing]], which is now an important tool for establishing [[independence result]]s in set theory.  ==== Symbolic logic ====  [[Leopold Löwenheim]] ([[#CITEREFL.C3.B6wenheim1915|1915]]) and [[Thoralf Skolem]] ([[#CITEREFSkolem1920|1920]]) obtained the [[Löwenheim–Skolem theorem]], which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a [[countable]] [[structure (mathematical logic)|model]]. This counterintuitive fact became known as [[Skolem's paradox]].  In his doctoral thesis, [[Kurt Gödel]] ([[#CITEREFGödel1929|1929]]) proved the [[completeness theorem]], which establishes a correspondence between syntax and semantics in [[first-order logic]]. Gödel used the completeness theorem to prove the [[compactness theorem]], demonstrating the finitary nature of first-order [[logical consequence]]. These results helped establish first-order logic as the dominant logic used by mathematicians.  In 1931, Gödel published ''[[On Formally Undecidable Propositions of Principia Mathematica and Related Systems]]'', which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as [[Gödel's incompleteness theorem]], establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic.  Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.  Gödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen ([[#CITEREFGentzen1936|1936]]) proved the consistency of arithmetic using a finitistic system together with a principle of [[transfinite induction]]. Gentzen's result introduced the ideas of [[cut elimination]] and [[proof-theoretic ordinal]]s, which became key tools in proof theory.  Gödel ([[#CITEREFGödel1958|1958]]) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intutitionistic arithmetic in higher types.  ====Beginnings of the other branches====  [[Alfred Tarski]] developed the basics of [[model theory]].  Beginning in 1935, a group of prominent mathematicians collaborated under the pseudonym [[Nicolas Bourbaki]] to publish a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words [[bijection, injection, and surjection|''bijection'', ''injection'', and ''surjection'']], and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.  The study of computability came to be known as recursion theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions.<ref>A detailed study of this terminology is given by Soare ([[#CITEREFSoare1996|1996]]).</ref> When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept &ndash; the [[computable function]] &ndash; had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.  Numerous results in recursion theory were obtained in the 1940s by [[Stephen Cole Kleene]] and [[Emil Leon Post]]. Kleene ([[#CITEREFKleene1943|1943]]) introduced the concepts of relative computability, foreshadowed by Turing ([[#CITEREFTuring1939|1939]]), and the [[arithmetical hierarchy]]. Kleene later generalized recursion theory to higher-order functionals. Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory. <!-- Perhaps it is better to stop this history around 1950 -->  == Formal logical systems {{anchor|Formal logic}} ==  At its core, mathematical logic deals with mathematical concepts expressed using formal [[logical system]]s. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language, or signature.  The systems of [[propositional logic]] and [[first-order logic]] are the most widely studied today, because of their applicability to [[foundations of mathematics]] and because of their desirable proof-theoretic properties.<ref>Ferreirós ([[#CITEREFFerreir.C3.B3s2001|2001]]) surveys the rise of first-order logic over other formal logics in the early 20th century.</ref>  Stronger classical logics such as [[second-order logic]] or [[infinitary logic]] are also studied, along with nonclassical logics such as [[intuitionistic logic]].  === First-order logic === {{Main|First-order logic}}  '''First-order logic''' is a particular [[logical system|formal system of logic]]. Its [[syntax]] involves only finite expressions as well-formed formulas, while its [[semantics]] are characterized by the limitation of all [[quantifiers]] to a fixed [[domain of discourse]].  Early results about formal logic established limitations of first-order logic. The [[Löwenheim–Skolem theorem]] (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to [[isomorphism]]. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.  [[Gödel's completeness theorem]] ([[#CITEREFG.C3.B6del1929|Gödel 1929]]) established the equivalence between semantic and syntactic definitions of [[logical consequence]] in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The [[compactness theorem]] first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model [[if and only if]] every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of [[model theory]], and they are a key reason for the prominence of first-order logic in mathematics.  [[Gödel's incompleteness theorems]] ([[#CITEREFG.C3.B6del1931|Gödel 1931]]) establish additional limits on first-order axiomatizations. The '''first incompleteness theorem''' states that for any sufficiently strong, effectively given logical system there exists a statement which is true but not provable within that system. Here a logical system is effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom. A logical system is sufficiently strong if it can express the [[Peano axioms]]. When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not [[elementary substructure|elementarily equivalent]], a stronger limitation than the one established by the Löwenheim–Skolem theorem. The '''second incompleteness theorem''' states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.  === Other classical logics ===  Many logics besides first-order logic are studied.  These include [[infinitary logics]], which allow for formulas to provide an infinite amount of information, and [[higher-order logic]]s, which include a portion of set theory directly in their semantics.  The most well studied infinitary logic is <math>L_{\omega_1,\omega}</math>. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of <math>L_{\omega_1,\omega}</math> such as :<math>(x = 0) \lor (x = 1) \lor (x = 2) \lor \cdots.</math>  Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.  The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.  Another type of logics are [[fixed-point logic]]s that allow [[inductive definition]]s, like one writes for [[primitive recursive function]]s.  One can formally define an extension of first-order logic &mdash; a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or [[fuzzy logic]]. [[Lindström's theorem]] implies that the only extension of first-order logic satisfying both the [[compactness theorem]] and the Downward [[Löwenheim–Skolem theorem]] is first-order logic.  === Nonclassical and modal logic ===  [[Modal logic]]s include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability ([[#CITEREFSolovay1976|Solovay 1976]]) and set-theoretic forcing ([[#CITEREFHamkinsL.C3.B6we|Hamkins and Löwe 2007]]).  [[Intuitionistic logic]] was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization.  Intuitionistic logic specifically does not include the [[law of the excluded middle]], which states that each sentence is either true or its negation is true.  Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is [[computable]]; this is not true in classical theories of arithmetic such as [[Peano arithmetic]].  === Algebraic logic ===  [[Algebraic logic]] uses the methods of [[abstract algebra]] to study the semantics of formal logics. A fundamental example is the use of [[Boolean algebra (structure)|Boolean algebras]] to represent [[truth value]]s in classical propositional logic, and the use of [[Heyting algebra]]s to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as [[cylindric algebra]]s.  == Set theory == {{Main|Set theory}}  '''[[Set theory]]''' is the study of [[Set (mathematics)|sets]], which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo ([[#CITEREFZermelo1908b|1908b]]), was extended slightly to become [[Zermelo–Fraenkel set theory]] (ZF), which is now the most widely used foundational theory for mathematics.  Other formalizations of set theory have been proposed, including [[von Neumann–Bernays–Gödel set theory]] (NBG), [[Morse–Kelley set theory]] (MK), and [[New Foundations]] (NF).  Of these, ZF, NBG, and MK are similar in describing a [[cumulative hierarchy]] of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of [[Kripke–Platek set theory]] is closely related to generalized recursion theory.  Two famous statements in set theory are the [[axiom of choice]] and the [[continuum hypothesis]]. The axiom of choice, first stated by Zermelo ([[#CITEREFZermelo1904|1904]]), was proved independent of ZF by Fraenkel ([[#CITEREFFraenkel1922|1922]]), but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set ''C'' that contains exactly one element from each set in the collection. The set ''C'' is said to "choose" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. [[Stefan Banach]] and [[Alfred Tarski]] (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the [[Banach–Tarski paradox]], is one of many counterintuitive results of the axiom of choice.  The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the [[constructible universe]] of set theory in which the continuum hypothesis must hold. In 1963, [[Paul Cohen (mathematician)|Paul Cohen]] showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory ([[#CITEREFCohen1966|Cohen 1966]]). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by [[W. Hugh Woodin]], although its importance is not yet clear ([[#CITEREFWoodin2001|Woodin 2001]]).  Contemporary research in set theory includes the study of [[large cardinal]]s and [[determinacy]].  Large cardinals are [[cardinal numbers]] with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an [[inaccessible cardinal]], already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high [[cardinality]], their existence has many ramifications for the structure of the real line.  ''Determinacy'' refers to the possible existence of winning strategies for certain two-player games (the games are said to be ''determined''). The existence of these strategies implies structural properties of the real line and other [[Polish space]]s.  == Model theory == {{Main|Model theory}}  '''[[Model theory]]''' studies the models of various formal theories.  Here a [[theory (mathematical logic)|theory]] is a set of formulas in a particular formal logic and [[signature (logic)|signature]], while a [[structure (mathematical logic)|model]] is a structure that gives a concrete interpretation of the theory. Model theory is closely related to [[universal algebra]] and [[algebraic geometry]], although the methods of model theory focus more on logical considerations than those fields.  The set of all models of a particular theory is called an [[elementary class]]; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.  The method of [[quantifier elimination]] can be used to show that definable sets in particular theories cannot be too complicated.  Tarski ([[#CITEREFTarski1948|1948]]) established quantifier elimination for [[real-closed field]]s, a result which also shows the theory of the field of real numbers is [[decidable set|decidable]]. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with [[o-minimal theory|o-minimal structure]]s.  [[Morley's categoricity theorem]], proved by [[Michael D. Morley]] (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.  A trivial consequence of the [[continuum hypothesis]] is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. [[Vaught conjecture|Vaught's conjecture]], named after [[Robert Lawson Vaught]], says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.  == Recursion theory == {{Main|Recursion theory}}  '''[[Recursion theory]]''', also called '''computability theory''', studies the properties of [[computable function]]s and the [[Turing degree]]s, which divide the uncomputable functions into sets which have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of [[Alonzo Church]] and [[Alan Turing]] in the 1930s, which was greatly extended by [[Stephen Cole Kleene|Kleene]] and [[Emil Leon Post|Post]] in the 1940s.  Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using [[Turing machine]]s, [[lambda calculus|&lambda; calculus]], and other systems.  More advanced results concern the structure of the Turing degrees and the [[lattice (order)|lattice]] of [[recursively enumerable set]]s.  Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as [[hyperarithmetical theory]] and [[alpha recursion theory|&alpha;-recursion theory]].  Contemporary research in recursion theory includes the study of applications such as [[algorithmic randomness]], [[computable model theory]], and [[reverse mathematics]], as well as new results in pure recursion theory.  === Algorithmically unsolvable problems ===  An important subfield of recursion theory studies algorithmic unsolvability; a [[decision problem]] or [[function problem]] is '''algorithmically unsolvable''' if there is no possible computable algorithm which returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the [[Entscheidungsproblem]] is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the [[halting problem]], a result with far-ranging implications in both recursion theory and computer science.  There are many known examples of undecidable problems from ordinary mathematics. The [[word problem for groups]] was proved algorithmically unsolvable by [[Pyotr Novikov]] in 1955 and independently by W. Boone in 1959.  The [[busy beaver]] problem, developed by [[Tibor Radó]] in 1962, is another well-known example.  [[Hilbert's tenth problem]] asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by [[Julia Robinson]], [[Martin Davis]] and [[Hilary Putnam]]. The algorithmic unsolvability of the problem was proved by [[Yuri Matiyasevich]] in 1970 (Davis 1973).  == Proof theory and constructive mathematics == {{Main|Proof theory}}  '''[[Proof theory]]''' is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including [[Hilbert-style deduction system]]s, systems of [[natural deduction]], and the [[sequent calculus]] developed by Gentzen.  The study of '''constructive mathematics''', in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of [[Impredicativity|predicative]] systems.  An early proponent of predicativism was [[Hermann Weyl]], who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).  Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the [[Gödel–Gentzen negative translation]] show that it is possible to embed (or ''translate'') classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.  Recent developments in proof theory include the study of [[proof mining]] by [[Ulrich Kohlenbach]] and the study of [[proof-theoretic ordinal]]s by Michael Rathjen.  ==Connections with computer science== {{Main|Logic in computer science}} The study of [[computability theory (computer science)|computability theory in computer science]] is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and [[feasible computability]], while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.  The theory of [[Program semantics|semantics of programming languages]] is related to [[model theory]], as is [[program verification]] (in particular, [[model checking]]). The [[Curry&ndash;Howard isomorphism]] between proofs and programs relates to [[proof theory]], especially [[intuitionistic logic]]. Formal calculi such as the [[lambda calculus]] and [[combinatory logic]] are now studied as idealized [[programming languages]].  Computer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as [[automated theorem proving]] and [[logic programming]].  [[Descriptive complexity theory]] relates logics to [[computational complexity]]. The first significant result in this area, [[Fagin's theorem]] (1974) established that [[NP (complexity)|NP]] is precisely the set of languages expressible by sentences of existential [[second-order logic]].  == Foundations of mathematics == {{Main|Foundations of mathematics}}  In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that [[Euclid]]'s axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of [[infinitesimal]]s, and the very definition of [[Function (mathematics)|function]], came into question in analysis, as pathological examples such as Weierstrass' nowhere-[[differentiable]] continuous function were discovered.  Cantor's study of arbitrary infinite sets also drew criticism. [[Leopold Kronecker]] famously stated "God made the integers; all else is the work of man," endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. [[David Hilbert]] argued in favor of the study of the infinite, saying "No one shall expel us from the Paradise that Cantor has created."  Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously-naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, [[non-Euclidean geometry]] can be proved consistent by defining ''point'' to mean a point on a fixed sphere and ''line'' to mean a [[great circle]] on the sphere. The resulting structure, a model of [[elliptic geometry]], satisfies the axioms of plane geometry except the parallel postulate.  With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of [[proof theory]]. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term ''finitary'' to refer to the methods he would allow but not precisely defining them. This project, known as [[Hilbert's program]], was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of [[transfinite induction]], and the techniques he developed to so do were seminal in proof theory.  A second thread in the history of foundations of mathematics involves [[nonclassical logic]]s and [[constructive mathematics]]. The study of constructive mathematics includes many different programs with various definitions of ''constructive''. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to [[natural numbers]], [[number-theoretic function]]s, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of [[mathematical analysis]]). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. <!-- ref "Varieties of constructive mathematics" -->  In the early 20th century, [[Luitzen Egbertus Jan Brouwer]] founded [[intuitionism]] as a philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to ''intuit'' the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the [[law of the excluded middle]], for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.  Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the [[BHK interpretation]] and [[Kripke model]]s, intuitionism became easier to reconcile with classical mathematics.  == See also == {{Portal|Logic}} * [[Knowledge representation]] * [[List of mathematical logic topics]] * [[List of computability and complexity topics]] * [[List of set theory topics]] * [[List of first-order theories]] * [[Logic symbols]] * [[Metalogic]]  ==Notes== {{Reflist}}  ==References== === Undergraduate texts === * {{Citation   |last1=Walicki   |first1=Michał   |title=Introduction to Mathematical Logic   |publisher=[[World Scientific Publishing]]   |location=Singapore   |isbn=978-981-4343-87-9 (pb.)   |year=2011}}.  * {{Citation   |last1=Boolos   |first1=George   |author1-link=en:George Boolos   |last2=Burgess   |first2=John   |last3=Jeffrey   |first3=Richard   |author3-link=en:Richard Jeffrey   |title=Computability and Logic   |publisher=[[Cambridge University Press]]   |location=Cambridge   |edition=4th   |isbn=978-0-521-00758-0 (pb.)   |year=2002}}. * {{Citation   |last1=Enderton   |first1=Herbert   |title=A mathematical introduction to logic   |publisher=[[Academic Press]]   |location=Boston, MA   |edition=2nd   |isbn=978-0-12-238452-3   |year=2001 }}. * {{Citation   |last1=Hamilton   |first1=A.G.   |title=Logic for Mathematicians   |publisher=Cambridge University Press   |location=Cambridge   |edition=2nd   |isbn=978-0-521-36865-0   |year=1988}}. *{{Citation   |last1=Ebbinghaus   |first1=H.-D.   |last2=Flum   |first2=J.   |last3=Thomas   |first3=W.   |doi=   |title=Mathematical Logic   |url=http://www.springer.com/mathematics/book/978-0-387-94258-2   |publisher=[[Springer Science Business Media|Springer]]   |location=[[New York City|New York]]   |edition=2nd   |isbn=0-387-94258-0   |year=1994 }}. * {{Citation   |last1=Katz   |first1=Robert   |title=Axiomatic Analysis   |publisher=[[D. C. Heath and Company]]   |location=Boston, MA   |year=1964}}. * {{Citation   |last1=Mendelson   |first1=Elliott   |title=Introduction to Mathematical Logic   |publisher=[[Chapman & Hall]]   |location=London   |edition=4th   |isbn=978-0-412-80830-2   |year=1997 }}. * {{Citation   |last=Rautenberg   |first=Wolfgang   |authorlink=Wolfgang Rautenberg   |doi=10.1007/978-1-4419-1221-3   |title=A Concise Introduction to Mathematical Logic   |url=http://www.springerlink.com/content/978-1-4419-1220-6/   |publisher=[[Springer Science Business Media]]   |location=[[New York City|New York]]   |edition=3rd   |isbn=978-1-4419-1220-6   |year=2010 }}. * {{Citation   |last1=Schwichtenberg   |first1=Helmut   |title=Mathematical Logic   |publisher=Mathematisches Institut der Universität München   |location=Munich, Germany   |url=http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf   |year=2003–2004 }}. * Shawn Hedman, ''A first course in logic: an introduction to model theory, proof theory, computability, and complexity'', Oxford University Press, 2004, ISBN 0-19-852981-3. Covers logics in close relation with [[computability theory]] and [[Computational complexity theory|complexity theory]]  === Graduate texts === * {{Citation   | last1=Andrews   | first1=Peter B.   | title=An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof   | publisher=Kluwer Academic Publishers   | location=Boston   | edition=2nd   | isbn=978-1-4020-0763-7   | year=2002 }}. * {{Citation   | editor1-last=Barwise   | editor1-first=Jon   | title=Handbook of Mathematical Logic   | publisher=[[Elsevier|North Holland]]   | series=Studies in Logic and the Foundations of Mathematics   | isbn=978-0-444-86388-1   | year=1989 }}. * {{Citation   | last1=Hodges   | first1=Wilfrid   | author1-link=Wilfrid Hodges   | title=A shorter model theory   | publisher=[[Cambridge University Press]]   | location=Cambridge | isbn=978-0-521-58713-6 | year=1997}}. * {{Citation   | last1=Jech   | first1=Thomas   | author1-link=en:Thomas Jech   | title=Set Theory: Millennium Edition   | publisher=[[Springer-Verlag]]   | location=Berlin, New York   | series=Springer Monographs in Mathematics   | isbn=978-3-540-44085-7   | year=2003 }}. * {{Citation   | last1=Shoenfield   | first1=Joseph R.   | title=Mathematical Logic   | origyear=1967   | publisher=[[A K Peters]]   | edition=2nd   | isbn=978-1-56881-135-2   | year=2001 }}. * {{Citation   | last1=Troelstra   | first1=Anne Sjerp   | author1-link=en:A. S. Troelstra   | last2=Schwichtenberg   | first2=Helmut   | title=Basic Proof Theory   | publisher=Cambridge University Press   | location=Cambridge   | edition=2nd   | series=Cambridge Tracts in Theoretical Computer Science   | isbn=978-0-521-77911-1   | year=2000 }}.  === Research papers, monographs, texts, and surveys === * {{Citation   | last1=Cohen   | first1=P. J.   | title=Set Theory and the Continuum Hypothesis   | publisher=W. A. Benjamin   | location=Menlo Park, CA   | year=1966 }}. * {{Citation   | last1=Davis   | first1=Martin   | author1-link=Martin Davis   | title=Hilbert's tenth problem is unsolvable   | year=1973   | journal=[[American Mathematical Monthly|The American Mathematical Monthly]]   | volume=80   | pages=233–269   | doi=10.2307/2318447   | issue=3   | publisher=The American Mathematical Monthly, Vol. 80, No. 3   | jstor=2318447 }}, reprinted as an appendix in Martin Davis, Computability and Unsolvability, Dover reprint 1982. [http://links.jstor.org/sici?sici=0002-9890%28197303%2980%3A3%3C233%3AHTPIU%3E2.0.CO%3B2-E JStor] *{{Citation  | title=Bolzano, Cauchy, Epsilon, Delta  | first1=Walter  | last1=Felscher  |journal=The American Mathematical Monthly  | volume = 107  | year = 2000  | pages = 844&ndash;862  | doi=10.2307/2695743  | issue = 9  | publisher=The American Mathematical Monthly, Vol. 107, No. 9  | jstor=2695743 }}. [http://links.jstor.org/sici?sici=0002-9890(200011)107%3A9%3C844%3ABCED%3E2.0.CO%3B2-L JSTOR] *{{Citation   | last1=Ferreirós   | first1=José   | title=The Road to Modern Logic-An Interpretation   |journal=Bulletin of Symbolic Logic   |volume=7   |year=2001   |pages=441&ndash;484   | doi=10.2307/2687794   |issue=4   | publisher=The Bulletin of Symbolic Logic, Vol. 7, No. 4   | jstor=2687794 }}. [http://links.jstor.org/sici?sici=1079-8986%28200112%297%3A4%3C441%3ATRTMLI%3E2.0.CO%3B2-O  JStor] * {{Citation  | last1 = Hamkins  | first1 = Joel David  | first2 = Benedikt  | last2 = Löwe  | title =  The modal logic of forcing  | journal = Transactions of the American Mathematical Society }}, to appear. [http://www.ams.org/tran/0000-000-00/S0002-9947-07-04297-3/home.html Electronic posting by the journal] * {{Citation   | last1=Katz   | first1=Victor J.   | title = A History of Mathematics   | year = 1998   | publisher = Addison–Wesley   | isbn = 0-321-01618-1 }}. * {{Citation   | last1=Morley   | first1=Michael   | author1-link=Michael D. Morley   | title=Categoricity in Power   | doi=10.2307/1994188   | year=1965   | journal=[[Transactions of the American Mathematical Society]]   | volume=114   | issue=2   | pages=514–538   | publisher=Transactions of the American Mathematical Society, Vol. 114, No. 2   | jstor=1994188 }}. *{{Citation   | last1 = Soare   | first1 = Robert I.   | title = Computability and recursion   | journal = Bulletin of Symbolic Logic   | volume = 2   | pages = 284–321   | doi = 10.2307/420992   | year = 1996   | issue = 3   | publisher = The Bulletin of Symbolic Logic, Vol. 2, No. 3   | jstor = 420992 }}. *{{Citation   | last1=Solovay   | first1= Robert M.   | author-link = Robert M. Solovay   | title = Provability Interpretations of Modal Logic   | journal = Israel Journal of Mathematics   | volume = 25   | year = 1976   | pages=287&ndash;304   | doi=10.1007/BF02757006   | issue=3–4 }}. *{{Citation   | last1=Woodin   | first1=W. Hugh   | author1-link=W. Hugh Woodin   | title=The Continuum Hypothesis, Part I   |journal=Notices of the American Mathematical Society   |volume=48   |year=2001   |issue=6 }}. [http://www.ams.org/notices/200106/fea-woodin.pdf PDF]  === Classical papers, texts, and collections === *{{Citation   | first1=Cesare   | last1=Burali-Forti   | year=1897|title = A question on transfinite numbers }}, reprinted in van Heijenoort 1976, pp.&nbsp;104&ndash;111. *{{citation   | first1=Richard   | last1=Dedekind   | year=1872   | title=Stetigkeit und irrationale Zahlen }}. English translation of title: "Consistency and irrational numbers". *{{Citation   | first1=Richard   | last1=Dedekind   | year=1888   | title=Was sind und was sollen die Zahlen?'' }} Two English translations: **1963 (1901). ''Essays on the Theory of Numbers''. Beman, W. W., ed. and trans. Dover. **1996. In ''From Kant to Hilbert: A Source Book in the Foundations of Mathematics'', 2 vols, Ewald, William B., ed., [[Oxford University Press]]: 787–832. *{{citation   | first1=Abraham A.   | last1=Fraenkel   | year=1922   | contribution=Der Begriff 'definit' und die Unabhängigkeit des Auswahlsaxioms   | title=Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-mathematische Klasse   | pages=253&ndash;257 }} (German), reprinted in English translation as "The notion of 'definite' and the independence of the axiom of choice", van Heijenoort 1976, pp.&nbsp;284&ndash;289.  *Frege Gottlob (1879), ''[[Begriffsschrift]], eine der arithmetischen nachgebildete Formelsprache des reinen Denkens''. Halle a. S.: Louis Nebert. Translation: ''Concept Script, a formal language of pure thought modelled upon that of arithmetic'', by S. Bauer-Mengelberg in [[Jean Van Heijenoort]], ed., 1967. ''From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931''. Harvard University Press. *Frege Gottlob (1884), ''Die Grundlagen der Arithmetik: eine logisch-mathematische Untersuchung über den Begriff der Zahl''. Breslau: W. Koebner. Translation: [[J. L. Austin]], 1974. ''The Foundations of Arithmetic: A logico-mathematical enquiry into the concept of number'', 2nd ed. Blackwell.  * {{citation   | author-link=Gerhard Gentzen   | first1=Gerhard   | last1=Gentzen   | year=1936   | title=Die Widerspruchsfreiheit der reinen Zahlentheorie   | journal=Mathematische Annalen   | volume=112   | pages=132&ndash;213   | doi=10.1007/BF01565428 }}, reprinted in English translation in Gentzen's ''Collected works'', M. E. Szabo, ed., North-Holland, Amsterdam, 1969.{{Specify|date=September 2009}} *{{Citation   | last1=Gödel   | first1=Kurt   | year = 1929   | title = Über die Vollständigkeit des Logikkalküls   | series = doctoral dissertation   | publisher = University Of Vienna }}. English translation of title: "Completeness of the logical calculus". * {{Citation   | last1=Gödel   | first1=Kurt   | year = 1930   | title = Die Vollständigkeit der Axiome des logischen Funktionen-kalküls   | journal =  Monatshefte für Mathematik und Physik   | volume = 37   | pages = 349&ndash;360   | doi=10.1007/BF01696781 }}. English translation of title: "The completeness of the axioms of the calculus of logical functions". * {{Citation   | last1=Gödel   | first1=Kurt   | title=Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I   | year=1931   | journal=Monatshefte für Mathematik und Physik   | volume=38   | issue=1   | pages=173–198   | doi=10.1007/BF01700692 }}, see [[On Formally Undecidable Propositions of Principia Mathematica and Related Systems]] for details on English translations. *{{Citation   | last1=Gödel   | first1=Kurt   | title=Über eine bisher noch nicht benützte Erweiterung des finiten Standpunktes   | year=1958   | journal=Dialectica. International Journal of Philosophy   | volume=12   | pages=280–287   | doi=10.1111/j.1746-8361.1958.tb01464.x   | issue=3–4 }}, reprinted in English translation in Gödel's ''Collected Works'', vol II, Soloman Feferman et al., eds. Oxford University Press, 1990.{{Specify|date=September 2009}} *{{citation   | editor-first=Jean   | editor-last=van Heijenoort   | editor-link=Jean van Heijenoort   | title = From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931   | edition = 3rd   | publisher = Harvard University Press   | location = Cambridge, Mass   | year = 1967, 1976 3rd printing with corrections   | id = (pbk.)   | isbn=0-674-32449-8 }} *{{citation   | last1=Hilbert   | first1=David   | year = 1899   | title = Grundlagen der Geometrie   | publisher = Teubner   | location = Leipzig }}, English 1902 edition (''The Foundations of Geometry'') republished 1980, Open Court, Chicago. *{{Citation   |first1=Hilbert   |last1=David   |year=1929   |title=Probleme der Grundlegung der Mathematik   |journal=Mathematische Annalen|volume=102|pages=1&ndash;9   |doi=10.1007/BF01782335 }}. Lecture given at the International Congress of Mathematicians, 3 September 1928. Published in English translation as "The Grounding of Elementary Number Theory", in Mancosu 1998, pp.&nbsp;266&ndash;273. * {{Citation   | last1=Kleene   | first1=Stephen Cole   | author1-link=en:Stephen Kleene   | title=Recursive Predicates and Quantifiers   | year=1943   | journal=American Mathematical Society Transactions   | volume=54   | issue=1   | pages=41–73   | doi=10.2307/1990131   | publisher=Transactions of the American Mathematical Society, Vol. 53, No. 1   | jstor=1990131 }}. * {{Citation   | first1 = Nikolai   | last1 = Lobachevsky   | authorlink = Nikolai Lobachevsky   | title = Geometrishe Untersuchungen zur Theorie der Parellellinien   | year = 1840 }} (German). Reprinted in English translation as "Geometric Investigations on the Theory of Parallel Lines" in ''Non-Euclidean Geometry'', Robert Bonola (ed.), Dover, 1955. ISBN 0-486-60027-0 * {{Citation   | last1=Löwenheim   | first1=Leopold   | author1-link=en:Leopold Löwenheim   | title=Über Möglichkeiten im Relativkalkül   | year=1915   | journal=Mathematische Annalen   | issn=0025-5831   | volume=76   | pages=447–470   | doi=10.1007/BF01458217   | issue=4}} (German). Translated as "On possibilities in the calculus of relatives" in [[Jean van Heijenoort]], 1967. ''A Source Book in Mathematical Logic, 1879–1931''. Harvard Univ. Press: 228–251. *{{Citation   |editor-last=Mancosu   |editor-first=Paolo   |year=1998   |title=From Brouwer to Hilbert. The Debate on the Foundations of Mathematics in   the 1920s   |publisher=Oxford University Press|publication-place=Oxford }}. *{{citation   |last1=Pasch   |first1=Moritz   |title=Vorlesungen über neuere Geometrie   |year=1882 }}. *{{citation   |first1=Giuseppe   |last1=Peano   |year=1888   |title=Arithmetices principia, nova methodo exposita }} (Italian), excerpt reprinted in English stranslation as "The principles of arithmetic, presented by a new method", van Heijenoort 1976, pp.&nbsp;83&nbsp;97. * {{Citation   |first1=Jules   |last1=Richard   |year=1905   |title=Les principes des mathématiques et le problème des ensembles   |journal=Revue générale des sciences pures et appliquées   |volume=16   |pages=541 }} (French), reprinted in English translation as "The principles of mathematics and the problems of sets", van Heijenoort 1976, pp.&nbsp;142&ndash;144. * {{Citation   | last1=Skolem   | first1=Thoralf   | author1-link=Thoralf Skolem   | title=Logisch-kombinatorische Untersuchungen über die Erfüllbarkeit oder Beweisbarkeit mathematischer Sätze nebst einem Theoreme über dichte Mengen   | year=1920   | journal=Videnskapsselskapet Skrifter, I. Matematisk-naturvidenskabelig Klasse   | volume=6   | pages=1–36}}. * {{Citation   | last1=Tarski   | first1=Alfred   | author1-link=Alfred Tarski   | title=A decision method for elementary algebra and geometry   | publisher=[[RAND Corporation]]   | location=Santa Monica, California   | year=1948}} *{{Citation   | last1=Turing   | first1=Alan M.   | author1-link=A. M. Turing   | title=Systems of Logic Based on Ordinals   | year=1939   | journal=[[Proceedings of the London Mathematical Society]]   | volume=45   | issue=2   | pages=161–228   | doi=10.1112/plms/s2-45.1.161 }} * {{Citation   |first1=Ernst   |last1=Zermelo   |year=1904   |title=Beweis, daß jede Menge wohlgeordnet werden kann   |journal=Mathematische Annalen|volume=59|pages=514&ndash;516   |doi=10.1007/BF01445300   |issue=4 }} (German), reprinted in English translation as "Proof that every set can be well-ordered", van Heijenoort 1976, pp.&nbsp;139&ndash;141. * {{Citation   |first1=Ernst   |last1=Zermelo   |year=1908a   |title=Neuer Beweis für die Möglichkeit einer Wohlordnung   |journal=[[Mathematische Annalen]]   |volume=65   |pages=107–128   |doi=10.1007/BF01450054   |issn=0025-5831 }} (German), reprinted in English translation as "A new proof of the possibility of a well-ordering", van Heijenoort 1976, pp.&nbsp;183&ndash;198. * {{Citation   | last1=Zermelo   | first1=Ernst   | year=1908b   | title=Untersuchungen über die Grundlagen der Mengenlehre   | journal=Mathematische Annalen   | volume=65   | pages=261–281   | doi=10.1007/BF01449999   | issue=2 }}.  == External links == * [http://settheory.net/world Logic and set theory around the world] * [http://home.swipnet.se/~w-33552/logic/home/index.htm Polyvalued logic] * ''[http://www.fecundity.com/logic/ forall x: an introduction to formal logic]'', by [[P.D. Magnus]], is a free textbook. * ''[http://euclid.trentu.ca/math/sb/pcml/ A Problem Course in Mathematical Logic]'', by Stefan Bilaniuk, is another free textbook. * Detlovs, Vilnis, and Podnieks, Karlis (University of Latvia) ''[http://www.ltn.lv/~podnieks/mlog/ml.htm Introduction to Mathematical Logic.]'' A hyper-textbook. *[[Stanford Encyclopedia of Philosophy]]: [http://plato.stanford.edu/entries/logic-classical/  Classical Logic] – by [[Stewart Shapiro]]. *Stanford Encyclopedia of Philosophy: [http://plato.stanford.edu/entries/modeltheory-fo/ First-order Model Theory ] – by [[Wilfrid Hodges]]. *The [http://www.ucl.ac.uk/philosophy/LPSG/ London Philosophy Study Guide] offers many suggestions on what to read, depending on the student's familiarity with the subject: **[http://www.ucl.ac.uk/philosophy/LPSG/MathLogic.htm Mathematical Logic] **[http://www.ucl.ac.uk/philosophy/LPSG/SetTheory.htm Set Theory & Further Logic] **[http://www.ucl.ac.uk/philosophy/LPSG/PhilMath.htm Philosophy of Mathematics]  {{Mathematics-footer}} {{Logic}}  [[Category:Mathematical logic| ]]  [[ar:منطق رياضي]] [[az:Riyazi məntiq]] [[be:Матэматычная логіка]] [[be-x-old:Матэматычная лёгіка]] [[bg:Математическа логика]] [[bs:Matematička logika]] [[ca:Lògica matemàtica]] [[cs:Matematická logika]] [[de:Mathematische Logik]] [[et:Matemaatiline loogika]] [[el:Μαθηματική λογική]] [[es:Lógica matemática]] [[eo:Matematika logiko]] [[fa:منطق ریاضی]] [[fr:Logique mathématique]] [[gd:Rianas matamataigeach]] [[ko:수리논리학]] [[hy:Մաթեմատիկական տրամաբանություն]] [[hi:गणितीय तर्कशास्त्र]] [[hr:Matematička logika]] [[io:Matematikala logiko]] [[id:Logika matematika]] [[it:Logica matematica]] [[he:לוגיקה מתמטית]] [[ka:მათემატიკური ლოგიკა]] [[lv:Matemātiskā loģika]] [[lij:Logica Matematica]] [[hu:Matematikai logika]] [[mk:Математичка логика]] [[ms:Logik matematik]] [[nl:Wiskundige logica]] [[ja:数理論理学]] [[no:Predikatslogikk]] [[nn:Matematisk logikk]] [[pl:Logika matematyczna]] [[pt:Lógica matemática]] [[ro:Logică matematică]] [[ru:Математическая логика]] [[sq:Logjika matematikore]] [[si:ගණිතමය තර්කණය]] [[sk:Matematická logika]] [[sl:Matematična logika]] [[sr:Математичка логика]] [[sh:Matematička logika]] [[sv:Matematisk logik]] [[tl:Matematikal na lohika]] [[th:คณิตตรรกศาสตร์]] [[tg:Мантиқи риёзӣ]] [[tr:Matematiksel mantık]] [[uk:Математична логіка]] [[ur:ریاضیاتی منطق]] [[vi:Logic toán]] [[zh-yue:數學邏輯]] [[zh:数理逻辑]]
'''Mathematical software''' is [[software]] used to [[mathematical model|model]], analyze or calculate numeric, symbolic or geometric data.{{fact|date=February 2012}}  ==Computer algebra systems== {{main|List of computer algebra systems}}  Many mathematical suites are [[computer algebra system]]s that use [[symbolic mathematics]].  They are designed to solve classical algebra equations and problems in human readable notation.  ==Statistics== {{main|List of statistical packages}} Many tools are available for statistical analysis of data. See also [[Comparison of statistical packages]].  ==Geometry== {{main|List of interactive geometry software}} {{main|List of information graphics software}}  ==Numerical analysis== {{main|List of numerical analysis software}}  The [[Netlib]] repository contains various collections of software routines for numerical problems, mostly in [[Fortran]] and [[C (programming language)|C]]. Commercial products implementing many different numerical algorithms include the [[IMSL Numerical Libraries|IMSL]], [[NMath]] and [[NAG_Numerical_Libraries|NAG libraries]]; a free alternative is the [[GNU Scientific Library]].  A different approach is taken by the [[Numerical Recipes]] library, where emphasis is placed on clear understanding of algorithms.  Many [[computer algebra system]]s (listed above) can also be used for numerical computations.  See also [[Comparison of numerical analysis software]].  ==Websites== Growing number of mathematical software is available in the web browser, without the need to download or install any code. Examples are [http://nclab.com NCLab] and [http://nb.sage.org Sage].  ==Programming libraries== Low-level mathematical libraries intended for use within other programming languages: * [[GNU Multi-Precision Library|GMP]], the [[GNU Multi-Precision Library]] for extremely fast [[arbitrary precision arithmetic]]. * [[Class Library for Numbers]], a high-level [[C  ]] library for [[arbitrary precision arithmetic]]. * [http://www.boost.org/doc/libs/1_48_0/libs/math/doc/html/index.html Boost.Math]  ==External links== * [http://mathstore.ac.uk/reviews/software.shtml Mathstore] published reviews of packages by United Kingdom [[Higher Education Academy]]'s ''Maths, Stats & OR Network''. * [http://web.archive.org/web/20000824122337/www.bham.ac.uk/ctimath/reviews/ CTI-Maths] Older reviews.  :{{portal|Software}}  [[Category:Mathematical software|*]]  [[bs:Matematički softver]] [[de:Mathematische Software]] [[es:Software matemático]] [[tr:Matematiksel yazılım]] [[zh:数学软件]]  {{software-type-stub}}
{{Use dmy dates|date=July 2012}} [[Image:Ybc7289-bw.jpg|thumb|250px|right|Babylonian clay tablet YBC 7289 (c. 1800–1600 BC) with annotations. The approximation of the [[square root of 2]] is four [[sexagesimal]] figures, which is about six [[decimal]] figures. 1   24/60   51/60<sup>2</sup>   10/60<sup>3</sup> = 1.41421296...<ref>[http://it.stlawu.edu/%7Edmelvill/mesomath/tablets/YBC7289.html Photograph, illustration, and description of the ''root(2)'' tablet from the Yale Babylonian Collection]</ref>]] '''Numerical analysis''' is the study of [[algorithm]]s that use numerical [[approximation]] (as opposed to general [[symbolic computation|symbolic manipulations]]) for the problems of [[mathematical analysis]] (as distinguished from [[discrete mathematics]]).  One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection(YBC 7289), which gives a sexagesimal numerical approximation of <math>\sqrt{2}</math>, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in carpentry and construction.<ref>The New Zealand Qualification authority specifically mentions this skill in document 13004 version 2, dated 17 October 2003 titled [http://www.nzqa.govt.nz/nqfdocs/units/pdf/13004.pdf CARPENTRY THEORY: Demonstrate knowledge of setting out a building]</ref>  Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of <math>\sqrt{2}</math>, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.  Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st&nbsp;century, the life sciences and even the arts have adopted elements of scientific computations. [[Ordinary differential equation]]s appear in the [[Celestial mechanics|movement of heavenly bodies (planets, stars and galaxies)]]; [[Mathematical optimization|optimization]] occurs in portfolio management; [[numerical linear algebra]] is important for data analysis; [[stochastic differential equation]]s and [[Markov chain]]s are essential in simulating living cells for medicine and biology.  Before the advent of modern computers numerical methods often depended on hand [[interpolation]] in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software [[algorithms]] for solving [[differential equations]].  ==General introduction== The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following.  * Advanced numerical methods are essential in making [[numerical weather prediction]] feasible. * Computing the trajectory of a spacecraft requires the accurate numerical solution of a system of [[ordinary differential equation]]s. * Car companies can improve the crash safety of their vehicles by using computer simulations of car crashes. Such simulations essentially consist of solving [[partial differential equation]]s numerically. * [[Hedge fund]]s (private investment funds) use tools from all fields of numerical analysis to calculate the value of stocks and derivatives more precisely than other market participants. * Airlines use sophisticated optimization algorithms to decide ticket prices, airplane and crew assignments and fuel needs. Historically, such algorithms were developed within the overlapping field of [[operations research]]. * Insurance companies use numerical programs for [[Actuary|actuarial]] analysis.  The rest of this section outlines several important themes of numerical analysis.  ===History===  The field of numerical analysis predates the invention of modern computers by many centuries. [[Linear interpolation]] was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like [[Newton's method]], [[Lagrange polynomial|Lagrange interpolation polynomial]], [[Gaussian elimination]], or [[Euler's method]].  To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients.  Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions.  The canonical work in the field is the [[NIST]] publication edited by [[Abramowitz and Stegun]], a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points.  The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.  The [[mechanical calculator]] was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.  ===Direct and iterative methods===  {| class="wikitable" style="float: right; width: 250px; margin-left: 1em;"  |-  | '''Direct vs iterative methods'''  Consider the problem of solving  :3''x''<sup>3</sup>   4 = 28  for the unknown quantity ''x''.  {| style="margin:auto;"  |  Direct method  |-  |  || 3''x''<sup>3</sup>   4 = 28.  |-  | ''Subtract 4'' || 3''x''<sup>3</sup> = 24.  |-  | ''Divide by 3'' || ''x''<sup>3</sup> = 8.  |-  | ''Take cube roots'' || ''x'' = 2.  |}  For the iterative method, apply the [[bisection method]] to ''f''(''x'') = 3''x''<sup>3</sup> &minus; 24. The initial values are ''a'' = 0, ''b'' = 3, ''f''(''a'') = &minus;24, ''f''(''b'') = 57.  {| style="margin:auto;"  |  Iterative method  |-  ! ''a'' !! ''b'' !! mid !! ''f''(mid)  |-  | 0 || 3 || 1.5 || &minus;13.875  |-  | 1.5 || 3 || 2.25 || 10.17...  |-  | 1.5 || 2.25 || 1.875 || &minus;4.22...  |-  | 1.875 || 2.25 || 2.0625 || 2.32...  |}  We conclude from this table that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.  ==== Discretization and numerical integration ====  [[Image:Schumacher (Ferrari) in practice at USGP 2005.jpg|right|125px]] In a two hour race, we have measured the speed of the car at three instants and recorded them in the following table.  {| style="margin:auto;" ! Time | 0:20 || 1:00 || 1:40 |- ! km/h | 140  || 150  || 180 |}  A '''discretization''' would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately (2/3h&nbsp;&times;&nbsp;140&nbsp;km/h)&nbsp;=&nbsp;93.3&nbsp;km. This would allow us to estimate the total distance traveled as 93.3&nbsp;km   100&nbsp;km   120&nbsp;km = 313.3&nbsp;km, which is an example of '''numerical integration''' (see below) using a [[Riemann sum]], because displacement is the [[integral]] of velocity.  '''Ill posed problem''': Take the function ''f''(''x'') = 1/(''x''&nbsp;&minus;&nbsp;1). Note that ''f''(1.1) = 10 and ''f''(1.001) = 1000: a change in ''x'' of less than 0.1 turns into a change in ''f''(''x'') of nearly 1000. Evaluating ''f''(''x'') near ''x'' = 1 is an ill-conditioned problem.  '''Well-posed problem''': By contrast, the function <math>f(x)=\sqrt{x}</math> is continuous and so evaluating it is well-posed, at least for ''x'' being not close to zero. |}  Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in [[Computer numbering formats|infinite precision arithmetic]]. Examples include [[Gaussian elimination]], the [[QR algorithm|QR]] factorization method for solving [[system of linear equations|systems of linear equations]], and the [[simplex method]] of [[linear programming]]. In practice, [[Floating point|finite precision]] is used and the result is an approximation of the true solution (assuming [[Numerically stable|stability]]).  In contrast to direct methods, [[iterative method]]s are not expected to terminate in a number of steps. Starting from an initial guess, iterative methods form successive approximations that [[Limit of a sequence|converge]] to the exact solution only in the limit. A [[convergence test]] is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include [[Newton's method]], the [[bisection method]], and [[Jacobi iteration]]. In computational matrix algebra, iterative methods are generally needed for large problems.  Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. [[GMRES]] and the [[conjugate gradient method]]. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.  ===Discretization===  Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called ''[[discretization]]''. For example, the solution of a [[differential equation]] is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.  ==Generation and propagation of errors== The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.  ===Round-off===  [[Round-off error]]s arise because it is impossible to represent all [[real number]]s exactly on a machine with finite memory (which is what all practical [[digital computer]]s are).  ===Truncation and discretization error===  [[Truncation]] errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a [[discretization error]] because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of <math>''3x^3 4=28''</math>, after 10 or so iterations, we conclude that the root is roughly 1.99 (for example). We therefore have a truncation error of 0.01.  Once an error is generated, it will generally propagate through the calculation. For instance, we have already noted that the operation   on a calculator (or a computer) is inexact. It follows that a calculation of the type a b c d e is even more inexact.  What does it mean when we say that the truncation error is created when we approximate a mathematical procedure?  We know that to integrate a function exactly requires one to find the sum of infinite trapezoids.  But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure.  Similarly, to differentiate a function, the differential element approaches to zero but numerically we can only choose a finite value of the differential element.  ===Numerical stability and well-posed problems===  [[Numerical stability]] is an important notion in numerical analysis. An algorithm is called ''numerically stable'' if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is ''[[condition number|well-conditioned]]'', meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is ''ill-conditioned'', then any small error in the data will grow to be a large error.  Both the original problem and the algorithm used to solve that problem can be ''well-conditioned'' and/or ''ill-conditioned'', and any combination is possible.  So an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation ''x''<sub>1</sub> to <math>\sqrt{2}</math>, for instance ''x''<sub>1</sub>=1.4, and then computing improved guesses ''x''<sub>2</sub>, ''x''<sub>3</sub>, etc.. One such method is the famous [[Babylonian method]], which is given by ''x''<sub>''k'' 1</sub> = ''x<sub>k</sub>''/2   1/''x<sub>k</sub>''. Another iteration, which we will call Method X, is given by ''x''<sub>''k''   1</sub> = (''x''<sub>''k''</sub><sup>2</sup>&minus;2)<sup>2</sup>   ''x''<sub>''k''</sub>.<ref>This is a [[fixed point iteration]] for the equation <math>x=(x^2-2)^2 x=f(x)</math>, whose solutions include <math>\sqrt{2}</math>. The iterates always move to the right since <math>f(x)\geq x</math>. Hence <math>x_1=1.4<\sqrt{2}</math> converges and <math>x_1=1.42>\sqrt{2}</math> diverges.</ref> We have calculated a few iterations of each scheme in table form below, with initial guesses ''x''<sub>1</sub> = 1.4 and ''x''<sub>1</sub> = 1.42.  {| class="wikitable"  |- ! Babylonian ! Babylonian ! Method X ! Method X  |-  | ''x''<sub>1</sub> = 1.4  | ''x''<sub>1</sub> = 1.42  | ''x''<sub>1</sub> = 1.4  | ''x''<sub>1</sub> = 1.42  |-  | ''x''<sub>2</sub> = 1.4142857...  | ''x''<sub>2</sub> = 1.41422535...  | ''x''<sub>2</sub> = 1.4016  | ''x''<sub>2</sub> = 1.42026896  |-  | ''x''<sub>3</sub> = 1.414213564...  | ''x''<sub>3</sub> = 1.41421356242...  | ''x''<sub>3</sub> = 1.4028614...  | ''x''<sub>3</sub> = 1.42056...  |-  |  |  | ...  | ...  |-  |  |  | ''x''<sub>1000000</sub> = 1.41421...  | ''x''<sub>28</sub> = 7280.2284...  |}  Observe that the Babylonian method converges fast regardless of the initial guess, whereas Method X converges extremely slowly with initial guess 1.4 and diverges for initial guess 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable. :'''Numerical stability''' is affected by the number of the significant digits the machine keeps on, if we use a machine that keeps on the first four floating-point digits, a good example on loss of significance is given by these two equivalent functions :<math>  f(x)=x\left(\sqrt{x 1}-\sqrt{x}\right) \text{ and } g(x)=\frac{x}{\sqrt{x 1} \sqrt{x}}. </math> :If we compare the results of :: <math> f(500)=500(\sqrt{501}-\sqrt{500})=500(22.3830-22.3607)=500(0.0223)=11.1500</math> :and :<math> \begin{alignat}{3}g(500)&=\frac{500}{\sqrt{501} \sqrt{500}}\\       &=\frac{500}{22.3830 22.3607}\\       &=\frac{500}{44.7437}=11.1748 \end{alignat} </math> : by looking to the two above results, we realize that '''[[loss of significance]]''' which is also called '''Subtractive Cancelation''' has a huge effect on the results, even though both functions are equivalent; to show that they are equivalent simply we need to start by f(x) and end with g(x), and so :: <math> \begin{alignat}{4} f(x)&=x(\sqrt{x 1}-\sqrt{x})\\     & =x(\sqrt{x 1}-\sqrt{x})\frac{(\sqrt{x 1} \sqrt{x})}{(\sqrt{x 1} \sqrt{x})}\\     &=x\frac{((\sqrt{x 1})^2-(\sqrt{x})^2)}{(\sqrt{x 1} \sqrt{x})}     &=\frac {x}{(\sqrt{x 1} \sqrt{x})} \end{alignat}</math> :The true value for the result is 11.174755..., which is exactly ''g''(500) = 11.1748 after rounding the result to 4 decimal digits. :Now imagine that lots of terms like these functions are used in the program; the error will increase as one proceeds in the program, unless one uses the suitable formula of the two functions each time one evaluates either ''f''(''x''), or ''g''(''x''); the choice is dependent on the parity of&nbsp;''x''. *The example is taken from Mathew; Numerical methods using matlab, 3rd ed.  ==Areas of study==  The field of numerical analysis is divided into different disciplines according to the problem that is to be solved.  ===Computing values of functions===  {| class="wikitable" style="float: right; width: 250px; clear: right; margin-left: 1em;"  | '''Interpolation''': We have observed the temperature to vary from 20 degrees Celsius at 1:00 to 14 degrees at 3:00. A linear interpolation of this data would conclude that it was 17 degrees at 2:00 and 18.5 degrees at 1:30pm.  '''Extrapolation''': If the [[gross domestic product]] of a country has been growing an average of 5% per year and was 100 billion dollars last year, we might extrapolate that it will be 105 billion dollars this year.  [[Image:Linear-regression.svg|right|100px|A line through 20 points]]  '''Regression''': In linear regression, given ''n'' points, we compute a line that passes as close as possible to those ''n'' points.  [[Image:LemonadeJuly2006.JPG|right|100px|How much for a glass of lemonade?]]  '''Optimization''': Say you sell lemonade at a [[lemonade stand]], and notice that at $1, you can sell 197 glasses of lemonade per day, and that for each increase of $0.01, you will sell one glass of lemonade less per day. If you could charge $1.485, you would maximize your profit, but due to the constraint of having to charge a whole cent amount, charging $1.49 per glass will yield the maximum income of $220.52 per day.  [[Image:Wind-particle.png|right|Wind direction in blue, true trajectory in black, Euler method in red.]]  '''Differential equation''': If you set up 100 fans to blow air from one end of the room to the other and then you drop a feather into the wind, what happens? The feather will follow the air currents, which may be very complex. One approximation is to measure the speed at which the air is blowing near the feather every second, and advance the simulated feather as if it were moving in a straight line at that same speed for one second, before measuring the wind speed again. This is called the [[Euler method]] for solving an ordinary differential equation. |}  One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the [[Horner scheme]], since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control [[round-off error]]s arising from the use of [[floating point]] arithmetic.  ===Interpolation, extrapolation, and regression===  [[Interpolation]] solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?  [[Extrapolation]] is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.  [[Regression analysis|Regression]] is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The [[least squares]]-method is one popular way to achieve this.  ===Solving equations and systems of equations===  Another fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation <math>2x 5=3</math> is linear while <math>2x^2 5=3</math> is not.  Much effort has been put in the development of methods for solving [[systems of linear equations]]. Standard direct methods, i.e., methods that use some [[matrix decomposition]] are [[Gaussian elimination]], [[LU decomposition]], [[Cholesky decomposition]] for [[symmetric matrix|symmetric]] (or [[hermitian matrix|hermitian]]) and [[positive-definite matrix]], and [[QR decomposition]] for non-square matrices. [[Iterative method]]s such as the [[Jacobi method]], [[Gauss–Seidel method]], [[successive over-relaxation]] and [[conjugate gradient method]] are usually preferred for large systems.  [[Root-finding algorithm]]s are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is [[derivative|differentiable]] and the derivative is known, then [[Newton's method]] is a popular choice. [[Linearization]] is another technique for solving nonlinear equations.  ===Solving eigenvalue or singular value problems=== Several important problems can be phrased in terms of [[eigenvalue decomposition]]s or [[singular value decomposition]]s. For instance, the [[image compression|spectral image compression]] algorithm<ref>[http://online.redwoods.cc.ca.us/instruct/darnold/maw/single.htm The Singular Value Decomposition and Its Applications in Image Compression]</ref> is based on the singular value decomposition. The corresponding tool in statistics is called [[principal component analysis]].  ===Optimization=== {{Main|Mathematical optimization}}  Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some [[Constraint (mathematics)|constraint]]s.  The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, [[linear programming]] deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the [[simplex method]].  The method of [[Lagrange multipliers]] can be used to reduce optimization problems with constraints to unconstrained optimization problems.  ===Evaluating integrals=== {{Main|Numerical integration}}  Numerical integration, in some instances also known as numerical [[quadrature (mathematics)|quadrature]], asks for the value of a definite [[integral]]. Popular methods use one of the [[Newton–Cotes formulas]] (like the midpoint rule or [[Simpson's rule]]) or [[Gaussian quadrature]]. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use [[Monte Carlo method|Monte Carlo]] or [[quasi-Monte Carlo method]]s (see [[Monte Carlo integration]]), or, in modestly large dimensions, the method of [[sparse grid]]s.  ===Differential equations=== {{main|Numerical ordinary differential equations|Numerical partial differential equations}}  Numerical analysis is also concerned with computing (in an approximate way) the solution of [[differential equation]]s, both ordinary differential equations and [[partial differential equation]]s.  Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a [[finite element method]], a [[finite difference]] method, or (particularly in engineering) a [[finite volume method]]. The theoretical justification of these methods often involves theorems from [[functional analysis]]. This reduces the problem to the solution of an algebraic equation.  ==Software== {{main|List of numerical analysis software|Comparison of numerical analysis software}}  Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The [[Netlib]] repository contains various collections of software routines for numerical problems, mostly in [[Fortran]] and [[C (programming language)|C]]. Commercial products implementing many different numerical algorithms include the [[IMSL Numerical Libraries|IMSL]] and [[Numerical Algorithms Group|NAG]] libraries; a free alternative is the [[GNU Scientific Library]].  There are several popular numerical computing applications such as [[MATLAB]], [[S-PLUS]], [[LabVIEW]], and [[IDL (programming language)|IDL]] as well as free and open source alternatives such as [[FreeMat]], [[Scilab]], [[GNU Octave]] (similar to Matlab), [[IT  ]] (a C   library), [[R (programming language)|R]] (similar to S-PLUS) and certain variants of [[Python (programming language)|Python]]. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.<ref>[http://www.sciviews.org/benchmark/ Speed comparison of various number crunching packages]</ref><ref>[http://www.scientificweb.com/ncrunch/ncrunch5.pdf Comparison of mathematical programs for data analysis] Stefan Steinhaus, ScientificWeb.com</ref>  Many [[computer algebra system]]s such as [[Mathematica]] also benefit from the availability of [[arbitrary precision arithmetic]] which can provide more accurate results.  Also, any [[spreadsheet]] software can be used to solve simple problems relating to numerical analysis.  ==See also== *[[Scientific computing]] *[[List of numerical analysis topics]] *[[Gram-Schmidt process]] *[[Numerical differentiation]] *[[Symbolic-numeric computation]] *[[Analysis of algorithms]] *[[Numerical Recipes]]  ==Notes== <references/>  ==References== <!-- This template can be used for additional references. *{{cite book |last= |first= |authorlink= |coauthors= |title= |year= |publisher= |location= |id= }} -->  *{{cite book |last=Gilat |first=Amos |authorlink= |coauthors= |title=MATLAB: An Introduction with Applications | edition=2nd edition |year=2004 |publisher=John Wiley & Sons |location= |isbn=0-471-69420-7 }} *{{cite book |last=Hildebrand |first=F. B. | authorlink=Francis B. Hildebrand | title=Introduction to Numerical Analysis | edition=2nd edition |year=1974 |publisher=McGraw-Hill |location= |isbn= 0-07-028761-9}} *{{cite book |last=Leader |first=Jeffery J. | title=Numerical Analysis and Scientific Computation |year=2004 |publisher=Addison Wesley |location= |isbn= 0-201-73499-0 }} * Trefethen, Lloyd N. (2006). [http://people.maths.ox.ac.uk/trefethen/NAessay.pdf "Numerical analysis"], 20 pages. In: Timothy Gowers and June Barrow-Green (editors), ''Princeton Companion of Mathematics'', Princeton University Press. *{{cite book |first=Nicholas J.|last=Higham |title=Accuracy and Stability of Numerical Algorithms (Society for Industrial and Applied Mathematics, ISBN 0-89871-355-2)|year=1966}} *{{cite book|author=Gene F. Golub and Charles F. van Loan|title=Matrix Computations, Third Edition (Johns Hopkins University Press, ISBN 0-8018-5413)|year=1986}} *{{cite book|author=J.H. Wilkinson M.A., D. Sc.|title=The Algebraic Eigenvalue Problem (Clarendon Press)|year=1965)}} *{{cite journal | author=Kahan, W. [1972] | title= “A survey of error-analysis,” in Info. Processing 71 (Proc. IFIP Congress 71 in Ljubljana), vol. 2, pp. 1214–39, North-Holland Publishing, Amsterdam}} (examples of the importance of accurate arithmetic).  ==External links== {{Wikibooks}}  '''Journals''' *[http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN362160546 Numerische Mathematik], volumes 1-66, Springer, 1959-1994 (searchable; pages are images). {{en icon}} {{de icon}} *[http://www.springerlink.com/content/0029-599X Numerische Mathematik at SpringerLink], volumes 1-112, Springer, 1959–2009 *[http://siamdl.aip.org/dbt/dbt.jsp?KEY=SJNAAM SIAM Journal on Numerical Analysis], volumes 1-47, SIAM, 1964–2009  '''Software and Code''' *[http://people.sc.fsu.edu/~tomek/Fortran/num_meth.html Numerical methods for Fortran programmers] *[http://www.apropos-logic.com/nc/ Java Number Cruncher] features free, downloadable code samples that graphically illustrate common numerical algorithms *[http://www.ifh.uni-karlsruhe.de/people/fenton/Lectures.html Excel Implementations] *[http://www.akiti.ca/Mathfxns.html Several Numerical Mathematical Utilities (in Javascript)]  '''Online Texts''' *[http://www.nr.com/oldverswitcher.html ''Numerical Recipes''], William H. Press (free, downloadable previous editions) *[http://kr.cs.ait.ac.th/~radok/math/mat7/stepsa.htm#Numerical%20Analysis ''First Steps in Numerical Analysis''], R.J.Hosking, S.Joe, D.C.Joyce, and J.C.Turner *[http://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/ ''Numerical Analysis for Engineering''], D. W. Harder *[http://www.phy.ornl.gov/csep/CSEP/TEXTOC.html ''CSEP'' (Computational Science Education Project)], [[U.S. Department of Energy]]  '''Online Course Material''' *[http://www.damtp.cam.ac.uk/user/fdl/people/sd103/lectures/nummeth98/index.htm#L_1_Title_Page Numerical Methods], Stuart Dalziel [[University of Cambridge]] *[http://www.math.upenn.edu/~wilf/DeturckWilf.pdf Lectures on Numerical Analysis], Dennis Deturck and Herbert S. Wilf [[University of Pennsylvania]] *[http://www.ifh.uni-karlsruhe.de/people/fenton/LectureNotes/Numerical-Methods.pdf Numerical methods], John D. Fenton [[University of Karlsruhe]] *[http://numericalmethods.eng.usf.edu/ Numerical Methods for Science, Technology, Engineering and Mathematics], Autar Kaw [[University of South Florida]] *[http://math.fullerton.edu/mathews/numerical.html Numerical Analysis Project], John H. Mathews [[California State University, Fullerton]] *[http://www.math.jct.ac.il/~naiman/nm/ Numerical Methods - Online Course], Aaron Naiman [[Jerusalem College of Technology]] *[http://www-teaching.physics.ox.ac.uk/computing/NumericalMethods/NMfP.pdf Numerical Methods for Physicists], Anthony O’Hare [[Oxford University]] *[http://kr.cs.ait.ac.th/~radok/math/mat7/stepsa.htm#Numerical%20Analysis Lectures in Numerical Analysis], R. Radok [[Mahidol University]] *[http://ocw.mit.edu/courses/mechanical-engineering/2-993j-introduction-to-numerical-analysis-for-engineering-13-002j-spring-2005/ Introduction to Numerical Analysis for Engineering], Henrik Schmidt [[Massachusetts Institute of Technology]]  {{Mathematics-footer}} {{Physics-footer}}  {{DEFAULTSORT:Numerical Analysis}} [[Category:Numerical analysis| Numerical analysis]] [[Category:Mathematical physics]] [[Category:Computational science]]  [[af:Numeriese analise]] [[ar:تحليل عددي]] [[an:Analís numerica]] [[be:Вылічальная матэматыка]] [[be-x-old:Вылічальная матэматыка]] [[bg:Числен анализ]] [[ca:Anàlisi numèrica]] [[cs:Numerická matematika]] [[da:Numerisk analyse]] [[de:Numerische Mathematik]] [[et:Arvutusmatemaatika]] [[el:Αριθμητική ανάλυση]] [[es:Análisis numérico]] [[eo:Cifereca analitiko]] [[fa:محاسبات عددی]] [[fr:Analyse numérique]] [[gl:Análise numérica]] [[ko:수치 해석]] [[hi:आंकिक विश्लेषण]] [[hr:Numerička analiza]] [[id:Analisis numeris]] [[it:Analisi numerica]] [[he:אנליזה נומרית]] [[ka:რიცხვითი ანალიზი]] [[kk:Есептеу математикасы]] [[la:Analysis numerica]] [[lt:Skaičiavimo metodai]] [[hu:Numerikus analízis]] [[ms:Analisis berangka]] [[mn:Тоон анализ]] [[nl:Numerieke wiskunde]] [[ja:数値解析]] [[no:Numerisk analyse]] [[oc:Analisi numerica]] [[pnb:نمبری انیلیسز]] [[pl:Analiza numeryczna]] [[pt:Análise numérica]] [[ro:Analiză numerică]] [[ru:Вычислительная математика]] [[sc:Anàlisi numèrica]] [[si:සංඛ්‍යාමය විශ්ලේෂණය]] [[simple:Numerical analysis]] [[sl:Numerična matematika]] [[sr:Нумеричка анализа]] [[sh:Numerička analiza]] [[su:Analisis numeris]] [[fi:Numeerinen analyysi]] [[sv:Beräkningsvetenskap]] [[ta:எண்சார் பகுப்பியல்]] [[tr:Sayısal yöntemler]] [[uk:Обчислювальна математика]] [[ur:عددی تحلیل]] [[vi:Giải tích số]] [[war:Ihapnon nga pag-analisa]] [[zh:数值分析]]
{{Refimprove|date=October 2011}} {{OS}} An '''operating system''' ('''OS''') is a set of software that  manages [[computer hardware]] resources and provides common [[operating system services|services]] for [[computer program]]s. The operating system is a vital component of the [[system software]] in a computer system. Application programs require an operating system to function.  [[Time-sharing]] operating systems schedule tasks for efficient use of the system and may also include accounting for cost allocation of processor time, mass storage, printing, and other resources.  For hardware functions such as input and output and [[dynamic memory allocation|memory allocation]], the operating system acts as an intermediary between programs and the computer hardware,<ref>{{cite book | last = Stallings | title = Operating Systems, Internals and Design Principles | publisher = Prentice Hall | year = 2005 | location = Pearson |page=6}}</ref><ref>{{cite book | last = Dhotre| first = I.A.| title = Operating Systems. | publisher = Technical Publications | year = 2009 |page=1}}</ref> although the application code is usually executed directly by the hardware and will frequently make a [[system call]] to an OS function or be interrupted by it. Operating systems can be found on almost any device that contains a computer—from [[cellular phone]]s and [[video game console]]s to [[supercomputer]]s and [[web server]]s.  Examples of popular modern operating systems include <!--Ordered alphabetically per discussion consensus; please do not re-order without discussing it on the talk page; see http://en.wikipedia.org/wiki/Talk:Operating_system#Market_share_changes.3B_the_alphabet_doesn.27t --> [[Android (operating system)|Android]], [[BSD]], [[iOS]], [[Linux]], [[Mac OS X]], [[Microsoft Windows]],<ref name="netapplications">{{cite web| url=http://marketshare.hitslink.com/operating-system-market-share.aspx?qprid=10| title=Operating System Market Share| publisher=Net Applications}}</ref> [[Windows Phone]], and [[IBM z/OS]]. All these, except Windows and z/OS, share roots in [[UNIX]].  ==Types== {{Unreferenced section|date=February 2012}} ;Real-time :A [[real-time operating system]] is a multitasking operating system that aims at executing real-time applications. Real-time operating systems often use specialized scheduling algorithms so that they can achieve a deterministic nature of behavior. The main objective of real-time operating systems is their quick and predictable response to events. They have an event-driven or time-sharing design and often aspects of both. An event-driven system switches between tasks based on their priorities or external events while time-sharing operating systems switch tasks based on clock interrupts.  ;Multi-user :A multi-user operating system allows multiple users to access a computer system concurrently. Time-sharing systems can be classified as multi-user systems as they enable multiple-user access to a computer through the sharing of time. Single-user operating systems, as opposed to multi-user operating systems, are usable by a single user at a time. Being able to use multiple accounts on a Windows operating system does not make it a multi-user system. Rather, only the network administrator is the real user. But for a UNIX-like operating system, it is possible for two users to log in at a time and this capability of the OS makes it a multi-user operating system.  ;Multi-tasking vs. single-tasking :When only a single program is allowed to run at a time, the system is grouped as a single-tasking system. However, when the operating system allows the execution of multiple tasks at one time, it is classified as a multi-tasking operating system. Multi-tasking can be of two types: pre-emptive or co-operative. In pre-emptive multitasking, the operating system slices the CPU time and dedicates one slot to each of the programs. Unix-like operating systems such as Solaris and Linux support pre-emptive multitasking, as does [[AmigaOS]]. Cooperative multitasking is achieved by relying on each process to give time to the other processes in a defined manner. [[16-bit]] versions of Microsoft Windows used cooperative multi-tasking. [[32-bit]] versions, both Windows NT and Win9x, used pre-emptive multi-tasking. Mac OS prior to OS X used to support cooperative multitasking.  ;Distributed {{further|Distributed system}} :A distributed operating system manages a group of independent computers and makes them appear to be a single computer. The development of networked computers that could be linked and communicate with each other gave rise to distributed computing. Distributed computations are carried out on more than one machine. When computers in a group work in cooperation, they make a distributed system.  ;Embedded :[[Embedded system|Embedded]] operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines like PDAs with less autonomy. They are able to operate with a limited number of resources. They are very compact and extremely efficient by design. Windows CE and Minix 3 are some examples of embedded operating systems.  ==History== {{Main|History of operating systems}} {{seealso|Resident monitor}} Early computers were built to perform a series of single tasks, like a calculator. Operating systems did not exist in their modern and more complex forms until the early 1960s.<ref name="google4">{{Cite book |title= Classic Operating Systems|editor1-first= Per Brinch|editor1-last= Hansen|year= 2001|publisher= Springer|location= |isbn= 0-387-95113-X|pages=4–7|url= http://books.google.com/?id=-PDPBvIPYBkC&lpg=PP1&pg=PP1#v=onepage&q}}</ref> Basic operating system features were developed in the 1950s, such as [[resident monitor]] functions that could automatically run different programs in succession to speed up processing.  Hardware features were added that enabled use of [[Runtime library|runtime libraries]], [[Programmable Interrupt Controller|interrupts]], and [[parallel processing]]. When [[personal computer]]s became popular in the 1980s, operating system were made for them similar in concept to those used on larger computers.  In the 1940s, the earliest electronic digital systems had no operating systems.  Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plug boards.  These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards.  After programmable general purpose computers were invented, machine languages (consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981). [[Image:IBM360-65-1.corestore.jpg|thumb|[[OS/360]] was used on most IBM mainframe computers beginning in 1966, including the computers that helped NASA put a man on the moon.]]  In the early 1950s, a computer could execute only one program at a time.  Each user had sole use of the computer for a limited period of time and would arrive at a scheduled time with program and data on punched paper cards and/or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that [[Alan Turing]] was a master of this on the early [[Manchester Mark 1]] machine, and he was already deriving the primitive conception of an operating system from the principles of the [[Universal Turing machine]].<ref name="google4"/>  Later machines came with libraries of [[Computer software|programs]], which would be linked to a user's program to assist in operations such as input and output and generating [[Machine code|computer code]] from human-readable [[Assembly language|symbolic code]]. This was the genesis of the modern-day computer system. However, machines still ran a single job at a time.  At Cambridge University in England the job queue was at one time a washing line from which tapes were hung with different colored clothes-pegs to indicate job-priority.{{citation needed|date=September 2010}}  ===Mainframes=== {{Main| Mainframe computer}} {{see also|History of IBM mainframe operating systems}}  Through the 1950s, many major features were pioneered in the field of operating systems, including [[batch processing]], input/output [[interrupt]], [[Data buffer|buffer]]ing, [[Computer multitasking|multitasking]], [[spooling]], [[runtime library|runtime libraries]], [[Linker (computing)|link-loading]], and programs for [[Sorting algorithm|sorting records]] in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications.  In 1959 the [[SHARE Operating System]] was released as an integrated utility for the [[IBM 704]], and later in the [[IBM 709|709]] and [[IBM 7090|7090]] mainframes, although it was quickly supplanted by [[IBM 7090/94 IBSYS|IBSYS]]/IBJOB on the 709, 7090 and 7094.  During the 1960s, IBM's [[OS/360]] introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines.  [[IBM]]'s current mainframe operating systems are [[History of IBM mainframe operating systems|distant descendants]] of this original system and applications written for OS/360 can still be run on modern machines.{{Citation needed|date=June 2010}}  [[OS/360]] also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and [[file locking]] during update. When the process is terminated for any reason, all of these resources are re-claimed by the operating system.  The alternative [[CP-67]] system for the [[IBM System/360 Model 67|S/360-67]] started a whole line of IBM operating systems focused on the concept of [[virtual machine]]s. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: COS/360 (Compatibility Operating System), [[DOS/360]] (Disk Operating System), [[TSS/360]] (Time Sharing System), [[TOS/360]] (Tape Operating System), [[BOS/360]] (Basic Operating System), and [[IBM Airline Control Program|ACP]] (Airline Control Program), as well as a few non-IBM systems: [[Michigan Terminal System|MTS]] (Michigan Terminal System), [[MUSIC/SP|MUSIC]] (Multi-User System for Interactive Computing), and [[ORVYL]] (Stanford Timesharing System).  [[Control Data Corporation]] developed the [[SCOPE (software)|SCOPE]] operating system in the 1960s, for [[batch processing]]. In cooperation with the University of Minnesota, the [[CDC Kronos|Kronos]] and later the [[NOS (software)|NOS]] operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the [[PLATO (computer system)|PLATO]] operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games. [[Burroughs Corporation]] introduced the [[B5000]] in 1961 with the [[Master Control Program|MCP]], ([[MCP (Burroughs Large Systems)|Master Control Program]]) operating system. The [[B5000]] was a [[stack machine]] designed to exclusively support high-level languages with no machine language or assembler, and indeed the [[Master Control Program|MCP]] was the first OS to be written exclusively in a high-level language – [[ESPOL]], a dialect of [[ALGOL]]. [[Master Control Program|MCP]] also introduced many other ground-breaking innovations, such as being the first commercial implementation of [[virtual memory]]. During development of the [[AS400]], [[IBM]] made an approach to Burroughs to licence MCP to run on the AS400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. [[Master Control Program|MCP]] is still in use today in the [[Unisys]] [[ClearPath/MCP]] line of computers.  UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems. Like all early main-frame systems, this was a batch-oriented system that managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.  General Electric and MIT developed General Electric Comprehensive Operating Supervisor (GECOS), which introduced the concept of ringed security privilege levels. After acquisition by Honeywell it was renamed to [[General Comprehensive Operating System]] (GCOS).  Digital Equipment Corporation developed many operating systems for its various computer lines, including [[TOPS-10]] and [[TOPS-20]] time sharing systems for the 36-bit PDP-10 class systems. Prior to the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early [[ARPANET]] community.  In the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying [[computer architecture]]s to appear to be the same as others in a series. In fact most 360s after the 360/40 (except the 360/165 and 360/168) were microprogrammed implementations. But soon other means of achieving application compatibility were proven to be more significant.  The enormous investment in software for these systems made since 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. The notable supported mainframe operating systems include: * [[MCP (Burroughs Large Systems)|Burroughs MCP]] – [[Burroughs large systems|B5000]], 1961 to [[Unisys]] Clearpath/MCP, present. * IBM [[OS/360]] – [[IBM System/360]], 1966 to IBM [[z/OS]], present. * IBM [[CP-67]] – [[IBM System/360]], 1967 to IBM [[z/VM]], present. * UNIVAC [[EXEC 8]] – [[UNIVAC 1108]], 1967, to [[Unisys OS 2200 operating system|OS 2200]] [[Unisys]] Clearpath Dorado, present.  ===Microcomputers=== [[Image:PC-DOS 1.10 screenshot.png|thumb|256px|PC-DOS was an early personal computer OS that featured a command line interface.]] [[Image:Apple Macintosh Desktop.png|thumb|256px|Mac OS by [[Apple Computer]] became the first widespread OS to feature a [[graphical user interface]]. Many of its features such as windows and icons would later become commonplace in GUIs.]] The first [[microcomputer]]s did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from [[Read-only memory|ROM]] and known as ''[[resident monitor|monitor]]s''. One notable early [[disk operating system]] was [[CP/M]], which was supported on many early microcomputers and was closely imitated by [[Microsoft]]'s [[MS-DOS]], which became wildly popular as the operating system chosen for the [[IBM PC]] (IBM's version of it was called IBM DOS or [[PC-DOS|PC DOS]]). In the '80s, Apple Computer Inc. (now [[Apple Inc.]]) abandoned its popular [[Apple II]] series of microcomputers to introduce the [[Apple Macintosh]] computer with an innovative [[Graphical User Interface]] (GUI) to the [[Mac OS]].  The introduction of the [[Intel 80386]] [[CPU]] chip with [[32-bit]] architecture and [[paging]] capabilities, provided personal computers with the ability to run [[Computer multitasking|multitasking]] operating systems like those of earlier [[minicomputer]]s and [[mainframe computer|mainframes]]. Microsoft responded to this progress by hiring [[Dave Cutler]], who had developed the [[OpenVMS|VMS]] operating system for [[Digital Equipment Corporation]]. He would lead the development of the [[Windows NT]] operating system, which continues to serve as the basis for Microsoft's operating systems line. [[Steve Jobs]], a co-founder of [[Apple Inc.]], started [[NeXT]] Computer Inc., which developed the [[Unix-like]] [[NEXTSTEP]] operating system. NEXTSTEP would later be acquired by [[Apple Inc.]] and used, along with code from [[FreeBSD]] as the core of Mac OS X.  The [[GNU Project]] was started by activist and programmer [[Richard Stallman]] with the goal of a complete [[free software]] replacement to the proprietary [[UNIX]] operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the [[GNU Hurd]] kernel proved to be unproductive. In 1991, Finnish computer science student [[Linus Torvalds]], with cooperation from volunteers collaborating over the Internet, released the first version of the [[Linux kernel]]. It was soon merged with the GNU [[user space]] components and [[system software]] to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the [[Free Software Foundation]] remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as [[BSD (operating system)|BSD]], is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and [[ported]] to many minicomputers, it eventually also gained a following for use on PCs, mainly as [[FreeBSD]], [[NetBSD]] and [[OpenBSD]].  ==Examples of operating systems== ===UNIX and UNIX-like operating systems=== <imagemap> File:Unix history-simple.png|256px|thumb|Evolution of [[Unix]] systems default [[Image:Unix history-simple.svg]] </imagemap> {{Main|Unix}} [[Ken Thompson]] wrote [[B (programming language)|B]], mainly based on [[BCPL]], which he used to write Unix, based on his experience in the [[MULTICS]] project. B was replaced by [[C (programming language)|C]], and Unix developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see [[History of operating systems|History]]).  The ''[[UNIX-like]]'' family is a diverse group of operating systems, with several major sub-categories including [[System V]], [[BSD (operating system)|BSD]], and [[Linux]]. The name "[[UNIX]]" is a trademark of [[The Open Group]] which licenses it for use with any operating system that has been shown to conform to their definitions. "UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.  Unix-like systems run on a wide variety of [[computer architecture]]s. They are used heavily for [[server (computing)|servers]] in business, as well as [[workstation]]s in academic and engineering environments. [[Free software|Free]] UNIX variants, such as [[Linux]] and [[Berkeley Software Distribution|BSD]], are popular in these areas.  Four operating systems are certified by the [[The Open Group]] (holder of the Unix trademark) as Unix. HP's [[HP-UX]] and IBM's [[AIX operating system|AIX]] are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, [[Sun Microsystems|Sun Microsystems's]] [[Solaris Operating System]] can run on multiple types of hardware, including [[x86]] and [[Sparc]] servers, and PCs. Apple's [[Mac OS X]], a replacement for Apple's earlier (non-Unix) Mac OS, is a [[hybrid kernel]]-based BSD variant derived from [[NeXTSTEP]], [[Mach (kernel)|Mach]], and [[FreeBSD]].  Unix interoperability was sought by establishing the [[POSIX]] standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.  ====BSD and its descendants==== [[Image:First Web Server.jpg|thumb|right|256px|The [[CERN httpd|first server]] for the [[World Wide Web]] ran on NeXTSTEP, based on BSD.]] {{Main|Berkeley Software Distribution}} A subgroup of the Unix family is the [[Berkeley Software Distribution]] family, which includes [[FreeBSD]], [[NetBSD]], and [[OpenBSD]], [[PC-BSD]]. These operating systems are most commonly found on [[webserver]]s, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The [[world wide web]] was also first demonstrated on a number of computers running an OS based on BSD called [[NextStep]].  BSD has its roots in Unix. In 1974, [[University of California, Berkeley]] installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkely received new [[VAX]] computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The [[Defense Advanced Research Projects Agency]] of the US [[United States Department of Defense|Department of Defense]] took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.  [[Steve Jobs]], upon leaving Apple Inc. in 1985, formed [[NeXT|NeXT Inc.]], a company that manufactured high-end computers running on a variation of BSD called [[NeXTSTEP]]. One of these computers was used by [[Tim Berners-Lee]] as the first webserver to create the World Wide Web.  Developers like [[Keith Bostic]] encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. Eventually, after two years of legal disputes, the BSD project came out ahead and spawned a number of free derivatives, such as [[FreeBSD]] and [[NetBSD]].  =====Mac OS X=====  [[File:Mac OSX Lion screen.png|256px|thumb|The standard user interface of Mac OS X]] {{Main|Mac OS X}} Mac OS X is a line of [[open core]] graphical operating systems developed, marketed, and sold by [[Apple Inc.]], the latest of which is pre-loaded on all currently shipping [[Macintosh]] computers. Mac OS X is the successor to the original [[Mac OS]], which had been Apple's primary operating system since 1984. Unlike its predecessor, Mac OS X is a [[UNIX]] operating system built on technology that had been developed at [[NeXT]] through the second half of the 1980s and up until Apple purchased the company in early 1997.  The operating system was first released in 1999 as [[Mac OS X Server 1.0]], with a desktop-oriented version ([[Mac OS X v10.0|Mac OS X v10.0 "Cheetah"]]) following in March 2001. Since then, six more distinct "client" and "server" editions of Mac OS X have been released, the most recent being [[OS X Mountain Lion|OS X 10.8 "Mountain Lion"]], which was first made available on February 16, 2012 for developers, and was then released to the public on July 25th 2012. Releases of Mac OS X are named after [[big cat]]s.  The server edition, [[Mac OS X Server]], is [[software architecture|architecturally]] identical to its desktop counterpart but usually runs on Apple's line of Macintosh [[server (computing)|server]] hardware. Mac OS X Server includes work group management and administration software tools that provide simplified access to key [[network service]]s, including a [[mail transfer agent]], a [[samba software|Samba server]], an [[LDAP]] server, a [[Domain Name System|domain name server]], and others. In [[Mac OS X Lion|Mac OS X v10.7 Lion]], all server aspects of Mac OS X Server have been integrated into the client version.<ref>{{cite web|url=http://www.apple.com/macosx/lion/ |title=OS X Mountain Lion - Move your Mac even further ahead |publisher=Apple |date= |accessdate=2012-08-07}}</ref>  ====Linux and GNU==== {{Main|GNU|Linux|Linux kernel}} [[File:Ubuntu 12.04 Final Live CD Screenshot.png|thumb|250px|[[Ubuntu (operating system)|Ubuntu]], desktop [[Linux distribution]]]] [[File:Android 4.0.png|thumb|180px|left|[[Android (operating system)|Android]], a popular mobile operating system using the Linux kernel]] [[Linux|Linux (or GNU/Linux)]] is a Unix-like operating system that was developed without any actual Unix code, unlike BSD and its variants. Linux can be used on a wide range of devices from supercomputers to wristwatches. The [[Linux kernel]] is released under an open source license, so anyone can read and modify its code. It has been modified to run on a large variety of electronics. Although estimates suggest that Linux is used on 1.82% of all personal computers,<ref>[[Usage share of operating systems]]</ref><ref name="StatCounter">{{cite web |title=Top 5 Operating Systems from January to April 2011 |url=http://gs.statcounter.com/#os-ww-monthly-201101-201104-bar |publisher=StatCounter |date = October 2009|accessdate=November 5, 2009}}</ref> it has been widely adopted for use in servers<ref>{{cite web|url=http://www.idc.com/about/viewpressrelease.jsp?containerId=prUS22360110&sectionId=null&elementId=null&pageType=SYNOPSIS |title=IDC report into Server market share |publisher=Idc.com |date= |accessdate=2012-08-07}}</ref> and embedded systems<ref>[http://www.linuxdevices.com/news/NS4920597981.html Linux still top embedded OS]</ref> (such as cell phones). Linux has superseded Unix in most places{{Which?|date=July 2010}}, and is used on the 10 most powerful supercomputers in the world.<ref>{{cite web|author=Tom Jermoluk |url=http://www.top500.org/list/2010/11/100 |title=TOP500 List – November 2010 (1–100) &#124; TOP500 Supercomputing Sites |publisher=Top500.org |date=2012-08-03 |accessdate=2012-08-07}}</ref> The Linux kernel is used in some popular distributions, such as [[Red Hat]],  [[Debian]], [[Ubuntu (operating system)|Ubuntu]], [[Linux Mint]] and [[Google]]'s [[Android (operating system)|Android]].  The GNU project is a mass collaboration of programmers who seek to create a completely free and open operating system that was similar to Unix but with completely original code. It was started in 1983 by [[Richard Stallman]], and is responsible for many of the parts of most Linux variants. Thousands of pieces of software for virtually every operating system are licensed under the [[GNU General Public License]]. Meanwhile, the Linux kernel began as a side project of [[Linus Torvalds]], a university student from Finland. In 1991, Torvalds began work on it, and posted information about his project on a newsgroup for computer students and programmers. He received a wave of support and volunteers who ended up creating a full-fledged kernel. Programmers from GNU took notice, and members of both projects worked to integrate the finished GNU parts with the Linux kernel in order to create a full-fledged operating system.  =====Google Chrome OS===== {{Main|Google Chrome OS}} Chrome is an operating system based on the Linux kernel and designed by [[Google]].  Since Chrome OS targets computer users who spend most of their time on the Internet, it is mainly a [[web browser]] with no ability to run applications. It relies on [[Internet application]]s (or [[Web app]]s) used in the web browser to accomplish tasks such as word processing and media viewing, as well as [[online storage]] for storing most files.  ===Microsoft Windows=== {{Main|Microsoft Windows}} [[File:Windows To Go USB Drive.png|thumb|Bootable [[Windows To Go]] USB flash drive|link=Windows To Go]] [[File:Windows 7.png|thumb|250px|[[Microsoft Windows 7]] Desktop]] Microsoft Windows is a family of [[proprietary software|proprietary]] operating systems designed by [[Microsoft Corporation]] and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers.<ref name="StatCounter"/><ref>{{cite news|title=Global Web Stats|url=http://marketshare.hitslink.com/operating-system-market-share.aspx?qprid=8|date = May 2011|publisher=Net Market Share, Net Applications |accessdate=2011-05-07}}</ref><ref name="w3cstats">{{cite news|title=Global Web Stats|url=http://www.w3counter.com/globalstats.php|date=September 2009|publisher=W3Counter, Awio Web Services|accessdate=2009-10-24}}</ref><ref>{{cite web |title=Operating System Market Share |url=http://marketshare.hitslink.com/operating-system-market-share.aspx?qprid=8 |publisher=Net Applications |date = October 2009|accessdate=November 5, 2009}}</ref> The newest version is [[Windows 7]] for workstations and [[Windows Server 2008 R2]] for servers. Windows 7 recently overtook Windows XP as most used OS.<ref name="w3schoolsOSStats">{{cite web|title=w3schools.com OS Platform Statistics|url=http://www.w3schools.com/browsers/browsers_os.asp|accessdate=October 30, 2011}}</ref><ref name="gstats2011">{{cite web|title=Stats Count Global Stats Top Five Operating Systems|url=http://gs.statcounter.com/#os-ww-monthly-201010-201110|accessdate=October 30, 2011}}</ref><ref name="globstats">{{cite web|title=Global statistics at w3counter.com|url=http://www.w3counter.com/globalstats.php|accessdate=23 January 2012}}</ref>  Microsoft Windows originated in 1985 as an [[operating environment]] running on top of [[MS-DOS]], which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, [[Windows 95]] was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS<ref>{{cite web|url=http://support.microsoft.com/kb/130179/EN-US |title=Troubleshooting MS-DOS Compatibility Mode on Hard Disks |publisher=Support.microsoft.com |date= |accessdate=2012-08-07}}</ref><ref>{{cite web|url=http://support.microsoft.com/kb/134748/en |title=Using NDIS 2 PCMCIA Network Card Drivers in Windows 95 |publisher=Support.microsoft.com |date= |accessdate=2012-08-07}}</ref> and 16 bits [[Windows 3.x]]<ref>{{cite web|url=http://support.microsoft.com/kb/163354/en |title=INFO: Windows 95 Multimedia Wave Device Drivers Must be 16 bit |publisher=Support.microsoft.com |date= |accessdate=2012-08-07}}</ref> drivers. [[Windows Me]], released in 2000, was the last version in the Win9x family. Later versions have all been based on the [[Windows NT]] [[kernel (computing)|kernel]]. Current versions of Windows run on [[IA-32]] and [[x86-64]] [[microprocessor]]s, although Windows 8 will support [[ARM]] architecture. In the past, Windows NT supported non-Intel architectures.  Server editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a [[server operating system]]. However, Windows' usage on servers is not as widespread as on personal computers, as Windows competes against Linux and BSD for server market share.<ref>{{cite web|url=http://news.netcraft.com/SSL-Survey/CMatch/osdv_all|title=Operating System Share by Groups for Sites in All Locations January 2009}}</ref><ref>{{cite web|url = http://blogs.zdnet.com/microsoft/?p=5408|title=Behind the IDC data: Windows still No. 1 in server operating systems|date=2010-02-26|publisher=ZDNet}}</ref>  ===Other===  There have been many operating systems that were significant in their day but are no longer so, such as [[AmigaOS]]; [[OS/2]] from IBM and Microsoft; [[Mac OS]], the non-Unix precursor to Apple's Mac OS X; [[BeOS]]; [[XTS-400|XTS-300]]; [[RISC OS]]; [[MorphOS]] and [[FreeMint]]. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. [[OpenVMS]] formerly from [[Digital Equipment Corporation|DEC]], is still under active development by [[Hewlett-Packard]]. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is [[MINIX]], while for example [[Singularity (operating system)|Singularity]] is used purely for research.  Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' [[Plan 9 from Bell Labs|Plan 9]].  ==Components== The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or complex as an Internet connection.  ===Kernel=== [[Image:Kernel Layout.svg|thumb|A kernel connects the application software to the hardware of a computer.]] {{Main|Kernel (computing)}} With the aid of the [[firmware]] and [[device driver]]s, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the [[RAM]], it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term [[non-volatile storage]] with [[file system]]s on such media as disks, tapes, flash memory, etc.  ====Program execution==== {{Main|Process (computing)}} The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system.  The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system [[kernel (computer science)|kernel]] which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices.  ====Interrupts==== {{Main|Interrupt}} [[Interrupt]]s are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment.  The alternative — having the operating system "watch" the various sources of input for events (polling) that require action — can be found in older systems with very small [[Call stack|stack]]s (50 or 60 bytes) but are unusual in modern systems with large stacks. [[Interrupt]]-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.  When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call.  In modern operating systems, interrupts are handled by the operating system's [[kernel (computer science)|kernel]]. Interrupts may come from either the computer's hardware or from the running program.  When a hardware device triggers an interrupt, the operating system's kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called [[device drivers|device driver]], which may be either part of the operating system's kernel, part of another program, or both. Device drivers may then relay information to a running program by various means.  A program may also trigger an interrupt to the operating system. If a program wishes to access hardware for example, it may interrupt the operating system's kernel, which causes control to be passed back to the kernel. The kernel will then process the request.  If a program wishes additional resources (or wishes to shed resources) such as memory, it will trigger an interrupt to get the kernel's attention.  ====Modes==== {{Main|Protected mode|Supervisor mode}} [[Image:Priv rings.svg|300px|thumb|right|Privilege rings for the [[x86]] available in [[protected mode]]. Operating systems determine which processes run in each mode.]] Modern CPUs support multiple modes of operation. [[CPU]]s with this capability use at least two modes: [[protected mode]] and [[supervisor mode]]. The supervisor mode is used by the operating system's kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is written and erased, and communication with devices like graphics cards. Protected mode, in contrast, is used for almost everything else. Applications operate within protected mode, and can only use hardware by communicating with the kernel, which controls everything in supervisor mode. [[CPU]]s might have other modes similar to protected mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.  When a computer first starts up, it is automatically running in [[supervisor mode]]. The first few programs to run on the computer, being the [[BIOS]] or [[Extensible Firmware Interface|EFI]], [[bootloader]], and the operating system have unlimited access to hardware - and this is required because, by definition, initializing a protected environment can only be done outside of one. However, when the operating system passes control to another program, it can place the CPU into [[protected mode]].  In [[protected mode]], programs may have access to a more limited set of the CPU's instructions. A user program may leave [[protected mode]] only by triggering an interrupt, causing control to be passed back to the [[kernel (computer science)|kernel]]. In this way the operating system can maintain exclusive control over things like access to hardware and memory.  The term "protected mode resource" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting (for example, by killing the program).  ====Memory management==== {{Main|Memory management}} Among other things, a multiprogramming operating system [[kernel (computer science)|kernel]] must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.  Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the [[kernel (computer science)|kernel]]'s memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.  [[Memory protection]] enables the [[kernel (computer science)|kernel]] to limit a process' access to the computer's memory. Various methods of memory protection exist, including [[memory segmentation]] and [[paging]]. All methods require some level of hardware support (such as the [[80286]] MMU), which doesn't exist in all computers.  In both segmentation and paging, certain [[protected mode]] registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses will trigger an interrupt which will cause the CPU to re-enter [[supervisor mode]], placing the [[kernel (computer science)|kernel]] in charge. This is called a [[segmentation violation]] or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the [[kernel (computer science)|kernel]] will generally resort to terminating the offending program, and will report the error.  Windows 3.1-Me had some level of memory protection, but programs could easily circumvent the need to use it. A [[general protection fault]] would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.  ====Virtual memory==== {{Main|Virtual memory}} {{Further|Page fault}} [[File:Virtual memory.svg|thumb|250px|Many operating systems can "trick" programs into using memory scattered around the hard disk and RAM as if it is one continuous chunk of memory, called virtual memory.]]  The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.  If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel will be interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a [[page fault]].  When the kernel detects a page fault it will generally adjust the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.  In modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called [[paging|swapping]], as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.  "Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.<ref name="Operating System">{{cite book|last=Stallings|first=William|title=Computer Organization & Architecture|year=2008|publisher=Prentice-Hall of India Private Limited|location=New Delhi|isbn=978-81-203-2962-1|page=267}}</ref>  ====Multitasking==== {{Main|Computer multitasking|Process management (computing)}} {{Further|Context switch|Preemptive multitasking|Cooperative multitasking}}  [[Computer multitasking|Multitasking]] refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.  An operating system [[kernel (computer science)|kernel]] contains a piece of software called a [[scheduling (computing)|scheduler]] which determines how much time each program will spend executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the [[Central processing unit|CPU]] and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a [[context switch]].  An early model which governed the allocation of time to programs was called [[cooperative multitasking]]. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an [[infinite loop]].  Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.  The philosophy governing [[preemptive multitasking]] is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A [[protected mode]] timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)  On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The [[AmigaOS]] is an exception, having pre-emptive multitasking from its very first version. [[Windows NT]] was the first version of [[Microsoft Windows]] which enforced preemptive multitasking, but it didn't reach the home user market until [[Windows XP]] (since [[Windows NT]] was targeted at professionals).  ====Disk access and file systems==== {{Main|Virtual file system}} [[File:Dolphin FileManager.png|thumb|256px|Filesystems allow users and programs to organize and sort files on a computer, often through the use of directories (or "folders")]] Access to data stored on disks is a central feature of all operating systems. Computers store data on [[Hard disk drives|disks]] using [[Computer file|files]], which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use out of the drive's available space. The specific way in which files are stored on a disk is called a [[file system]], and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a [[directory tree]].  Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.  While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like [[UNIX]] and [[Linux]] support a technology known as a [[virtual file system]] or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or [[file system]]s, allowing them to be accessed through a common [[application programming interface]] (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific [[device driver]]s and file system drivers.  A connected [[data storage device|storage device]], such as a [[hard drive]], is accessed through a [[device driver]]. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of [[block device]]s.  When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.  Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, [[case sensitivity]], and the presence of various kinds of [[file attribute]]s makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, [[NTFS]] in Windows and [[ext3]] and [[ReiserFS]] in Linux. However, in practice, third party drives are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through [[NTFS-3g]], and ext2/3 and ReiserFS are available in Windows through third-party software).  Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use [[ISO 9660]] or [[Universal Disk Format|UDF]], and as of [[Windows Vista]], [[NTFS]] is the only file system which the operating system can be installed on.  It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD,DVD...), a USB flash drive, or even contained within a file located on another file system.  ====Device drivers==== {{Main|Device driver}}  A [[device driver]] is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.  The key design goal of device drivers is [[abstraction]]. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver will ensure that the device appears to operate as usual from the operating system's point of view.  Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.  ===Networking=== {{Main|Computer network}} Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common [[computer network|network]] for sharing resources such as [[remote procedure call|computing]], files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as [[Secure Shell|SSH]] which allows networked users direct access to a computer's command line interface.  Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's [[IP address|network address]]. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.  Many operating systems support one or more vendor-specific or open networking protocols as well, for example, [[Systems Network Architecture|SNA]] on [[IBM]] systems, [[DECnet]] on systems from [[Digital Equipment Corporation]], and Microsoft-specific protocols ([[Server message block|SMB]]) on Windows. Specific protocols for specific tasks may also be supported such as [[Network File System (protocol)|NFS]] for file access. Protocols like [[ESound]], or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.  ===Security=== {{Main|Computer security}} A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.  The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between "privileged" and "non-privileged", systems commonly have a form of requester ''identity'', such as a user name. To establish identity there may be a process of ''authentication''. Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester '''identity''' is ''authorization''; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.  In addition to the allow/disallow model of security, a system with a high level of security will also offer auditing options. These would allow tracking of requests for access to resources (such as, "who has been reading this file?"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.  External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States [[Government of the United States|Government]] [[United States Department of Defense|Department of Defense]] (DoD) created the ''[[Trusted Computer System Evaluation Criteria]]'' (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select [[trusted operating system]]s being considered for the processing, storage and retrieval of sensitive or [[classified information]].  Network services include offerings such as file sharing, print services, email, web sites, and [[file transfer protocol]]s (FTP), most of which can have compromised security. At the front line of security are hardware devices known as [[firewall (networking)|firewalls]] or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.  An alternative strategy, and the only [[sandbox (computer security)|sandbox]] strategy available in systems that do not meet the [[Popek and Goldberg virtualization requirements]], is the operating system not running user programs as native code, but instead either [[emulator|emulates]] a processor or provides a host for a [[p-code machine|p-code]] based system such as Java.  Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing.  ===User interface=== [[File:Command line.png|thumb|256px|A screenshot of the [[Bash (Unix shell)|Bourne Again Shell]] command line. Each command is typed out after the 'prompt', and then its output appears below, working its way down the screen. The current command prompt is at the bottom.]] {{Main|User interface}} Every computer that is to be operated by an individual requires a [[user interface]]. The user interface is not actually a part of the operating system&mdash;it generally runs in a separate program usually referred to as a [[shell (computing)|shell]], but is essential if human interaction is to be supported.  The user interface requests services from the operating system that will acquire data from [[input device|input hardware devices]], such as a [[keyboard (computing)|keyboard]], [[mouse (computing)|mouse]] or [[credit card|credit card reader]], and requests operating system services to display [[Command-line interface#Command prompt|prompt]]s, [[status message]]s and such on [[output device|output hardware devices]], such as a [[computer monitor|video monitor]] or [[printer (computing)|printer]]. The two most common forms of a user interface have historically been the [[command-line interface]], where computer commands are typed out line-by-line, and the [[graphical user interface]], where a visual environment (most commonly a [[WIMP (computing)|WIMP]]) is present.  ====Graphical user interfaces==== [[File:KDE 4.png|thumb|left|256px|A screenshot of the [[K Desktop Environment|KDE]] graphical user interface. Programs take the form of images on the screen, and the files, folders (directories), and applications take the form of icons and symbols. A mouse is used to navigate the computer.]] Most of the modern computer systems support [[graphical user interface]]s (GUI), and often include them. In some computer systems, such as the original implementation of [[Mac OS]], the GUI is integrated into the [[kernel (computer science)|kernel]].  While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of [[context switch]]es required for the GUI to perform its output functions. Other operating systems are [[modularity (programming)|modular]], separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and Mac OS X are also built this way. Modern releases of Microsoft Windows such as [[Windows Vista]] implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between [[Windows NT 4.0]] and [[Windows Server 2003]] exist mostly in kernel space. [[Windows 9x]] had very little distinction between the interface and the kernel.  Many computer operating systems allow the user to install or create any user interface they desire. The [[X Window System]] in conjunction with [[GNOME]] or [[KDE]] is a commonly found setup on most Unix and [[Unix-like]]  (BSD, Linux, Solaris) systems. A number of [[Windows shell replacement]]s have been released for Microsoft Windows, which offer alternatives to the included [[Windows shell]], but the shell itself cannot be separated from Windows.  Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to [[Common Open Software Environment|COSE]] and [[Common Desktop Environment|CDE]] failed for  various reasons, and were eventually eclipsed by the widespread adoption of GNOME and KDE. Prior to [[free software]]-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).  Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.<ref name="intro-date">Poisson, Ken. [http://www.islandnet.com/~kpolsson/compsoft/soft1998.htm "Chronology of Personal Computer Software"]. Retrieved on 2008-05-07. Last checked on 2009-03-30.</ref>  ==Real-time operating systems== {{Main|Real-time operating system}}  A real-time operating system (RTOS) is a multitasking operating system intended for applications with fixed deadlines ([[real-time computing]]). Such applications include some small [[embedded system]]s, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.  An early example of a large-scale real-time operating system was [[Transaction Processing Facility]] developed by [[American Airlines]] and [[International Business Machines|IBM]] for the [[Sabre Airline Reservations System]].  Embedded systems that have fixed deadlines use a [[real-time operating system]] such as [[VxWorks]], [[PikeOS]], [[eCos]], [[QNX]], [[MontaVista Linux]] and [[RTLinux]].  [[Windows CE]] is a [[real-time operating system]] that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.{{Citation needed|date=March 2009}} [[Symbian OS]] also has an RTOS kernel (EKA2) starting with version 8.0b.  Some embedded systems use operating systems such as [[Palm OS]], [[BSD (operating system)|BSD]], and [[Linux]], although such operating systems do not support real-time computing.  ==Operating system development as a hobby== {{see also|Hobbyist operating system development}}  Operating system development is one of the most complicated activities in which a computing hobbyist may engage. A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and [[software development|active developers]]. <ref> {{cite web |url=http://www.osnews.com/story/22638/My_OS_Is_Less_Hobby_than_Yours |work=Osnews |title=My OS is less hobby than yours |date=December 21, 2009 |accessdate=December 21, 2009}} </ref>  In some cases, hobby development is in support of a "[[Homebrew Computer Club|homebrew]]" computing device, for example, a simple [[single-board computer]] powered by a [[6502 microprocessor]].  Or, development may be for an architecture already in widespread use.  Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system.  In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.  Examples of a hobby operating system include [[ReactOS]] and [[Syllable (operating system)|Syllable]].  ==Diversity of operating systems and portability== Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise  [[software maintenance|maintained]]. <!--There really ought to be a discussion of ''software modules'' somewhere, such as those that are neither API's nor Plug-Ins (not sure what those are), but which are either hard (on cartridge), soft (on diskette), or otherwise installable by downloading). -->  This cost in supporting operating systems diversity can be avoided by instead writing applications against [[software platform]]s like [[Java (software platform)|Java]] or [[Qt (toolkit)|Qt]]. These abstractions have already borne the cost of adaptation to specific operating systems and their [[system library|system libraries]].  Another approach is for operating system vendors to adopt standards. For example, [[POSIX]] and [[Operating system abstraction layer|OS abstraction layer]]s provide commonalities that reduce porting costs.  ==See also== {{multicol}} * [[Comparison of operating systems]] * [[Handheld computers]] * [[Hypervisor]] * [[Interruptible operating system]] * [[List of important publications in computer science#Operating systems|List of important publications in operating systems]] * [[List of operating systems]] * [[Microcontroller]] {{multicol-break}} {{Portal|Computer Science|Information technology|Computer networking}} * [[Network operating system]] * [[Object-oriented operating system]] * [[Operating System Projects]] * [[PCjacking]] * [[System image]] * [[Timeline of operating systems]] * [[Usage share of operating systems]] {{multicol-end}}  ==References== {{Reflist|colwidth=30em}}  ==Further reading== {{Refbegin}} * {{cite journal | last =Auslander | first =Marc A.| coauthors = Larkin, David C.; Scherr, Allan L.| title = The evolution of the MVS Operating System | publisher = IBM J. Research & Development | year=1981 | url=http://www.research.ibm.com/journal/rd/255/auslander.pdf }} * {{cite book | last = Deitel | first = Harvey M. | coauthors = Deitel, Paul; Choffnes, David | title = Operating Systems | publisher = Pearson/Prentice Hall | year = | isbn = 978-0-13-092641-8 }} * {{cite book | last = Bic| first = Lubomur F. | coauthors = Shaw, Alan C. | title = Operating Systems | publisher = [[Prentice Hall]] | year = 2003 | location = Pearson  }} * {{cite book | last = Silberschatz | first = Avi | coauthors = Galvin, Peter; Gagne, Greg | title = Operating Systems Concepts | publisher = [[John Wiley & Sons]] | year = 2008 | isbn = 0-470-12872-0 }} {{Refend}}  ==External links== {{Wiktionary}} {{Commons category|Screenshots by operating system|Screenshots of operating systems}} {{Wikiversity|at=Topic:Operating systems|Operating Systems}} * {{dmoz|Computers/Software/Operating_Systems|Operating Systems}} * [http://www.cbi.umn.edu/iterations/haigh.html Multics History] and the history of operating systems * [http://computer.howstuffworks.com/operating-system.htm How Stuff Works - Operating Systems] * [http://whatsmyos.com Help finding your Operating System type and version]  {{Operating System}} {{Systems}}  {{DEFAULTSORT:Operating System}} [[Category:Operating systems|*]] [[Category:American inventions]]  {{Link GA|no}}  [[ace:OS]] [[af:Bedryfstelsel (inligtingstegnologie)]] [[als:Betriebssystem]] [[am:የሲስተም አሰሪ]] [[ar:نظام تشغيل]] [[an:Sistema operativo]] [[as:অপাৰেটিং চিষ্টেম]] [[ast:Sistema operativu]] [[az:Əməliyyat sistemləri]] [[bn:অপারেটিং সিস্টেম]] [[zh-min-nan:Chok-gia̍p hē-thóng]] [[ba:Операцион система]] [[be:Аперацыйная сістэма]] [[be-x-old:Апэрацыйная сыстэма]] [[bg:Операционна система]] [[bs:Operativni sistem]] [[br:Reizhiad korvoiñ]] [[ca:Sistema operatiu]] [[cv:Операци системи]] [[cs:Operační systém]] [[cy:System weithredu]] [[da:Styresystem]] [[de:Betriebssystem]] [[et:Operatsioonisüsteem]] [[el:Λειτουργικό σύστημα]] [[es:Sistema operativo]] [[eo:Operaciumo]] [[eu:Sistema eragile]] [[fa:سیستم‌عامل]] [[fr:Système d'exploitation]] [[fur:Sisteme operatîf]] [[ga:Córas oibriúcháin]] [[gl:Sistema operativo]] [[ko:운영 체제]] [[hy:Օպերացիոն համակարգ]] [[hi:प्रचालन तन्त्र]] [[hsb:Dźěłowy system]] [[hr:Operacijski sustav]] [[io:Funcionanta sistemo]] [[ilo:Sistema ti panangpaandar]] [[id:Sistem operasi]] [[ia:Systema de operation]] [[is:Stýrikerfi]] [[it:Sistema operativo]] [[he:מערכת הפעלה]] [[jv:Sistem operasi komputer]] [[kn:ಕಾರ್ಯನಿರ್ವಹಣ ಸಾಧನ]] [[ka:ოპერაციული სისტემა]] [[csb:Òperacjowô systema]] [[kk:Операциялық жүйелер]] [[sw:Mfumo wa uendeshaji]] [[ku:Pergala xebitandinê]] [[lo:ລະບົບປະຕິບັດການ]] [[la:Systema administrativum computatrale]] [[lv:Operētājsistēma]] [[lb:Betribssystem (Computer)]] [[lt:Operacinė sistema]] [[ln:Litámbwisi-mokonzi]] [[lmo:Sistema uperatif]] [[hu:Operációs rendszer]] [[mk:Оперативен систем]] [[mg:Mpandrindra milina]] [[ml:ഓപ്പറേറ്റിങ്ങ്‌ സിസ്റ്റം]] [[mr:संचालन प्रणाली]] [[arz:نظام تشغيل]] [[ms:Sistem pengendalian]] [[mn:Үйлдлийн систем]] [[my:ကွန်ပျူတာ စက်လည်ပတ်ရေး စနစ်]] [[nl:Besturingssysteem]] [[new:अपरेटिङ सिस्टम]] [[ja:オペレーティングシステム]] [[no:Operativsystem]] [[nn:Operativsystem]] [[oc:Sistèma operatiu]] [[mhr:Операционло системе]] [[or:ଅପରେଟିଂ ସିଷ୍ଟମ]] [[uz:Ishlatuv tizimi]] [[pnb:اوپریٹنگ سسٹم]] [[ps:چليز غونډال]] [[nds:Bedriefssystem]] [[pl:System operacyjny]] [[pt:Sistema operativo]] [[kaa:Operatsion sistema]] [[ksh:Bedriefsystem]] [[ro:Sistem de operare]] [[qu:Llamk'aykuna llika]] [[rue:Операчна сістема]] [[ru:Операционная система]] [[sah:Операциялыыр система]] [[sq:Sistemi operativ]] [[si:පරිගණක මෙහෙයුම් පද්ධති]] [[simple:Operating system]] [[sk:Operačný systém]] [[sl:Operacijski sistem]] [[szl:Uoperacyjno systyma]] [[so:Operating system]] [[ckb:سیستەمی بەکارخەری]] [[sr:Оперативни систем]] [[sh:Operativni sistem]] [[su:Sistim Operasi]] [[fi:Käyttöjärjestelmä]] [[sv:Operativsystem]] [[tl:Sistemang operatibo]] [[ta:இயக்கு தளம்]] [[kab:A nagraw n w'ammud]] [[tt:Операцион система]] [[te:ఆపరేటింగ్ సిస్టమ్]] [[th:ระบบปฏิบัติการ]] [[tg:Системаи амалӣ]] [[tr:İşletim sistemi]] [[uk:Операційна система]] [[ur:اشتغالی نظام]] [[vec:Sistema operativo]] [[vi:Hệ điều hành]] [[fiu-vro:Opõrats'oonisüstem]] [[wa:Sistinme d' operance]] [[war:Sistema operatibo]] [[wo:Nosteg doxiin]] [[yi:אפערירן סיסטעם]] [[yo:Sístẹ̀mù ìṣiṣẹ́ kọ̀mpútà]] [[zh-yue:作業系統]] [[diq:Sistemo operatif]] [[bat-smg:Uoperacėnė sėstema]] [[zh:操作系统]]
{{Other uses}}  In [[machine learning]], '''pattern recognition''' is the assignment of a label to a given input value. An example of pattern recognition is [[classification (machine learning)|classification]], which attempts to assign each input value to one of a given set of ''classes'' (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are [[regression analysis|regression]], which assigns a real-valued output to each input; [[sequence labeling]], which assigns a class to each member of a sequence of values (for example, [[part of speech tagging]], which assigns a [[part of speech]] to each word in an input sentence); and [[parsing]], which assigns a [[parse tree]] to an input sentence, describing the [[syntactic structure]] of the sentence.  Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to do "fuzzy" matching of inputs. This is opposed to ''[[pattern matching]]'' algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is [[regular expression]] matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many [[text editor]]s and [[word processor]]s. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output to the sort provided by pattern-recognition algorithms.  Pattern recognition is studied in many fields, including [[psychology]], [[psychiatry]], [[ethology]], [[cognitive science]], [[Three-phase traffic theory|traffic flow]] and [[computer science]].  ==Overview== Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. ''[[Supervised learning]]'' assumes that a set of ''training data'' (the ''[[training set]]'') has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a ''model'' that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with [[Occam's Razor]]). [[Unsupervised learning]], on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has recently been explored is [[semi-supervised learning]], which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled ''is'' the training data.  Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as ''[[data clustering|clustering]]'', based on the common perception of the task as involving no training data to speak of, and of grouping the input data into ''clusters'' based on some inherent similarity measure (e.g. the [[distance]] between instances, considered as vectors in a multi-dimensional [[vector space]]), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in [[community ecology]], the term "classification" is used to refer to what is commonly known as "clustering".  The piece of input data for which an output value is generated is formally termed an ''instance''. The instance is formally described by a [[feature vector|vector]] of ''features'', which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate [[space (mathematics)|multidimensional space]], and methods for manipulating vectors in [[vector space]]s can be correspondingly applied to them, such as computing the [[dot product]] or the angle between two vectors.) Typically, features are either [[categorical data|categorical]] (also known as [[nominal data|nominal]], i.e. consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), [[ordinal data|ordinal]] (consisting of one of a set of ordered items, e.g. "large", "medium" or "small"), [[integer|integer-valued]] (e.g. a count of the number of occurrences of a particular word in an email) or [[real number|real-valued]] (e.g. a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be ''discretized'' into groups (e.g. less than 5, between 5 and 10, or greater than 10).  Many common pattern recognition algorithms are ''probabilistic'' in nature, in that they use [[statistical inference]] to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, oftentimes probabilistic algorithms also output a [[probability]] of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the ''N''-best labels with associated probabilities, for some value of ''N'', instead of simply a single best label. When the number of possible labels is fairly small (e.g. in the case of [[classification (machine learning)|classification]]), ''N'' may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms: *They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in [[probability theory]]. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.) *Correspondingly, they can ''abstain'' when the confidence of choosing any particular output is too low. *Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of ''error propagation''.  Techniques to transform the raw feature vectors are sometimes used prior to application of the pattern-matching algorithm. For example, [[feature extraction]] algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as [[principal components analysis]] (PCA). [[Feature selection]] algorithms, attempt to directly prune out redundant or irrelevant features. The distinction between the two is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.  == Problem statement (supervised version)== Formally, the problem of [[supervised learning|supervised]] pattern recognition can be stated as follows: Given an unknown function <math>g:\mathcal{X}\rightarrow\mathcal{Y}</math> (the ''ground truth'') that maps input instances <math>\boldsymbol{x} \in \mathcal{X}</math> to output labels <math>y \in \mathcal{Y}</math>, along with training data <math>\mathbf{D} = \{(\boldsymbol{x}_1,y_1),\dots,(\boldsymbol{x}_n, y_n)\}</math> assumed to represent accurate examples of the mapping, produce a function <math>h:\mathcal{X}\rightarrow\mathcal{Y}</math> that approximates as closely as possible the correct mapping <math>g</math>. (For example, if the problem is filtering spam, then <math>\boldsymbol{x}_i</math> is some representation of an email and <math>y</math> is either "spam" or "non-spam"). In order for this to be a well-defined problem, "approximates as closely as possible" needs to be defined rigorously. In [[decision theory]], this is defined by specifying a [[loss function]] that assigns a specific value to "loss" resulting from producing an incorrect label. The goal then is to minimize the [[expected value|expected]] loss, with the expectation taken over the [[probability distribution]] of <math>\mathcal{X}</math>. In practice, neither the distribution of <math>\mathcal{X}</math> nor the ground truth function <math>g:\mathcal{X}\rightarrow\mathcal{Y}</math> are known exactly, but can be computed only empirically by collecting a large number of samples of <math>\mathcal{X}</math> and hand-labeling them using the correct value of <math>\mathcal{Y}</math> (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of [[classification (machine learning)|classification]], the simple [[zero-one loss function]] is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and is equivalent to computing the [[accuracy]] of the classification procedure over the set of test data (i.e. counting up the fraction of instances that the learned function <math>h:\mathcal{X}\rightarrow\mathcal{Y}</math> labels correctly. The goal of the learning procedure is to maximize this test accuracy on a "typical" test set.  For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e. to estimate a function of the form :<math>p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = f\left(\boldsymbol{x};\boldsymbol{\theta}\right)</math> where the [[feature vector]] input is <math>\boldsymbol{x}</math>, and the function ''f'' is typically parameterized by some parameters <math>\boldsymbol{\theta}</math>. In a [[discriminative model|discriminative]] approach to the problem, ''f'' is estimated directly. In a [[generative model|generative]] approach, however, the inverse probability <math>p({\boldsymbol{x}|\rm label})</math> is instead estimated and combined with the [[prior probability]] <math>p({\rm label}|\boldsymbol\theta)</math> using [[Bayes' rule]], as follows: :<math>p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label}) p({\rm label|\boldsymbol\theta})}{\sum_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta)}.</math>  When the labels are [[continuous distribution|continuously distributed]] (e.g. in [[regression analysis]]), the denominator involves [[integral|integration]] rather than summation:  :<math>p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label}) p({\rm label|\boldsymbol\theta})}{\int_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta) \operatorname{d}L}.</math>  The value of <math>\boldsymbol\theta</math> is typically learned using [[maximum a posteriori]] (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data and to find the simplest possible model. Essentially, this combines [[maximum likelihood]] estimation with a [[regularization (mathematics)|regularization]] procedure that favors simpler models over more complex models. In a [[Bayesian inference|Bayesian]] context, the regularization procedure can be viewed as placing a [[prior probability]] <math>p(\boldsymbol\theta)</math> on different values of <math>\boldsymbol\theta</math>. Mathematically:  :<math>\boldsymbol\theta^* = \arg \max_{\boldsymbol\theta} p(\boldsymbol\theta|\mathbf{D})</math>  where <math>\boldsymbol\theta^*</math> is the value used for <math>\boldsymbol\theta</math> in the subsequent evaluation procedure, and <math>p(\boldsymbol\theta|\mathbf{D})</math>, the [[posterior probability]] of <math>\boldsymbol\theta</math>, is given by  :<math>p(\boldsymbol\theta|\mathbf{D}) = \left[\prod_{i=1}^n p(y_i|\boldsymbol{x}_i,\boldsymbol\theta) \right] p(\boldsymbol\theta).</math>  In the [[Bayesian statistics|Bayesian]] approach to this problem, instead of choosing a single parameter vector <math>\boldsymbol{\theta}^*</math>, the probability of a given label for a new instance <math>\boldsymbol{x}</math> is computed by integrating over all possible values of <math>\boldsymbol\theta</math>, weighted according to the posterior probability:  :<math>p({\rm label}|\boldsymbol{x}) = \int p({\rm label}|\boldsymbol{x},\boldsymbol\theta)p(\boldsymbol{\theta}|\mathbf{D}) \operatorname{d}\boldsymbol{\theta}.</math>  ==Uses== [[File:800px-Cool Kids of Death Off Festival p 146-face selected.jpg|thumb|200px|The face was automatically detected by special software.]] Within medical science, pattern recognition is the basis for [[computer-aided diagnosis]] (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.  Other typical applications of pattern recognition techniques are automatic [[speech recognition]], [[document classification|classification of text into several categories]] (e.g. spam/non-spam email messages), the [[handwriting recognition|automatic recognition of handwritten postal codes]] on postal envelopes, [[facial recognition system|automatic recognition of images]] of human faces, or handwriting image extraction from medical forms.<ref>{{cite journal|last=Milewski|first=Robert|coauthors=Govindaraju, Venu|title=Binarization and cleanup of handwritten text from carbon copy medical form images|journal=Pattern Recognition|date=31 March 2008|volume=41|issue=4|pages=1308–1315|doi=10.1016/j.patcog.2007.08.018|url=http://dl.acm.org/citation.cfm?id=1324656}}</ref> The last two examples form the subtopic [[image analysis]] of pattern recognition that deals with digital images as input to pattern recognition systems.<ref name=duda2001>Richard O. Duda, [[Peter E. Hart]], David G. Stork (2001) ''Pattern classification'' (2nd edition), Wiley, New York, ISBN 0-471-05669-3</ref><ref>R. Brunelli, ''Template Matching Techniques in Computer Vision: Theory and Practice'', Wiley, ISBN 978-0-470-51706-2, 2009 </ref>  The method of signing one's name was captured with stylus and overlay starting in 1990.{{citation needed|date=January 2011}} The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..{{citation needed|date=January 2011}}  Neural networks (neural net classifiers) have many real-world applications in image processing, a few examples: * identification and authentication: e.g., license plate recognition,<ref>[http://anpr-tutorial.com/ THE AUTOMATIC NUMBER PLATE RECOGNITION TUTORIAL] http://anpr-tutorial.com/ </ref> fingerprint analysis and face detection/verification;<ref>[http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/mitchell/ftp/faces.html Neural Networks for Face Recognition] Companion to Chapter 4 of the textbook Machine Learning. </ref> * medical diagnosis: e.g., screening for cervical cancer (Papnet<ref>[http://health-asia.org/papnet-for-cervical-screening/ PAPNET For Cervical Screening] http://health-asia.org/papnet-for-cervical-screening/</ref>) or breast tumors; * defence: various navigation and guidance systems, target recognition systems, etc.  For a discussion of the aforementioned applications of neural networks in image processing, see e.g. <ref>{{Cite journal| author=Egmont-Petersen, M., de Ridder, D., Handels, H. | year=2002 | title=Image processing with neural networks - a review | journal=Pattern Recognition | volume=35 | pages=2279&ndash;2301 | doi = 10.1016/S0031-3203(01)00178-9 | issue=10 }}</ref>.  ==Algorithms== Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as [[generative model|generative]] or [[discriminative model|discriminative]].  ===[[Classification (machine learning)|Classification]] algorithms ([[supervised learning|supervised]] algorithms predicting [[categorical data|categorical]] labels)=== *[[Maximum entropy classifier]] (aka [[logistic regression]], [[multinomial logistic regression]]): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.) *[[Naive Bayes classifier]] *[[Decision tree]]s, [[decision list]]s *[[Support vector machine]]s *[[Variable_kernel_density_estimation#Use_for_statistical_classification|Kernel estimation]] and [[K-nearest-neighbor]] algorithms *[[Perceptron]]s *[[Neural network]]s (multi-level perceptrons)  ===[[Cluster analysis|Clustering]] algorithms ([[unsupervised learning|unsupervised]] algorithms predicting [[categorical data|categorical]] labels)=== *Categorical [[mixture model]]s *[[K-means clustering]] *[[Hierarchical clustering]] (agglomerative or divisive) *[[Kernel principal component analysis]] (Kernel PCA) *[[Deep learning|Deep learning methods]]  ===[[Regression analysis|Regression]] algorithms (predicting [[real number|real-valued]] labels)=== Supervised: *[[Linear regression]] and extensions *[[Neural network]]s *[[Gaussian process regression]] (kriging)  Unsupervised: *[[Principal components analysis]] (PCA) *[[Independent component analysis]] (ICA)  ===Categorical [[sequence labeling]] algorithms (predicting sequences of [[categorical data|categorical]] labels)=== Supervised: *[[Hidden Markov model]]s (HMMs) *[[Maximum entropy Markov model]]s (MEMMs) *[[Conditional random field]]s (CRFs)  Unsupervised: *[[Hidden Markov model]]s (HMMs)  ===Real-valued [[sequence labeling]] algorithms (predicting sequences of [[real number|real-valued]] labels)=== Supervised (?): *[[Kalman filter]]s *[[Particle filter]]s  Unsupervised: *??  ===[[Parsing]] algorithms (predicting [[tree structure]]d labels)=== Supervised and unsupervised: *[[Probabilistic context free grammar]]s (PCFGs)  ===General algorithms for predicting arbitrarily-structured labels=== *[[Bayesian network]]s *[[Markov random field]]s  ===[[Ensemble learning]] algorithms (supervised [[meta-algorithm]]s for combining multiple learning algorithms together)=== *[[Bootstrap aggregating]] ("bagging") *[[Boosting]] *[[Ensemble averaging]] *[[Mixture of experts]], [[hierarchical mixture of experts]]  ==See also== * [[Perceptual learning]] * [[Cache language model]] * [[Compound term processing]] * [[Computer-aided diagnosis]] * [[Data mining]] * [[List of numerical analysis software]] * [[List of numerical libraries]] * [[Machine learning]] * [[Neocognitron]] * [[Predictive analytics]] * [[Prior knowledge for pattern recognition]] * [[Sequence mining]] * [[Template matching]] * [[Thin-slicing]] *[[Perception]]  ==References== {{FOLDOC}} {{reflist}}  ==Further reading== *{{cite book|last=Fukunaga|first=Keinosuke|title=Introduction to Statistical Pattern Recognition|edition=2nd|year=1990|publisher=Academic Press|location=Boston|isbn=0-12-269851-7}} *{{cite book|last=Bishop|first=Christopher|authorlink=Christopher Bishop|title=Pattern Recognition and Machine Learning|year=2006|publisher=Springer|location=Berlin|isbn=0-387-31073-8}} *{{cite book|last1=Koutroumbas|first1=Konstantinos|last2=Theodoridis|first2=Sergios|title=Pattern Recognition|edition=4th|year=2008|publisher=Academic Press|location=Boston|isbn=1-59749-272-8}} *{{cite book|last1=Hornegger|first1=Joachim|last2=Paulus|first2=Dietrich W. R.|title=Applied Pattern Recognition: A Practical Introduction to Image and Speech Processing in C  |edition=2nd|year=1999|publisher=Morgan Kaufmann Publishers|location=San Francisco|isbn=3-528-15558-2}} *{{cite book|last=Schuermann|first=Juergen|title=Pattern Classification: A Unified View of Statistical and Neural Approaches|year=1996|publisher=Wiley|location=New York|isbn=0-471-13534-8}} *{{cite book|editor=Godfried T. Toussaint|title=Computational Morphology|year=1988|publisher=North-Holland Publishing Company|location=Amsterdam}} *{{cite book|last1=Kulikowski|first1=Casimir A.|last2=Weiss|first2=Sholom M.|title=Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems|series=Machine Learning|year=1991|publisher=Morgan Kaufmann Publishers|location=San Francisco|isbn=1-55860-065-5}} *{{cite paper|last1=Jain|first1=Anil.K.|last2=Duin|first2=Robert.P.W.|last3=Mao|first3=Jianchang|title=Statistical pattern recognition: a review|year=2000|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence | volume=22 | pages=4&ndash;37 | doi = 10.1109/34.824819 | issue=1}} *[http://www.egmont-petersen.nl/classifiers.htm An introductory tutorial to classifiers (introducing the basic terms, with numeric example)]  ==External links== * [http://www.iapr.org The International Association for Pattern Recognition] * [http://cgm.cs.mcgill.ca/~godfried/teaching/pr-web.html List of Pattern Recognition web sites] * [http://www.jprr.org Journal of Pattern Recognition Research] * [http://www.docentes.unal.edu.co/morozcoa/docs/pr.php Pattern Recognition Info] * [http://www.sciencedirect.com/science/journal/00313203 Pattern Recognition] (Journal of the Pattern Recognition Society) * [http://www.worldscinet.com/ijprai/mkt/archive.shtml International Journal of Pattern Recognition and Artificial Intelligence] * [http://www.inderscience.com/ijapr International Journal of Applied Pattern Recognition] * [http://www.openpr.org.cn/ Open Pattern Recognition Project], intended to be an open source platform for sharing algorithms of pattern recognition  [[Category:Machine learning]] [[Category:Formal sciences]]  [[ar:تمييز الأنماط]] [[ca:Reconeixement de patrons]] [[de:Mustererkennung]] [[el:Αναγνώριση προτύπων]] [[es:Reconocimiento de patrones]] [[fa:بازشناخت الگو]] [[fr:Reconnaissance de formes]] [[ko:패턴 인식]] [[id:Pengenalan pola]] [[it:Riconoscimento di pattern]] [[jv:Pangenalan pola]] [[kk:Бейнені айырып тану]] [[lt:Atpažinimo teorija]] [[ms:Pengecaman pola]] [[nl:Patroonherkenning]] [[ja:パターン認識]] [[pl:Rozpoznawanie wzorców]] [[pt:Reconhecimento de padrões]] [[ru:Теория распознавания образов]] [[sr:Prepoznavanje obrazaca]] [[fi:Hahmontunnistus]] [[tl:Pagkilala ng paterno]] [[th:การรู้จำแบบ]] [[tr:Örüntü tanıma]] [[uk:Теорія розпізнавання образів]] [[vi:Nhận dạng mẫu]] [[zh:模式识别]]
'''Performance tuning''' is the improvement of [[system]] [[Computer performance|performance]]. This is typically a computer application, but the same methods can be applied to economic markets, bureaucracies or other complex systems. The motivation for such activity is called a performance problem, which can be real or anticipated. Most systems will respond to increased [[Load (computing)|load]] with some degree of decreasing performance. A system's ability to accept higher load is called [[scalability]], and modifying a system to handle a higher load is synonymous to performance tuning.  Systematic tuning follows these steps: # Assess the problem and establish numeric values that categorize acceptable behavior. # Measure the performance of the system before modification. # Identify the part of the system that is critical for improving the performance. This is called the [[bottleneck]]. # Modify that part of the system to remove the bottleneck. # Measure the performance of the system after modification.  This is an instance of the measure-evaluate-improve-learn cycle from [[quality assurance]].  A performance problem may be identified by slow or unresponsive systems. This usually occurs because high system [[Load (computing)|loading]], causing some part of the system to reach a limit in its ability to respond. This limit within the system is referred to as a [[bottleneck]].  A handful of techniques are used to improve performance. Among them are code optimization, load balancing, caching strategy, distributed computing and self-tuning.  == Performance analysis == : ''See the main article at [[Performance analysis]]'' Performance analysis, commonly known as profiling, is the investigation of a program's behavior using information gathered as the program executes. Its goal is to determine which sections of a program to optimize.  A profiler is a performance analysis tool that measures the behavior of a program as it executes, particularly the frequency and duration of function calls. Performance analysis tools existed at least from the early 1970s. Profilers may be classified according to their output types, or their methods for data gathering.  == Performance engineering == : ''See the main article at [[Performance engineering]]''  Performance engineering is the discipline encompassing roles, skills, activities, practices, tools, and deliverables used to meet the [[non-functional requirements]] of a designed system, such as increase business revenue, reduction of system failure, delayed projects, and avoidance of unnecessary usage of resources or work.  Several common activities have been identified in different methodologies: * Identification of critical [[business processes]]. * Elaboration of the processes in [[use cases]] and system volumetrics. * System construction, including performance tuning. * Deployment of the constructed system. * Service management, including activities performed after the system has been deployed.  == Code optimization == :''See the main article at [[Optimization (computer science)]]''.  Some optimizations include improving the code so that work is done once before a loop rather than inside a loop or replacing a call to a simple [[selection sort]] with a call to the more complicated algorithm for a [[quicksort]].  == Caching strategy == {{main|Cache (computing)}}  Caching is a fundamental method of removing performance bottlenecks that are the result of slow access to data. Caching improves performance by retaining frequently used information in high speed memory, which reduces access time and thus improves performance. Caching is an effective manner of improving performance in situations where the principle of [[locality of reference]] The methods used to determine which data is stored in progressively faster storage are collectively called '''caching strategies.'''  == Load balancing == {{main|Load balancing (computing)}}  A system can consist of independent components, each able to service requests.  If all the requests are serviced by one of these systems (or a small number) while others remain idle then time is wasted waiting for used system to be available.  Arranging so all systems are used equally is referred to as [[Load balancing (computing)|load balancing]] and can improve over-all performance.  Load balancing is often used to achieve further gains from a distributed system by intelligently selecting which machine to run an operation on based on how busy all potential candidates are, and how well suited each machine is to the type of operation that needs to be performed.  == Distributed computing == {{main|Distributed computing}}  [[Distributed computing]] is used for increasing the potential for parallel execution on modern CPU architectures continues, the use of distributed systems is essential to achieve performance benefits from the available [[Parallel computing|parallelism]].  High performance [[Computer cluster|cluster computing]] is a well known use of distributed systems for performance improvements.  Distributed computing and clustering can negatively impact latency while simultaneously increasing load on shared resources, such as database systems. To minimize latency and avoid bottlenecks, distributed computing can benefit significantly from distributed [[cache (computing)|caches]].  == Self-tuning == {{main|Self-tuning}}  A self-tuning system is capable of optimizing its own internal running parameters in order to maximize or minimize the fulfillment of an [[objective function]]; typically the maximization of [[efficiency]] or [[error#Science and engineering|error]] minimization.  Self-tuning systems typically exhibit [[non-linear]] [[adaptive control]].  Self-tuning systems have been a hallmark of the aerospace industry for decades, as this sort of feedback is necessary to generate [[Optimization (mathematics)#Multi-objective optimization|optimal multi-variable control]] for nonlinear processes.  == Bottlenecks == The bottleneck is the part of a system which is at capacity. Other parts of the system will be idle waiting for it to perform its task.  In the process of finding and removing bottlenecks, it is important to prove their existence, such as by sampling, before acting to remove them. There is a strong temptation to ''guess''. Guesses are often wrong, and investing only in guesses can itself be a bottleneck.{{Citation needed|date=April 2011}}  == See also == * [[Performance Application Programming Interface]]  == References ==  {{reflist}}  == External links == * [http://www.performancewiki.com/home.html Quick performance tuning tips from Linux file systems to WebSphere heap size.] - Easy-to-understand and easy-to-apply software tuning tips. * [http://www.articlecity.com/articles/computers_and_internet/article_3255.shtml Tweaking Your System Performance In Windows XP] - Easy to read guide on tuning computer performance. * [http://www.vertexbuffer.com Online CPU and GPU] - Optimization for video game technology * [http://www.performancetroubleshooting.com performancetroubleshooting.com] - Tips on Optimizing your performance * [http://howtospeed.com/ Guide to speed up your pc] - 18 tips for speeding up computer performance.  [[Category:Computer hardware tuning]] [[Category:Network performance]]
{{About|personal computers in general|hardware components of personal computers|Personal computer hardware}} {{pp-move-indef}} {{Infobox  |title   =Personal computer |image   =[[File:Computer-aj aj ashton 01.svg|250px]] |caption =An illustration of a personal [[desktop computer]] }} A '''personal computer''' ('''PC''') is any general-purpose [[computer]] whose size, capabilities, and original sales price make it useful for individuals, and which is intended to be operated directly by an [[end-user]] with no intervening computer operator. This contrasted with the batch processing or [[time-sharing system|time-sharing]] models which allowed larger, more expensive [[minicomputer]] and [[mainframe computer|mainframe]] systems to be used by many people, usually at the same time. Large data processing systems require a full-time staff to operate efficiently.   Software applications for personal computers include, but are not limited to, [[word processing]], [[spreadsheets]], [[databases]], [[Web browser]]s and [[e-mail]] clients, [[digital media]] playback, [[Personal computer game|games]], and myriad personal productivity and special-purpose software applications.  Modern personal computers often have connections to the [[Internet]],  allowing access to the [[World Wide Web]] and a wide range of other resources. Personal computers may be connected to a [[local area network]] (LAN), either by a cable or a wireless connection. A personal computer may be a [[desktop computer]] or  a [[laptop]], [[tablet computer|tablet]], or a [[handheld PC]].   Early PC owners usually had to write their own programs to do anything useful with the machines, even lacking an [[operating system]]. The very earliest microcomputers, equipped with a [[front panel]], required hand-loading of a [[booting|bootstrap]] program to load programs from external storage ([[paper tape]], cassettes, or eventually diskettes).  Before very long, automatic booting from permanent read-only memory became universal.   Today's users have access to a wide range of [[commercial software]] and [[free software]], which is provided in ready-to-run or ready-to-[[compiler|compile]] form. Since the early 1990s, the [[Microsoft]] Windows™ operating systems and [[Intel]] hardware have dominated much of the personal computer market, first with [[MS-DOS]] and then with the "[[Wintel]]" (Windows   Intel) combination. Popular alternatives to Microsoft's [[Windows]] operating systems include [[Apple Inc.|Apple's]] [[Mac OS X]] and the [[Free_and_open_source_software|free open-source]] [[Linux]] and [[Berkeley_Software_Distribution|BSD]] operating systems. [[AMD]] provides the major alternative to Intel's [[central processing units]]. Applications and games for PCs are typically developed and distributed independently from the hardware or OS manufacturers, whereas software for many mobile phones and other portable systems is approved and distributed through a centralized online store.<ref>{{citation |author=Conlon, Tom |url=http://www.popsci.com/gadgets/article/2010-01/ipad%E2%80%99s-closed-system-sometimes-i-hate-being-right |title=The iPad’s Closed System: Sometimes I Hate Being Right |publisher=Popular Science |date=29 January 2010 |quote=The iPad is not a personal computer in the sense that we currently understand. |accessdate=2010-10-14}}</ref><ref>{{citation |url=http://gawker.com/5539717/steve-jobs-offers-world-freedom-from-porn?skyline=true&s=i |title=Steve Jobs Offers World 'Freedom From Porn' |publisher=Gawker.com |date=15 May 2010 |quote=some traditional PC folks feel like their world is slipping away. |accessdate=2010-10-14}}</ref>  In July and August 2011, marketing businesses and journalists began to talk about the 'Post-PC Era', in which the desktop form factor was being replaced with more portable computing such as [[netbook]]s, [[Notebook computer|notebooks]], [[Tablet computer|tablets]], and [[smartphone]]s.<ref>{{cite web|url=http://www.zdnet.com/blog/virtualization/the-dawn-of-the-post-pc-era-not/3714?tag=nl.e539|title=The Dawn of the Post-PC Era. Not.|last=Hess|first=Ken|date=September 1, 2011<!--, 3:00am PDT-->|accessdate=1 September 2011}}</ref>  == History == {{Refimprove section|date=September 2008}} {{Main|History of personal computers}} {{See also|Microcomputer revolution}} The [[Programma 101]], released in 1965, was the first commercial "[[desktop computer]]",<ref>"'Desk-top' computer is typewriter size". Business Week. October 23, 1965.</ref> but today would usually be considered a printing [[programmable calculator]].  In what was later to be called [[The Mother of All Demos]], [[SRI International|SRI]] researcher [[Douglas Engelbart]] in 1968 gave a  preview of what would become the staples of daily working life in the 21st century - e-mail, hypertext, word processing, video conferencing, and the mouse. The demonstration required technical support staff and a mainframe time-sharing computer that were far too costly for individual business use at the time.  By the early 1970s, people in academic or research institutions had the opportunity for single-person use of a [[LINC|computer system]] in interactive mode for extended durations, although these systems would still have been too expensive to be owned by a single person.  In the 1970s [[HP 9800 series desktop computers|Hewlett Packard]] introduced fully [[BASIC]] programmable computers that fit entirely on top of a desk, including a keyboard, a small one-line display and printer. The [[Xerox Alto]], developed in 1973 at [[Xerox|Xerox's]] [[PARC (company)|Palo Alto Research Center (PARC)]], had a graphical user interface ([[GUI]]) that later served as inspiration for [[Apple Inc.|Apple Computer]]'s [[Macintosh]], and [[Microsoft]]'s [[Windows]] operating system. The [[Wang 2200]] of 1973 had a full-size [[cathode ray tube]] (CRT) and cassette tape storage. The [[IBM 5100]] in 1975 had a small CRT display and could be programmed in BASIC and [[APL (programming language)|APL]]. These were generally expensive specialized computers sold for business or scientific uses. The introduction of the [[microprocessor]], a single [[integrated circuit|chip]] with all the circuitry that formerly occupied large cabinets, led to the proliferation of personal computers after 1975.  Early personal computers&nbsp;&mdash; generally called [[microcomputers]]&nbsp;&mdash; were sold often in [[Electronic kit|kit]] form and in limited volumes, and were of interest mostly to hobbyists and technicians. Minimal programming was done with toggle switches to enter instructions, and output was provided by front panel lamps. Practical use required adding peripherals such as keyboards, [[computer display]]s, disk drives, and printers. [[Micral]] N was the earliest commercial, non-kit microcomputer based on a microprocessor, the Intel 8008. It was built starting in 1972 and about 90,000 units were sold. In 1976 [[Steve Jobs]] and [[Steve Wozniak]] sold the [[Apple I|Apple I computer]] circuit board, which was fully prepared and contained about 30 chips. The first successfully mass marketed personal computer was the [[Commodore PET]] introduced in January 1977.  It was soon followed by the [[Apple II series|Apple II]] (usually referred to as the "Apple") in June 1977, and the TRS-80 from Radio Shack in November 1977.  Mass-market ready-assembled computers allowed a wider range of people to use computers, focusing more on software applications and less on development of the processor hardware.  Into the beginning of the 1980s, [[home computer]]s were further developed for household use, with software for personal productivity, programming and games. They typically could be used with a [[television]] already in the home as the computer display, with low-detail blocky graphics and a limited color range, and text about 40 characters wide by 25 characters tall. One such machine, the [[Commodore 64]], totaled 17 million units sold, making it the best-selling single personal computer model of all time.<ref name="Reimer1">{{cite web |url=http://www.jeremyreimer.com/total_share.html |title=Personal Computer Market Share: 1975–2004 |date=2 November 2009 |last=Reimer |first=Jeremy |accessdate=2009-07-17}}</ref> Another such computer, the [[NEC PC-9801|NEC PC-98]], sold more than 18 million units.<ref>{{cite journal|title=Computing Japan|journal=Computing Japan|year=1999|volume=54-59|url=http://books.google.co.uk/books?id=oP61AAAAIAAJ|accessdate=6 February 2012|page=18|publisher=LINC Japan|quote=...its venerable PC 9800 series, which has sold more than 18 million units over the years, and is the reason why NEC has been the number one PC vendor in Japan for as long as anyone can remember.}}</ref>  Somewhat larger and more expensive systems (for example, running [[CP/M]]), or sometimes a home computer with additional interfaces and devices, although still low-cost compared with [[minicomputer]]s and [[Mainframe computer|mainframe]]s, were aimed at office and small business use, typically using "high resolution" monitors capable of at least 80 column text display, and often no graphical or color drawing capability.  [[Workstation]]s were characterized by high-performance processors and graphics displays, with large local disk storage, networking capability, and running under a multitasking operating system. [[File:IBM PC 5150.jpg|thumb|[[IBM 5150]] as of 1981]]  Eventually, due to the [[influence of the IBM PC on the personal computer market]], personal computers and home computers lost any technical distinction. Business computers acquired color graphics capability and sound, and home computers and game systems users used the same processors and operating systems as office workers. Mass-market computers had graphics capabilities and memory comparable to dedicated workstations of a few years before. Even local area networking, originally a way to allow business computers to share expensive mass storage and peripherals, became a standard feature of personal computers used at home.  In 1982 "The Computer" was named [[Time Person of the Year|Machine of the Year]] by [[Time (magazine)|Time Magazine]].  ===Market and sales=== {{See also|Market share of leading PC vendors}} [[File:Personal computers (million) ITU.png|thumb|Personal computers worldwide in million distinguished by developed and developing world]] In 2001, 125 million personal computers were shipped in comparison to 48 thousand in 1977. More than 500 million personal computers were in use in 2002 and one [[1,000,000,000 (number)|billion]] personal computers had been sold worldwide from the mid-1970s up to this time. Of the latter figure, 75 percent were professional or work related, while the rest were sold for personal or home use. About 81.5 percent of personal computers shipped had been [[desktop computer]]s, 16.4 percent [[laptop]]s and 2.1 percent [[Server (computing)|server]]s. The United States had received 38.8 percent (394 million) of the computers shipped, Europe 25 percent and 11.7 percent had gone to the Asia-Pacific region, the fastest-growing market as of 2002. The second billion was expected to be sold by 2008.<ref name="cnet">{{cite news|last=Kanellos |first=Michael |url=http://news.cnet.com/2100-1040-940713.html |title=personal computers: More than 1 billion served |publisher=cnet news |date=30 June 2002 |accessdate=2010-10-14}}</ref> Almost half of all the households in [[Western Europe]] had a personal computer and a computer could be found in 40 percent of homes in United Kingdom, compared with only 13 percent in 1985.<ref>{{cite news|url=http://news.bbc.co.uk/1/hi/sci/tech/2077986.stm |title=Computers reach one billion mark |publisher=BBC News |date=1 July 2002 |accessdate=2010-10-14}}</ref>  The global personal computer shipments were 350.9 million units in 2010,<ref name="cp2011-09-12">[http://www.gmanews.tv/story/210464/global-pc-shipments-grew-138-percent-in-2010-gartner-study Global PC shipments grew 13.8 percent in 2010 —Gartner study], 01/13/2011, retrieved at September 12, 2011</ref> 308.3 million units in 2009<ref name="sn2011-09-12">[http://itmanagement.earthweb.com/mowi/article.php/3884641/Laptop-Sales-Soaring-Amid-Wider-PC-Growth-Gartner.htm Laptop Sales Soaring Amid Wider PC Growth: Gartner], May 27, 2010, Andy Patrizio, earthweb.com, retrieved at September 12, 2011</ref> and 302.2 million units in 2008.<ref name="seo2011-09-12">[http://www.zdnet.com/blog/itfacts/worldwide-pc-shipments-in-2008/15672 Worldwide PC Shipments in 2008], March 16, 2009, ZDNet, retrieved at September 12, 2011</ref><ref name="set2011-09-12">[http://www.internetnews.com/hardware/article.php/3796381/PC Sales Up for 2008 but Barely.htm PC Sales Up for 2008, but Barely], January 14, 2009, Andy Patrizio, internetnews.com, retrieved at September 12, 2011</ref> The shipments were 264 million units in the year 2007, according to [[iSuppli]],<ref name="cw2009-01-13">{{citation |url=http://www.pcworld.com/article/133102/isuppli_raises_2007_computer_sales_forecast.html |title=ISuppli Raises 2007 Computer Sales Forecast] |publisher=pcworld.com |accessdate=13 January 2009}}</ref> up 11.2 percent from 239 million in 2006.<ref name="mw2009-01-13">{{citation |url=http://www.macworld.co.uk/business/news/index.cfm?newsid=18326 |title=iSuppli raises 2007 computer sales forecast |publisher=macworld.co.uk |accessdate=13 January 2009}}</ref> In 2004, the global shipments were 183 million units, an 11.6 percent increase over 2003.<ref name="nf2009-01-13">{{citation |url=http://www.newsfactor.com/story.xhtml?story_id=30526&full_skip=1 |title=Global PC Sales Leveling Off |publisher=newsfactor.com |accessdate=13 January 2009}}</ref> In 2003, 152.6 million computers were shipped, at an estimated value of $175 billion.<ref name="cnet2009-01-13">{{citation |url=http://news.cnet.com/HP-back-on-top-of-PC-market/2100-1003_3-5141213.html |title=HP back on top of PC market |accessdate=13 January 2009}}</ref> In 2002, 136.7 million PCs were shipped, at an estimated value of $175 billion.<ref name="cnet2009-01-13"/> In 2000, 140.2 million personal computers were shipped, at an estimated value of $226 billion.<ref name="cnet2009-01-13"/> Worldwide shipments of personal computers surpassed the 100-million mark in 1999, growing to 113.5 million units from 93.3 million units in 1998.<ref name="lat2009-01-13">{{cite news |url=http://articles.latimes.com/2000/jan/24/business/fi-57038 |title=Dell Passes Compaq as Top PC Seller in U.S |publisher=Los Angeles Times |accessdate=13 January 2009 |first=Nona |last=Yates |date=24 January 2000}}</ref> In 1999, Asia had 14.1 million units shipped.<ref name="za2009-01-13">{{citation |url=http://www.zdnetasia.com/news/hardware/0,39042972,13025480,00.htm |title=Economic recovery bumps AP 1999 PC shipments to record high |publisher=zdnetasia.com |accessdate=13 January 2009}}</ref>  For 2011, global PC shipments are expected to reach 364 million units, a 3.8% growth comparing to 2010.<ref name="shel2011-09-12">[http://www.marketwatch.com/story/gartner-lowers-2011-forecast-for-pc-shipments-2011-09-08 Gartner lowers 2011 forecast for PC shipments], September  8, 2011, Tess Stynes, marketwatch.com, retrieved at September 12, 2011</ref>  As of June 2008, the number of personal computers in use worldwide hit one billion, while another billion is expected to be reached by 2014. Mature markets like the United States, [[Western Europe]] and Japan accounted for 58 percent of the worldwide installed PCs. The [[emerging market]]s were expected to double their installed PCs by 2012 and to take 70 percent of the second billion PCs. About 180 million computers (16 percent of the existing installed base) were expected to be replaced and 35 million to be dumped into landfill in 2008. The whole installed base grew 12 percent annually.<ref>{{cite press release|url=http://www.gartner.com/it/page.jsp?id=703807 |title=Gartner Says More than 1 Billion PCs In Use Worldwide and Headed to 2 Billion Units by 2014 |publisher=Gartner |date=23 June 2008 |accessdate=2010-10-14}}</ref><ref>{{cite news|url=http://www.reuters.com/article/technologyNews/idUSL2324525420080623 |title=Computers in use pass 1 billion mark: Gartner |publisher=Reuters |author=Tarmo Virki |date=23 June 2008 |accessdate=2010-10-14}}</ref>  Based on IDC data for Q2 2011, for the first time China surpassed US in PC shipments by 18.5 million and 17.7 million respectively. It is reflects the rising of emerging markets as well as the relative stagnation of mature regions.<ref>{{cite web |url=http://old.news.yahoo.com/s/ap/20110823/ap_on_hi_te/us_techbit_pc_shipments_china |title=China hits tech milestone: PC shipments pass US |date=August 23, 2011}}</ref>  In the [[developed world]], there has been a vendor tradition to keep adding functions to maintain high prices of personal computers. However, since the introduction of the [[One Laptop per Child]] foundation and its low-cost [[OLPC XO-1|XO-1]] laptop, the computing industry started to pursue the price too. Although introduced only one year earlier, there were 14 million [[netbook]]s sold in 2008.<ref>{{cite web |url=http://www.olpcnews.com/use_cases/technology/4p_computing_olpc_impact.html |title=4P Computing - Negroponte's 14 Million Laptop Impact |publisher=OLPC News |date=11 December 2008 |accessdate=2010-10-14}}</ref> Besides the regular computer manufacturers, companies making especially rugged versions of computers have sprung up, offering alternatives for people operating their machines in extreme weather or environments.<ref>{{cite web |url=http://www.ruggedpcreview.com/2_leaders.html |title=Rugged PC leaders |author=Conrad H. Blickenstorfer |publisher=Ruggedpcreview.com |accessdate=2010-10-14}}</ref>  [[Deloitte]] consulting firm predicted that in 2011, [[smartphones]] and [[tablet computers]] as computing devices would surpass the PCs sales.<ref>Tablets, smartphones to outsell PCs http://news.yahoo.com/s/afp/20110210/tc_afp/itinternettelecomequipmentmobileconsumerproduct</ref>  === Average selling price === Selling prices of personal computers, unlike other consumer commodities, steadily declined due to lower costs of production and manufacture. Capabilities of the computers also increased.  In 1975, an Altair kit sold for only around US $400, but required customers to solder components into circuit boards; peripherals required to interact with the system in alphanumeric form instead of blinking lights would add another $2000, and the resultant system was only of use to hobbyists.<ref name=Sussman85>Marvin B. Sussman  ''Personal Computers and the Family'' Routledge, 1985 ISBN 0-86656-361-X  page 90</ref>  At their introduction in 1981, the US $1,795 price of the [[Osborne 1]] and its competitor [[Kaypro]] was considered an attractive price point; these systems had text-only displays and only floppy disks for storage. By  1982, [[Michael Dell]] observed that a personal computer system selling at retail for about $3,000 US was made of components that cost the dealer about $600; typical gross margin on a computer unit was around $1,000.<ref>Kateri M. Drexler ''Icons of business: an encyclopedia of mavericks, movers, and shakers, Volume 1'',Greenwood Publishing Group, 2007 ISBN 0-313-33863-9  page 102</ref> The total value of personal computer purchases in the US in 1983 was about $4 billion, comparable to total sales of pet food.  By late 1998, the average selling price of personal computer systems in the United States had dropped below $1000.<ref>[http://www.pcworld.com/article/9150/average_pc_price_drops_below_1000.html Nancy Weil , ''Average PC Price drops below $1000'', ''PC World'' December 1998, retrieved 2010 Nov 17]</ref>  For [[Microsoft Windows]] systems, the [[average selling price]] (ASP) showed a decline in 2008/2009, possibly due to low-cost [[netbook]]s, drawing $569 for [[desktop computer]]s and $689 for [[laptop]]s at U.S. retail in August 2008. In 2009, ASP had further fallen to $533 for desktops and to $602 for notebooks by January and to $540 and $560 in February.<ref>{{cite web|url=http://www.eweek.com/c/a/Windows/Netbooks-Are-Destroying-the-Laptop-Market-and-Microsoft-Needs-to-Act-Now-863307/ |title=Netbooks Are Destroying the Laptop Market and Microsoft Needs to Act Now |publisher=eWeek.com |author=Joe Wilcox |date=16 April 2009 |accessdate=2010-10-14}}</ref> According to research firm NPD, the average selling price of all Windows portable PCs has fallen from $659 in October 2008 to $519 in October 2009.<ref>{{cite web |url=http://www.cio.com/article/509556/Falling_PC_Prices_Pit_Microsoft_Against_PC_Makers |title=Falling PC Prices Pit Microsoft Against PC Makers |author=Shane O'Neill |date=2 December 2009 |accessdate=2010-10-14}}</ref>  == Types == ===Stationary=== ==== Workstation ==== [[File:SPARCstation 1.jpg|right|thumb|Sun [[SPARCstation]]  1 , 25&nbsp;MHz [[RISC]] processor from early 1990s]]  {{Main|Workstation}} A workstation is a high-end personal computer designed for technical or scientific applications. Intended primarily to be used by one person at a time, they are commonly connected to a [[local area network]] and run multi-user [[operating system]]s. Workstations are used for tasks such as [[computer-aided design]], drafting and modeling, computation-intensive scientific and engineering calculations,  image processing, [[architecture|architectural]] modeling, and [[computer graphics]] for animation and motion picture visual effects.<ref>{{cite encyclopedia |last=Ralston | first=Anthony | coauthors=Reilly, Edwin | encyclopedia=Encyclopedia of Computer Science | title=Workstation | edition=Third Edition | year=1993 | publisher=Van Nostrand Reinhold | location=New York | isbn=0-442-27679-6}}</ref>  ==== Desktop computer ==== {{Main|Desktop computer}} [[File:Desktop personal computer.jpg|thumb|right|[[Dell OptiPlex]] [[desktop computer]]]]  Prior to the wide spread usage of PCs, a computer that could fit on a [[desk]] was remarkably small. Today the phrase usually indicates a particular style of [[computer case]]. Desktop computers come in a variety of  styles ranging from large vertical [[tower case]]s to [[small form factor]] models that can be tucked behind an [[LCD monitor]]. In this sense, the term 'desktop' refers specifically to a horizontally oriented case, usually intended to have the display screen placed on top to save space on the desk top. Most modern desktop computers have separate screens and keyboards.  ===== Gaming Computer ===== {{Main|Gaming computer}} A gaming computer is a standard desktop computer that typically has high-performance hardware, such as a more powerful [[video card]], processor, and memory, in order to handle the requirements of demanding [[PC game|video games]]. A number of companies, such as [[Alienware]], manufacture prebuilt gaming computers, and companies such as [[Razer]] and [[Logitech]] market mice, keyboards, and headsets geared towards gamers.  ===== Single unit ===== {{further2|[[All-in-one computer]]}} Single unit PCs (also known as all-in-one PCs) are a subtype of desktop computers, which combine the monitor and case of the computer within a single unit. The monitor often utilizes a [[touchscreen]] as an optional method of user input, however detached keyboards and mice are normally still included. The inner components of the PC are often located directly behind the monitor, and many are built similarly to laptops.  ==== Nettop ==== {{Main|Nettop}}   A subtype of desktops, called [[Nettop (computer)|nettop]]s, was introduced by [[Intel]] in February 2008 to describe low-cost, lean-function, desktop computers. A similar subtype of laptops (or notebooks) are the [[netbook]]s (see below). The product line features the new [[Intel Atom]] processor which specially enables them to consume less power and to be built into small enclosures.  ==== Home theater PC ==== {{Main|Home theater PC}} [[File:Home theater PC front with keyboard.jpg|right|thumb|[[Antec]] Fusion V2 [[home theater PC]] with [[Keyboard (computing)|keyboard]] on top.]]  A home theater PC (HTPC) is a convergence device that combines the functions of a personal computer and a [[digital video recorder]].  It is connected to a [[television]] or a television-sized [[computer display]] and is often used as a digital photo, music, video player, TV receiver and digital video recorder. Home theater PCs are also referred to as media center systems or [[media server]]s. The general goal in a HTPC is usually to combine many or all components of a [[home cinema|home theater]] setup into one box.  They can be purchased pre-configured with the required hardware and software needed to add television programming to the PC, or can be cobbled together out of discrete components as is commonly done with [[MythTV]], [[Windows Media Center]], [[GB-PVR]], [[SageTV]], Famulent or [[LinuxMCE]]. More recently, home theatre PCs have been given the ability to connect to services that play movies and TV shows on demand.  ===Mobile=== ==== Laptop ==== {{Main|Laptop}}  [[File:MSI Laptop computer.jpg|thumb|A modern laptop computer]]   A laptop computer or simply [[laptop]], also called a notebook computer, is a small personal computer designed for portability. Usually all of the interface hardware needed to operate the laptop, such as [[USB|USB ports]] (previously [[parallel port|parallel]] and [[serial port]]s), graphics card, sound channel, etc., are built in to a single unit. Laptops contain high capacity [[battery (electricity)|batteries]] that can power the device for extensive periods of time, enhancing portability. Once the battery charge is depleted, it will have to be recharged through a power outlet. In the interest of saving power, weight and space, they usually share RAM with the video channel, slowing their performance compared to an equivalent desktop machine. For this reason, Desktop or Gaming computers are generally preferred to laptop PCs for gaming purposes.   One main drawback of the laptop is sometimes, due to the size and configuration of components, relatively little can be done to upgrade the overall computer from its original design. Internal upgrades are either not manufacturer recommended, can damage the laptop if done with poor care or knowledge, or in some cases impossible, making the desktop PC more modular. Some internal upgrades, such as memory and hard disks upgrades are often easy, a display or keyboard upgrade is usually impossible. The laptop has the same access as the desktop to the wide variety of devices, such as external displays, mice, cameras, storage devices and keyboards, which may be attached externally through USB ports and other less common ports such as external video.  A subtype of notebooks, called [[subnotebook]]s, are computers with most of the features of a standard laptop computer but smaller. They are larger than [[hand-held computer]]s, and usually run full versions of desktop/laptop operating systems. [[Ultra-Mobile PC]]s (UMPC) are usually considered subnotebooks, or more specifically, subnotebook [[tablet computer|Tablet PC]]s (see below). [[Netbook]]s are sometimes considered in this category, though they are sometimes separated in a category of their own (see below).  ===== Desktop replacement ===== {{Main|Desktop replacement computer}} [[File:Acer Aspire 8920 Gemstone by Georgy.JPG|thumb|An Acer 18'4 inch screen Desktop Replacement Laptop]]   A '''desktop replacement computer''' ('''DTR''') is a personal computer that provides the full capabilities of a [[desktop computer]] while remaining [[Mobile computing|mobile]].  They are often larger, bulkier [[laptop]]s. Because of their increased size, this class of computer usually includes more powerful components and a larger display than generally used in smaller portable computers and can have a relatively limited battery capacity (or none at all). Some use a limited range of desktop components to provide better performance at the expense of battery life. These are sometimes called '''desknotes''', a [[portmanteau]] of the words "desktop" and "notebook," though the term is also applied to desktop replacement computers in general.<ref>[http://www.news.com/2100-1040-979763.html Desktop notebooks stake their claim], accessed October 19, 2007</ref>  ==== Netbook ==== {{Main|Netbook}} [[File:HP 2133 Mini-Note PC (front view compare with pencil).jpg|thumb|An [[HP]] [[netbook]]]] [[Netbooks]] (also called mini notebooks or [[subnotebook]]s) are a rapidly evolving<ref name="cnnogg">{{cite news   | title = Time to drop the Netbook label   | publisher = CNN   | author = Erica Ogg   | url = http://www.cnn.com/2009/TECH/ptech/08/20/cnet.drop.netbook.label/index.html   | date = 20 August 2009}}</ref>  category of small, light and inexpensive [[laptop|laptop computer]]s suited for general computing and accessing [[web application|web-based applications]]; they are often marketed as "companion devices," that is, to augment a user's other computer access.<ref name="cnnogg"/>  [[Walt Mossberg]] called them a "relatively new category of small, light, minimalist and cheap laptops."<ref name="wsj">{{cite news   | title = New Netbook Offers Long Battery Life and Room to Type   | publisher = The Wall Street Journal Online, Personal Technology   | author = Walt Mossberg   | url = http://online.wsj.com/article/SB10001424052970203674704574332522805119180.html   | date = 6 August 2009}}</ref> By August 2009, [[CNET]] called netbooks "nothing more than smaller, cheaper notebooks.".<ref name="cnnogg"/>  Initially, their primary defining characteristic was the lack of an [[optical disc]] drive, requiring it to be a separate and external device. This has become less important as [[flash memory]] devices have gradually increased in capacity, replacing the writable optical disc (e.g. [[CD-RW]], [[DVD-RW]]) as a transportable storage medium.  At their inception in late 2007 &mdash; as smaller notebooks optimized for low weight and low cost<ref name="btwsj">{{cite news   | title = Cheap PCs Weigh on Microsoft   | publisher = Business Technologies, The Wall Street Journal   | date = 8 December 2008   | url = http://blogs.wsj.com/biztech/2008/12/08/cheap-pcs-weigh-on-microsoft/}}</ref> &mdash; netbooks omitted key features (e.g., the [[optical drive]]), featured smaller screens and keyboards, and offered reduced specification and computing power. Over the course of their evolution, netbooks have ranged in size from below 5 in<ref>{{cite web |url=http://www.elitezoom.com/umid-netbook-only-48-display.html |title=UMID Netbook Only 4.8″ |publisher=Elitezoom.com |accessdate=2010-10-14}}</ref> to over 13 in,<ref>{{cite web |url=http://www.futurelooks.com/ces-2009-msi-unveils-the-x320-macbook-air-clone-netbook/ |title=CES 2009 - MSI Unveils the X320 "MacBook Air Clone" Netbook |publisher=Futurelooks.com |date=2009-01-07 |accessdate=2010-10-14}}</ref> and from {{nowrap|~1 kg}} ({{nowrap|2-3 pounds}}).  Often significantly less expensive than other [[laptop]]s,<ref name="pricegrabber">{{cite book |title=Netbook Trends and Solid-State Technology Forecast |publisher=pricegrabber.com |page=7 |url=https://mr.pricegrabber.com/Netbook_Trends_and_SolidState_Technology_January_2009_CBR.pdf |accessdate=2009-01-28}}</ref> by mid-2009, netbooks had been offered to users "free of charge", with an extended service contract purchase of a cellular data plan.<ref>{{citation |url=http://www.nytimes.com/2009/04/02/technology/02netbooks.html |title=Light and Cheap, Netbooks Are Poised to Reshape PC Industry |publisher=The New York Times |date=1 April 2009 |quote=AT&T announced on Tuesday that customers in Atlanta could get a type of compact PC called a netbook for just 50 US$ if they signed up for an Internet service plan... 'The era of a perfect Internet computer for 99 US$ is coming this year,' said Jen-Hsun Huang, the chief executive of Nvidia, a maker of PC graphics chips that is trying to adapt to the new technological order. |accessdate=2010-10-14}}</ref>  In the short period since their appearance, netbooks have grown in size and features, now converging with new smaller, lighter notebooks. By mid 2009, CNET noted "the specs are so similar that the average shopper would likely be confused as to why one is better than the other," noting "the only conclusion is that there really is no distinction between the devices."<ref name="cnnogg"/>  ==== Tablet PC ==== {{Main|Tablet computer}} [[File:Tablet.jpg|right|150px|thumb|[[HP]] [[Compaq]] [[tablet computer|tablet PC]] with rotating/removable keyboard.]]  A tablet PC is a [[Laptop|notebook]] or slate-shaped [[mobile computer]].<!-- hid the following unsourced and incomprehensible (what does "their" refer to?) clause: first introduced by [[Pen computing]] in the early 90s with their PenGo Tablet Computer and popularized by Microsoft.--> Its [[touchscreen]] or [[graphics tablet/screen hybrid]] technology allows the user to operate the computer with a [[Stylus (computing)|stylus]] or digital pen, or a fingertip, instead of a keyboard or mouse. The form factor offers a more mobile way to interact with a computer. Tablet PCs are often used where normal notebooks are impractical or unwieldy, or do not provide the needed functionality. Recently, tablet PCs have been given operating systems normally used on phones, like [[Android (operating system)|Android]] or [[iOS]]. This gives them many of the same uses as a phone, but with more power and functionality.  ==== Ultra-mobile PC ==== {{Main|Ultra-mobile PC}} [[File:UMPC Samsung-Q1-Ultra.JPG|thumb|[[Samsung Q1]] [[Ultra-Mobile PC]].]] The ultra-mobile PC (UMPC) is a specification for a [[small form factor]] of [[tablet computer|tablet PC]]s. It was developed as a joint development exercise by [[Microsoft]], [[Intel]], and [[Samsung]], among others. Current UMPCs typically feature the Windows XP, Windows Vista, Windows 7, or Linux [[operating system]] and low-voltage Intel [[Atom]] or [[VIA C7-M]] processors.  ==== Pocket PC ==== {{Main|Pocket PC}} [[File:O2xda2i.jpg|thumb|right|110px|An O<sub>2</sub> [[pocket PC]]]] A pocket PC is a hardware specification for a handheld-sized computer ([[personal digital assistant]]) that runs the [[Microsoft]] [[Windows Mobile]] [[operating system]]. It may have the capability to run an alternative [[operating system]] like [[NetBSD]] or [[Linux]]. It has many of the capabilities of modern desktop [[IBM PC compatible|PCs]].  Currently there are tens of thousands of [[software|applications]] for handhelds adhering to the Microsoft Pocket PC specification, many of which are [[freeware]]. Some of these devices also include [[mobile phone]] features and thus actually represent a [[smartphone]]. Microsoft compliant Pocket PCs can also be used with many other add-ons like [[Global Positioning System|GPS receivers]], [[barcode]] readers, [[RFID]] readers, and cameras. In 2007, with the release of Windows Mobile 6, Microsoft dropped the name Pocket PC in favor of a new naming scheme. Devices without an integrated phone are called Windows Mobile Classic instead of Pocket PC. Devices with an integrated phone and a touch screen are called Windows Mobile Professional.<ref>[http://www.pocketpcmag.com/_archives/jun07/newwmdev.aspx New Windows Mobile 6 Devices :: Jun/Jul 2007]{{Dead link|date=October 2010}}</ref>  == Hardware == [[File:Personal computer, exploded 6.svg|thumb|300px|An [[exploded view]] of a modern personal computer and peripherals: <ol> <li>[[Image scanner|Scanner]] <li>[[Central processing unit|CPU]] ([[Microprocessor]]) <li>[[Primary storage]] ([[Random access memory|RAM]]) <li>[[Expansion card]]s ([[graphics card]]s, etc.) <li>[[Power supply unit (computer)|Power supply]] <li>[[Optical disc|Optical disc drive]] <li>[[Secondary storage]] ([[Hard disk]]) <li>[[Motherboard]] <li>[[Loudspeaker|Speakers]] <li>[[Computer display|Monitor]] <li>[[System software]] <li>[[Application software]] <li>[[Computer keyboard|Keyboard]] <li>[[Mouse (computing)|Mouse]] <li>[[External hard disk drive|External hard disk]] <li>[[Computer printer|Printer]] </ol>]] {{Main|Personal computer hardware}}  Mass-market consumer computers use highly standardized components and so are simple for an end user to assemble into a working system. A typical [[desktop computer]] consists of a [[computer case]] which holds the [[Computer power supply|power supply]], [[motherboard]], [[hard disk drive|hard disk]] and often an [[optical disc drive]]. External devices such as a [[computer monitor]] or [[visual display unit]], [[computer keyboard|keyboard]], and a [[pointing device]] are usually found in a personal computer.  The motherboard connects all processor, memory and peripheral devices together. The [[RAM]], graphics card and processor are mounted directly onto the motherboard. The [[central processing unit]] microprocessor chip plugs into a [[cpu socket|socket]]. Expansion memory plugs into memory sockets. Some motherboards have the video display adapter, sound and other peripherals integrated onto the motherboard.  Others use [[expansion slot]]s for graphics cards, network cards, or other [[Input/output|I/O]] devices.  Disk drives for mass storage are connected to the mother board with a cable, and to the power supply through another cable. Usually disk drives are mounted in the same case as the motherboard; formerly, expansion chassis were made for additional disk storage.  The graphics and sound card can have a [[break out box]] to keep the analog parts away from the [[electromagnetic radiation]] inside the computer case. For really large amounts of data, a [[tape drive]] can be used or (extra) hard disks can be put together in an external case.  The keyboard and the mouse are external devices plugged into the computer through connectors on an I/O panel on the back of the computer. The monitor is also connected to the I/O panel, either through an onboard port on the motherboard, or a port on the graphics card.  The [[computer hardware|hardware]] capabilities of personal computers can sometimes be extended by the addition of [[expansion card]]s connected via an expansion [[computer bus|bus]]. Some standard peripheral buses often used for adding expansion cards in personal computers as of 2005 are [[Peripheral Component Interconnect|PCI]], [[Accelerated Graphics Port|AGP]] (a high-speed PCI bus dedicated to graphics adapters), and [[PCI Express]]. Most personal computers as of 2005 have multiple physical [[Peripheral Component Interconnect|PCI]] expansion slots. Many also include an AGP bus and expansion slot or a PCI Express bus and one or more expansion slots, but few PCs contain both buses.  Toxic chemicals, such as lead and mercury, are found in certain pieces of PC hardware.  [[Lead]] is found in the [[Cathode ray tube]] (CRT), which is located inside the [[Computer monitor|monitor]], as well as in the [[Printed circuit board]]. While daily PC users are not exposed to these toxic elements, danger arises in [[computer recycling]], and thus stems the controversy of [[electronic waste]] (e-Waste). The process of computer recycling involves workers responsible for manually breaking down computers, which leads to inevitable exposure to these toxic chemicals and serious health issues. Lead is known to cause damage to the central nervous system, kidneys, and slow down child brain development.  [[Mercury (element)|Mercury]], another toxic element found in PCs, is located in the screen's [[fluorescent lamp]], laser light generators in the [[optical disk drive]], and in mercury batteries in the circuit board and [[expansion cards]]. Even the smallest amount of mercury, if it is inhaled or digested can cause serious brain damage. Other chemicals found in PCs are [[cadmium]], [[chromium]], [[Polyvinyl chloride]] (PVC), and [[barium]], which are also found in the [[motherboard]] and other crucial pieces of the computer. In each individual computer, about 17% of the computer is lead, [[copper]], [[zinc]], mercury, and cadmium, 23% is [[plastic]] (which usually gets burned and causes extreme air pollution), and 14% is [[Aluminium]] and 20% is [[Iron]] (the last two do not cause extreme health risks, however they are natural elements and do not decompose, which means they continue to take up space unless they are recycled into new computers). Although there is not a huge amount of these hazardous chemicals in each individual computer, even the smallest amount of lead or mercury can ruin a community’s drinking water, or cause serious brain damage.  === Computer case === {{Main|Computer case}} [[File:Stripped-computer-case.JPG|right|thumb|A stripped [[ATX]] case lying on its side.]] A computer case is the enclosure that contains the main components of a [[computer]]. Cases are usually constructed from [[steel]] or [[aluminum]], although other materials such as [[wood]] and  [[plastic]] have been used. Cases can come in many different sizes, or ''[[motherboard form factor|form factor]]s''. The size and shape of a computer case is usually determined by the form factor of the [[motherboard]] that it is designed to accommodate, since this is the largest and most central component of most computers. Consequently, personal computer form factors typically specify only the ''internal'' dimensions and layout of the case.  Form factors for [[rack-mounted]] and [[blade server|blade]] [[server (computing)|servers]] may include precise ''external'' dimensions as well, since these cases must themselves fit in specific enclosures.  Currently, the most popular form factor for desktop computers is [[ATX]], although [[microATX]] and [[small form factor]]s have become very popular for a variety of uses.  Companies like [[Shuttle Inc.]] and [[Acer (company)|AOpen]] have popularized small cases, for which [[FlexATX]] is the most common motherboard size.  === Power supply unit === {{Main|Power supply unit (computer)}} [[File:PSU-Open1.jpg|right|thumb|Computer [[power supply]] unit with top cover removed.]] The power supply unit converts [[mains electricity|general purpose electric current from the mains]] to [[direct current]] for the other components of the computer.  ===Processor=== {{Main|Central processing unit}} [[File:AMD 64X2 Dual-Core.jpg|thumb|right|[[AMD]] [[Athlon 64 X2]] [[Central processing unit|CPU]].]] The central processing unit, or CPU, is that part of a computer which executes software [[computer program|program]] instructions. In older computers this circuitry was formerly on several [[printed circuit board]]s, but in PCs is a single integrated circuit. Nearly all PCs contain a type of CPU known as a [[microprocessor]]. The microprocessor often plugs into the motherboard using one of many different types of sockets. [[IBM PC compatible]] computers use an [[x86]]-compatible processor, usually made by [[Intel]], [[AMD]], [[VIA Technologies]] or [[Transmeta]]. Apple Macintosh computers were initially built with the [[Motorola]] 680x0 family of processors, then switched to the [[PowerPC]] series (a [[RISC]] architecture jointly developed by [[Apple Computer]], [[IBM]] and [[Motorola]]), but as of 2006, Apple switched again, this time to x86-compatible processors by [[Intel]]. Modern CPUs are equipped with a [[computer fan|fan]] attached via [[heat sink]].  === Motherboard === {{Main|Motherboard}} [[File:Asus a8n VMCSM02.jpg|thumb|right|A [[motherboard]]]] The motherboard, also referred to as systemboard or mainboard, is the primary [[circuit board]] within a personal computer. Many other components connect directly or indirectly to the motherboard. Motherboards usually contain one or more CPUs, supporting circuitry - usually [[integrated circuit]]s (ICs) - providing the interface between the [[central processing unit|CPU]] memory and input/output peripheral circuits, main memory, and facilities for initial setup of the computer immediately after power-on (often called boot [[firmware]] or, in IBM PC compatible computers, a [[BIOS]]). <!-- BIOS is software, remember? --> In many portable and embedded personal computers, the motherboard houses nearly all of the PC's core components. Often a motherboard will also contain one or more peripheral buses and physical connectors for expansion purposes. Sometimes a secondary [[daughter board]] is connected to the motherboard to provide further expandability or to satisfy space constraints.  === Main memory === {{Main|Primary storage}} [[File:DDRSDRAM400-1GB.jpg|thumb|right|1GB [[DDR SDRAM]] PC-3200 module]] A PC's main memory is fast storage that is directly accessible by the CPU, and is used to store the currently executing program and immediately needed data. PCs use [[semiconductor]] [[random access memory]] (RAM) of various kinds such as [[dynamic random access memory|DRAM]], SDRAM or [[static random access memory|SRAM]] as their primary storage. Which exact kind depends on cost/performance issues at any particular time. Main memory is much faster than mass storage devices like [[hard disk]]s or [[optical disc]]s, but is usually [[volatile memory|volatile]], meaning it does not retain its contents (instructions or data) in the absence of power, and is much more expensive for a given capacity than is most mass storage. Main memory is generally not suitable for long-term or archival data storage.  === Hard disk === {{Main|Hard disk drive}} [[File:Hdd.jpg|thumb|right|A [[Western Digital]] 250 GB [[hard disk drive]].]] Mass storage devices store programs and data even when the power is off; they do require power to perform read and write functions during usage. Although [[flash memory]] has dropped in cost, the prevailing form of mass storage in personal computers is still the [[hard disk]]. <!-- Generalize and clean up this article section. --> The disk drives use a sealed head/disk assembly (HDA) which was first introduced by IBM's "Winchester" disk system. The use of a sealed assembly allowed the use of positive air pressure to drive out particles from the surface of the disk, which improves reliability.  If the mass storage controller provides for expandability, a PC may also be upgraded by the addition of extra hard disk or [[optical disc drive]]s. For example, [[Blu-ray|BD-ROM]]s, [[DVD-RW]]s, and various optical disc recorders may all be added by the user to certain PCs. Standard internal storage device connection interfaces are [[Parallel ATA|PATA]], [[Serial ATA]], [[SCSI]]  === Video card === {{Main|Video card}} [[File:PowerColor Radeon X850XT PE.jpg|thumb|right|[[ATI Technologies|ATI]] [[Radeon]] [[video card]]]]  The [[video card]] - otherwise called a graphics card, graphics adapter or video adapter - processes and renders the graphics output from the computer to the [[computer display]], and is an essential part of the modern computer. On older models, and today on budget models, graphics circuitry tended to be integrated with the motherboard but, for modern flexible machines, they are supplied in [[Peripheral Component Interconnect|PCI]], [[Accelerated Graphics Port|AGP]], or [[PCI Express]] format.  When the IBM PC was introduced, most existing business-oriented personal computers used text-only display adapters and had no graphics capability. Home computers at that time had graphics compatible with television signals, but with low resolution by modern standards owing to the limited memory available to the eight-bit processors available at the time.  === Visual display unit === {{Main|Visual display unit}} [[File:Flat monitor.svg|thumb|right|A flat-panel [[LCD monitor]].]] A visual display unit (or monitor) is a piece of [[electrical equipment]], usually separate from the computer case, which displays viewable [[image]]s generated by a [[computer]] without producing a permanent record. The word "monitor" is used in other contexts; in particular in [[television broadcasting]], where a [[television]] picture is displayed to a high standard on a [[broadcast reference monitor]]. A computer display device is usually either a [[cathode ray tube]] or some form of flat panel such as a [[TFT LCD]]. The monitor comprises the display device, [[electronic circuit|circuitry]] to generate a picture from [[electronics|electronic]] [[signal (electrical engineering)|signals]] sent by the computer, and an [[enclosure (electrical)|enclosure]] or case. Within the computer, either as an integral part or a plugged-in [[Expansion card]], there is circuitry to convert internal [[data (computing)|data]] to a format compatible with a monitor.  The images from monitors originally contained only text, but as [[Graphical user interface]]s emerged and became common, they began to display more images and multimedia content.  === Keyboard === {{Main|Keyboard (computing)}} [[File:ModelM.jpg|thumb|right|A computer keyboard]] In computing, a [[Keyboard (computing)|keyboard]] is an arrangement of buttons that each correspond to a function, letter, or number.  They are the primary devices of inputting text.  In most cases, they contain an array of keys specifically organized with the corresponding letters, numbers, and functions printed or engraved on the button.  They are generally designed around an operators language, and many different versions for different languages exist. In English, the most common layout is the [[QWERTY]] layout, which was originally used in [[typewriter]]s.  They have evolved over time, and have been modified for use in computers with the addition of function keys, number keys, arrow keys, and OS specific keys.  Often, specific functions can be achieved by pressing multiple keys at once or in succession, such as inputting characters with accents or opening a task manager.  Programs use keyboard shortcuts very differently and all use different keyboard shortcuts for different program specific operations, such as refreshing a [[web page]] in a [[web browser]] or selecting all text in a word processor.  === Mouse === {{Main|Mouse (computing)}} [[File:Assorted computer mice - MfK Bern.jpg|thumb|Computer mice built between 1986 and 2007]] A [[Mouse (computing)|Mouse]] on a computer is a small, slideable device that users hold and slide around to point at, click on, and sometimes drag objects on screen in a graphical user interface using a pointer on screen.  Almost all Personal Computers have mice.  It may be plugged into a computer's rear mouse socket, or as a [[USB]] device, or, more recently, may be connected wirelessly via a USB antenna or Bluetooth antenna.  In the past, they had a single button that users could press down on the device to "click" on whatever the pointer on the screen was hovering over.  Now, however, many Mice have two or three buttons(possibly ''more''); a "right click" function button on the mouse, which performs a secondary action on a selected object, and a scroll wheel, which users can rotate using their fingers to "scroll" up or down.  The scroll wheel can also be pressed down, and therefore be used as a third button. Some mouse wheels may be ''tilted'' from side to side to allow sideways scrolling. Different programs make use of these functions differently, and may scroll horizontally by default with the scroll wheel, open different menus with different buttons, among others. These functions may be user defined through software utilities.  Mice traditionally detected movement and communicated with the computer with an internal "mouse ball"; and used optical [[encoders]] to detect rotation of the ball and tell the computer where the mouse has moved.  However, these systems were subject to low durability, accuracy and required internal cleaning.  Modern mice use optical technology to directly trace movement of the surface under the mouse and are much more accurate, durable and almost maintenance free.  They work on a wider variety of surfaces and can even operate on walls, ceilings or other non-horizontal surfaces.  ===Other components=== [[File:Computer Workstation Variables.jpg|thumb|right|200px|Proper ergonomic design of personal computer workplace is necessary to prevent repetitive strain injuries, which can develop over time and can lead to long-term disability.<ref>Berkeley Lab. [http://www.lbl.gov/ehs/pub811/hazards/ergonomics.html ''Integrated Safety Management: Ergonomics''].{{Dead link|date=October 2010}} Website. Retrieved 9 July 2008.</ref>]] ;Mass storage All computers require either fixed or removable storage for their operating system, programs and user generated material.<br /> Formerly the  5¼&nbsp;inch and 3½&nbsp;inch [[floppy drive]] were the principal forms of removable storage for backup of user files and distribution of software.  As memory sizes increased, the capacity of the floppy did not keep pace; the [[Zip drive]] and other higher-capacity removable media were introduced but never became as prevalent as the floppy drive.  By the late 1990s the [[optical drive]], in [[CD]] and later [[DVD]] and [[Blu-ray Disc]], became the main method for software distribution, and writeable media provided backup and file interchange. Floppy drives have become uncommon in desktop personal computers since about 2000, and were dropped from many laptop systems even earlier.<ref group=note>The [[NeXT]] computer introduced in 1988 did not include a floppy drive, which at the time was unusual.</ref>  Early [[home computer]]s used [[Compact Cassette|compact audio cassettes]] for file storage; these were at the time a very low cost storage solution, but were displaced by floppy disk drives when manufacturing costs dropped, by the mid 1980s.  A second generation of tape recorders was provided when [[Videocassette recorder]]s were pressed into service as backup media for larger disk drives.  All these systems were less reliable and slower than purpose-built magnetic tape drives. Such tape drives were uncommon in consumer-type personal computers but were a necessity in business or industrial use.  Interchange of data such as photographs from digital cameras is greatly expedited by installation of a [[card reader]], which often is compatible with several forms of [[flash memory]]. It is usually faster and more convenient to move large amounts of data by removing the card from the mobile device, instead of communicating with the mobile device through a [[USB]] interface.  A [[USB flash drive]] today performs much of the data transfer and backup functions formerly done with floppy drives, [[Zip disk]]s and other devices.  Main-stream current operating systems for personal computers provide standard support for flash drives, allowing interchange even between computers using different processors and operating systems. The compact size and lack of moving parts or dirt-sensitive media, combined with low cost for high capacity, have made flash drives a popular and useful accessory for any personal computer user.  The operating system (e.g.: Microsoft Windows, Mac OS, Linux or many others) can be located on any storage, but typically it is on a hard disks. A [[Live CD]] is the running of a OS directly from a CD. While this is slow compared to storing the OS on a hard drive, it is typically used for installation of operating systems, demonstrations, system recovery, or other special purposes. Large flash memory is currently more expensive than hard drives of similar size (as of mid-2008) but are starting to appear in laptop computers because of their low weight, small size and low power requirements.  ;Computer communications * [[Softmodem|Internal modem card]] * [[Modem]] * [[Network card|Network adapter card]] * [[Router (computing)|Router]]  ;Common [[peripheral]]s and adapter cards * [[Headset (audio)|Headset]] * [[Joystick]] * [[Microphone]] * [[Computer printer|Printer]] * [[Image scanner|Scanner]] * [[Sound card|Sound adapter card]] as a separate card rather than located on the motherboard * [[Loudspeaker|Speaker]]s * [[Webcam]]  == Software == {{Main|Computer software}} [[File:OpenOffice.org Writer.png|thumb|A [[screenshot]] of the [[OpenOffice.org Writer]] software]] Computer software is a general term used to describe a collection of [[computer program]]s, [[Algorithm|procedures]] and documentation that perform some tasks on a computer system.<ref>{{cite web | title = Wordreference.com: WordNet 2.0 | publisher = Princeton University, Princeton, NJ | url = http://www.wordreference.com/definition/software | accessdate = 2007-08-19 }}</ref> The term includes [[application software]] such as [[word processor]]s which perform productive tasks for users, [[system software]] such as [[operating system]]s, which interface with [[computer hardware]] to provide the necessary services for application software, and [[middleware]] which controls and co-ordinates [[Distributed computing|distributed systems]].  Software applications for [[word processing]], [[Internet]] browsing, [[Internet fax]]ing, [[e-mail]] and other digital messaging, [[multimedia]] playback, [[computer game]] play and [[computer programming]] are common. The user of a modern personal computer may have significant knowledge of the operating environment and application programs, but is not necessarily interested in programming nor even able to write programs for the computer. Therefore, most [[software]] written primarily for personal computers tends to be designed with simplicity of use, or "[[usability|user-friendliness]]" in mind. However, the [[software industry]] continuously provide a wide range of new products for use in personal computers, targeted at both the expert and the non-expert user.  === Operating system === {{Main|Operating system}}{{See also|Usage share of operating systems}} An operating system (OS) manages computer resources and provides programmers with an [[User interface|interface]] used to access those resources. An operating system processes system data and user input, and responds by allocating and managing tasks and internal system resources as a service to users and programs of the system. An operating system performs basic tasks such as controlling and allocating [[Computer data storage|memory]], prioritizing system requests, controlling [[input/output|input and output]] devices, facilitating [[computer networking]] and managing files.  Common contemporary desktop OSs are [[Microsoft Windows]], [[Mac OS X]], [[Linux]], [[Solaris (operating system)|Solaris]] and [[FreeBSD]]. Windows, Mac, and Linux all have server and personal variants. With the exception of Microsoft Windows, the designs of each of the aforementioned OSs were inspired by, or directly inherited from, the [[Unix]] operating system. [[Unix]] was developed at [[Bell Labs]] beginning in the late 1960s and spawned the development of numerous free and proprietary operating systems.  ==== Microsoft Windows ==== {{Main|Microsoft Windows}} Microsoft Windows is the collective brand name of several [[software]] [[operating system]]s by [[Microsoft]]. Microsoft first introduced an operating environment named ''Windows'' in November 1985 as an add-on to [[MS-DOS]] in response to the growing interest in [[graphical user interface]]s (GUIs)<ref name=aboutcomnov>{{cite web |url=http://inventors.about.com/od/mstartinventions/a/Windows.htm?rd=1 |title=The Unusual History of Microsoft Windows |author=Mary Bellis |publisher=About.com |accessdate=2010-10-14}}</ref><ref name=linuxworld>{{cite web |url=http://www.linuxworld.com.au/index.php/id;940707233;fp;2;fpid;1 |title=IDC: Consolidation to Windows won't happen |publisher=Linuxworld |accessdate=2010-10-14}}</ref> generated by Apple's [[1984 (advertisement)|1984]] introduction of the [[Macintosh]]. The most recent client version of Windows is [[Windows 7]] and [[Windows Server 2008 R2]] which was available at retail on October 22, 2009.  ==== OS X ==== {{Main|OS X}} OS X (formerly Mac OS X) is a line of [[operating system]]s developed, marketed, and sold by [[Apple Inc.]]. OS X is the successor to the original [[Mac OS]], which had been Apple's primary operating system since 1984. OS X is a Unix-based [[graphical user interfaces|graphical]] operating system. The most recent version of OS X is [[OS X Mountain Lion]].  ====AmigaOS==== {{Main|AmigaOS}} AmigaOS is the default native operating system of the Amiga personal computer. It was developed first by Commodore International, and initially introduced in 1985 with the Amiga 1000. Early versions (1.0-3.9) run on the Motorola 68k series of 16-bit and 32-bit microprocessors, while the newer AmigaOS 4 runs only on PowerPC microprocessors. On top of a preemptive multitasking kernel called Exec, it includes an abstraction of the Amiga's unique hardware, a disk operating system called AmigaDOS, a windowing system API called Intuition and a graphical user interface called Workbench. A command line interface called AmigaShell is also available and integrated into the system. The GUI and the CLI complement each other and share the same privileges. The current holder of the Amiga intellectual properties is Amiga Inc. They oversaw the development of AmigaOS 4 but did not develop it themselves, contracting it instead to Hyperion Entertainment. On 20 December 2006, Amiga Inc terminated Hyperion's license to continue development of AmigaOS 4. However, in 30 September 2009, Hyperion was granted an exclusive, perpetual, worldwide right to AmigaOS 3.1 in order to use, develop, modify, commercialize, distribute and market AmigaOS 4.x and subsequent versions of AmigaOS (including AmigaOS 5).  ==== Linux ==== {{Main|Linux}} [[File:KDE 4.png|thumb|right|A [[Linux distribution]] running [[KDE Plasma Desktop]].]] Linux is a family of [[Unix-like]] computer [[operating system]]s. Linux is one of the most prominent examples of [[free software]] and [[open source]] development: typically all underlying [[source code]] can be freely modified, used, and redistributed by anyone.<ref>{{cite web | title = Linux Online ─ About the Linux Operating System | url = http://www.linux.org/info/index.html | publisher = Linux.org | accessdate = 2007-07-06 }}</ref> The name "Linux" comes from the [[Linux kernel]], started in 1991 by [[Linus Torvalds]]. The system's [[System software|utilities]] and [[library (computer science)|libraries]] usually come from the [[GNU]] operating system, announced in 1983 by [[Richard Stallman]]. The GNU contribution is the basis for the [[GNU/Linux naming controversy|alternative name]] GNU/Linux.<ref name="lsag">{{cite book | url = http://www.tldp.org/LDP/sag/html/sag.html#GNU-OR-NOT | title = Linux System Administrator's Guide | chapter = 1.1 | edition = version 0.9 | year = 2004 | accessdate = 2007-01-18 | first = Alex | last = Weeks }}</ref>  Known for its use in [[server (computing)|server]]s as part of the [[LAMP (software bundle)|LAMP]] application stack, Linux is supported by corporations such as [[Dell]], [[Hewlett-Packard]], [[IBM]], [[Novell]], [[Oracle Corporation]], [[Red Hat]], [[Canonical Ltd.]] and [[Sun Microsystems]]. It is used as an operating system for a wide variety of [[computer hardware]], including [[desktop computer]]s, [[netbooks]], [[supercomputers]],<ref>{{cite news | title = Linux rules supercomputers | url = http://www.forbes.com/home/enterprisetech/2005/03/15/cz_dl_0315linux.html | last = Lyons | first = Daniel | accessdate = 2007-02-22 | work=Forbes | date=15 March 2005}}</ref> video game systems, such as the [[PlayStation 3]], several [[arcade games]], and [[embedded devices]] such as [[mobile phone]]s, [[portable media player]]s, [[Router (computing)|routers]], and [[stage lighting]] systems.  === Applications === {{Unreferenced section|date=June 2008}} {{Main|Application software}} [[File:Gimp-gnome-2.2.8.png|thumb|right|[[GIMP]] [[raster graphics editor]]]] A computer user will apply application software to carry out a specific task.  [[System software]] supports applications and provides common services such as memory management, network connectivity, or device drivers; all of which may be used by applications but which are not directly of interest to the end user.  A simple, if imperfect [[analogy]] in the world of hardware would be the relationship of an electric light bulb (an application) to an electric power generation plant (a system).  The power plant merely generates electricity, not itself of any real use until harnessed to an application like the electric light that performs a service that benefits the user.  Typical examples of software applications are [[word processor]]s, [[spreadsheet]]s, and [[media player (application software)|media player]]s. Multiple applications bundled together as a package are sometimes referred to as an ''application suite''. [[Microsoft Office]] and [[OpenOffice.org]], which bundle together a word processor, a spreadsheet, and several other discrete applications, are typical examples. The separate applications in a suite usually have a [[user interface]] that has some commonality making it easier for the user to learn and use each application. And often they may have some capability to interact with each other in ways beneficial to the user. For example, a spreadsheet might be able to be embedded in a word processor document even though it had been created in the separate spreadsheet application.  [[End-user development]] tailors systems to meet the user's specific needs. User-written software include spreadsheet templates, word processor macros, scientific simulations, graphics and animation scripts. Even email filters are a kind of user software. Users create this software themselves and often overlook how important it is.  ==Electronic waste== Personal computers can become a large contributor to the 50 million tons of waste of discarded electronic goods that is being generated annually, according to the United Nations Environment Programme. To solve the  [[electronic waste]] issue affecting developing countries and the environment, an [[Extended producer responsibility]] act was implemented. Organizations, such as the [[Silicon Valley Toxics Coalition]], [[Basel Action Network]], Toxics Link India, SCOPE, and [[Greenpeace]] contribute to the strategy. The [[Silicon Valley Toxics Coalition]] and BAN (Basel Action Network) teamed up with 32 electronic recyclers in the US and Canada to create an e-steward program due to the lack of national legislation and regulation for the exporting and importing of electronic waste. The creation of an e-steward program created another option for manufacturers and customers to dispose of their electronic waste properly. The Silicon Valley Toxics Coalition founded the Electronics TakeBack Coalition, which is a coalition that advocates for the production of environmentally friendly products. The TakeBack Coalition works with policy makers, recyclers, and smart businesses to get manufacturers to take full responsibility of their products. There are organizations opposing EPR, such as the [[Reason Foundation]] opposes the EPR for two main reasons.  The first reason is that the EPR relies on the idea that is the manufacturers pay for the harm they do to the environment, they will learn their lesson.  The second reason is that the EPR assumes the current design practices are environmentally inefficient.  The Reason Foundation claims that manufacturers are moving toward reduced material use and energy use instead. {{Main|Computer recycling}}  == See also == {{Portal|Computer Science|Electronics}} * [[Computer virus]] * [[Desktop computer]] * [[Desktop replacement computer]] * [[e-waste]] * [[Information and communication technologies for development]] * [[List of computer system manufacturers]] * [[Market share of leading PC vendors]] * [[Personal Computer Museum]] * [[Public computer]] * [[Quiet PC]] * [[PC game]] * [[Trashware]] * [[Computer case]]  == Notes == <references group=note/>  ==References== {{reflist|colwidth=30em}}  ==Further reading== * ''Accidental Empires: How the boys of [[Silicon Valley]] make their millions, battle foreign competition, and still can't get a date'', Robert X. Cringely, Addison-Wesley Publishing, (1992), ISBN 0-201-57032-7  ==External links== {{Commons category|Personal computers}} {{Wikiversity|Introduction to Computers/Personal}} {{Wiktionary}} {{Wikiquote}} * [[How Stuff Works]] pages: ** [http://videos.howstuffworks.com/harvard-extension-schools-computer-science-e-1-understand/1290-dissecting-a-pc-video.htm Dissecting a PC] ** [http://computer.howstuffworks.com/pc.htm How PCs Work] ** [http://videos.howstuffworks.com/labratstv/3548-how-to-upgrade-your-computer-video.htm How to Upgrade Your Computer] * [[wikihow:Build-a-Computer|How to Build a Computer]] {{Computer sizes}} {{Time Persons of the Year 1976-2000}}  {{DEFAULTSORT:Personal Computer}} [[Category:Office equipment]] [[Category:Personal computers|*]] [[Category:American inventions]] [[Category:Classes of computers]]  {{Link FA|ka}} [[ar:حاسوب شخصي]] [[bn:ব্যক্তিগত কম্পিউটার]] [[zh-min-nan:Kò-jîn tiān-náu]] [[be:Персанальны камп'ютар]] [[be-x-old:Пэрсанальны кампутар]] [[bg:Персонален компютър]] [[ca:Ordinador personal]] [[cs:Osobní počítač]] [[da:Personlig computer]] [[de:Personal Computer]] [[nv:Béésh bee akʼeʼelchí áłtsisígíí]] [[et:Personaalarvuti]] [[el:Προσωπικός υπολογιστής]] [[es:Computadora personal]] [[eo:Persona komputilo]] [[fa:رایانه شخصی]] [[fo:Eginteldur]] [[fr:Ordinateur personnel]] [[fy:Persoanlike kompjûter]] [[fur:Ordenadôr personâl]] [[ga:Ríomhaire pearsanta]] [[ko:개인용 컴퓨터]] [[hy:Անհատական համակարգիչ]] [[hi:व्यक्तिगत संगणक]] [[hr:Osobno računalo]] [[id:Komputer pribadi]] [[ia:Computator personal]] [[is:Einkatölva]] [[it:Personal computer]] [[he:מחשב אישי]] [[ka:პერსონალური კომპიუტერი]] [[kk:Дербес компьютер]] [[ku:Komputera kesanî]] [[lv:Personālais dators]] [[lt:Asmeninis kompiuteris]] [[hu:Személyi számítógép]] [[mk:Личен сметач]] [[ml:പെഴ്സണൽ കമ്പ്യൂട്ടർ]] [[mr:व्यक्तिगत संगणक]] [[xmf:პერსონალური კომპიუტერი]] [[ms:Komputer peribadi]] [[mn:Персональ компьютер]] [[nl:Personal computer]] [[ja:パーソナルコンピュータ]] [[no:Personlig datamaskin]] [[nn:Personleg datamaskin]] [[uz:Shaxsiy kompyuter]] [[ps:ځاني سولګر]] [[pl:Komputer osobisty]] [[pt:Computador pessoal]] [[ro:Computer personal]] [[ru:Персональный компьютер]] [[stq:Personal Computer]] [[sq:Kompjuteri personal]] [[si:පෞද්ගලික පරිගණකය]] [[simple:Personal computer]] [[sk:Osobný počítač]] [[sr:Лични рачунар]] [[sh:Osobni kompjuter]] [[fi:Henkilökohtainen tietokone]] [[sv:Persondator]] [[ta:தனி மேசைக் கணினி]] [[th:คอมพิวเตอร์ส่วนบุคคล]] [[tg:Компутари фардӣ]] [[tr:Kişisel bilgisayar]] [[uk:Персональний комп'ютер]] [[ur:ذاتی شمارندہ]] [[vi:Máy tính cá nhân]] [[fiu-vro:Personalpuutri]] [[vls:Personal computer]] [[war:Kompyuter personal]] [[yi:פערזענלעכער קאמפיוטער]] [[zh-yue:個人電腦]] [[zh:个人电脑]]
{{Use dmy dates|date=April 2012}} {{Programming language lists}} A '''programming language''' is an [[Formal language|artificial language]] designed to communicate [[Machine instruction|instructions]] to a [[machine]], particularly a [[computer]].  Programming languages can be used to create [[program (machine)|programs]] that control the behavior of a machine and/or to express [[algorithm]]s precisely.  The earliest programming languages predate the [[History of computing hardware|invention of the computer]], and were used to direct the behavior of machines such as [[Jacquard loom]]s and [[player piano]]s. Thousands of different programming languages have been created, mainly in the computer field, with many more being created every year. Most programming languages describe computation in an [[imperative programming|imperative]] style, i.e., as a sequence of commands, although some languages, such as those that support [[functional programming]] or [[logic programming]], use alternative forms of description.  The description of a programming language is usually split into the two components of [[Syntax (programming languages)|syntax]] (form) and [[semantics]] (meaning). Some languages are defined by a specification document (for example, the [[C (programming language)|C]] programming language is specified by an [[International Organization for Standardization|ISO]] Standard), while other languages, such as [[Perl]] 5 and earlier, have a dominant [[Programming language implementation|implementation]] that is used as a [[reference implementation|reference]].    ==Definitions== A programming language is a notation for writing [[computer program|programs]], which are specifications of a computation or [[algorithm]].<ref name="Aaby 2004">{{cite book|last=Aaby|first=Anthony|title=Introduction to Programming Languages|year=2004|url=http://burks.brighton.ac.uk/burks/pcinfo/progdocs/plbook/index.htm}}</ref> Some, but not all, authors restrict the term "programming language" to those languages that can express ''all'' possible algorithms.<ref name="Aaby 2004"/><ref>In mathematical terms, this means the programming language is [[Turing completeness|Turing-complete]] {{cite book | last=MacLennan | first=Bruce J. | title=Principles of Programming Languages | page=1 | publisher=Oxford University Press | year=1987 | isbn=0-19-511306-3 }}</ref> Traits often considered important for what constitutes a programming language include: * ''Function and target:'' A ''computer programming language'' is a language<ref name="Fischer">Steven R. Fischer, ''A history of language'', Reaktion Books, 2003, ISBN 1-86189-080-X, p. 205</ref> used to write [[computer program]]s, which involve a [[computer]] performing some kind of computation<ref name=sigplan>{{cite web|author=[[Association for Computing Machinery|ACM]] SIGPLAN|title=Bylaws of the Special Interest Group on Programming Languages of the Association for Computing Machinery|url=http://www.acm.org/sigs/sigplan/sigplan_bylaws.htm|accessdate=19 June 2006|year=2003}}, ''The scope of SIGPLAN is the theory, design, implementation, description, and application of computer programming languages - languages that permit the specification of a variety of different computations, thereby providing the user with significant control (immediate or delayed) over the computer's operation.''</ref> or [[algorithm]] and possibly control external devices such as [[printer (computing)|printers]], [[disk drive]]s, [[robot]]s,<ref name="robots">{{cite web|url=http://www.cs.brown.edu/people/tld/courses/cs148/02/programming.html |title=Programming Robots |accessdate=23 September 2006 |last=Dean |first=Tom |date= |year=2002 |work=Building Intelligent Robots |publisher=Brown University Department of Computer Science}}</ref> and so on. For example [[PostScript]] programs are frequently created by another program to control a computer printer or display. More generally, a programming language may describe computation on some, possibly abstract, machine. It is generally accepted that a complete specification for a programming language includes a description, possibly idealized, of a machine or processor for that language.<ref name=nara2>R. Narasimahan, Programming Languages and Computers: A Unified Metatheory, pp. 189--247 in Franz Alt, Morris Rubinoff (eds.) Advances in computers, Volume 8, Academic Press, 1994, ISBN 012012108, p.193 : "a complete specification of a programming language must, by definition, include a specification of a processor--idealized, if you will--for that language." [the source cites many references to support this statement]</ref> In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way.<ref>{{cite book|last=Ben Ari|first=Mordechai|title=Understanding Programming Languages|publisher=John Wiley and Sons| year=1996|quote=Programs and languages can be deﬁned as purely formal mathematical objects. However, more people are interested in programs than in other mathematical objects such as groups, precisely because it is possible to use the program—the sequence of symbols—to control the execution of a computer. While we highly recommend the study of the theory of programming, this text will generally limit itself to the study of programs as they are executed on a computer.}}</ref> Programming languages differ from [[natural language]]s in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines. * ''Abstractions:'' Programming languages usually contain [[abstraction (computer science)|abstractions]] for defining and manipulating [[data structure]]s or controlling the [[control flow|flow of execution]]. The practical necessity that a programming language support adequate abstractions is expressed by the [[abstraction principle (programming)|abstraction principle]];<ref>David A. Schmidt, ''The structure of typed programming languages'', MIT Press, 1994, ISBN 0-262-19349-3, p. 32</ref> this principle is sometimes formulated as recommendation to the programmer to make proper use of such abstractions.<ref>{{cite book|last=Pierce|first=Benjamin|title=Types and Programming Languages|publisher=MIT Press|year=2002|isbn=0-262-16209-1|page=339}}</ref> * ''Expressive power:'' The [[theory of computation]] classifies languages by the computations they are capable of expressing. All [[Turing completeness|Turing complete]] languages can implement the same set of [[algorithm]]s. [[SQL|ANSI/ISO SQL]] and [[Charity (programming language)|Charity]] are examples of languages that are not Turing complete, yet often called programming languages.<ref>{{cite web | author=Digital Equipment Corporation | title=Information Technology - Database Language SQL (Proposed revised text of DIS 9075) | url=http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt | work=ISO/IEC 9075:1992, Database Language SQL | accessdate=29 June 2006}}</ref><ref>{{cite web|author=The Charity Development Group|title=The CHARITY Home Page | url=http://pll.cpsc.ucalgary.ca/charity1/www/home.html | month=December|year=1996|accessdate=29 June 2006}}, ''Charity is a categorical programming language...'', ''All Charity computations terminate.''</ref>  [[Markup languages]] like [[XML]], [[HTML]] or [[troff]], which define [[structured data]], are not generally considered programming languages.<ref>[http://www.w3.org/XML/1999/XML-in-10-points.html XML in 10 points] [[W3C]], 1999, ''XML is not a programming language.''</ref><ref>{{cite book|last=Powell|first=Thomas|title=HTML & XHTML: the complete reference|publisher=McGraw-Hill|year=2003|isbn=0-07-222942-X|page=25|quote=''HTML is not a programming language.''}}</ref><ref>{{cite book|last1=Dykes|first1=Lucinda|first2=Ed|last2=Tittel|title=XML For Dummies, 4th Edition|publisher=Wiley|year=2005|isbn=0-7645-8845-1|page=20|quote=''...it's a markup language, not a programming language.''}}</ref> Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. [[XSLT]], for example, is a [[Turing completeness|Turing complete]] XML dialect.<ref>{{cite web|url=http://www.ibm.com/developerworks/library/x-xslt/ |title=What kind of language is XSLT? |publisher=Ibm.com |date= |accessdate=3 December 2010}}</ref><ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms767587(VS.85).aspx |title=XSLT is a Programming Language |publisher=Msdn.microsoft.com |date= |accessdate=3 December 2010}}</ref><ref>{{cite book|last=Scott|first=Michael|title=Programming Language Pragmatics|publisher=[[Morgan Kaufmann]]|year=2006|isbn=0-12-633951-1|page=802|quote=''XSLT, though highly specialized to the transformation of XML, is a Turing-complete programming language.''}}</ref> Moreover, [[LaTeX]], which is mostly used for structuring documents, also contains a Turing complete subset.<ref>http://tobi.oetiker.ch/lshort/lshort.pdf</ref><ref>{{cite book|last=Syropoulos|first=Apostolos|coauthors=Antonis Tsolomitis, Nick Sofroniou|title=Digital typography using LaTeX|publisher=Springer-Verlag|year = 2003|isbn=0-387-95217-9|page=213|quote=''TeX is not only an excellent typesetting engine but also a real programming language.''}}</ref>  The term ''computer language'' is sometimes used interchangeably with programming language.<ref>Robert A. Edmunds, The Prentice-Hall standard glossary of computer terminology, Prentice-Hall, 1985, p. 91</ref> However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages.<ref>Pascal Lando, Anne Lapujade, Gilles Kassel, and Frédéric Fürst, ''[http://www.loa-cnr.it/ICSOFT2007_final.pdf Towards a General Ontology of Computer Programs]'', [http://dblp.uni-trier.de/db/conf/icsoft/icsoft2007-1.html ICSOFT 2007], pp. 163-170</ref> In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.<ref>S.K. Bajpai, ''Introduction To Computers And C Programming'', New Age International, 2007, ISBN 81-224-1379-X, p. 346</ref> Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.<ref>R. Narasimahan, Programming Languages and Computers: A Unified Metatheory, pp. 189--247 in Franz Alt, Morris Rubinoff (eds.) Advances in computers, Volume 8, Academic Press, 1994, ISBN 012012108, p.215: "[...] the model [...] for computer languages differs from that [...] for programming languages in only two respects. In a computer language, there are only finitely many names--or registers--which can assume only finitely many values--or states--and these states are not further distinguished in terms of any other attributes. [author's footnote:] This may sound like a truism but its implications are far reaching. For example, it would imply that any model for programming languages, by fixing certain of its parameters or features, should be reducible in a natural way to a model for computer languages."</ref> [[John C. Reynolds]] emphasizes that [[formal specification]] languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.<ref>John C. Reynolds, ''Some thoughts on teaching programming and programming languages'', [[SIGPLAN]] Notices, Volume 43, Issue 11, November 2008, p.109</ref>  ==Elements== All programming languages have some [[language primitive|primitive]] building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.  ===Syntax=== [[Image:Python add5 parse.png|thumb|right|367px|[[Parse tree]] of Python code with inset tokenization]] [[Image:Python add5 syntax.svg|thumb|right|292px|[[Syntax highlighting]] is often used to aid programmers in recognizing elements of source code. The language above is [[Python (programming language)|Python]].]]  {{Main|Syntax (programming languages)}}  A programming language's surface form is known as its [[syntax (programming languages)|syntax]]. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more [[visual programming language|graphical]] in nature, using visual relationships between symbols to specify a program.  The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either [[Formal semantics of programming languages|formal]] or hard-coded in a [[Reference implementation (computing)|reference implementation]]). Since most languages are textual, this article discusses textual syntax.  Programming language syntax is usually defined using a combination of [[regular expression]]s (for [[lexical analysis|lexical]] structure) and [[Backus–Naur Form]] (for [[context-free grammar|grammatical]] structure). Below is a simple grammar, based on [[Lisp (programming language)|Lisp]]: <pre> expression ::= atom | list atom ::= number | symbol number ::= [ -]?['0'-'9']  symbol ::= ['A'-'Z'<nowiki>'</nowiki>a'-'z'].* list ::= '(' expression* ')' </pre>  This grammar specifies the following: * an ''expression'' is either an ''atom'' or a ''list''; * an ''atom'' is either a ''number'' or a ''symbol''; * a ''number'' is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign; * a ''symbol'' is a letter followed by zero or more of any characters (excluding whitespace); and * a ''list'' is a matched pair of parentheses, with zero or more ''expressions'' inside it.  The following are examples of well-formed token sequences in this grammar: '<code>12345</code>', '<code>()</code>', '<code>(a b c232 (1))</code>'  Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit [[undefined behavior]]. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.  Using [[natural language]] as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false: * "[[Colorless green ideas sleep furiously]]." is grammatically well-formed but has no generally accepted meaning. * "John is a married bachelor." is grammatically well-formed but expresses a meaning that cannot be true.  The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation <tt>*p >> 4</tt> has no meaning for a value having a complex type and <tt>p->im</tt> is not defined because the value of <tt>p</tt> is the [[pointer (computer programming)|null pointer]]):  <source lang="c"> complex *p = NULL; complex abs_p = sqrt(*p >> 4   p->im); </source>  If the [[type declaration]] on the first line were omitted, the program would trigger an error on compilation, as the variable "p" would not be defined. But the program would still be syntactically correct, since type declarations provide only semantic information.  The grammar needed to specify a programming language can be classified by its position in the [[Chomsky hierarchy]]. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are [[context-free grammar]]s.<ref>{{cite book|author = Michael Sipser | year = 1996 | title = [[Introduction to the Theory of Computation]] | publisher = PWS Publishing | isbn = 0-534-94728-X |authorlink=Michael Sipser}} Section 2.2: Pushdown Automata, pp.101–114.</ref> Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an [[undecidable problem]], and generally blur the distinction between parsing and execution.<ref>Jeffrey Kegler, "[http://www.jeffreykegler.com/Home/perl-and-undecidability Perl and Undecidability]", ''The Perl Review''. Papers 2 and 3 prove, using respectively [[Rice's theorem]] and direct reduction to the [[halting problem]], that the parsing of Perl programs is in general undecidable.</ref> In contrast to [[Lisp macro|Lisp's macro system]] and Perl's <code>BEGIN</code> blocks, which may contain general computations, C macros are merely string replacements, and do not require code execution.<ref>Marty Hall, 1995, [http://www.apl.jhu.edu/~hall/Lisp-Notes/Macros.html Lecture Notes: Macros], [[PostScript]] [http://www.apl.jhu.edu/~hall/Lisp-Notes/Macros.ps version]</ref>  === Semantics === The term [[Semantics#Computer_science|Semantics]] refers to the meaning of languages, as opposed to their form ([[#Syntax|syntax]]).  ====Static semantics==== The static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.<ref name="Aaby 2004"/> For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every [[identifier]] is declared before it is used (in languages that require such declarations) or that the labels on the arms of a [[case statement]] are distinct.<ref>Michael Lee Scott, ''Programming language pragmatics'', Edition 2, Morgan Kaufmann, 2006, ISBN 0-12-633951-1, p. 18-19</ref> Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that [[subroutine]] calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a [[logic]] called a [[type system]]. Other forms of [[static code analysis|static analyses]] like [[data flow analysis]] may also be part of static semantics. Newer programming languages like [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]] have [[definite assignment analysis]], a form of data flow analysis, as part of their static semantics.  ====Dynamic semantics==== {{main|Semantics of programming languages}} Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the [[evaluation strategy|strategy]] by which expressions are evaluated to values, or the manner in which [[control flow|control structures]] conditionally execute [[Statement (computer science)|statements]]. The ''dynamic semantics'' (also known as ''execution semantics'') of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into [[formal semantics of programming languages]], which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.  ====Type system==== {{Main|Data type|Type system|Type safety}}  A type system defines how a programming language classifies values and expressions into ''types'', how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any [[Decidability (logic)|decidable]] type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have ''type loopholes'', usually unchecked [[Type conversion#Explicit type conversion|casts]] that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to [[type checking|type check]] programs, but a number of languages, usually functional ones, [[type inference|infer types]], relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as ''[[type theory]]''.  =====Typed versus untyped languages===== A language is ''typed'' if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types.<ref name="typing">{{cite web | url=http://www.acooke.org/comp-lang.html | author=Andrew Cooke | title=Introduction To Computer Languages | accessdate=13 July 2012}}</ref> For example, the data represented by "<code>this text between the quotes</code>" is a [[String literal|string]]. In most programming languages, dividing a number by a string has no meaning. Most modern programming languages will therefore reject any program attempting to perform such an operation. In some languages, the meaningless operation will be detected when the program is compiled ("static" type checking), and rejected by the compiler, while in others, it will be detected when the program is run ("dynamic" type checking), resulting in a runtime [[Exception handling|exception]].  A special case of typed languages are the ''single-type'' languages. These are often scripting or markup languages, such as [[REXX]] or [[Standard Generalized Markup Language|SGML]], and have only one data type—most commonly character strings which are used for both symbolic and numeric data.  In contrast, an ''untyped language'', such as most [[assembly language]]s, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths.<ref name="typing"/> High-level languages which are untyped include [[BCPL]] and some varieties of [[Forth (programming language)|Forth]].  In practice, while few languages are considered typed from the point of view of [[type theory]] (verifying or rejecting ''all'' operations), most modern languages offer a degree of typing.<ref name="typing"/> Many production languages provide means to bypass or subvert the type system (see [[Type conversion#Explicit type conversion|casting]]).  =====Static versus dynamic typing===== In ''[[Type system|static typing]]'', all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2 2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.<ref name="typing"/>  Statically typed languages can be either ''[[Manifest typing|manifestly typed]]'' or ''[[Type inference|type-inferred]]''. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable [[declaration (computer science)|declarations]]). In the second case, the compiler ''infers'' the types of expressions and declarations based on context. Most mainstream statically typed languages, such as [[C  ]], [[C Sharp (programming language)|C#]] and [[Java (programming language)|Java]], are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as [[Haskell (programming language)|Haskell]] and [[ML (programming language)|ML]]. However, many manifestly typed languages support partial type inference; for example, [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]] both infer types in certain limited cases.<ref>Specifically, instantiations of [[generic programming|generic]] types are inferred for certain expression forms. Type inference in Generic Java—the research language that provided the basis for Java 1.5's bounded [[polymorphism in object-oriented programming|parametric polymorphism]] extensions—is discussed in two informal manuscripts from the Types mailing list: [http://www.seas.upenn.edu/~sweirich/types/archive/1999-2003/msg00849.html Generic Java type inference is unsound] ([[Alan Jeffrey]], 17 December 2001) and [http://www.seas.upenn.edu/~sweirich/types/archive/1999-2003/msg00921.html Sound Generic Java type inference] ([[Martin Odersky]], 15 January 2002). C#'s type system is similar to Java's, and uses a similar partial type inference scheme.</ref>  ''[[Type system|Dynamic typing]]'', also called ''latent typing'', determines the type-safety of operations at runtime; in other words, types are associated with ''runtime values'' rather than ''textual expressions''.<ref name="typing"/> As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type [[Software bug|errors]] cannot be automatically detected until a piece of code is actually executed, potentially making [[debugging]] more difficult. [[Lisp (programming language)|Lisp]], [[Perl]], [[Python (programming language)|Python]], [[JavaScript]], and [[Ruby (programming language)|Ruby]] are dynamically typed.  =====Weak and strong typing===== ''[[Weak typing]]'' allows a value of one type to be treated as another, for example treating a [[String (computer science)|string]] as a number.<ref name="typing"/> This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at [[compile time]] and even at [[Run time (program lifecycle phase)|run-time]].  ''[[Strongly typed programming language|Strong typing]]'' prevents the above. An attempt to perform an operation on the wrong type of value raises an error.<ref name="typing"/> Strongly typed languages are often termed ''type-safe'' or ''[[type safety|safe]]''.  An alternative definition for "weakly typed" refers to languages, such as [[Perl]] and [[JavaScript]], which permit a large number of implicit type conversions. In JavaScript, for example, the expression <code>2 * x</code> implicitly converts <code>x</code> to a number, and this conversion succeeds even if <code>x</code> is <code>null</code>, <code>undefined</code>, an <code>Array</code>, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. ''Strong'' and ''static'' are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term ''strongly typed'' to mean ''strongly, statically typed'', or, even more confusingly, to mean simply ''statically typed''. Thus [[C (programming language)|C]] has been called both strongly typed and weakly, statically typed.<ref>{{cite web | url=http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-4.html | title=Revised Report on the Algorithmic Language Scheme | date=20 February 1998 | accessdate=9 June 2006}}</ref><ref>{{cite web | url=http://citeseer.ist.psu.edu/cardelli85understanding.html | title=On Understanding Types, Data Abstraction, and Polymorphism | author=[[Luca Cardelli]] and [[Peter Wegner]] | work=Manuscript (1985) | accessdate=9 June 2006}}</ref>  ===Standard library and run-time system=== {{main|Standard library}}  Most programming languages have an associated core [[Library (computing)|library]] (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.  A language's core library is often treated as part of the language by its users, although the designers may have treated it as a separate entity. Many language specifications define a core that must be made available in all implementations, and in the case of standardized languages this core library may be required. The line between a language and its core library therefore differs from language to language. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in [[Java (programming language)|Java]], a string literal is defined as an instance of the <tt>java.lang.String</tt> class; similarly, in [[Smalltalk]], an [[anonymous function]] expression (a "block") constructs an instance of the library's <tt>BlockContext</tt> class. Conversely, [[Scheme (programming language)|Scheme]] contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.  ==Design and implementation== Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing ''language families'' of related languages branching one from another.<ref name="Fischer"/><ref name="levenez">{{cite web|author=Éric Lévénez|title=Computer Languages History|year=2011|url=http://www.levenez.com/lang/}}</ref> But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition.<ref>{{cite web|url=http://www.cs.cornell.edu/info/Projects/Nuprl/cs611/fall94notes/cn2/subsection3_1_3.html|author=Jing Huang|title=Artificial Language vs. Natural Language}}</ref> By contrast, natural languages have changing meanings given by their users in different communities. While [[constructed languages]] are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.  Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages.  Many have eventually fallen into disuse. Although there have been attempts to design one "universal" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role.<ref>IBM in first publishing PL/I, for example, rather ambitiously titled its manual ''The universal programming language PL/I'' (IBM Library; 1966). The title reflected IBM's goals for unlimited subsetting capability: ''PL/I is designed in such a way that one can isolate subsets from it satisfying the requirements of particular applications.'' ({{cite web | url=http://www.encyclopediaofmath.org/index.php?title=PL/I&oldid=19175 | title= PL/I | work=Encyclopedia of Mathematics | accessdate=29 June 2006}}). [[Ada (programming language)|Ada]] and [[UNCOL]] had similar early goals.</ref> The need for diverse programming languages arises from the diversity of contexts in which languages are used: * Programs range from tiny scripts written by individual hobbyists to huge systems written by hundreds of [[programmer]]s. * Programmers range in expertise from novices who need simplicity above all else, to experts who may be comfortable with considerable complexity. * Programs must balance speed, size, and simplicity on systems ranging from [[microcontroller]]s to [[supercomputer]]s. * Programs may be written once and not change for generations, or they may undergo continual modification. * Finally, programmers may simply differ in their tastes: they may be accustomed to discussing problems and expressing them in a particular language.  One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of [[Abstraction (computer science)|abstraction]]. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.<ref>Frederick P. Brooks, Jr.: ''The Mythical Man-Month,'' Addison-Wesley, 1982, pp. 93-94</ref>  [[Natural language processing|Natural language processors]] have been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. [[Edsger W. Dijkstra]] took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as "foolish".<ref>Dijkstra, Edsger W. [http://www.cs.utexas.edu/users/EWD/transcriptions/EWD06xx/EWD667.html On the foolishness of "natural language programming."] EWD667.</ref> [[Alan Perlis]] was similarly dismissive of the idea.<ref>{{cite web | last=Perlis | first=Alan | url=http://www-pu.informatik.uni-tuebingen.de/users/klaeren/epigrams.html | title=Epigrams on Programming | work=SIGPLAN Notices Vol. 17, No. 9 | date=September 1982 | pages=7-13}}</ref> Hybrid approaches have been taken in [[Structured English]] and [[SQL]].  A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language ''specification'' and ''implementation''.  ===Specification=== {{Main|Programming language specification}} The '''specification''' of a programming language is intended to provide a definition that the language [[programmer|users]] and the [[programming language implementation|implementors]] can use to determine whether the behavior of a [[computer program|program]] is correct, given its [[source code]].  A programming language specification can take several forms, including the following: * An explicit definition of the syntax, static semantics, and execution semantics of the language. While syntax is commonly specified using a formal grammar, semantic definitions may be written in [[natural language]] (e.g., as in the [[C (programming language)|C language]]), or a [[formal semantics of programming languages|formal semantics]] (e.g., as in [[Standard ML]]<ref>{{cite book  | last = Milner  | first = R.  | authorlink = Robin Milner  | coauthors = [[Mads Tofte|M. Tofte]], [[Robert Harper (computer scientist)|R. Harper]] and D. MacQueen.  | title = The Definition of Standard ML (Revised)  | publisher = MIT Press  | year = 1997  | isbn = 0-262-63181-4 }}</ref> and [[Scheme (programming language)|Scheme]]<ref>{{cite web|first=Richard |last=Kelsey|coauthors=William Clinger and Jonathan Rees|title=Section 7.2 Formal semantics|work=Revised<sup>5</sup> Report on the Algorithmic Language Scheme|url = http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-10.html#%_sec_7.2| year=1998|month=February|accessdate=9 June 2006}}</ref> specifications). * A description of the behavior of a [[compiler|translator]] for the language (e.g., the [[C  ]] and [[Fortran]] specifications). The syntax and semantics of the language have to be inferred from this description, which may be written in natural or a formal language. * A [[reference implementation|''reference'' or ''model'' implementation]], sometimes [[Meta-circular evaluator|written in the language being specified]] (e.g., [[Prolog]] or [[REXX|ANSI REXX]]<ref>[[American National Standards Institute|ANSI]] — Programming Language Rexx, X3-274.1996</ref>). The syntax and semantics of the language are explicit in the behavior of the reference implementation.  ===Implementation=== {{Main|Programming language implementation}} An '''implementation''' of a programming language provides a way to execute that program on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: ''[[compiler|compilation]]'' and ''[[interpreter (computing)|interpretation]]''. It is generally possible to implement a language using either technique.  The output of a [[compiler]] may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of [[BASIC]] compile and then execute the source a line at a time.  Programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.{{Citation needed|date=October 2008}}  One technique for improving the performance of interpreted programs is [[just-in-time compilation]]. Here the [[virtual machine]], just before execution, translates the blocks of [[bytecode]] which are going to be used to machine code, for direct execution on the hardware.  ==Usage== Thousands of different programming languages have been created, mainly in the computing field.<ref>{{cite web | accessdate=1 June 2009 | url = http://hopl.murdoch.edu.au/ | title=HOPL: an interactive Roster of Programming Languages | publisher = [[Murdoch University]] | location=Australia |quote=This site lists 8512 languages.}}</ref> Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness.  When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using [[pseudocode]], which interleaves natural language with code written in a programming language.  A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A [[programmer]] uses the [[Abstraction (computer science)|abstractions]] present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called [[language primitive|primitives]]).<ref>{{cite web |url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-10.html|title=Structure and Interpretation of Computer Programs|author=Abelson, Sussman, and Sussman|accessdate=3 March 2009}}</ref> ''[[Programming]]'' is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.  Programs for a computer might be [[Execution (computing)|executed]] in a [[Batch processing|batch process]] without human interaction, or a user might type [[Command (computing)|commands]] in an [[Session (computer science)|interactive session]] of an [[Interpreter (computing)|interpreter]]. In this case the "commands" are simply programs, whose execution is chained together. When a language is used to give commands to a software application (such as a [[Shell (computing)|shell]]) it is called a [[scripting language]].{{Citation needed|date=March 2009}}  ===Measuring language usage=== {{Main|Measuring programming language popularity}}  It is difficult to determine which programming languages are most widely used, and what usage means varies by context. One language may occupy the greater number of programmer hours, a different one have more lines of code, and a third utilize the most CPU time. Some languages are very popular for particular kinds of applications. For example, [[COBOL]] is still strong{{citation needed|date=November 2011}} in the corporate data center, often on large [[Mainframe computer|mainframes]]; [[Fortran]] in scientific and engineering applications; and [[C (programming language)|C]] in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.  Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed: * counting the number of job advertisements that mention the language<ref>http://www.computerweekly.com/Articles/2007/09/11/226631/sslcomputer-weekly-it-salary-survey-finance-boom-drives-it-job.htm</ref> * the number of books sold that teach or describe the language<ref>{{cite web|url=http://radar.oreilly.com/archives/2006/08/programming_language_trends_1.html |title=Counting programming languages by book sales |publisher=Radar.oreilly.com |date=2 August 2006 |accessdate=3 December 2010}}</ref> * estimates of the number of existing lines of code written in the language—which may underestimate languages not often found in public searches<ref>Bieman, J.M.; Murdock, V., Finding code on the World Wide Web: a preliminary investigation, Proceedings First IEEE International Workshop on Source Code Analysis and Manipulation, 2001</ref> * counts of language references (i.e., to the name of the language) found using a web search engine.  Combining and averaging information from various internet sites, langpop.com claims that<ref>{{cite web|url=http://www.langpop.com/ |title=Programming Language Popularity |publisher=Langpop.com |date= |accessdate=3 December 2010}}</ref> in 2008 the 10 most cited programming languages are (in alphabetical order): [[C (programming language)|C]], [[C  ]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[JavaScript]], [[Perl]], [[PHP]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], and [[SQL]].  ==Taxonomies== {{details|Categorical list of programming languages}} There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.  The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple [[Thread (computer science)|threads]] in parallel). [[Python (programming language)|Python]] is an object-oriented [[scripting language]].  In broad strokes, programming languages divide into ''[[programming paradigm]]s'' and a classification by ''intended domain of use''. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called [[imperative programming]] languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of [[declarative programming]].<ref>Carl A. Gunter, ''Semantics of Programming Languages: Structures and Techniques'', MIT Press, 1992, ISBN 0-262-57095-5, p. 1</ref> More refined paradigms include [[procedural programming]], [[object-oriented programming]], [[functional programming]], and [[logic programming]]; some languages are hybrids of paradigms or multi-paradigmatic. An [[assembly language]] is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these).<ref>{{cite web|url=http://tunes.org/wiki/programming_20languages.html|title=TUNES: Programming Languages}}</ref> Some general purpose languages were designed largely with educational goals.<ref>{{cite journal|last=Wirth|first=Niklaus|authorlink=Niklaus Wirth|title=Recollections about the development of Pascal|journal=Proc. 2nd [[SIGPLAN|ACM SIGPLAN]] conference on history of programming languages|pages=333–342|year=1993|url=http://portal.acm.org/citation.cfm?id=155378|accessdate=30 June 2006|doi=10.1145/154766.155378|isbn=0-89791-570-4}}</ref>  A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use [[English language]] keywords, while [[Non-English-based programming languages|a minority do not]]. Other languages may be classified as being [[Esoteric programming language|deliberately esoteric]] or not.  ==History== [[File:Bangalore India Tech books for sale IMG 5261.jpg|thumb|230px|right|A selection of textbooks that teach programming, in languages both popular and obscure. These are only a few of the thousands of programming languages and dialects that have been designed in history.]]  {{Main|History of programming languages|Programming language generations}}  ===Early developments=== The first programming languages predate the modern computer. The 19th century saw the invention of "programmable" [[loom]]s and [[player piano]] scrolls, both of which implemented examples of [[domain-specific language]]s. By the beginning of the twentieth century, punch cards encoded data and directed mechanical processing. In the 1930s and 1940s, the formalisms of [[Alonzo Church]]'s [[lambda calculus]] and [[Alan Turing]]'s [[Turing machine]]s provided mathematical abstractions for expressing [[algorithm]]s; the lambda calculus remains influential in language design.<ref>Benjamin C. Pierce writes: :"... the lambda calculus has seen widespread use in the specification of programming language features, in language design and implementation, and in the study of type systems." {{cite book  | last=Pierce  | first=Benjamin C.  | authorlink=Benjamin C. Pierce  | title=Types and Programming Languages  | publisher=[[MIT Press]]  | year=2002  | isbn=0-262-16209-1  | page=52 }}</ref>  In the 1940s, the first electrically powered digital computers were created. The first [[high-level programming language]] to be designed for a computer was [[Plankalkül]], developed for the German [[Z3 (computer)|Z3]] by [[Konrad Zuse]] between 1943 and 1945. However, it was not implemented until 1998 and 2000.<ref> [[Raúl Rojas|Rojas, Raúl]], et al. (2000). "Plankalkül: The First High-Level Programming Language and its Implementation". Institut für Informatik, Freie Universität Berlin, Technical Report B-3/2000. [http://www.zib.de/zuse/Inhalt/Programme/Plankalkuel/Plankalkuel-Report/Plankalkuel-Report.htm (full text)]</ref>  Programmers of early 1950s computers, notably [[UNIVAC I]] and [[IBM 701]], used [[Machine code|machine language programs]], that is, the [[First-generation programming language|first generation language]] (1GL). 1GL programming was quickly superseded by similarly machine-specific, but [[mnemonic]], [[Second-generation programming language|second generation]] languages (2GL) known as [[assembly language]]s or "assembler". Later in the 1950s, assembly language programming, which had evolved to include the use of [[macro instruction]]s, was followed by the development of [[Third-generation programming language|"third generation" programming languages]] (3GL), such as [[Fortran|FORTRAN]], [[Lisp (programming language)|LISP]], and [[COBOL]].<ref>Linda Null, Julia Lobur, ''The essentials of computer organization and architecture'', Edition 2, Jones & Bartlett Publishers, 2006, ISBN 0-7637-3769-0, p. 435</ref> 3GLs are more abstract and are "portable", or at least implemented similarly on computers that do not support the same native machine code. Updated versions of all of these 3GLs are still in general use, and each has strongly influenced the development of later languages.<ref name="influences">{{cite web  | url=http://www.oreilly.com/news/graphics/prog_lang_poster.pdf  | format=PDF  | title=History of programming languages  | author=O'Reilly Media  | accessdate=5 October 2006  |authorlink=O'Reilly Media }}</ref> At the end of the 1950s, the language formalized as [[ALGOL|ALGOL 60]] was introduced, and most later programming languages are, in many respects, descendants of Algol.<ref name="influences"/> The format and use of the early programming languages was heavily influenced by the [[Computer programming in the punch card era|constraints of the interface]].<ref>Frank da Cruz. [http://www.columbia.edu/acis/history/cards.html IBM Punch Cards] [http://www.columbia.edu/acis/history/index.html Columbia University Computing History].</ref>  ===Refinement=== The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use, though many aspects were refinements of ideas in the very first [[Third-generation programming language]]s: * [[APL (programming language)|APL]] introduced ''[[array programming]]'' and influenced [[functional programming]].<ref>Richard L. Wexelblat: ''History of Programming Languages'', Academic Press, 1981, chapter XIV.</ref> * [[PL/I]] (NPL) was designed in the early 1960s to incorporate the best ideas from FORTRAN and COBOL. * In the 1960s, [[Simula]] was the first language designed to support ''[[object-oriented programming]]''; in the mid-1970s, [[Smalltalk]] followed with the first "purely" object-oriented language. * [[C (programming language)|C]] was developed between 1969 and 1973 as a ''[[system programming]]'' language, and remains popular.<ref>{{cite web | url=http://www.cs.berkeley.edu/~flab/languages.html | author=François Labelle | title=Programming Language Usage Graph | work=[[SourceForge]] | accessdate=21 June 2006}}. This comparison analyzes trends in number of projects hosted by a popular community programming repository. During most years of the comparison, C leads by a considerable margin; in 2006, Java overtakes C, but the combination of C/C   still leads considerably.</ref> * [[Prolog]], designed in 1972, was the first ''[[logic programming]]'' language. * In 1978, [[ML (programming language)|ML]] built a polymorphic type system on top of Lisp, pioneering ''[[Type system|statically typed]] [[functional programming]]'' languages. Each of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.  The 1960s and 1970s also saw considerable debate over the merits of ''[[structured programming]]'', and whether programming languages should be designed to support it.<ref>{{cite journal | title=The Semicolon Wars | journal=American Scientist | first1=Brian | last1=Hayes | volume=94 | issue=4 | year=2006 | pages=299–303}}</ref> [[Edsger W. Dijkstra|Edsger Dijkstra]], in a famous 1968 letter published in the [[Communications of the ACM]], argued that [[Goto|GOTO]] statements should be eliminated from all "higher level" programming languages.<ref>{{cite journal|last=Dijkstra|first=Edsger W.|authorlink=Edsger Dijkstra|title=Go To Statement Considered Harmful|journal=Communications of the ACM|volume=11|issue=3|month=March|year=1968|pages=147–148|url=http://www.acm.org/classics/oct95/|accessdate=29 June 2006|doi=10.1145/362929.362947}}{{dead link|date=June 2011}}</ref>  The 1960s and 1970s also saw expansion of techniques that reduced the footprint of a program as well as improved productivity of the programmer and user. The [[Computer programming in the punch card era|card deck]] for an early [[Fourth-generation programming language|4GL]] was a lot smaller for the same functionality expressed in a [[Third-generation programming language|3GL deck]].  ===Consolidation and growth=== The 1980s were years of relative consolidation. [[C  ]] combined object-oriented and systems programming. The United States government standardized [[Ada (programming language)|Ada]], a systems programming language derived from [[Pascal (programming language)|Pascal]] and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called [[Fifth-generation programming language|"fifth generation" languages]] that incorporated logic programming constructs.<ref> Tetsuro Fujise, Takashi Chikayama Kazuaki Rokusawa, Akihiko Nakase (December 1994). "KLIC: A Portable Implementation of KL1" ''Proc. of FGCS '94, ICOT'' Tokyo, December 1994. [http://www.icot.or.jp/ARCHIVE/HomePage-E.html KLIC is a portable implementation of a concurrent logic programming language [[KL1]].]</ref> The functional languages community moved to standardize [[ML (programming language)|ML]] and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decade.  One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of ''modules'', or large-scale organizational units of code. [[Modula-2]], Ada, and ML all developed notable module systems in the 1980s, although other languages, such as [[PL/I]], already had extensive support for modular programming. Module systems were often wedded to [[generic programming]] constructs.<ref>{{cite web|author=Jim Bender|url=http://readscheme.org/modules/|title=Mini-Bibliography on Modules for Functional Programming Languages|work=ReadScheme.org|accessdate=27 September 2006|date=15 March 2004}}</ref>  The rapid growth of the [[Internet]] in the mid-1990s created opportunities for new languages. [[Perl]], originally a Unix scripting tool first released in 1987, became common in dynamic [[website]]s. [[Java (programming language)|Java]] came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of "[[Write once, run anywhere]]" ([[UCSD Pascal]] had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements to existing languages and paradigms, and largely based on the C family of programming languages.  Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity ([[mixin]]s, [[Delegation (programming)|delegates]], [[aspect-oriented programming|aspects]]), and database integration such as Microsoft's [[Language Integrated Query|LINQ]].  The [[Fourth-generation programming language|4GLs]] are examples of languages which are domain-specific, such as [[SQL]], which manipulates and returns [[set (computer science)|sets]] of data rather than the scalar values which are canonical to most programming languages. [[Perl]], for example, with its '[[here document]]' can hold multiple 4GL programs, as well as multiple JavaScript programs, in part of its own perl code and use variable interpolation in the 'here document' to support multi-language programming.<ref>Wall, ''Programming Perl'' ISBN 0-596-00027-8 p.66</ref>  ==See also== {{portal|Computer science|Computer programming}} {{Wikipedia books|Programming Languages}} * [[Comparison of programming languages (basic instructions)]] * [[Comparison of programming languages]] * [[Computer programming]] * [[Computer science]] and [[Outline of computer science]] * [[Educational programming language]] * [[Invariant based programming]] * [[Lists of programming languages]] * [[List of programming language researchers]] * [[Literate programming]] * [[Dialect (computing)]] * [[Programming language theory]] * [[Pseudocode]] * [[Scientific language]] * [[Software engineering]] and [[List of software engineering topics]] {{Clear}}  ==References== {{Reflist|30em}}  ==Further reading== {{see also|History of programming languages#Further reading}} {{refbegin|2}} * {{cite book|last1=Abelson|first1=Harold |authorlink1=Harold Abelson|last2=Sussman|first2=Gerald Jay |authorlink2=Gerald Jay Sussman|title=[[Structure and Interpretation of Computer Programs]]|url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-4.html|edition=2nd|year=1996|publisher=MIT Press}} * [[Raphael Finkel]]: ''[http://www.nondot.org/sabre/Mirrored/AdvProgLangDesign/ Advanced Programming Language Design]'', Addison Wesley 1995. * [[Daniel P. Friedman]], [[Mitchell Wand]], [[Christopher T. Haynes]]: ''[[Essentials of Programming Languages]]'', The MIT Press 2001. * Maurizio Gabbrielli and Simone Martini: "Programming Languages: Principles and Paradigms", Springer, 2010. * [[David Gelernter]], [[Suresh Jagannathan]]: ''Programming Linguistics'', [[The MIT Press]] 1990. * [[Ellis Horowitz]] (ed.): ''Programming Languages, a Grand Tour'' (3rd ed.), 1987. * Ellis Horowitz: ''Fundamentals of Programming Languages'', 1989. * [[Shriram Krishnamurthi]]: ''[[Programming Languages: Application and Interpretation]]'', [http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/ online publication]. * [[Bruce J. MacLennan]]: ''Principles of Programming Languages: Design, Evaluation, and Implementation'', [[Oxford University Press]] 1999. * [[John C. Mitchell]]: ''Concepts in Programming Languages'', [[Cambridge University Press]] 2002. * [[Benjamin C. Pierce]]: ''[[Types and Programming Languages]]'', The MIT Press 2002. * [[Terrence W. Pratt]] and [[Marvin V. Zelkowitz]]: ''Programming Languages: Design and Implementation'' (4th ed.), Prentice Hall 2000. * [[Peter H. Salus]]. ''Handbook of Programming Languages'' (4 vols.). Macmillan 1998. * [[Ravi Sethi]]: ''Programming Languages: Concepts and Constructs'', 2nd ed., [[Addison-Wesley]] 1996. * [[Michael L. Scott]]: ''Programming Language Pragmatics'', [[Morgan Kaufmann Publishers]] 2005. * [[Robert W. Sebesta]]: ''Concepts of Programming Languages'', 9th ed., Addison Wesley 2009. * [[Franklyn Turbak]] and [[David Gifford]] with [[Mark Sheldon]]: ''Design Concepts in Programming Languages'', The MIT Press 2009. * [[Peter Van Roy]] and [[Seif Haridi]]. ''[[Concepts, Techniques, and Models of Computer Programming]]'', The MIT Press 2004. * [[David A. Watt]]. ''Programming Language Concepts and Paradigms''. Prentice Hall 1990. * David A. Watt and [[Muffy Thomas]]. ''Programming Language Syntax and Semantics''. Prentice Hall 1991. * David A. Watt. ''Programming Language Processors''. Prentice Hall 1993. * David A. Watt. ''Programming Language Design Concepts''. John Wiley & Sons 2004. {{refend}}  ==External links== {{Sister project links|wikt=programming language|commons=Category:Programming languages|v=Programming languages|q=Programming languages|s=no|b=Subject:Computer programming languages}} * [http://www.99-bottles-of-beer.net/ 99 Bottles of Beer] A collection of implementations in many languages. * {{dmoz|Computers/Programming/Languages|Computer Programming Languages}}  {{Programming language}} {{Computer language}}  {{DEFAULTSORT:Programming Language}} [[Category:Programming language topics| ]] [[Category:Notation]]  {{Link FA|he}}  [[af:Programmeertaal]] [[als:Programmiersprache]] [[am:የፕሮግራም ቋንቋ]] [[ar:لغة برمجة]] [[an:Luengache de programación]] [[ast:Llinguaxe de programación]] [[az:Proqramlaşdırma dilləri]] [[bn:প্রোগ্রামিং ভাষা]] [[zh-min-nan:Thêng-sek gí-giân]] [[be:Мова праграмавання]] [[be-x-old:Мова праграмаваньня]] [[bg:Език за програмиране]] [[bs:Programski jezik]] [[br:Yezh programmiñ]] [[ca:Llenguatge de programació]] [[cv:Компьютер чĕлхи]] [[cs:Programovací jazyk]] [[cy:Iaith rhaglennu]] [[da:Programmeringssprog]] [[de:Programmiersprache]] [[et:Programmeerimiskeel]] [[el:Γλώσσα προγραμματισμού]] [[es:Lenguaje de programación]] [[eo:Programlingvo]] [[eu:Programazio-lengoaia]] [[fa:زبان‌های برنامه‌نویسی]] [[fr:Langage de programmation]] [[gl:Linguaxe de programación]] [[ko:프로그래밍 언어]] [[hy:Ծրագրավորման լեզու]] [[hi:प्रोग्रामिंग भाषा]] [[hsb:Programěrowanske rěče]] [[hr:Programski jezik]] [[io:Programifo-lingui]] [[ilo:Lengguahe ti panangprograma]] [[id:Bahasa pemrograman]] [[ia:Linguage de programmation]] [[is:Forritunarmál]] [[it:Linguaggio di programmazione]] [[he:שפת תכנות]] [[ka:პროგრამირების ენა]] [[kk:Бағдарламалау тілі]] [[la:Lingua programmandi]] [[lv:Programmēšanas valoda]] [[lb:Programméiersprooch]] [[lt:Programavimo kalba]] [[jbo:samplabau]] [[hu:Programozási nyelv]] [[mk:Програмски јазик]] [[ml:പ്രോഗ്രാമിംഗ് ഭാഷ]] [[mr:प्रोग्रॅमिंग भाषा]] [[arz:لغة برمجه]] [[ms:Bahasa pengaturcaraan]] [[mn:Програмчлалын хэл]] [[nl:Programmeertaal]] [[ne:कम्प्युटर भाषा]] [[ja:プログラミング言語]] [[no:Programmeringsspråk]] [[nn:Programmeringsspråk]] [[oc:Lengatge de programacion]] [[mhr:Программлымаш йылме]] [[pnb:کمپیوٹر بولی]] [[pl:Język programowania]] [[pt:Linguagem de programação]] [[ro:Limbaj de programare]] [[rue:Язык проґрамованя]] [[ru:Язык программирования]] [[sah:Программалааhын тыла]] [[sq:Gjuhë programimi]] [[si:ක්‍රමලේඛන භාෂාව]] [[simple:Programming language]] [[sk:Programovací jazyk]] [[sl:Programski jezik]] [[ckb:زمانی بەرنامەسازی]] [[sr:Програмски језик]] [[sh:Programski jezik]] [[su:Basa program]] [[fi:Ohjelmointikieli]] [[sv:Programspråk]] [[tl:Wikang pamprograma]] [[ta:நிரல் மொழி]] [[kab:Timeslayin n usihel]] [[tt:Программалау теле]] [[te:ప్రోగ్రామింగు భాష]] [[th:ภาษาโปรแกรม]] [[tg:Забони барномасозӣ]] [[tr:Programlama dili]] [[bug:ᨅᨔ ᨀᨚᨇᨘᨈᨛᨑᨛ]] [[uk:Мова програмування]] [[ur:برمجہ زبان]] [[vi:Ngôn ngữ lập trình]] [[war:Pinulongan hin programa]] [[yi:פראגראמירן שפראך]] [[yo:Èdè Ìṣèlànà Kọ̀mpútà]] [[zh-yue:程式語言]] [[bat-smg:Pruogramavėma kalba]] [[zh:编程语言]]
{{refimprove|date=December 2009}}  In [[Integrated_circuit_design#Digital_design|digital circuit design]], '''register-transfer level''' ('''RTL''') is a design abstraction which models a [[synchronous circuit|synchronous]] [[digital circuit]] in terms of the flow of digital signals (data) between [[hardware register]]s, and the [[Boolean logic|logical operations]] performed on those signals.   Register-transfer-level abstraction is used in [[hardware description language]]s (HDLs) like [[Verilog]] and [[VHDL]] to create high-level representations of a circuit, from which lower-level representations and ultimately actual wiring can be derived.  Design at the RTL level is typical practice in modern digital design.<ref> {{cite book  | title = Digital Design with RTL Design, Verilog and VHDL  | edition = 2nd  | author = Frank Vahid  | publisher = John Wiley and Sons  | year = 2010  | isbn = 978-0-470-53108-2  | page = 247  | url = http://books.google.com/books?id=-YayRpmjc20C&pg=PA247  }}</ref>  ==RTL description==  [[File:Register transfer level - example toggler.svg|right|thumb|300px |Example of a simple circuit with a toggling output. The inverter forms the combinational logic in this circuit, and the register holds the state.]]  A synchronous circuit consists of two kinds of elements: registers and [[combinational logic]]. Registers (usually implemented as D [[Flip-flop (electronics)|flip-flop]]s) synchronize the circuit's operation to the edges of the clock signal, and are the only elements in the circuit that have memory properties. Combinational logic performs all the logical functions in the circuit and it typically consists of [[logic gate]]s.  For example, a very simple synchronous circuit is shown in the figure. The [[Inverter (logic gate)|inverter]] is connected from the output of a register to the register's input, to create a circuit that changes its state on each rising edge of the clock. In this circuit, the combinational logic consists of the inverter.  When designing digital integrated circuits with a [[hardware description language]], the designs are usually engineered at a higher level of abstraction than transistor level ([[logic family|logic families]]) or logic gate level.  In HDLs the designer declares the registers (which roughly correspond to variables in computer programming languages), and describes the combination logic by using constructs that are familiar from programming languages such as if-then-else and arithmetic operations. This level is called ''register-transfer level''. The term refers to the fact that RTL focuses on describing the flow of signals between registers.  As an example, the circuit mentioned above can be described in VHDL as follows:  <source lang="vhdl"> D <= not Q;   process(clk) begin     if rising_edge(clk) then         Q <= D;     end if; end process; </source>  Using an [[Electronic design automation|EDA]] tool for synthesis, this description can usually be directly translated to an equivalent hardware implementation file for an [[Application-specific integrated circuit|ASIC]] or an [[FPGA]]. The [[logic synthesis|synthesis]] tool also performs logic optimization.  At the register-transfer level, some types of circuits can be recognized. If there is a cyclic path of logic from a register's output to its input (or from a set of registers outputs to its inputs), the circuit is called a [[finite state machine|state machine]] or can be said to be [[sequential logic]]. If there are logic paths from a register to another without a cycle, it is called a [[pipeline (computing)|pipeline]].  ==RTL in the circuit design cycle==  RTL is used in the [[Digital Logic|logic design]] phase of the [[integrated circuit design]] cycle.  An RTL description is usually converted to a [[netlist|gate-level description]] of the circuit by a [[logic synthesis]] [[Software tool|tool]]. The synthesis results are then used by [[Placement (EDA)|placement]] and [[Routing (EDA)|routing]] tools to create a physical [[integrated circuit|layout]].  [[Logic simulation]] tools may use a design's RTL description to verify its correctness.  ==See also==  *[[Electronic design automation]] *[[Electronic system level]] *[[Integrated circuit design]] *[[Synchronous circuit]] *[[Algorithmic State Machine]]  ==References==  {{reflist}}  {{DEFAULTSORT:Register Transfer Level}} [[Category:Electronic design automation]]  [[de:Registertransferebene]] [[fr:Register Transfer Level]] [[it:Register transfer level]] [[ja:レジスタ転送レベル]] [[pt:Register transfer level]] [[ru:Уровень регистровых передач]] [[zh:寄存器传输级]]
{{Multiple issues|no footnotes = October 2011|external links = March 2012|refimprove = October 2011|copy edit = October 2011}}  '''Reliability engineering''' is an [[engineering]] field that deals with the study, evaluation, and [[Product lifecycle management|life-cycle management]] of [[wikt:reliability|reliability]]: the ability of a [[system]] or component to perform its required functions under stated conditions for a specified period of time.<ref>Institute of Electrical and Electronics Engineers (1990) IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries. New York, NY ISBN 1-55937-079-3</ref> Reliability engineering is a sub-discipline within [[systems engineering]]. Reliability is often measured as [[probability]] of failure, frequency of failures, or in terms of [[availability]], a probability derived from reliability and maintainability. [[Maintainability]] and [[maintenance]] are often important parts of reliability engineering.   Reliability engineering is closely related to [[safety engineering]], in that they use common methods for their analysis and may require input from each other. Reliability engineering focuses on costs of failure caused by system downtime, cost of spares, repair equipment, personnel and cost of warranty claims. The focus of safety engineering is normally not on cost, but on preserving life and nature, and therefore deals only with particular dangerous system failure modes.  Reliability engineering for complex systems requires a different, more elaborate systems approach than reliability for non-complex systems. Reliability analysis has important links with function analysis, requirements specification, systems design, hardware design, software design, [[manufacturing]], testing, maintenance, transport, storage, spare parts, [[operations research]], human factors, technical documentation, training and more. Effective reliability engineering requires experience, broad engineering skills and knowledge from many different fields of engineering.  == Overview == [[File:Reliability block diagram.png|300px|thumb|right|A reliability block diagram showing a 1oo3 (1 out of 3)redundant designed subsystem]] '''Reliability''' may be defined in several ways. * The idea that something is fit for a purpose with respect to time. * The capacity of a device or system to perform as designed. * The resistance to failure of a device or system. * The ability of a device or system to perform a required function under stated conditions for a specified period of [[time]]. * The probability that a [[functional unit]] will perform its required function for a specified interval under stated conditions. * The ability of something to [[Failing badly|fail well]].  Reliability engineering is a special discipline within [[Systems engineering]]. Reliability engineers rely heavily on [[statistics]], [[probability theory]], and [[reliability theory]] to set requirements, measure or predict reliability and give advice on improvements for reliability performance. Many engineering techniques are used in reliability engineering, such as Reliability [[Hazard analysis]], [[Failure mode and effects analysis]] (FMEA), [[Fault tree analysis]], Reliability Prediction, [[Weibull distribution]] analysis, thermal management, reliability testing and [[accelerated life testing]]. Because of the large number of reliability techniques, their expense, and the varying degrees of reliability required for different situations, most projects develop a [[reliability program plan]] to specify the reliability tasks that will be performed for that specific system.  The function of reliability engineering is to: * develop the reliability requirements for the product  * establish an adequate life-cycle reliability program  * show that corrective measures (reliability risk mitigation) produce reliability improvements  * perform appropriate analyses and tasks to ensure that  ** the product will meet its requirements  ** the unreliability risk is controlled and brought to an acceptable level.   In line with the creation of safety cases for safety, it needs to provide a robust set of (statistical) evidence and justification material to verify if the top reliability or availability requirements can or have been met. The goal is to first identify the reliability hazards, assess the risk associated with them and to control the risk to an acceptable level. What is acceptable is determined by the managing authority or customers. These tasks are normally managed by a reliability engineer or manager, who may hold an [[school accreditation|accredited]] engineering degree and has additional reliability-specific education and training.  Reliability engineering is closely associated with maintainability engineering and [[logistic engineering|logistics engineering]], e.g. [[Integrated Logistics Support]] (ILS). Many problems from other fields, such as [[security engineering]] and [[safety engineering]], can also be approached using common reliability engineering techniques.   Many types of [[engineering]] employ reliability engineers and use the tools and methodology of reliability engineering. Reliability [[engineering]] is performed throughout the entire [[new product development|life cycle]] of a system, including development, test, production and operation.  == Reliability theory == {{main|Reliability theory|Failure rate}}  Reliability theory is the foundation of reliability engineering. For engineering purposes, reliability is defined as the [[probability]] that a device will perform its intended function during a specified period of time under stated conditions. Mathematically, this may be expressed as,  :<math>R(t)=Pr\{T>t\}=\int_{t}^{\infty} f(x)\, dx \ \!</math>,  :where <math>f(x) \!</math> is the failure [[probability density function]] and <math>t</math> is the length of the period of time (which is assumed to start from time zero).  Reliability engineering is concerned with four key elements of this definition:  # Reliability is a probability. This means that failure is regarded as a [[random]] phenomenon: it is a recurring event, and we do not express any information on individual failures, the causes of failures, or relationships between failures, except that the likelihood for failures to occur varies over time according to the given probability function.  Reliability engineering is concerned with meeting the specified probability of success, at a specified statistical [[confidence interval|confidence level]]. # Reliability is predicated on "intended function:" Generally, this is taken to mean operation without [[failure]]. However, even if no individual part of the system fails, but the system as a whole does not do what was intended, then it is still charged against the system reliability. The system requirements specification is the criterion against which reliability is measured. # Reliability applies to a specified period of time. In practical terms, this means that a system has a specified chance that it will operate without failure before time <math>t \!</math>. Reliability engineering ensures that components and materials will meet the requirements during the specified time.  Units other than time may sometimes be used. The automotive industry might specify reliability in terms of miles, the military might specify reliability of a gun for a certain number of rounds fired.  A piece of mechanical equipment may have a reliability rating value in terms of cycles of use. # Reliability is restricted to operation under stated (or explicitly defined) conditions. This constraint is necessary because it is impossible to design a system for unlimited conditions. A [[Mars Rover]] will have different specified conditions than a family car. The operating environment must be addressed during design and testing.  Also, that same rover may be required to operate in varying conditions requiring additional scrutiny.  == Reliability program plan == Many tasks, methods and tools can be used to achieve reliability. Every system requires a different level of reliability. A commercial [[airliner]] must operate under a wide range of conditions. The consequences of failure are grave, but there is a correspondingly higher budget. A pencil sharpener may be more reliable than an airliner, but has a much different set of operational conditions, insignificant consequences of failure and a much lower budget.  A [[reliability program plan]] (RPP) is used to document exactly what "best practices" (tasks, methods, tools, analyses and tests) are required for a particular (sub)system, as well as clarify customer requirements for reliability assessment. For large scale, complex systems, the Reliability Program Plan is a distinctive [[document]].   For simple systems, it may be combined with the [[systems engineering]] management plan or an [[integrated logistics support]] management plan. A reliability program plan is essential for a successful reliability, [[availability]] and [[maintainability]] (RAM) program and is developed early during system development and refined over the systems life-cycle. It specifies not only what the reliability engineer does, but also the tasks performed by other [[Stakeholder (corporate)|stakeholders]]. A reliability program plan is approved by top program management, which is responsible for identifying resources for its implementation.  Technically, often, the main objective of a Reliability Program Plan is to evaluate and improve [[availability]] of a system and not reliability. Reliability needs to be evaluated and improved related to both availability '''and''' the cost of ownership (due to cost of spar parts, maintenance man-hours, transport costs etc.). Often a trade-off is needed between the two. There might be a maximum ratio between availability and cost of ownership.   Whether availability or Cost of Ownership is more important depends on the use of the system. For example, a system that is a critical link in a production system – for example a big oil platform – is normally allowed to have a very high cost of ownership if this translates to even a minor increase in availability, as the unavailability of the platform directly results in a massive loss of revenue which can easily exceed the basic cost of ownership.   Testability of a system should also be addressed in the plan as this is the link between reliability and maintainability. The maintenance (the maintenance concept / strategy) can influence the reliability of a system (e.g. by preventive maintenance), although it can never bring it above the inherent reliability. Maintainability influences the availability of a system – in theory this can be almost unlimited if one would be able to repair a fault in a very short time.  A proper reliability plan should normally always address RAMT analysis in its total context. RAMT stands in this case for Reliability, Availability, Maintainability/Maintenance and Testability in context to users needs with regard to the technical [[requirements]] (as translated from the needs).  === Reliability requirements === For any system, one of the first tasks of reliability engineering is to adequately specify the reliability and maintainability requirements as defined by the stakeholders in terms of their overall [[availability]] needs. Reliability requirements address the system itself, including test and assessment requirements, and associated tasks and documentation. Reliability requirements are included in the appropriate system/subsystem requirements specifications, test plans and contract statements.  Maintainability requirements address system issue of costs as well as repair time.  Evaluation of the effectiveness of corrective measures is part of a [[FRACAS]] process that is usually part of a good RPP.  ===Reliability prediction and improvement=== Reliability prediction is the combination of the creation of a proper reliability model together with estimating (and justifying) the input parameters for this model (like failure rates for a particular failure mode or event and the mean time to repair the system for a particular failure) and finally to provide a system (or part) level estimate for the output reliability parameters (system availability or a particular functional failure frequency).  Some recognized authors on reliability – e.g. Patrick O'Conner, R. Barnard and others – have argued that too much emphasis is often given to the prediction of reliability parameters and more effort should be devoted to prevention of failure (reliability improvement). The reason for this is that prediction of reliability based on historic data can be very misleading, as a comparison is only valid for exactly the same designs, products under exactly the same loads / context. Even a minor change in detail in any of these could have major effects on reliability. Furthermore, normally the most unreliable and important items (most interesting candidates for a reliability investigation) are most often subjected to many modifications and changes. Also, to perform a proper quantitative reliability prediction for systems is extremely difficult and expensive if done by testing. On part level, results can be obtained often with higher confidence as many samples might be used for the available testing financial budget, however unfortunately these tests might lack validity on system level due to the assumptions that had to be made for part level testing.   Testing for reliability should be done to create failures, learn from them and to improve the system / part. The general conclusion is drawn that an accurate and an absolute prediction – by field data comparison or testing – of reliability is in most cases not possible. A exception might be failures due to wear-out problems like fatigue failures. In the introduction of Mil. Std. 785 it is written that reliability prediction should be used with great caution if not only used for comparison in trade-off studies.  Furthermore, based on the latest insights in [[Reliability centered maintenance]] (RCM), most (complex) system failures do no occur due to wear-out issues (e.g. a number of 4% has been provided, refer to RCM page). The failures are often a result of combinations of more and multi-type events or failures. The results of these studies have shown that the majority of failures follow a constant failure rate model, for which prediction of the value of the parameters is often problematic and very time consuming (for a high level reliability – part level). Testing these constant failure rates at system level, by for example mil. handbook 781 type of testing{{clarify|date=October 2011}}, is not practical and can be extremely misleading.  Despite all the concerns, there will always be a need for the prediction of reliability.{{says who|date=October 2011}} These numbers can be used as a [[Key performance indicator]] (KPI) or to estimate the need for spares, man-power, availability of systems etc.  Reliability predictions: * help assess the effect of product reliability on maintenance activity and the quantity of spare units required for acceptable field performance of any particular system. For example, predictions of the frequency of unit level maintenance actions can be obtained. Reliability prediction can be used to size spare populations. * provide necessary input to system-level reliability models. System-level reliability models can subsequently be used to predict, for example, frequency of system outages in steady-state, frequency of system outages during early life, expected downtime per year, and system availability. * provide necessary input to unit and system-level Life Cycle Cost Analyses. Life cycle cost studies determine the cost of a product over its entire life. Therefore, how often a unit will have to be replaced needs to be known. Inputs to this process include unit and system failure rates. This includes how often units and systems fail during the first year of operation as well as in later years. * assist in deciding which product to purchase from a list of competing products. As a result, it is essential that reliability predictions be based upon a common procedure. * can be used to set factory test standards for products requiring a reliability test. Reliability predictions help determine how often the system should fail. * are needed as input to the analysis of complex systems such as switching systems and digital cross-connect systems.  It is necessary to know how often different parts of the system are going to fail even for redundant components. * can be used in design trade-off studies. For example, a supplier could look at a design with many simple devices and compare it to a design with fewer devices that are newer but more complex. The unit with fewer devices is usually more reliable. * can be used to set achievable in-service performance standards against which to judge actual performance and stimulate action.  The telecommunications industry has devoted much time over the years to concentrate on developing reliability models for electronic equipment. One such tool is the Automated Reliability Prediction Procedure (ARPP), which is an Excel-spreadsheet software tool that automates the reliability prediction procedures in "SR-332, Reliability Prediction Procedure for Electronic Equipment"<ref>[http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=SR-332& SR-332, Reliability Prediction Procedure for Electronic Equipment.], [http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=FD-ARPP-01& FD-ARPP-01] telecom-info.telcordia.com</ref> provides suppliers and manufacturers with a tool for making Reliability Prediction Procedure (RPP) calculations. It also provides a means for understanding RPP calculations through the capability of interactive examples provided by the user.  The RPP views electronic systems as hierarchical assemblies. Systems are constructed from units that, in turn, are constructed from devices. The methods presented predict reliability at these three hierarchical levels: # ''Device'': A basic component (or part) # ''Unit'': Any assembly of devices. This may include, but is not limited to, circuit packs, modules, plug-in units, racks, power supplies and ancillary equipment. Unless otherwise dictated by maintenance considerations, a unit will usually be the lowest level of replaceable assemblies/devices. The RPP is aimed primarily at reliability prediction of units. # ''Serial System'': Any assembly of units for which the failure of any single unit will cause a failure of the system or overall mission.  === System reliability parameters === Requirements are specified using reliability [[parameter]]s. The most common reliability parameter is the [[mean time to failure]] (MTTF), which can also be specified as the [[failure rate]] (this is expressed as a frequency or Conditional Probability Density Function (PDF)) or the number of failures during a given period. These parameters are very useful for systems that are operated frequently, such as most [[vehicle]]s, machinery, and [[electronics|electronic]] equipment. Reliability increases as the MTTF increases. The MTTF is usually specified in hours, but can also be used with other units of measurement, such as miles or cycles.     In other cases, reliability is specified as the probability of mission success. For example, reliability of a scheduled aircraft flight can be specified as a dimensionless probability or a percentage, as in [[system safety]] engineering.  A special case of mission success is the single-shot device or system. These are devices or systems that remain relatively dormant and only operate once. Examples include automobile [[airbags]], thermal [[battery (electricity)|batteries]] and [[missiles]]. Single-shot reliability is specified as a probability of one-time success, or is subsumed into a related parameter. Single-shot missile reliability may be specified as a [[requirement]] for the probability of a hit. For such systems, [[Safety Integrity Level|the probability of failure on demand (PFD)]] is the reliability [[Measurement|measure]] – which actually is a unavailability number. This PFD is derived from failure rate (a frequency of occurrence) and mission time for non-repairable systems.   For repairable systems, it is obtained from failure rate and mean-time-to-repair (MTTR) and test interval. This measure may not be unique for a given system as this measure depends on the kind of demand. In addition to system level requirements, reliability requirements may be specified for critical subsystems. In most cases, reliability parameters are specified with appropriate statistical [[confidence interval]]s.  ===Reliability modelling=== Reliability modelling is the process of predicting or understanding the reliability of a component or system prior to its implementation. Two types of analysis that are often used to model a system reliability behavior are Fault Tree Analysis and Reliability Block diagrams. On component level the same analysis can be used together with others. The input for the models can come from many sources: Testing, Earlier operational experience field data or Data Handbooks from the same or mixed industries can be used. In all cases, the data must be used with great caution as predictions are only valid in case the same product in the same context is used. Often predictions are only made to compare alternatives.  For part level predictions, two separate fields of investigation are common: * The [[physics of failure]] approach uses an understanding of physical failure mechanisms involved, such as mechanical [[crack propagation]] or chemical [[corrosion]] degradation or failure; * The [[parts stress modelling]] approach is an empirical method for prediction based on counting the number and type of components of the system, and the stress they undergo during operation. [[Software reliability]] is a more challenging area that must be considered when it is a considerable component to system functionality.  For systems with a clearly defined failure time (which is sometimes not given for systems with a drifting parameter), the [[empirical distribution function]] of these failure times can be determined. This is done in general in an experiment with increased (or accelerated) stress. These experiments can be divided into two main categories:  * Early failure rate studies determine the distribution with a decreasing failure rate over the first part of the [[bathtub curve]].   (The bathtub curve only holds for hardware failures, not software.) Here in general only moderate stress is necessary. The stress is applied for a limited period of time in what is called a censored test. Therefore, only the part of the distribution with early failures can be determined.  * In so-called zero defect experiments, only limited information about the failure distribution is acquired. Here the stress, stress time or the sample size is so low that not a single failure occurs. Due to the insufficient sample size, only an upper limit of the early failure rate can be determined. At any rate, it looks good for the customer if there are no failures.  In a study of the intrinsic failure distribution, which is often a material property, higher (material) stresses are necessary to get failure in a reasonable period of time. Several degrees of stress have to be applied to determine an acceleration model. The empirical failure distribution is often parametrized with a [[Weibull distribution|Weibull]] or a [[Log-normal distribution|log-normal]] model.  It is a general [[praxis (process)|praxis]] to model the early (hardware) failure rate with an exponential distribution. This less complex model for the failure distribution has only one parameter: the constant failure rate. In such cases, the [[Chi-squared distribution]] can be used to find the [[goodness of fit]] for the estimated failure rate. Compared to a model with a decreasing failure rate, this is quite pessimistic (important remark: this is not the case if less hours / load cycles are tested than service life in a wear-out type of test, in this case the opposite is true and assuming a more constant failure rate than there is in reality can be dangerous). Sensitivity analysis should be conducted in this case.  === Reliability test requirements === Reliability test requirements can follow from any analysis for which the first estimate of failure probability, failure mode or effect needs to be justified. Evidence can be generated with some level of confidence by testing. With software-based systems, the probability is a mix of software and hardware-based failures. Testing reliability requirements is problematic for several reasons. A single test is in most cases insufficient to generate enough statistical data. Multiple tests or long-duration tests are usually very expensive. Some tests are simply impractical, and environmental conditions can be hard to predict over a systems life-cycle.    Reliability engineering is used to design a realistic and affordable test program that provides empirical evidence that the system meets its reliability requirements. Statistical [[confidence interval|confidence levels]] are used to address some of these concerns. A certain parameter is expressed along with a corresponding confidence level: for example, an [[MTBF]] of 1000 hours at 90% confidence level. From this specification, the reliability engineer can, for example, design a test with explicit criteria for the number of hours and number of failures until the requirement is met or failed. Other type tests are also possible, including at 95% and 99%.  The combination of reliability parameter value and confidence level greatly affects the development cost and the risk to both the customer and producer. Care is needed to select the best combination of requirements – e.g. cost-effectiveness. Reliability testing may be performed at various levels, such as [[wikt:component|component]], [[subsystem]] and [[system]]. Also, many factors must be addressed during testing and operation, such as extreme temperature and humidity, shock, vibration, or other environmental factors (like loss of signal, cooling or power; or other catastrophes such as fire, floods, excessive heat, physical or security violations or other myriad forms of damage or degradation).    Reliability engineering must assess the root cause of failures and devise corrective actions. Reliability engineering determines an effective [[test strategy]] so that all parts are exercised in relevant environments in order to assure the best possible reliability under understood conditions. For systems that must last many years, reliability engineering may be used to design accelerated life tests.  === Requirements for reliability tasks === Reliability engineering must also address requirements for various reliability tasks and documentation during system development, test, production, and operation. These requirements are generally specified in the contract statement of work and depend on how much leeway the customer wishes to provide to the contractor. Reliability tasks include various analyses, planning, and failure reporting. Task selection depends on the criticality of the system as well as cost. A critical system may require a formal failure reporting and review process throughout development, whereas a non-critical system may rely on final test reports. The most common reliability program tasks are documented in reliability program standards, such as MIL-STD-785 and IEEE 1332. Failure reporting analysis and corrective action systems are a common approach for product/process reliability monitoring.  == Design for reliability == Design For Reliability (DFR), is an emerging discipline that refers to the process of designing reliability into designs.  This process encompasses several tools and practices and describes the order of their deployment that an organization needs to have in place to drive reliability and improve [[maintainability]] in products, towards a objective of improved [[availability]], lower sustainment costs, and maximum product utilization or lifetime. Typically, the first step in the DFR process is to establish the system’s availability requirements.  Reliability must be "designed in" to the system. During system [[design]], the top-level reliability requirements are then allocated to subsystems by design engineers, maintainers, and reliability engineers working together.  Reliability design begins with the development of a (system) [[mathematical model|model]]. Reliability models use '''block diagrams''' and '''fault trees''' to provide a graphical means of evaluating the relationships between different parts of the system. These models ''may'' incorporate predictions based on failure rates taken from historical data. While the (input data) predictions are often not accurate in an absolute sense, they are valuable to assess relative differences in design alternatives.  [[File:Fault tree.png|thumb|280px|A Fault Tree Diagram]]  One of the most important design techniques is '''[[redundancy (engineering)|redundancy]]'''. This means that if one part of the system fails, there is an alternate success path, such as a backup system. The reason why this is the ultimate design choice is related to the fact that to provide absolute high confidence reliability evidence for new parts / items is often not possible or extremely expensive. By creating redundancy, together with a high level of failure monitoring and the avoidance of common cause failures, even a system with relative bad single channel (part) reliability, can be made highly reliable (mission reliability) on system level. No testing of reliability has to be required for this. Furthermore, by using redundancy and the use of dissimilar design and manufacturing processes (different suppliers) for the single independent channels, less sensitivity for quality issues (early childhood failures) is created and very high levels of reliability can be achieved at all moments of the development cycles (early life times and long term).  An automobile brake light might use two light bulbs. If one bulb fails, the brake light still operates using the other bulb. Redundancy significantly increases system reliability, and is often the only viable means of doing so. However, building redundancy on higher (complex) system level may be very difficult and expensive, and is therefore limited to critical parts of the system (e.g. multi engine aircraft). On lower levels, redundancy is often rather simple and straight forward (e.g. use of redundant bolt connections).  Another design technique, '''physics of failure''', relies on understanding the physical processes of stress, strength and failure at a very detailed level. Then the material or component can be re-designed to reduce the probability of failure. Another common design technique is '''component [[derating]]''': Selecting components whose tolerance significantly exceeds the expected stress, as using a heavier gauge wire that exceeds the normal specification for the expected [[electrical current]]. Another effective way to deal with unreliability issues is to perform analysis to be able to predict degradation and being able to prevent unscheduled down events / failures from occurring. [[Reliability centered maintenance|RCM]] (Reliability Centered Maintenance) programs can be used for this.  Many tasks, techniques and analyses are specific to particular industries and applications. Commonly these include:  :* Built-in test (BIT) (Testability analysis) :* [[Failure mode and effects analysis]] (FMEA) :* Reliability simulation modeling :* Reliability [[Hazard analysis]] :* Reliability Block Diagram analysis :* [[Fault tree analysis]] :* [[Root cause analysis]] :* Sneak circuit analysis :* Accelerated Testing :* Reliability Growth analysis :* [[Weibull distribution|Weibull]] analysis :* [[Thermal analysis]] by Finite Element Analysis (FEA) and / or Measurement :* Thermal induced, shock and vibration fatigue analysis by FEA and / or Measurement :* Electromagnetic analysis :* [[Statistical interference]] :* Avoid [[Single Point of Failure]] :* Functional Analysis (like Function FMEA) :* Predictive and preventive maintenance: Reliability Centered Maintenance (RCM) analysis :* Testability analysis :* Failure diagnostics analysis (normally also incorporated in FMEA) :* Human error analysis :* Operational Hazard analysis / :* Manual screening :* [[Integrated Logistics Support]]  Results are presented during the system design reviews and logistics reviews. Reliability is just one requirement among many system requirements. Engineering trade studies are used to determine the [[Optimization (mathematics)|optimum]] balance between reliability and other requirements and constraints.  == Reliability testing == [[File:Reliability sequential test plan.png|thumb|right|300px|A Reliability Sequential Test Plan]] The purpose of reliability testing is to discover potential problems with the design as early as possible and, ultimately, provide confidence that the system meets its reliability requirements.  Reliability testing may be performed at several levels. Complex systems may be tested at component, circuit board, unit, assembly, subsystem and system levels. (The test level nomenclature varies among applications.) For example, performing environmental stress screening tests at lower levels, such as piece parts or small assemblies, catches problems before they cause failures at higher levels. Testing proceeds during each level of integration through full-up system testing, developmental testing, and operational testing, thereby reducing program risk. System reliability is calculated at each test level. Reliability growth techniques and failure reporting, analysis and corrective active systems (FRACAS) are often employed to improve reliability as testing progresses. The drawbacks to such extensive testing are time and expense. [[Customer Value|Customers]] may choose to accept more [[risk]] by eliminating some or all lower levels of testing.  Another type of tests are called Sequential Probability Ratio type of tests. These tests use both a statistical type 1 and type 2 error, combined with a discrimination ratio as main input (together with the R requirement). This test (see for examples  [[mil. std. 781]]) sets – independently – before the start of the test both the risk of incorrectly accepting a bad design (type 2 error) and the risk of incorrectly rejecting a good design (type 1 error) together with the discrimination ratio and the required minimum reliability parameter. The test is therefore more controllable and provides more information from a quality and business point of view. The number of test samples is not fixed, but it is said{{Citation needed|date=September 2011}} that this test is in general more efficient (requires less samples) and provides more information than for example zero failure testing.  It is not always feasible to test all system requirements. Some systems are prohibitively expensive to test; some [[failure mode]]s may take years to observe; some complex interactions result in a huge number of possible test cases; and some tests require the use of limited test ranges or other resources. In such cases, different approaches to testing can be used, such as accelerated life testing, [[design of experiments]], and [[simulation]]s.  The desired level of statistical confidence also plays an important role in reliability testing. Statistical confidence is increased by increasing either the test time or the number of items tested. Reliability test plans are designed to achieve the specified reliability at the specified [[confidence interval|confidence level]] with the minimum number of test units and test time. Different test plans result in different levels of risk to the producer and consumer. The desired reliability, statistical confidence, and risk levels for each side influence the ultimate test plan. Good test requirements ensure that the customer and developer agree in advance on how reliability requirements will be tested.  A key aspect of reliability testing is to define "[[failure]]". Although this may seem obvious, there are many situations where it is not clear whether a failure is really the fault of the system. Variations in test conditions, operator differences, weather and unexpected situations create differences between the customer and the system developer. One strategy to address this issue is to use a '''scoring conference''' process. A scoring conference includes representatives from the customer, the developer, the test organization, the reliability organization, and sometimes independent observers. The scoring conference process is defined in the statement of work. Each test case is considered by the group and "scored" as a success or failure. This scoring is the official result used by the reliability engineer.  As part of the requirements phase, the reliability engineer develops a test strategy with the customer. The test strategy makes trade-offs between the needs of the reliability organization, which wants as much data as possible, and constraints such as cost, schedule and available resources. Test plans and procedures are developed for each reliability test, and results are documented in official reports.  == Accelerated testing == The purpose of accelerated life testing is to induce field failure in the laboratory at a much faster rate by providing a harsher, but nonetheless representative, environment. In such a test, the product is expected to fail in the lab just as it would have failed in the field—but in much less time. The main objective of an accelerated test is either of the following: :* To discover failure modes :* To predict the normal field life from the high [[stress testing|stress]] lab life An '''Accelerated testing''' program can be broken down into the following steps: :* Define objective and scope of the test :* Collect required information about the product :* Identify the stress(es) :* Determine level of stress(es) :* Conduct the accelerated test and analyze the collected data.  Common way to determine a life stress relationship are :* Arrhenius Model :* Eyring Model :* Inverse Power Law Model :* Temperature-Humidity Model :* Temperature Non-thermal Model  == Software reliability == Software reliability is a special aspect of reliability engineering. System reliability, by definition, includes all parts of the system, including hardware, software, supporting infrastructure (including critical external interfaces), operators and procedures. Traditionally, reliability engineering focuses on critical hardware parts of the system. Since the widespread use of digital [[integrated circuit]] technology, software has become an increasingly critical part of most [[electronics]] and, hence, nearly all present day systems.   There are significant differences, however, in how software and hardware behave.  Most hardware unreliability is the result of a component or [[material]] failure that results in the system not performing its intended function. Repairing or replacing the hardware component restores the system to its original operating state.  However, software does not fail in the same sense that hardware fails. Instead, software unreliability is the result of unanticipated results of software operations. Even relatively small software programs can have astronomically large [[combinations]] of inputs and states that are infeasible to exhaustively test. Restoring software to its original state only works until the same combination of inputs and states results in the same unintended result. Software reliability engineering must take this into account.  Despite this difference in the source of failure between software and hardware — software does not wear out — some in the software reliability engineering community believe statistical models used in hardware reliability are nevertheless useful as a measure of software reliability, describing what we experience with software: the longer software is run, the higher the probability that it will eventually be used in an untested manner and exhibit a latent defect that results in a failure ([[Shooman]] 1987), (Musa 2005), (Denney 2005).  (Of course, that assumes software is a constant, which it seldom is.)  As with hardware, software reliability depends on good requirements, design and implementation. Software reliability engineering relies heavily on a disciplined [[software engineering]] process to anticipate and design against [[unintended consequence]]s. There is more overlap between software [[quality assurance|quality engineering]] and software reliability engineering than between hardware quality and reliability. A good software development plan is a key aspect of the software reliability program. The software development plan describes the design and coding standards, [[software peer review|peer reviews]], [[unit test]]s, [[configuration management]], [[software metrics]] and software models to be used during software development.  A common reliability metric is the number of software faults, usually expressed as faults per thousand lines of code. This metric, along with software execution time, is key to most software reliability models and estimates. The theory is that the software reliability increases as the number of faults (or fault density) goes down. Establishing a direct connection between fault density and mean-time-between-failure is difficult, however, because of the way software faults are distributed in the code, their severity, and the probability of the combination of inputs necessary to encounter the fault. Nevertheless, fault density serves as a useful indicator for the reliability engineer. Other software metrics, such as complexity, are also used.  This metric remains controversial, since changes in software development and verification practices can have dramatic impact on overall defect rates.  Testing is even more important for software than hardware. Even the best software development process results in some software faults that are nearly undetectable until tested. As with hardware, software is tested at several levels, starting with individual units, through integration and full-up system testing. Unlike hardware, it is inadvisable to skip levels of software testing. During all phases of testing, software faults are discovered, corrected, and re-tested. Reliability estimates are updated based on the fault density and other metrics. At a system level, mean-time-between-failure data can be collected and used to estimate reliability. Unlike hardware, performing exactly the same test on exactly the same software configuration does not provide increased statistical confidence. Instead, software reliability uses different metrics, such as [[code coverage]].  Eventually, the software is integrated with the hardware in the top-level system, and software reliability is subsumed by system reliability. The Software Engineering Institute's [[Capability Maturity Model]] is a common means of assessing the overall software development process for reliability and quality purposes.  == Reliability engineering vs safety engineering ==  Reliability engineering differs from [[safety engineering]] with respect to the kind of hazards that are considered. Reliability engineering is in the end only concerned with cost. It relates to all Reliability hazards that could transform into incidents with a particular level of loss of revenue for the company or the customer. These can be cost due to loss of production due to system unavailability, unexpected high or low demands for spares, repair costs, man hours, (multiple) re-designs, interruptions on normal production (e.g. due to high repair times or due to unexpected demands for non-stocked spares) and many other indirect costs.   Safety engineering, on the other hand, is more specific and regulated. It relates to only very specific and system Safety Hazards that could potentially lead to severe accidents. The related functional reliability requirements are sometimes extremely high. It deals with unwanted dangerous events (for life and environment) in the same sense as reliability engineering, but does normally not directly look at cost and is not concerned with repair actions after failure / accidents (on system level). Another difference is the level of impact of failures on society and the control of governments. Safety engineering is often strictly controlled by governments (e.g. Nuclear, Aerospace, Defense, Rail and Oil industries).  Furthermore, safety engineering and reliability engineering may even have contradicting requirements. This relates to system level architecture choices {{citation needed|date=October 2011}}. For example, in train signal control systems it is common practice to use a fail-safe system design concept. In this concept the so called "wrong side failures" need to be fully controlled to a extreme low failure rate. These failures are related to possible severe effects, like frontal collisions (2* GREEN lights). Systems are designed in a way that the far majority of failures will simply result in a temporary or total loss of signals or open contacts of relays and generate RED lights for all trains. This is the safe state. All trains are stopped immediately. This fail-safe logic might unfortunately lower the reliability of the system. The reason for this is the higher risk of false tripping as any full or temporary, intermittent failure is quickly latched in a shut-down (safe)state. Different solutions are available for this issue. See chapter Fault Tolerance below.  === Fault Tolerance === Reliability can be increased here by using a 2oo2 (2 out of 2) redundancy on part or system level, but this does in turn lower the safety levels (more possibilities for Wrong Side and undetected dangerous Failures). Fault tolerant voting systems (e.g. 2oo3 voting logic) can increase both reliability and safety on a system level. In this case the so called "operational" or "mission" reliability as well as the safety of a system can be increased. This is also common practice in Aerospace systems that need continued availability and do ''not'' have a fail safe mode (e.g. flight computers and related electrical and / or mechanical and / or hydraulic steering functions need always to be working. There are no safe fixed positions for rudder or other steering parts when the aircraft is flying).  === Basic Reliability and Mission (Operational) Reliability === The above example of a 2oo3 fault tolerant system increases both mission reliability as well as safety. However, the "basic" reliability of the system will in this case still be lower than a non redundant (1oo1) or 2oo2 system! Basic reliability refers to all failures, including those that might not result in system failure, but do result in maintenance repair actions, logistic cost, use of spares, etc. For example, the replacement or repair of 1 channel in a 2oo3 voting system that is still operating with one failed channel (which in this state actually has become a 1oo2 system) is contributing to basic unreliability but not mission unreliability. Also, for example, the failure of the taillight of a aircraft is not considered as a mission loss failure, but does contribute to the basic unreliability.  === Detectability and Common Cause Failures === When using fault tolerant (redundant architectures) systems or systems that are equipped with protection functions, detectability of failures and avoidance of common cause failures become paramount for safe functioning and/or mission reliability.  == Reliability operational assessment ==  After a system is produced, reliability engineering monitors, assesses and corrects deficiencies. Monitoring includes electronic and visual surveillance of critical parameters identified during the fault tree analysis design stage. The data are constantly analyzed using statistical techniques, such as [[Weibull distribution|Weibull]] analysis and [[linear regression]], to ensure the system reliability meets requirements. Reliability data and estimates are also key inputs for system [[logistics]]. Data collection is highly dependent on the nature of the system. Most large organizations have [[quality control]] groups that collect failure data on vehicles, equipment and machinery. Consumer product failures are often tracked by the number of returns. For systems in dormant storage or on standby, it is necessary to establish a formal surveillance program to inspect and test random samples. Any changes to the system, such as field upgrades or recall repairs, require additional reliability testing to ensure the reliability of the modification. Since it is not possible to anticipate all the failure modes of a given system, especially ones with a human element, failures will occur. The reliability program also includes a systematic [[root cause analysis]] that identifies the causal relationships involved in the failure such that effective corrective actions may be implemented. When possible, system failures and corrective actions are reported to the reliability engineering organization.  One of the most common methods to apply to a reliability operational assessment are [[Failure Reporting, Analysis and Corrective Action Systems]] (FRACAS). This systematic approach develops a reliability, safety and logistics assessment based on Failure / Incident reporting, management, analysis and corrective/preventive actions. Organizations today are adopting this method and utilize commercial systems such as a Web based FRACAS application enabling an organization to create a failure/incident data repository from which statistics can be derived to view accurate and genuine reliability, safety and quality performances.  It is extremely important to have one common source FRACAS system for all end items. Also, test results should be able to be captured here in a practical way. Failure to adopt one easy to handle (easy data entry for field engineers and repair shop engineers)and maintain integrated system is likely to result in a FRACAS program failure.  Some of the common outputs from a FRACAS system includes: Field MTBF, MTTR, Spares Consumption, Reliability Growth, Failure/Incidents distribution by type, location, part no., serial no, symptom etc.  The use of past data to predict the reliability of new comparable systems/items can be misleading as reliability is a function of the context of use and can be affected by small changes in the designs/manufacturing.  == Reliability organizations == Systems of any significant complexity are developed by [[organizations]] of people, such as a commercial [[company (law)|company]] or a [[government]] agency. The reliability engineering organization must be consistent with the company's [[organizational structure]]. For small, non-critical systems, reliability engineering may be informal. As complexity grows, the need arises for a formal reliability function. Because reliability is important to the customer, the customer may even specify certain aspects of the reliability organization.  There are several common types of reliability organizations. The [[project manager]] or chief [[engineer]] may employ one or more reliability engineers directly. In larger organizations, there is usually a product assurance or [[specialty engineering]] organization, which may include reliability, [[maintainability]], [[Quality (business)|quality]], [[safety]], [[human factors]], [[logistics]], etc. In such case, the reliability engineer reports to the product assurance manager or specialty engineering manager.  In some cases, a company may wish to establish an independent reliability organization. This is desirable to ensure that the system reliability, which is often expensive and time consuming, is not unduly slighted due to budget and schedule pressures. In such cases, the reliability engineer works for the project day-to-day, but is actually employed and paid by a separate organization within the company.  Because reliability engineering is critical to early system design, it has become common for reliability engineers, however the organization is structured, to work as part of an [[integrated product team]].  ==Certification== {{Globalize|section|date=June 2012}} The [[American Society for Quality]] has a program to become a Certified Reliability Engineer, CRE. Certification is based on education, experience, and a certification test: periodic re-certification is required. The body of knowledge for the test includes: reliability management, design evaluation, product safety, statistical tools, design and development, modeling, reliability testing, collecting and using data, etc.  Another highly respected certification program is the [http://www.reliabilityprofessional.org/index.htm CRP] (Certified Reliability Professional).  To achieve certification, candidates must complete a series of courses focused on important Reliability Engineering topics, successfully apply the learned body of knowledge in the workplace and publicly present this expertise in an industry conference or journal.  == Reliability engineering education == {{Globalize|section|date=June 2012}} Some universities offer graduate degrees in reliability engineering. Other reliability engineers typically have an engineering degree, which can be in any field of engineering, from an [[school accreditation|accredited]] [[university]] or [[college]] program. Many engineering programs offer reliability courses, and some universities have entire reliability engineering programs. A reliability engineer may be registered as a [[Professional Engineer]] by the state, but this is not required by most employers. There are many professional conferences and industry training programs available for reliability engineers. Several professional organizations exist for reliability engineers, including the [[IEEE Reliability Society]], the [http://www.asq.org American Society for Quality (ASQ)], and the [http://www.sre.org Society of Reliability Engineers (SRE)].  == See also == {{Portal|Engineering}} {{colbegin|2}} * [[Brittle Systems]] * [[Burn-in]] * [[Failing badly]] * [[Human reliability]] * [[Integrated Logistics Support]] * [[Highly accelerated stress test]] * [[Highly Accelerated Life Test]] * [[Logistic engineering]] * [[Performance engineering]] * [[Professional engineer]] * [[Product qualification]] * [[Quality assurance]] * [[Reliability (disambiguation)]] * [[Reliable system design]] * [[Reliability theory]] * [[Reliability theory of aging and longevity]] * [[Risk assessment]] * [[Redundancy (total quality management)]] * [[Security engineering]] * [[Single point of failure]] (SPOF) * [[Software engineering]] * [[Software Reliability Testing]] * [[Systems engineering]] * [[Temperature cycling]] * [[Spurious trip level]] * [[Safety integrity level]] {{colend}}  == References == {{Reflist}}  == Further reading == * Blanchard, Benjamin S. (1992), ''Logistics Engineering and Management'' (Fourth Ed.), Prentice-Hall, Inc., Englewood Cliffs, New Jersey. * Breitler, Alan L. and Sloan, C. (2005), Proceedings of the American Institute of Aeronautics and Astronautics (AIAA) Air Force T&E Days Conference, Nashville, TN, December, 2005: System Reliability Prediction: towards a General Approach Using a Neural Network. * Ebeling, Charles E., (1997), ''An Introduction to Reliability and Maintainability Engineering'', McGraw-Hill Companies, Inc., Boston. * Denney, Richard (2005) Succeeding with Use Cases: Working Smart to Deliver Quality. Addison-Wesley Professional Publishing. ISBN . Discusses the use of software reliability engineering in [[use case]] driven software development. * Gano, Dean L. (2007), "Apollo Root Cause Analysis" (Third Edition), Apollonian Publications, LLC., Richland, Washington * [[Oliver Wendell Holmes, Sr.|Holmes, Oliver Wendell]], Sr. [[s:The Deacon's Masterpiece|The Deacon's Masterpiece]] * Kapur, K.C., and Lamberson, L.R., (1977), ''Reliability in Engineering Design'', John Wiley & Sons, New York. * Kececioglu, Dimitri, (1991) "Reliability Engineering Handbook", Prentice-Hall, Englewood Cliffs, New Jersey *[[Trevor Kletz]] (1998) ''Process Plants: A Handbook for Inherently Safer Design'' CRC ISBN 1-56032-619-0 * Leemis, Lawrence, (1995) ''Reliability: Probabilistic Models and Statistical Methods'', 1995, Prentice-Hall. ISBN 0-13-720517-1 * {{Cite book|author=[[Frank Lees]] |title=Loss Prevention in the Process Industries|edition=3rdEdition|publisher=Elsevier|year=2005|isbn=978-0-7506-7555-0}} * MacDiarmid, Preston; Morris, Seymour; et al., (1995), ''Reliability Toolkit: Commercial Practices Edition'', Reliability Analysis Center and Rome Laboratory, Rome, New York. * Modarres, Mohammad; Kaminskiy, Mark; Krivtsov, Vasiliy (1999), "Reliability Engineering and Risk Analysis: A Practical Guide, CRC Press, ISBN 0-8247-2000-8. * Musa, John (2005) Software Reliability Engineering: More Reliable Software Faster and Cheaper, 2nd. Edition, AuthorHouse. ISBN * Neubeck, Ken (2004) "Practical Reliability Analysis", Prentice Hall, New Jersey * Neufelder, Ann Marie, (1993), ''Ensuring Software Reliability'', Marcel Dekker, Inc., New York. * O'Connor, Patrick D. T. (2002), ''Practical Reliability Engineering'' (Fourth Ed.), John Wiley & Sons, New York. * Shooman, Martin, (1987), ''Software Engineering: Design, Reliability, and Management'', McGraw-Hill, New York. * Tobias, Trindade, (1995), ''Applied Reliability'', Chapman & Hall/CRC, ISBN 0-442-00469-9 * [http://www.springer.com/series/6917 Springer Series in Reliability Engineering] * Nelson, Wayne B., (2004), ''Accelerated Testing – Statistical Models, Test Plans, and Data Analysis'', John Wiley & Sons, New York, ISBN 0-471-69736-2 * Bagdonavicius, V., Nikulin, M., (2002), "Accelerated Life Models. Modeling and  Statistical analysis", CHAPMAN&HALL/CRC, Boca Raton, ISBN 1-58488-186-0  === US standards, specifications, and handbooks === * [http://www.everyspec.com/USAF/TORs/TOR2007-8583-6889_14232/ Aerospace Report Number: TOR-2007(8583)-6889] ''Reliability Program Requirements for Space Systems'', [[The Aerospace Corporation]] (10 Jul 2007) * [http://www.everyspec.com/DoD/DoD-PUBLICATIONS/DOD_3235x1-H_15048/ DoD 3235.1-H (3rd Ed)] ''Test and Evaluation of System Reliability, Availability, and Maintainability (A Primer)'', U.S. Department of Defense (March 1982) . * [http://www.everyspec.com/NASA/NASA-GSFC/GSFC-Code-Series/GSFC_431_REF_000370_2297/  NASA GSFC 431-REF-000370] ''Flight Assurance Procedure: Performing a Failure Mode and Effects Analysis'', [[National Aeronautics and Space Administration]] [[Goddard Space Flight Center]] (10 Aug 1996). * [http://ieeexplore.ieee.org/xpl/standardstoc.jsp?isnumber=15567  IEEE 1332–1998] ''IEEE Standard Reliability Program for the Development and Production of Electronic Systems and Equipment'', [[Institute of Electrical and Electronics Engineers]] (1998). * [http://www.everyspec.com/NASA/NASA-JPL/JPL_D-5703_JUL1990_15049/  JPL D-5703] ''Reliability Analysis Handbook'', [[National Aeronautics and Space Administration]] [[Jet Propulsion Laboratory]] (July 1990). * [http://www.everyspec.com/MIL-STD/MIL-STD-0700-0799/MIL-STD-785B_23780/ MIL-STD-785B] ''Reliability Program for Systems and Equipment Development and Production'', U.S. Department of Defense (15 Sep 1980).  (*Obsolete, superseded by ANSI/GEIA-STD-0009-2008 titled ''Reliability Program Standard for Systems Design, Development, and Manufacturing'', 13 Nov 2008) * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0200-0299/MIL-HDBK-217F_14591/ MIL-HDBK-217F] ''Reliability Prediction of Electronic Equipment'', U.S. Department of Defense (2 Dec 1991). * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0200-0299/MIL-HDBK-217F_NOTICE-1_14589/ MIL-HDBK-217F (Notice 1)] ''Reliability Prediction of Electronic Equipment'', U.S. Department of Defense (10 Jul 1992). * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0200-0299/MIL-HDBK-217F_NOTICE-2_14590/ MIL-HDBK-217F (Notice 2)] ''Reliability Prediction of Electronic Equipment'', U.S. Department of Defense (28 Feb 1995). * [http://www.everyspec.com/MIL-STD/MIL-STD-0500-0699/MIL-STD-690D_15050/ MIL-STD-690D] ''Failure Rate Sampling Plans and Procedures'', U.S. Department of Defense (10 Jun 2005). * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0300-0499/MIL-HDBK-338B_15041/  MIL-HDBK-338B] ''Electronic Reliability Design Handbook'', U.S. Department of Defense (1 Oct 1998). * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-2000-2999/MIL-HDBK-2173_15046/ MIL-HDBK-2173] ''Reliability-Centered Maintenance (RCM) Requirements for Naval Aircraft, Weapon Systems, and Support Equipment'', U.S. Department of Defense (30 JAN 1998);  (superseded by [http://www.barringer1.com/mil_files/NAVAIR-00-25-403.pdf NAVAIR 00-25-403]). * [http://www.everyspec.com/MIL-STD/MIL-STD-1500-1599/MIL_STD_1543B_166/ MIL-STD-1543B] ''Reliability Program Requirements for Space and Launch Vehicles'', U.S. Department of Defense (25 Oct 1988). * [http://www.everyspec.com/MIL-STD/MIL-STD-1600-1699/MIL_STD_1629A_1556/ MIL-STD-1629A] ''Procedures for Performing a Failure Mode Effects and Criticality Analysis'', U.S. Department of Defense (24 Nov 1980). * [http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0700-0799/MIL_HDBK_781A_1933/ MIL-HDBK-781A] ''Reliability Test Methods, Plans, and Environments for Engineering Development, Qualification, and Production'', U.S. Department of Defense (1 Apr 1996). * [http://www.everyspec.com/USN/NSWC/NSWC-06_RELIAB_HDBK_2006_15051/ NSWC-06 (Part A & B)] ''Handbook of Reliability Prediction Procedures for Mechanical Equipment'', [[Naval Surface Warfare Center]] (10 Jan 2006).  === UK standards === In the UK, there are more up to date standards maintained under the sponsorship of UK MOD as Defence Standards. The relevant Standards include:  DEF STAN 00-40   Reliability and Maintainability (R&M) *PART 1: Issue 5: Management Responsibilities and Requirements for Programmes and Plans *PART 4: (ARMP-4)Issue 2: Guidance for Writing NATO R&M Requirements Documents *PART 6: Issue 1: IN-SERVICE R & M *PART 7 (ARMP-7) Issue 1: NATO R&M Terminology Applicable to ARMP’s  DEF STAN 00-42  RELIABILITY AND MAINTAINABILITY ASSURANCE GUIDES *PART 1: Issue 1: ONE-SHOT DEVICES/SYSTEMS *PART 2: Issue 1: SOFTWARE *PART 3: Issue 2: R&M CASE *PART 4: Issue 1: Testability *PART 5: Issue 1: IN-SERVICE RELIABILITY DEMONSTRATIONS DEF STAN 00-43 RELIABILITY AND MAINTAINABILITY ASSURANCE ACTIVITY *PART 2: Issue 1: IN-SERVICE MAINTAINABILITY DEMONSTRATIONS DEF STAN 00-44  RELIABILITY AND MAINTAINABILITY DATA COLLECTION AND CLASSIFICATION *PART 1: Issue 2: MAINTENANCE DATA & DEFECT REPORTING IN THE ROYAL NAVY, THE ARMY AND THE ROYAL AIR FORCE *PART 2: Issue 1: DATA CLASSIFICATION AND INCIDENT SENTENCING – GENERAL *PART 3: Issue 1: INCIDENT SENTENCING – SEA *PART 4: Issue 1: INCIDENT SENTENCING – LAND DEF STAN 00-45 Issue 1: RELIABILITY CENTERED MAINTENANCE  DEF STAN 00-49 Issue 1: RELIABILITY AND MAINTAINABILITY MOD GUIDE TO TERMINOLOGY DEFINITIONS  These can be obtained from [http://www.dstan.mod.uk DSTAN]. There are also many commercial standards, produced by many organisations including the SAE, MSG, ARP, and IEE.  === French standards ===  * FIDES [http://fides-reliability.org]. The FIDES methodology (UTE-C 80-811) is based on the physics of failures and supported by the analysis of test data, field returns and existing modelling. * UTE-C 80–810 or RDF2000 [http://www.ute-fr.com/FR/]. The RDF2000 methodology is based on the French telecom experience.  === International standards === * [http://tc56.iec.ch/about/standards0_1.htm TC 56 Standards: Dependability]  == External links == {{External links|date=August 2010}} * [http://www.asq.org/ American Society for Quality] * [http://www.sei.cmu.edu/ Carnegie Mellon Software Engineering Institute] * [http://www.ieee.org/portal/index.jsp?pageID=relsoc_home IEEE Reliability Society] * [http://satc.gsfc.nasa.gov/support/OSMASAS_SEP01/Application_and_Improvement_of_SW_Reliability_Models.html NASA Hardware and Software Reliability report] * NIST/SEMATECH, "Engineering Statistics Handbook", [http://www.itl.nist.gov/div898/handbook/index.htm] * [http://www.sre.org/ Society of Reliability Engineers] * [http://www.enre.umd.edu/ University of Maryland Reliability Engineering Program] * [http://www.theriac.org/ Reliability Information Analysis Center] * [http://www.uncertainty-in-engineering.net/ Models and methods regarding reliability analysis] * [http://www.dstan.mod.uk/ UK Defence Standardization Organisation's Home on the Web] * [http://www.crr.umd.edu/ Center for Risk and Reliability at University of Maryland, College Park] * [http://www.weibull.com/ On-line Reliability Engineering Resources for the Reliability Professional] * [http://www.eurelnet.org/ EURELNET European Reliability Network – Failure Mechanisms and Materials Database] * [http://www.imdr.fr/submitted/document_site/Method_sheets_EN_343.pdf Institut pour la Maitrise des Risques : Method Sheets, english version]  {{Design}} {{Systems Engineering}} {{Statistics|applications|state=collapsed}}  [[Category:Design for X]] [[Category:Failure]] [[Category:Reliability engineering| ]] [[Category:Systems engineering]] [[Category:Software quality]] [[Category:Engineering statistics]] [[Category:Survival analysis]] [[Category:Materials science]]  [[ar:هندسة الوثوقية]] [[cs:Teorie spolehlivosti]] [[es:Fiabilidad de sistemas]] [[fr:Ingénierie de fiabilité]] [[ko:신뢰성 공학]] [[hi:विश्वसनीयता इंजीनियरी]] [[hr:Inženjerstvo pouzdanosti]] [[it:Ingegneria dell'affidabilità]] [[nl:Reliability engineering]] [[ja:信頼性工学]] [[pt:Engenharia de confiabilidade]] [[ru:Надёжность]] [[scn:Affidabbilitati 'ngigniristica]] [[fi:Luotettavuustekniikka]] [[sv:Funktionssäkerhet]] [[tr:Güvenilirlik mühendisliği]]
'''Software engineering''' ('''SE''') is the application of a systematic, disciplined, quantifiable  approach to the design, development, operation, and maintenance of [[software]], and the study of these approaches; that is, the application of [[engineering]] to software.<ref name="BoDu04">[[Software Engineering Body of Knowledge|SWEBOK]] {{Cite book| editors = Pierre Bourque and Robert Dupuis |   title = Guide to the Software Engineering Body of Knowledge - 2004 Version | publisher = [[IEEE Computer Society]] | year = 2004 | pages = 1–1 | isbn = 0-7695-2330-7 | url = http://www.swebok.org | author = executive editors, Alain Abran, James W. Moore ; editors, Pierre Bourque, Robert Dupuis.}}</ref> <ref>{{cite web  | last = ACM  | year = 2006  | url = http://computingcareers.acm.org/?page_id=12  | title = Computing Degrees & Careers  | publisher = ACM  | accessdate = 2010-11-23 }}</ref> <ref>  {{cite book | last = Laplante | first = Phillip | title = What Every Engineer Should Know about Software Engineering | publisher = CRC | location = Boca Raton    | year = 2007 | isbn = 978-0-8493-7228-5 | url = http://books.google.com/?id=pFHYk0KWAEgC&lpg=PP1&dq=What%20Every%20Engineer%20Should%20Know%20about%20Software%20Engineering.&pg=PA1#v=onepage&q&f=false | accessdate = 2011-01-21 }} </ref> The term ''software engineering'' first appeared in the 1968 NATO Software Engineering Conference, and was meant to provoke thought regarding the perceived "[[software crisis]]" at the time.<ref>{{cite conference   | first = Naur   | last = Peter   | coauthors = Brian Randell   | title = Software Engineering: Report of a conference sponsored by the NATO Science Committee   | publisher = Scientific Affairs Division, NATO   | date = 7–11 October 1968   | location = Garmisch, Germany   | url = http://homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1968.PDF   |format=PDF| doi =   | id =   | accessdate = 2008-12-26}} </ref><ref>{{cite web | url = http://homepages.cs.ncl.ac.uk/brian.randell/NATO/NATOReports/index.html | title = The 1968/69 NATO Software Engineering Reports | accessdate = 2008-10-11 | last = Randell | first = Brian | authorlink = Brian Randell |date = 10 August 2001 | work = Brian Randell's University Homepage | publisher = The School of the Computer Sciences, Newcastle University | quote = The idea for the first NATO Software Engineering Conference, and in particular that of adopting the then practically unknown term "software engineering" as its (deliberately provocative) title, I believe came originally from Professor Fritz Bauer.}}</ref>  ''[[Software development]]'', a much used and more generic term, does not necessarily subsume the engineering paradigm. Although it is questionable what impact it has had on actual software development over the last 40 years,<ref>{{cite web|url=http://alistair.cockburn.us/The end of software engineering and the start of economic-cooperative gaming |title=The end of software engineering and the start of economic-cooperative gaming |publisher=Alistair.cockburn.us |date= |accessdate=2012-03-25}}</ref><ref>{{cite web|url=http://cat.inist.fr/?aModele=afficheN&cpsidt=15417224 |title=35 years on: to what extent has software engineering design achieved its goals? |publisher=Cat.inist.fr |date= |accessdate=2012-03-25}}</ref> the field's future looks bright according to [[Money Magazine]] and [[Salary.com]], which rated "[[software engineer]]" as the best job in the United States in 2006.<ref>{{cite web   | last = Kalwarski  | first = Tara  | coauthors = Daphne Mosher, Janet Paskin and Donna Rosato  | year = 2006  | url = http://money.cnn.com/magazines/moneymag/bestjobs/2006/  | title = Best Jobs in America  | work = MONEY Magazine  | publisher = CNN  | accessdate = 2006-04-20 }}</ref> In 2012, software engineering was again ranked as the best job in the United States, this time by CareerCast.com.<ref>{{cite web|url=http://online.wsj.com/article/SB10001424052702303772904577336230132805276.html|title=Best and Worst Jobs of 2012 |publisher=online.wsj.com |date= |accessdate=2012}}</ref>  ==History== {{Main|History of software engineering}}  When the first modern digital [[computer]]s appeared in the early 1940s,<ref>{{Cite book|last=Leondes|title= intelligent systems: technology and applications|year=2002| publisher=CRC Press| isbn =978-0-8493-1121-5 | quote = }}</ref> the instructions to make them operate were wired into the machine.  Practitioners quickly realized that this design was not flexible and came up with the "stored program architecture" or [[von Neumann architecture]].  Thus the division between "hardware" and "software" began with [[abstraction (computer science)|abstraction]] being used to deal with the complexity of computing.  [[Programming languages]] started to appear in the 1950s and this was also another major step in abstraction.  Major languages such as [[Fortran]], [[ALGOL]], and [[COBOL]] were released in the late 1950s to deal with scientific, algorithmic, and business problems respectively.  [[Edsger Dijkstra|E.W. Dijkstra]] wrote his seminal paper, "Go To Statement Considered Harmful",<ref>{{Cite journal   | last = Dijkstra   | first = E. W.   | authorlink = Edsger_Dijkstra   | title = Go To Statement Considered Harmful   | journal = [[Communications of the ACM]]   | volume = 11   | issue = 3   | pages = 147–148   | month = March | year = 1968   | url = http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.PDF   | accessdate = 2009-08-10   | doi = 10.1145/362929.362947}} </ref> in 1968 and [[David Parnas]] introduced the key concept of [[modularity]] and [[information hiding]] in 1972<ref> {{Cite journal | last = Parnas | first = David | authorlink = David Parnas | journal = [[Communications of the ACM]] | volume = 15 | issue = 12 | pages = 1053–1058 | url = http://www.acm.org/classics/may96/ | title = On the Criteria To Be Used in Decomposing Systems into Modules | month = December | year = 1972 | accessdate = 2008-12-26 | doi = 10.1145/361598.361623 }}</ref> to help programmers deal with the ever increasing complexity of [[software systems]].  A software system for managing the hardware called an [[operating system]] was also introduced, most notably by [[Unix]] in 1969. In 1967, the [[Simula]] language introduced the [[object-oriented programming]] paradigm.  These advances in [[software]] were met with more advances in [[computer hardware]].  In the mid 1970s, the [[microcomputer]] was introduced, making it economical for hobbyists to obtain a [[computer]] and write [[software]] for it.  This in turn led to the now famous [[Personal Computer]] (PC) and [[Microsoft Windows]].  The [[Software Development Process|Software Development Life Cycle]] or SDLC was also starting to appear as a consensus for centralized construction of software in the mid 1980s. The late 1970s and early 1980s saw the introduction of several new Simula-inspired object-oriented programming languages, including [[Smalltalk]], [[Objective-C]], and [[C  ]].  [[Open-source]] software started to appear in the early 90s in the form of [[Linux]] and other software introducing the "bazaar" or decentralized style of constructing software.<ref>[[Eric S. Raymond|Raymond, Eric S.]] ''[[The Cathedral and the Bazaar]]''. ed 3.0. 2000.</ref>  Then the [[World Wide Web]] and the popularization of the [[Internet]] hit in the mid 90s, changing the engineering of software once again.  [[Distributed systems]] gained sway as a way to design systems, and the [[Java (programming language)|Java]] programming language was introduced with its own [[virtual machine]] as another step in [[abstraction]].  [[Programmers]] collaborated and wrote the [[Agile Manifesto]], which favored more lightweight processes to create cheaper and more timely software.  The current definition of ''software engineering'' is still being debated by practitioners today as they struggle to come up with ways to produce software that is "cheaper, better, faster". [[Spend management|Cost reduction]] has been a primary focus of the IT industry since the 1990s.  [[Total cost of ownership]] represents the costs of more than just acquisition. It includes things like productivity impediments, upkeep efforts, and resources needed to support infrastructure.  ==Profession== {{Main|Software engineer}}  Legal requirements for the licensing or certification of professional software engineers vary around the world.  In the UK, the [[British Computer Society]] licenses software engineers and members of the society can also become [[Chartered Engineer (UK)|Chartered Engineers]] (CEng), while in some areas of Canada, such as Alberta, Ontario,<ref>{{cite conference | first = N.S.W. | last = Williams | title = Professional Engineers Ontario's approach to licensing software engineering practitioners | booktitle = Software Engineering Education and Training, 2001 Proceedings. 14th Conference on | pages = 77–78 | publisher = [[IEEE]]| date = 19–21 February 2001 | location = Charlotte, NC | url = | accessdate = }}</ref> and Quebec, software engineers can hold the Professional Engineer (P.Eng)designation and/or the Information Systems Professional (I.S.P.) designation; however, there is no legal requirement to have these qualifications.  The [[IEEE Computer Society]] and the [[Association for Computing Machinery|ACM]], the two main professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's ''Guide to the Software Engineering Body of Knowledge - 2004 Version'', or [[SWEBOK]], defines the field and describes the knowledge the IEEE expects a practicing [[software engineer]] to have.  The IEEE also promulgates a "Software Engineering Code of Ethics".<ref>{{cite web|url=http://www.computer.org/cms/Computer.org/Publications/code-of-ethics.pdf |title='&#39;Software Engineering Code of Ethics'&#39; |format=PDF |date= |accessdate=2012-03-25}}</ref>  ===Employment=== In 2004, the [[Bureau of Labor Statistics|U. S. Bureau of Labor Statistics]] counted 760,840 [[software engineer]]s holding jobs in the [[United States|U.S.]]; in the same time period there were some 1.4 million practitioners employed in the U.S. in all other engineering disciplines combined.<ref>Bureau of Labor Statistics, U.S. Department of Labor, [ftp://ftp.bls.gov/pub/news.release/ocwage.txt ''USDL 05-2145: Occupational Employment and Wages, November 2004''], Table 1.</ref> Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.<ref>{{cite web|url=http://computingcareers.acm.org/?page_id=12|title=Software Engineering|accessdate=2008-02-01}}</ref>  Many [[software engineer]]s work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as [[freelancer]]s. Some organizations have specialists to perform each of the tasks in the [[software development process]]. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry ([[Requirements analysis|analysts]], [[Software architecture|architects]], [[Software developer|developers]], [[Software testing|testers]], [[technical support]], [[middleware analyst]]s, [[Project management|managers]]) and in academia ([[educator]]s, [[researcher]]s).  Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Injuries in these occupations are rare. However, like other workers who spend long periods in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.<ref>{{cite web|url=http://www.bls.gov/oco/ocos303.htm#training|title=Computer Software Engineers and Computer Programmers|accessdate=2009-12-17}}</ref>  ===Certification=== The [[Software Engineering Institute]] offers certifications on specific topics like Security, [[Process improvement]] and [[Software architecture]].<ref>{{cite web|url=http://www.sei.cmu.edu/certification/ |title=SEI certification page |publisher=Sei.cmu.edu |date= |accessdate=2012-03-25}}</ref> [[Apple certification programs|Apple]], [[Information Technology Architect Certification|IBM]], [[Microsoft Certified Professional|Microsoft]] and other companies also sponsor their own certification examinations.  Many [[Information technology|IT]] [[Professional certification (Computer technology)|certification]] programs are oriented toward specific technologies, and managed by the vendors of these technologies.<ref>{{cite web|url=http://www.informit.com/articles/article.aspx?p=1180991|title=The Top 10 Problems with IT Certification in 2008|last=Wyrostek|first=Warren |date=March 14, 2008|work=InformIT|accessdate=2009-03-03}}</ref> These certification programs are tailored to the institutions that would employ people who use these technologies.  Broader certification of general software engineering skills is available through various professional societies. {{As of|2006}}, the [[IEEE]] had certified over 575 software professionals as a [[Certified Software Development Professional]] (CSDP).<ref name="ieee2006">{{cite web|author=IEEE Computer Society|url=http://www.ifip.org/minutes/GA2006/Tab18b-US-IEEE.pdf|format=PDF|title=2006 IEEE computer society report to the IFIP General Assembly|accessdate=2007-04-10}}</ref> In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).<ref>{{cite web|author=IEEE|url=http://www.computer.org/portal/web/certification/csda|title=CSDA|accessdate=2010-04-20}}</ref>  The [[Association for Computing Machinery|ACM]] had a professional certification program in the early 1980s,{{Citation needed|date=March 2009}} which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.<ref name="acm2000">{{cite web|url=http://www.cs.wm.edu/~coppit/csci690-spring2004/papers/selep_main.pdf|title=A Summary of the ACM Position on Software Engineering as a Licensed Engineering  Profession|last=ACM|date=July 17, 2000|publisher=Association for Computing Machinery (ACM)|accessdate=2009-03-03|quote=At its meeting in May 2000, the Council further concluded that the framework of a licensed professional engineer, originally developed for civil engineers, does not match the professional industrial practice of software engineering. Such licensing practices would give false assurances of competence even if the body of knowledge were mature; and would preclude many of the most qualified software engineers from becoming licensed.}} {{Dead link|date=October 2010|bot=H3llBot}}</ref>  In 2012, [http://validatedguru.com Validated Guru] began offering the ''Certified Software Developer (VGCSD)'' certification; which is heavily influenced by the global community.  In the U.K. the [[British Computer Society]] has developed a legally recognized professional certification called ''Chartered IT Professional (CITP)'', available to fully qualified Members (''MBCS''). Software engineers may be eligible for membership of the [[Institution of Engineering and Technology]] and so qualify for Chartered Engineer status.  In Canada the [[Canadian Information Processing Society]] has developed a legally recognized professional certification called ''Information Systems Professional (ISP)''.<ref>{{cite web|author=Canadian Information Processing Society|url=http://www.cips.ca/standards/isp|title=I.S.P. Designation|accessdate=2007-03-15}}</ref> In Ontario, Canada, Software Engineers who graduate from a ''Canadian Engineering Accreditation Board (CEAB)'' accredited program, successfully complete PEO's (''Professional Engineers Ontario'') Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the ''Professional Engineers Ontario'' and can become Professional Engineers P.Eng.<ref>{{cite web|url=http://www.peo.on.ca |title=Professional Engineers Ontario: Welcome to PEO's website |publisher=Peo.on.ca |date= |accessdate=2012-03-25}}</ref>  ===Impact of globalization=== The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / timezone difference that prevented human interaction between clients and developers, but also the lower quality of the software developed by the outsourcing companies and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the [[developed world]] avoid education related to software engineering because of the fear of [[offshore outsourcing]] (importing software products or services from other countries) and of being displaced by [[Foreign Worker Visa|foreign visa workers]].<ref>{{cite web|author=Patrick Thibodeau |url=http://www.computerworld.com/printthis/2006/0,4814,111202,00.html |title=As outsourcing gathers steam, computer science interest wanes |publisher=Computerworld.com |date=2006-05-05 |accessdate=2012-03-25}}</ref> Although statistics do not currently show a threat to software engineering itself; a related career, [[computer programming]] does appear to have been affected.<ref>{{cite web|url=http://www.bls.gov/oco/ocos110.htm#outlook |title=Computer Programmers |publisher=Bls.gov |date= |accessdate=2012-03-25}}</ref><ref>{{cite web|last=Mullins |first=Robert |url=http://www.infoworld.com/article/07/03/13/HNslowsoftdev_1.html |title=Software developer growth slows in North America|publisher=InfoWorld |date=2007-03-13 |accessdate=2012-03-25}}</ref>  Nevertheless, the ability to smartly leverage offshore and near-shore resources via the [[follow-the-sun]] workflow has improved the overall operational capability of many organizations.<ref>{{cite web|url=http://www.cognizant.com/html/content/news/GartnerMQ_Cognizant.pdf |title=Gartner Magic Quadrant |publisher=Cognizant.com |date= |accessdate=2012-03-25}}</ref> When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.  ==Education== A knowledge of [[programming]] is a pre-requisite to becoming a software engineer. In 2004 the [[IEEE Computer Society]] produced the [[SWEBOK]], which has been published as ISO/IEC Technical Report 19759:2004, describing the body of knowledge that they believe should be mastered  by a graduate software engineer with four years of experience.<ref>{{cite book | editor1-last = Abran |editor1-first=  Alain | title = Guide to the Software Engineering Body of Knowledge | publisher = IEEE Computer Society | location = Los Alamitos |year= 2005 | isbn = 0-7695-2330-7 | url= http://www.computer.org/portal/web/swebok |accessdate=2010-09-13|origyear= 2004|chapter= Chapter 1: Introduction to the Guide|chapterurl= http://www.computer.org/portal/web/swebok/html/ch1 |quote= The total volume of cited literature is intended to be suitable for mastery through the completion of an undergraduate education plus four years of experience.}}</ref> Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the [[CCSE]], and updated in 2004.<ref>{{cite web|url=http://sites.computer.org/ccse/ |title=SE2004 Software Engineering Curriculum |publisher=Sites.computer.org |date=2003-09-30 |accessdate=2012-03-25}}</ref> A number of universities have Software Engineering degree programs; {{As of|2010|lc=on}}, there were 244 Campus programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.<ref>[http://www.gradschools.com/search-programs/software-engineering] Degree programs in Software Engineering</ref>  In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through [[military service]] in software engineering.  ==Comparison with other disciplines== Major differences between software engineering and other engineering disciplines, according to some researchers, result from the costs of fabrication.<ref>{{cite book | last1 = Young | first1 = Michal  | last2 = Faulk | first2 =Stuart  | year = 2010 |title= Proceedings of the FSE/SDP workshop on Future of software engineering research (FoSER '10) |format=PDF | accessdate = 2011-02-25 |publisher=ACM| isbn=978-1-4503-0427-6|doi =10.1145/1882362.1882451| pages= 439–442 |chapter= Sharing What We Know About Software Engineering |chapterurl= http://www.ics.uci.edu/~jajones/papers/p439.pdf | quote = The essential distinction between software and other engineered artifacts has always been the absence of fabrication cost. In conventional engineering of physical artifacts, the cost of materials and fabrication has dominated the cost of design and placed a check on the complexity of artifacts that can be designed. When one bottleneck is removed, others appear, and software engineering has therefore faced the essential challenges of complexity and the cost of design to an extent that conventional engineering has not. Software engineering has focused on issues in managing complexity, from process to modular design to cost-effective verification, because that is the primary leverage point when the costs of materials and fabrication are nil.}}</ref>  ==Subdisciplines== Software engineering can be divided into ten subdisciplines. They are:<ref name="BoDu04"/> * [[Software requirements]]: The elicitation, analysis, specification, and validation of [[requirements]] for [[software]]. * [[Software design]]: The process of defining the architecture, components, interfaces, and other characteristics of a system or component.  It is also defined as the result of that process. * [[Software construction]]: The detailed creation of working, meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. * [[Software testing]]: The dynamic verification of the behavior of a program on a finite set of test cases, suitably selected from the usually infinite executions domain, against the expected behavior. * [[Software maintenance]]: The totality of activities required to provide cost-effective support to software. * [[Software configuration management]]: The identification of the configuration of a system at distinct points in time for the purpose of systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration throughout the system life cycle. * Software engineering management: The application of management activities—planning, coordinating, measuring, monitoring, controlling, and reporting—to ensure that the development and maintenance of software is systematic, disciplined, and quantified. * [[Software engineering process]]: The definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. * Software engineering tools and methods: The computer-based tools that are intended to assist the software life cycle processes, see [[Computer Aided Software Engineering]], and the methods which impose structure on the software engineering activity with the goal of making the activity systematic and ultimately more likely to be successful. * [[Software quality]]: The degree to which a set of inherent characteristics fulfills requirements.  ==Related disciplines== Software engineering is a direct subfield of computer science and has some relations with [[management science]]. It is also considered a part of overall [[systems engineering]].  ===Systems engineering=== [[Systems engineering|Systems engineers]] deal primarily with the overall system requirements and design, including hardware and human issues. They are often concerned with partitioning functionality to hardware, software or human operators.  Therefore, the output of the systems engineering process serves as an input to the software engineering process.  ===Computer software engineers=== Computer Software Engineers are usually systems level (software engineering, information systems) computer science or software level computer engineering graduates. This term also includes general computer science graduates with a few years of practical on the job experience involving software engineering.  ==See also== {{Portal|Software|Software Testing}} {{Main|Outline of software engineering}} * [[Bachelor of Science in Information Technology]] * [[Bachelor of Software Engineering]] * [[List of software engineering conferences]] * [[List of software engineering publications]] * [[Software craftsmanship]]  ==References== {{Reflist|30em}}  ==Further reading== * {{Cite book|last= Ghezzi|first=Carlo   | title = Fundamentals of Software Engineering|origyear = 1991   | edition = 2nd (International) | year = 2003   | publisher = Pearson Education @ Prentice-Hall | coauthors = Mehdi Jazayeri, Dino Mandrioli}} *{{Cite book|last= Jalote|first=Pankaj|authorlink=Pankaj Jalote   | title = An Integrated Approach to Software Engineering |origyear = 1991   | url = http://www.springer.com/east/home?SGWisbn=5-102-22-52090005-0&changeHeader=true   | edition = 3rd | year = 2005   | publisher =  Springer | location =   | isbn = 0-387-20881-X }} *{{Cite book|last= Pressman|first=Roger S|authorlink=Roger S. Pressman   | title = Software Engineering: A Practitioner's Approach |origyear =   | edition = 6th | year = 2005   | publisher = McGraw-Hill | location = Boston, Mass   | isbn = 0-07-285318-2}} *{{Cite book|last= Sommerville|first=Ian|authorlink=Ian Sommerville (academic)   | title = Software Engineering|origyear = 1982   | url = http://www.pearsoned.co.uk/HigherEducation/Booksby/Sommerville/   | edition = 8th | year = 2007   | publisher = Pearson Education | location = Harlow, England   | isbn = 0-321-31379-8 }}  ==External links== {{Commons category|Software engineering}} {{wikibooks|Introduction to Software Engineering}} * [http://www.acm.org/education/education/curric_vols/CC2005-March06Final.pdf Computing Curricula 2005: The Overview Report] by The Joint Task Force for Computing Curricula ACM/AIS/IEEE-CS * [http://sites.computer.org/ccse/ Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering] by The Joint Task Force on Computing Curricula ACM/IEEE-CS * [http://www.acmtyc.org/WebReports/SEreport/ Guidelines for Associate-Degree Transfer Curriculum in Software Engineering] by The  ACM Two-Year College Education Committee and The IEEE Computer Society/ACM Joint Task Force on Software Engineering * [http://www.swebok.org/ Guide to the Software Engineering Body of Knowledge] * [http://www.bls.gov/oco/ocos267.htm Computer Software Engineers] - Definition and statistics from the U.S. [[Bureau of Labor Statistics]] * [http://StudentProjectGuide.info/ A Student's Guide to Software Engineering Projects] - a free online guide for students taking SE project courses * [http://OpenSDLC.org/ The Open Systems Engineering and Software Development Life Cycle Framework] OpenSDLC.org the integrated Creative Commons SDLC {{Software engineering}} {{Computer science}} {{Technology}}  {{DEFAULTSORT:Software Engineering}} [[Category:Software engineering| ]]  [[af:Sagteware-ingenieurswese]] [[am:የሶፍትዌር አሠራር]] [[ar:هندسة البرمجيات]] [[ast:Inxeniería del software]] [[bg:Софтуерно инженерство]] [[bs:Softverski inženjering]] [[ca:Enginyeria de programari]] [[cs:Softwarové inženýrství]] [[da:Softwareudvikling]] [[de:Softwaretechnik]] [[el:Μηχανική λογισμικού]] [[es:Ingeniería de software]] [[eu:Software-ingeniaritza]] [[fa:مهندسی نرم‌افزار]] [[fr:Génie logiciel]] [[ga:Innealtóireacht bogearraí]] [[gv:Jeshaghteyrys cooid vog]] [[gl:Enxeñaría de software]] [[ko:소프트웨어 공학]] [[hy:Ծրագրային ապահովման ճարտարագիտություն]] [[hi:सॉफ्टवेयर इंजीनियरी]] [[hr:Programsko inženjerstvo]] [[id:Rekayasa perangkat lunak]] [[is:Hugbúnaðarverkfræði]] [[it:Ingegneria del software]] [[he:הנדסת תוכנה]] [[kk:Бағдарламалық жасақтама инжинирингі]] [[sw:Uundaji bidhaa pepe za tarakilishi]] [[ku:Endezyariya nivîsbariyê]] [[lo:Software engineering]] [[lv:Programminženierija]] [[lt:Programų inžinerija]] [[mk:Софтверско инженерство]] [[ml:സോഫ്റ്റ്‌വെയർ എഞ്ചിനീയറിങ്]] [[ms:Pembangunan perisian]] [[mn:Програм хангамжийн инженерчлэл]] [[nl:Software engineering]] [[ja:ソフトウェア工学]] [[no:Programvareutvikling]] [[pl:Inżynieria oprogramowania]] [[pt:Engenharia de software]] [[ro:Inginerie software]] [[ru:Инженерия программного обеспечения]] [[sq:Inxhinieria Softuerike]] [[si:මෘදුකාංග ඉංජිනේරු ශිල්පය]] [[sk:Softvérové inžinierstvo]] [[sr:Инжењеринг софтвера]] [[su:Rékayasa software]] [[fi:Ohjelmistotuotanto]] [[sv:Programvaruutveckling]] [[tl:Software engineering]] [[ta:மென்பொருட் பொறியியல்]] [[th:วิศวกรรมซอฟต์แวร์]] [[ti:ሶፍትዌር ምህንድስና]] [[tr:Yazılım mühendisliği]] [[uk:Програмна інженерія]] [[vi:Công nghệ phần mềm]] [[war:Inhenyerya hin software]] [[yi:ווייכווארג אינזשעניריע]] [[zh:软件工程]]
{{about|accurate manipulation of mathematical expressions|simultaneous exploration of the many paths computer execution can take|symbolic execution}}  '''Symbolic computation''' or '''algebraic computation''' or '''computer algebra''' relates to [[algorithm]]s and [[software]] for manipulating [[mathematics|mathematical]] [[expression (mathematics)|expressions]] and [[equation]]s in [[symbol|symbolic]] form, as opposed to manipulating the approximations of specific [[numerical analysis|numerical]] quantities represented by those symbols. [[Software]] applications that perform symbolic calculations are called '''[[computer algebra system]]s'''.   These systems might be used for symbolic [[symbolic integration|integration]] or [[derivative|differentiation]], substitution of one expression into another, simplification of an expression, etc., for most operations of [[calculus]] and, more generally, for every computation with mathematical objects for which algorithms are known. Computer algebra software is widely used in many scientific and engineering domains.  Symbolic computation is also sometimes referred to as '''symbolic manipulation''', '''symbolic processing''', '''symbolic mathematics''', or '''symbolic algebra''', but these terms also refer to non-computational manipulation.  == See also == * [[Automated theorem prover]] * [[Computer-assisted proof]] * [[Proof checker]] * [[Model checker]] * [[Symbolic-numeric computation]] * [[Symbolic simulation]]  == References == {{reflist}}  * [http://www.risc.jku.at/about/editorial/editorial.pdf Symbolic Computation (An Editorial)], Bruno Buchberger, Journal of Symbolic Computation (1985) 1, pp. 1-6.  * [http://www.csd.uwo.ca/~watt/pub/reprints/2006-tc-sympoly.pdf Making Computer Algebra More Symbolic (Invited)], Stephen M. Watt, pp. 43-49, Proc. Transgressive Computing 2006: A conference in honor or Jean Della Dora, (TC 2006), April 24-26 2006, Granada Spain.  == External links ==  * [http://www.cs.mu.oz.au/~schachte/lpanalysis.html A Gentle Introduction to Static Analysis and Logic Programming] showing an example of application of symbolic computation to perform static program analysis. * [http://www.symbolicnet.org/toc.html Information on Symbolic Computing] A good site for beginners * [[jHepWork]] A free program for symbolic calculations * [http://integrals.wolfram.com Wolfram Integrator] — Free online symbolic integration with [[Mathematica]] * [http://user.mendelu.cz/marik/maw/index.php?lang=en&form=integral Mathematical Assistant on Web] — symbolic computations online. Allows to integrate in small steps (with hints for next step (integration by parts, substitution, partial fractions, application of formulas and others), powered by [[Maxima (software)|Maxima]] * [http://wims.unice.fr/wims/wims.cgi?module=tool/analysis/function.en Function Calculator] from [[WWW Interactive Multipurpose Server|WIMS]] * [http://www.numberempire.com/integralcalculator.php Online integral calculator]  {{FOLDOC}} {{math-stub}}  [[Category:Computer algebra]]  [[ar:حساب رمزي]] [[cs:Symbolický výpočet]] [[de:Symbolische Mathematik]] [[fa:محاسبات نمادین]] [[fr:Calcul formel]] [[ko:기호계산]] [[nl:Symbolische wiskunde]] [[pl:Obliczenia symboliczne]] [[pt:Matemática simbólica]] [[ru:Символьные вычисления]] [[fi:Symbolinen laskenta]]
{{refimprove|date=August 2010}}  A '''system administrator''', '''IT systems administrator''', '''systems administrator''', or '''sysadmin''' is a person employed to maintain and operate a computer system and/or network. System administrators may be members of an information technology (IT) or Electronics and Communication Engineering department.  The duties of a system administrator are wide-ranging, and vary widely from one organization to another. Sysadmins are usually charged with installing, supporting and maintaining [[server (computing)|server]]s or other computer systems, and planning for and responding to service outages and other problems. Other duties may include [[Scripting language|scripting]] or light [[computer programming|programming]], [[project management]] for systems-related projects, supervising or training computer operators, and being the consultant for computer problems beyond the knowledge of [[technical support]] staff. To perform his or her job well, a system administrator must demonstrate a blend of technical skills and responsibility.  ==Related fields== Many organizations staff other jobs related to system administration. In a larger company, these may all be separate positions within a computer support or Information Services (IS) department. In a smaller group they may be shared by a few sysadmins, or even a single person.  * A [[database administrator]] ''(DBA) maintains'' a [[database]] system, and is responsible for the integrity of the data and the efficiency and performance of the system. * A [[network administrator]] maintains network infrastructure such as [[network switch|switches]] and [[network router|router]]s, and diagnoses problems with these or with the behavior of network-attached computers. * A [[computer security|security administrator]] is a specialist in computer and network security, including the administration of security devices such as firewalls, as well as consulting on general security measures. * A [[web administrator]] maintains web server services (such as Apache or IIS) that allow for internal or external access to web sites. Tasks include managing multiple sites, administering security, and configuring necessary components and software. Responsibilities may also include software [[change management]]. * [[Technical support]] staff respond to individual users' difficulties with computer systems, provide instructions and sometimes training, and diagnose and solve common problems. * A [[computer operator]] performs routine maintenance and upkeep, such as changing backup tapes or replacing failed drives in a [[RAID]]. Such tasks usually require physical presence in the room with the computer; and while less skilled than sysadmin tasks require a similar level of trust, since the operator has access to possibly sensitive data. * A [[postmaster (computing)|postmaster]] is the administrator of a [[mail server]].  In some organizations, a person may begin as a member of technical support staff or a computer operator, then gain experience on the job to be promoted to a sysadmin position.  == Training ==  Unlike many other professions, there is no single path to becoming a system administrator. Many system administrators have a degree in a related field: [[computer science]], [[information technology]], [[computer engineering]], information system management, or even a trade school program. Other schools have offshoots of their Computer Science program specifically for system administration.   Some schools have started offering undergraduate degrees in System Administration. The first, [[Rochester Institute of Technology]][http://nssa.rit.edu/~nssa/?q=node/8] started in 1992. Others such as [[Rensselaer Polytechnic Institute]], [[University of New Hampshire]][http://www.cs.unh.edu/bsit.htm], [[Marist College]], and [[Drexel University]] have more recently offered degrees in Information Technology. [[Symbiosis Institute of Computer Studies and Research (SICSR)]] in Pune, India offers Masters degree in Computers Applications with a specialization in System Administration. The [[University of South Carolina]][http://www.hrsm.sc.edu/iit/] offers an Integrated Information Technology B.S. degree specializing in [[Microsoft]] product support.  {{As of|2011}}, only five U.S. universities, Rochester Institute of Technology [http://nssa.rit.edu/~nssa/nssa/grad/index.maml], [[New York City College of Technology]], [[Tufts]], [[Michigan Tech]], and [[Florida State University]] [http://www.cs.fsu.edu/current/grad/cnsa_ms.php] have [[graduate school|graduate]] programs in system administration.{{Citation needed|date=February 2007}} In [[Norway]], there is a special English-taught MSc program organized by [[Oslo University College]] [http://www.hio.no/Studietilbud/Masterstudier/Master-Programme-in-Network-and-System-Administration] in cooperation with [[Oslo University]], named "Masters programme in Network and System Administration." [[University of Amsterdam]] (UvA) offers a similar program in cooperation with [[Hogeschool van Amsterdam]] (HvA) named "Master System and Network Engineering". In [[Israel]], the [[Israel Defense Forces|IDF]]'s ntmm course in considered a prominent way to train System administrators. <ref>[https://www.os3.nl UvA Master SNE homepage]</ref> However, many other schools offer related graduate degrees in fields such as network systems and computer security.   One of the primary difficulties with teaching system administration as a formal university discipline, is that the industry and technology changes much faster than the typical textbook and coursework certification process. By the time a new textbook has spent years working through approvals and committees, the specific technology for which it is written may have changed significantly or become obsolete.  In addition, because of the practical nature of system administration and the easy availability of [[open-source]] [[Server (computing)|server]] software, many system administrators enter the field self-taught. Some learning institutions are reluctant to, what is in effect, teach hacking to undergraduate level students.  Generally, a prospective  will be required to have some experience with the computer system he or she is expected to manage. In some cases, candidates are expected to possess industry certifications such as the Microsoft [[Microsoft Certified Systems Administrator|MCSA]], [[MCSE]], [[MCITP]], Red Hat [[Red Hat Certified Engineer|RHCE]], Novell [[Certified Novell Administrator|CNA]], [[Certified Novell Engineer|CNE]], Cisco [[CCNA]] or [[CompTIA]]'s [[A%2B_certification#A.2B|A ]] or [[Network%2B#Network.2B|Network ]], [[Sun Certified]] [[SCNA]], [[Linux Professional Institute]] among others.  Sometimes, almost exclusively in smaller sites, the role of system administrator may be given to a skilled user in addition to or in replacement of his or her duties.  For instance, it is not unusual for a mathematics or computing teacher to serve as the system administrator of a secondary school.  == Skills == ''Some of this section is from the [http://www.bls.gov/oco/ocos305.htm Occupational Outlook Handbook], 2010-11 Edition, which is in the [[public domain]] as a [[work of the United States Government]].'' The ''subject matter'' of system administration includes computer systems and the ways people use them in an organization. This entails a knowledge of [[operating system]]s and [[computer application|application]]s, as well as hardware and software [[troubleshooting]], but also knowledge of the purposes for which people in the organization use the computers.  Perhaps the most important skill for a system administrator is [[problem solving]] -- frequently under various sorts of constraints and stress. The sysadmin is on call when a computer system goes down or malfunctions, and must be able to quickly and correctly diagnose what is wrong and how best to fix it.  System administrators are not [[software engineer]]s or [[software development|developer]]s. It is not usually within their duties to design or write new application software. However, sysadmins must understand the behavior of software in order to deploy it and to troubleshoot problems, and generally know several [[programming language]]s used for scripting or automation of routine tasks.   Particularly when dealing with [[Internet]]-facing or business-critical systems, a sysadmin must have a strong grasp of [[computer security]]. This includes not merely deploying software patches, but also preventing break-ins and other security problems with preventive measures. In some organizations, computer security administration is a separate role responsible for overall security and the upkeep of [[Firewall (computing)|firewall]]s and [[intrusion detection system]]s, but all sysadmins are generally responsible for the security of computer systems.   == Duties of a system administrator == A system administrator's responsibilities might include:  * Analyzing [[Computer data logging|system logs]] and identifying potential issues with computer systems. * Introducing and integrating new technologies into existing [[data center]] environments. * Performing routine audits of systems and software. * Performing [[backup]]s. * Applying [[operating system]] updates, patches, and configuration changes. * Installing and configuring new [[Computer hardware|hardware]] and [[Computer software|software]]. * Adding, removing, or updating [[user account]] information, resetting [[password]]s, etc. * Answering technical queries and dealing with often frustrated users. * Responsibility for [[computer security|security]]. * Responsibility for [[documentation|documenting]] the configuration of the system. * [[Troubleshooting]] any reported problems. * System [[performance tuning]]. * Ensuring that the network infrastructure is up and running.  In larger organizations, some tasks listed above may be divided among different system administrators or members of different organizational groups. For example, a dedicated individual(s) may apply all system upgrades, a [[Quality control|Quality Assurance (QA)]] team may perform testing and validation, and one or more [[technical writer]]s may be responsible for all technical documentation written for a company.  In smaller organizations, the system administrator can also perform any number of duties elsewhere associated with other fields:  *[[Technical support]] *[[Database administrator]] (DBA) *[[Network administrator]]/analyst/specialist *[[Application analyst]] *[[Computer Security|Security administrator]] *[[Programmer]]  System administrators, in larger organizations, tend not to be [[systems architect|system architect]]s, system engineers, or [[system designer]]s.  However, like many roles in this field, [[wikt:demarcation|demarcations]] between system administration and other technical roles often are not well defined in smaller organizations.  Even in larger organizations, senior system administrators often have skills in these other areas as a result of their working experience.  In smaller organizations, IT/computing specialties are less often discerned in detail, and the term ''system administrator'' is used in a rather generic way &mdash; they are the people who know how the computer systems work and can respond when something fails.  == System Administrator privileges ==  The term "system administrator" may also be used to describe a security privilege which is assigned to a user or users of a specific computer, server, network or other IT System.   The Administrator level of system access permits that user to gain access to, and perform high level configuration features of the system.    This user privilege level is more commonly referred to within a computer or IT system as "administrator" (without the epithet "system"). It may also be called [[superuser]] or root.  For example a computer may have a user named "Administrator" or "Root" which has a security level sufficient to install software, or give other users access to the system.  Alternatively a user of a system may be assigned to an "Administrators" group, membership of which grants them the same privilege as the Administrator user.  These users may be referred to as System Administrators, referring only to the system privilege level, rather than the job function.  For security reasons, the name of an Administrator user or Administrators security group is often changed locally so that it is less easy to guess, in order to reduce [[Vulnerability (computing)|system vulnerability]] to access by [[Hacker (computer security)|hackers]].{{Citation needed|date=March 2012}}  ==See also== {{Portal|Information technology|Computer Science}} * [[Red Hat Certification Program]] * [[Apple certification]] * [[Microsoft Certified Professional]] * [[Application Service Management]] * [[Bastard Operator From Hell]] (BOFH) * [[Forum administrator]] * [[LISA (conference)]] * [[superuser]] * [[System Administrator Appreciation Day]] * [[League of Professional System Administrators]] * [[SAGE (organization)]] * [[Sysop]]  ==References== {{Reflist}}  == Further reading == <!-- alphabetical order please --> * Essential System Administration (O'Reilly), 3rd Edition, 2001, by [[Æleen Frisch]] * Essential Linux Administration (Cengage Press): A Comprehensive Guide for Beginners, 2011 by [[Chuck Easttom]] * Principles of Network and System Administration (J. Wiley & Sons), 2000,2003(2ed), by [[Mark Burgess (computer scientist)|Mark Burgess]] * The Practice of System and Network Administration (Addison-Wesley), 2nd Edition (July 5, 2007), by [[Tom Limoncelli|Thomas A. Limoncelli]], [[Christine Hogan]] and [[Strata R. Chalup]] * Time Management for System Administrators (O'Reilly), 2005, by [[Tom Limoncelli|Thomas A. Limoncelli]] * UNIX and Linux System Administration Handbook (Prentice Hall), 4th Edition, 2010, by [[Evi Nemeth]], [[Garth Snyder]], [[Trent R. Hein]], [[Ben Whaley]]  == External links == {{Commons category|System administrator}}   {{Wiktionary|sysadmin}}   * [http://www.ideosphere.com/fx-bin/Claim?claim=ITJOBS "The Future of IT Jobs in America" article] * The US Department of Labor's description of [http://www.bls.gov/oco/ocos305.htm "Computer Network"] and [http://www.bls.gov/oco/ocos305.htm "Systems, and Database Administrators"] and statistics for employed [http://www.bls.gov/oco/oco2001.htm#emply "System administrator"] * [http://www.bsdcertification.org/ BSD Certification] * [http://www.itil.co.uk/ ITIL] for [[ITIL]] certification (part of [[Office of Government Commerce]]) * [http://www.sun.com/bigadmin/newsletter/ BigAdmin Newsletter] * [http://www.artduweb.com/ Art du web.com : the website of system administrator] * [http://www.administration-systeme.com/ Administration système : Vie et mort des sysadm ;)] * [http://linux-administration-pro.com Linux System Administration Information] * [http://www.isystemadmin.com A System Admin site to share Tools, Script, Plugins and Books] * [https://lopsa.org/content/sabok Guide to the System Administration Body of Knowledge] * [http://www.xiitec.com/blog/ Info Blog For System Administrator]  {{Use British English|date=June 2012}} {{Use dmy dates|date=June 2012}}    {{DEFAULTSORT:System Administrator}} [[Category:System administration|*]] [[Category:Computer occupations]] [[Category:Computer systems]]  [[ar:مسؤول النظام]] [[bg:Системен администратор]] [[bs:IT administrator]] [[ca:Administrador de sistemes]] [[da:Systemadministrator]] [[de:Systemadministrator]] [[es:Administrador de sistemas]] [[eu:Sistema administratzaile]] [[fr:Administrateur systèmes]] [[ko:시스템 관리자]] [[it:Sistemista]] [[he:מנהל מערכת]] [[kk:Жүйелік әкімші]] [[nl:Systeembeheerder]] [[ja:システムアドミニストレータ]] [[no:Systemadministrator]] [[mhr:Системвиктарыше]] [[pl:Administrator (informatyka)]] [[pt:Administrador de sistemas]] [[ru:Системный администратор]] [[simple:System administrator]] [[sl:Administrator operacijskega sistema]] [[sr:Sistemski administrator]] [[fi:Ylläpitäjä]] [[sv:Systemadministratör]] [[th:ผู้ดูแลระบบ]] [[uk:Системний адміністратор]] [[zh:系统管理员]]
{{Unreferenced|date=October 2008}} [[File:Imagen binaria.jpg|thumb|right|300px|Monochrome black/white image]] In [[imaging science]], '''image processing''' is any form of [[signal processing]] for which the input is an image, such as a [[photograph]] or [[video frame]]; the [[output]] of image processing may be either an image or a set of characteristics or [[parameter]]s related to the image.  Most image-processing techniques involve treating the image as a [[two-dimensional]] [[signal (electrical engineering)|signal]] and applying standard signal-processing techniques to it.   Image processing usually refers to [[digital image processing]], but [[Optical engineering|optical]] and [[analog image processing]] also are possible. This article is about general techniques that apply to all of them. The ''acquisition'' of images (producing the input image in the first place) is referred to as [[Imaging science|imaging]].  Closely related to image processing are [[computer graphics]] and [[computer vision]]. In computer graphics, images are manually ''made'' from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from ''natural'' scenes, as in most animated movies. Computer vision, on the other hand, is often considered ''high-level'' image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).  In modern sciences and technologies, images also gain much broader scopes due to the ever growing importance of scientific visualization (of often large-scale complex scientific/experimental data). Examples include [[microarray]] data in genetic research, or real-time multi-asset portfolio trading in finance.      ==See also== <!-- Please put links that apply strictly to DIGITAL image processing in its own article. --> * [[Photo manipulation]] * [[Near sets]] * [[Multidimensional systems]] * [[Comparison of image processing software]]  ==Further reading== * {{cite book  |author = Tinku Acharya and Ajoy K. Ray  |title =  Image Processing - Principles and Applications  |year = 2006  |url = http://books.google.co.in/books?id=smBw4-xvfrIC&lpg=PP1&dq=image%20processing%20ajoy%20ray&pg=PP1#v=onepage&q=&f=false  |publisher =  Wiley InterScience }} * {{cite book  |author = Wilhelm Burger and Mark J. Burge  |title = Digital Image Processing: An Algorithmic Approach Using Java  |publisher = [[Springer Science Business Media|Springer]]  |year = 2007  |url = http://www.imagingbook.com/  |isbn=1-84628-379-5 and ISBN 3-540-30940-3 }} * {{cite book  |author=R. Fisher, K Dawson-Howe, A. Fitzgibbon, C. Robertson, E. Trucco  |title=Dictionary of Computer Vision and Image Processing  |publisher=John Wiley  |year=2005  |isbn=0-470-01526-8 }} * {{cite book  |author=Bernd Jähne  |title=Digital Image Processing  |publisher=Springer  |year=2002  |isbn=3-540-67754-2 }} * {{cite book  |author=Tim Morris  |title=Computer Vision and Image Processing  |publisher=Palgrave Macmillan  |year=2004  |isbn=0-333-99451-5 }} * {{cite book  |author=Tony F. Chan and Jackie (Jianhong) Shen  |title=Image Processing and Analysis - Variational, PDE, Wavelet, and Stochastic Methods  |publisher=Society of Industrial and Applied Mathematics  |year=2005  |isbn=0-89871-589-X  }} * {{cite book  |author=Milan Sonka, Vaclav Hlavac and Roger Boyle  |title=Image Processing, Analysis, and Machine Vision  |publisher=PWS Publishing  |year=1999  |isbn=0-534-95393-X }}  ==External links== <!-- Put links about DIGITAL image processing in its own article. --> * [http://www.archive.org/details/Lectures_on_Image_Processing Lectures on Image Processing], by Alan Peters. Vanderbilt University. Updated 15 September 2011. * [http://www.hindawi.com/journals/ivp/ EURASIP Journal on Image and Video Processing] &mdash; Open Access journal on Image Processing * [http://www.ipol.im/ Image Processing On Line] – Open access journal with image processing algorithms, open source implementations and demonstrations * [http://www.computervisiononline.com/ Computer Vision Online] A good source for source codes, software packages, datasets, etc. related to image processing *[http://image-processing-is-fun.blogspot.com/ Image processing is fun!] A mathematical image processing blog * [http://splab.cz/immi IMMI - Rapidminer Image Mining Extension] - tool for image processing and image mining * [http://www.shinoe.com/population Population] open-source image processing library in C   * [http://iprg.co.in IPRG] Open group related to image processing research resources * [http://union-d.ru/projects/imagetools Image Tools] .NET/MONO Batch image processing tool *[http://intopii.com/into/ Open source cross-platform image processing framework in C   with JavaScript support]   [[Category:Image processing| *]]  [[ar:معالجة الصور الرقمية]] [[bs:Procesiranje slike]] [[ca:Retoc fotogràfic digital]] [[cs:Zpracování obrazu]] [[de:Bildverarbeitung]] [[el:Επεξεργασία εικόνας]] [[es:Retoque fotográfico]] [[fa:پردازش تصویر]] [[fr:Traitement d'images]] [[fy:Byldbewurking]] [[ko:영상 처리]] [[id:Pengolahan citra]] [[it:Ritocco fotografico]] [[he:עיבוד תמונה]] [[mn:Зургийн боловсруулалт]] [[my:ပုံရိပ်သခြင်း]] [[nl:Beeldverwerking]] [[ja:画像処理]] [[pt:Processamento de imagem]] [[ru:Обработка изображений]] [[simple:Image processing]] [[sr:Обрада снимака]] [[su:Pengolahan citra]] [[sv:Bildbehandling]] [[th:การประมวลผลภาพ]] [[tr:Görüntü işleme]] [[vi:Xử lý ảnh]] [[zh:图像处理]]
'''Microcode''' is a layer of hardware-level instructions or data structures involved in the implementation of higher level [[machine code]] instructions in many computers and other processors; it resides in special high-speed memory and translates machine instructions into sequences of detailed circuit-level operations. It helps separate the machine instructions from the underlying [[electronics]] so that instructions can be designed and altered more freely. It also makes it feasible to build complex multi-step instructions while still reducing the complexity of the electronic circuitry compared to other methods. Writing microcode is often called '''microprogramming''' and the microcode in a particular processor implementation is sometimes called a '''microprogram'''.  Modern microcode is normally written by an engineer during the processor design phase and stored in a ROM ([[read-only memory]]) or PLA ([[programmable logic array]])<ref>{{cite journal|last1=Manning|first1=B.M.|last2=Mitby|first2=J.S|last3=Nicholson|first3=J.O.|title=Microprogrammed Processor Having PLA Control Store|journal=IBM Technical Disclosure Bulletin|volume=22|issue=6|date=1979-11|url=http://www.computerhistory.org/collections/accession/102660026}}</ref> structure, or a combination of both.<ref>Often denoted a ROM/PLA control store in the context of usage in a CPU; {{cite web|title=J-11: DEC's fourth and last PDP-11 microprocessor design ... features ... ROM/PLA control store|url=http://simh.trailing-edge.com/semi/j11.html}}</ref> However, machines also exist which have some (or all) microcode stored in [[Static Random Access Memory|SRAM]] or [[flash memory]]. This is traditionally denoted a "writeable [[control store]]" in the context of computers. Complex digital processors may also employ more than one (possibly microcode based) control unit in order to delegate sub-tasks which must be performed (more or less) asynchronously in parallel. Microcode is generally not visible or changeable by a normal programmer, not even by an [[assembly language|assembly]] programmer. Unlike machine code which often retains some [[backwards compatibility|compatibility]] among different processors in a family, microcode only runs on the exact [[electronic circuit]]ry for which it is designed, as it constitutes an inherent part of the particular processor design itself.  More extensive microcoding has also been used to allow small and simple [[microarchitecture]]s to [[emulator|emulate]] more powerful architectures with wider [[word length]], more [[execution unit]]s and so on; a relatively simple way to achieve software compatibility between different products in a processor family.  Some hardware vendors, especially [[IBM]], use the term as a synonym for ''[[firmware]]'', so that all code in a device, whether microcode or [[machine code]], is termed microcode (such as in a [[hard drive]] for instance, which typically contains both).<ref>[http://download.boulder.ibm.com/ibmdl/pub/software/server/firmware/73lzx.html "Microcode Update for SCSI Hard Disk"]</ref>  == Overview == The elements composing a microprogram exist on a lower conceptual level than a normal application program. Each element is differentiated by the "micro" prefix to avoid confusion: microinstruction, microassembler, microprogrammer, microarchitecture, etc.  The microcode usually does not reside in the [[main memory]], but in a special high speed memory, called the [[control store]]. It might be either [[read-only memory|read-only]] or [[read-write memory]]. In the latter case the microcode would be loaded into the control store from some other storage medium as part of the initialization of the CPU, and it could be altered to correct bugs in the instruction set, or to implement new machine instructions.  Microprograms consist of series of microinstructions. These microinstructions control the CPU at a very fundamental level of hardware circuitry. For example, a single typical microinstruction might specify the following operations:  * Connect Register 1 to the "A" side of the [[Arithmetic logic unit|ALU]] * Connect Register 7 to the "B" side of the [[Arithmetic logic unit|ALU]] * Set the ALU to perform [[two's-complement]] addition * Set the ALU's carry input to zero * Store the result value in Register 8 * Update the "condition codes" with the ALU status flags ("Negative", "Zero", "Overflow", and "Carry") * Microjump to Micro[[Program counter|PC]] nnn for the next microinstruction  To simultaneously control all processor's features in one cycle, the microinstruction is often wider than 50 bits, e.g., 128 bits on a 360/85 with an emulator feature. Microprograms are carefully designed and optimized for the fastest possible execution, since a slow microprogram would yield a slow machine instruction which would in turn cause all programs using that instruction to be slow.  == The reason for microprogramming == Microcode was originally developed as a simpler method of developing the control logic for a computer.  Initially CPU [[instruction set]]s were "[[Hardwired control|hardwired]]". Each step needed to fetch, decode and execute the machine instructions (including any operand address calculations, reads and writes) was controlled directly by [[combinatorial logic]] and rather minimal [[sequential circuit|sequential]] state machine circuitry. While very efficient, the need for powerful instruction sets with multi-step addressing and complex operations (''see below'') made such "hard-wired" processors difficult to design and debug; highly encoded and varied-length instructions can contribute to this as well, especially when very irregular encodings are used.  Microcode simplified the job by allowing much of the processor's behaviour and programming model to be defined via microprogram routines rather than by dedicated circuitry. Even late in the design process, microcode could easily be changed, whereas hard wired CPU designs were very cumbersome to change, so this greatly facilitated CPU design.  From the 1940s to the late 1970s, much programming was done in [[assembly language]]; higher level instructions meant greater programmer productivity, so an important advantage of microcode was the relative ease by which powerful machine instructions could be defined.<ref>The ultimate extension of this were "Directly Executable High Level Language" designs. In these each statement of a high level language such as [[PL/I]] would be entirely and directly executed by microcode, without compilation. The [[IBM Future Systems project]] and [[Data General]] Fountainhead Processor were examples of this.</ref> During the 1970s, CPU speeds grew more quickly than memory speeds and numerous techniques such as [[memory block transfer]], [[memory pre-fetch]] and [[Memory hierarchy|multi-level cache]]s were used to alleviate this. High level machine instructions, made possible by microcode, helped further, as fewer more complex machine instructions require less memory bandwidth. For example, an operation on a character string could be done as a single machine instruction, thus avoiding multiple instruction fetches.  Architectures with instruction sets implemented by complex microprograms included the [[IBM]] [[System/360]] and [[Digital Equipment Corporation]] [[VAX]]. The approach of increasingly complex microcode-implemented instruction sets was later called [[Complex instruction set computer|CISC]]. An alternate approach, used in many [[microprocessor]]s, is to use [[Programmable logic array|PLA]]s or [[Read-only memory|ROM]]s (instead of combinatorial logic) mainly for instruction decoding, and let a simple state machine (without much, or any, microcode) do most of the sequencing.<ref>The [[MOS Technology 6502]] is an example of a microprocessor using a PLA for instruction decode and sequencing.  The PLA is visible in photomicrographs of the chip, such as those at the [http://www.visual6502.org/images/6502/ Visual6502.org project] (across top edge of die photo), and the operation of the FPGA can be seen in the transistor-level simulation on that site. </ref>  The various practical uses of microcode and related techniques (such as PLAs) have been numerous over the years, as well as approaches to where, and to which extent, it should be used. It is still used in modern CPU designs.  === Other benefits === A processor's microprograms operate on a more primitive, totally different and much more hardware-oriented architecture than the assembly instructions visible to normal programmers. In coordination with the hardware, the microcode implements the programmer-visible architecture. The underlying hardware need not have a fixed relationship to the visible architecture. This makes it possible to implement a given instruction set architecture on a wide variety of underlying hardware micro-architectures.  Doing so is important if binary program compatibility is a priority. That way previously existing programs can run on totally new hardware without requiring revision and recompilation. However there may be a performance penalty for this approach. The tradeoffs between application backward compatibility vs CPU performance are hotly debated by CPU design engineers.  The IBM System/360 has a 32-bit architecture with 16 general-purpose registers, but most of the System/360 implementations actually use hardware that implemented a much simpler underlying microarchitecture; for example, the System/360 Model 30 had 8-bit data paths to the [[arithmetic logic unit]] (ALU) and main memory and implemented the general-purpose registers in a special unit of higher-speed [[core memory]], and the System/360 Model 40 had 8-bit data paths to the ALU and 16-bit data paths to main memory and also implemented the general-purpose registers in a special unit of higher-speed core memory.  The Model 50 and Model 65 had full 32-bit data paths; the Model 50 implemented the general-purpose registers in a special unit of higher-speed core memory<ref>{{cite book|title=IBM System/360 Model 50 Functional Characteristics|url=http://bitsavers.org/pdf/ibm/360/funcChar/A22-6898-1_360-50_funcChar_1967.pdf|publisher=[[IBM]]|page=7|year=1967|accessdate=2011-09-20}}</ref> and the Model 65 implemented the general-purpose registers in faster transistor circuits.{{Citation needed|date=September 2011}}  In this way, microprogramming enabled IBM to design many System/360 models with substantially different hardware and spanning a wide range of cost and performance, while making them all architecturally compatible.  This dramatically reduced the amount of unique system software that had to be written for each model.  A similar approach was used by Digital Equipment Corporation in their VAX family of computers. Initially a 32-bit [[Transistor-transistor logic|TTL]] processor in conjunction with supporting microcode implemented the programmer-visible architecture. Later VAX versions used different microarchitectures, yet the programmer-visible architecture did not change.  Microprogramming also reduced the cost of field changes to correct defects ([[Computer bug|bug]]s) in the processor; a bug could often be fixed by replacing a portion of the microprogram rather than by changes being made to [[hardware logic]] and wiring.  ==History==  In 1947, the design of the [[Whirlwind (computer)|MIT Whirlwind]] introduced the concept of a [[control store]] as a way to simplify computer design and move beyond ''[[ad hoc]]'' methods. The control store was a [[diode matrix]]: a two-dimensional lattice, where one dimension accepted "control time pulses" from the CPU's internal clock, and the other connected to control signals on gates and other circuits. A "pulse distributor" would take the pulses generated by the CPU clock and break them up into eight separate time pulses, each of which would activate a different row of the lattice. When the row was activated, it would activate the control signals connected to it.<ref>{{Cite paper | author=Everett, R.R., and Swain, F.E. | title=Whirlwind I Computer Block Diagrams | publisher=MIT Servomechanisms Laboratory | year=1947 | version=Report R-127 | url=http://www.cryptosmith.com/wp-content/uploads/2009/05/whirlwindr-127.pdf |format=PDF| accessdate=2006-06-21}} </ref>  Described another way, the signals transmitted by the control store are being played much like a [[player piano]] roll. That is, they are controlled by a sequence of very wide words constructed of [[bit]]s, and they are "played" sequentially. In a control store, however, the "song" is short and repeated continuously.  In 1951 [[Maurice Wilkes]] enhanced this concept by adding ''conditional execution'', a concept akin to a [[Conditional (programming)|conditional]] in computer software. His initial implementation consisted of a pair of matrices, the first one generated signals in the manner of the Whirlwind control store, while the second matrix selected which row of signals (the microprogram instruction word, as it were) to invoke on the next cycle. Conditionals were implemented by providing a way that a single line in the control store could choose from alternatives in the second matrix. This made the control signals conditional on the detected internal signal. Wilkes coined the term '''microprogramming''' to describe this feature and distinguish it from a simple control store.  ==Examples of microprogrammed systems== * In common with many other complex mechanical devices, [[Charles Babbage|Charles Babbage's]] [[analytical engine]] used banks of cams to control each operation, i.e. it had a read-only control store. As such it deserves to be recognised as the first microprogrammed computer to be designed, even if it has not yet been realised in hardware. * The [[EMIDEC 1100]]<ref>{{cite web|url=http://www.emidec.org.uk/ |title=EMIDEC 1100 computer |publisher=Emidec.org.uk |date= |accessdate=2010-04-26}}</ref> reputedly used a hard-wired control store consisting of wires threaded through ferrite cores, known as 'the laces'. * Most models of the IBM System/360 series were microprogrammed: :* The Model 25 was unique among System/360 models in using the top 16k bytes of core storage to hold the control storage for the microprogram. The 2025 used a 16-bit microarchitecture with seven control words (or microinstructions). At power up, or full system reset, the microcode was loaded from the card reader. The IBM 1410 emulation for this model was loaded this way. :* The [[IBM 2030|Model 30]], the slowest model in the line, used an 8-bit microarchitecture with only a few hardware registers; everything that the programmer saw was emulated by the microprogram. The microcode for this model was also held on special punched cards, which were stored inside the machine in a dedicated reader per card, called "CROS" units (Capacitor Read-Only Storage). A second CROS reader was installed for machines ordered with 1620 emulation. :* The Model 40 used 56-bit control words. The 2040 box implements both the System/360 main processor and the multiplex channel (the I/O processor). This model used "TROS" dedicated readers similar to "CROS" units, but with an inductive pickup (Transformer Read-only Store). :* The Model 50 had two internal datapaths which operated in parallel: a 32-bit datapath used for arithmetic operations, and an 8-bit data path used in some logical operations. The control store used 90-bit microinstructions. :* The Model 85 had separate instruction fetch (I-unit) and execution (E-unit) to provide high performance. The I-unit is hardware controlled. The E-unit is microprogrammed; the control words are 108 bits wide on a basic 360/85 and wider if an emulator feature is installed. * The [[NCR 315]] was microprogrammed with hand wired ferrite cores (a [[Read-only memory|ROM]]) pulsed by a sequencer with conditional execution. Wires routed through the cores were enabled for various data and logic elements in the processor. * The Digital Equipment Corporation [[PDP-11]] processors, with the exception of the PDP-11/20, were microprogrammed.<ref>{{cite book|author=Daniel P. Siewiorek, [[C. Gordon Bell]], [[Allen Newell]]|title=Computer Structures: Principles and Examples|publisher=[[McGraw-Hill|McGraw-Hill Book Company]]|location=[[New York, NY]]|year=1982|isbn=0-07-057302-6}}</ref> * Many systems from the [[Burroughs Corporation|Burroughs]] were microprogrammed: :* The B700 "microprocessor" executed application-level opcodes using sequences of 16-bit microinstructions stored in main memory, each of these was either a register-load operation or mapped to a single 56-bit "nanocode" instruction stored in read-only memory. This allowed comparatively simple hardware to act either as a mainframe peripheral controller or to be packaged as a standalone computer. :* The [[B1700]] was implemented with radically different hardware including bit-addressable main memory but had a similar multi-layer organisation.  The operating system would preload the interpreter for whatever language was required. These interpreters presented different virtual machines for [[COBOL]], [[Fortran]], etc. * [[Microdata Corporation|Microdata]] produced computers in which the microcode was accessible to the user; this allowed the creation of custom assembler level instructions. Microdata's [[Pick operating system|Reality]] operating system design made extensive use of this capability. * The [[Nintendo 64]]'s [[Nintendo 64#Reality Co-Processor|Reality Co-Processor]], which serves as the console's [[graphics processing unit]] and audio processor, utilized microcode; it is possible to implement new effects or tweak the processor to achieve the desired output. Some well-known examples of custom microcode include [[Factor 5]]'s N64 port of the ''[[Indiana Jones and the Infernal Machine]]'', ''[[Star Wars: Rogue Squadron]]'' and ''[[Star Wars: Battle for Naboo]]''. * The VU0 and VU1 vector units in the [[Sony]] [[PlayStation 2]] are microprogrammable; in fact, VU1 was ''only'' accessible via microcode for the first several generations of the SDK.  ==Implementation== Each microinstruction in a microprogram provides the bits which control the functional elements that internally compose a CPU.  The advantage over a hard-wired CPU is that internal CPU control becomes a specialized form of a computer program.  Microcode thus transforms a complex electronic design challenge (the control of a CPU) into a less-complex programming challenge.  To take advantage of this, computers were divided into several parts:  A [[microsequencer]] picked the next word of the [[control store]].  A sequencer is mostly a counter, but usually also has some way to jump to a different part of the control store depending on some data, usually data from the [[instruction register]] and always some part of the control store.  The simplest sequencer is just a register loaded from a few bits of the control store.  A [[processor register|register]] set is a fast memory containing the data of the central processing unit.  It may include the program counter, stack pointer, and other numbers that are not easily accessible to the application programmer.  Often the register set is a triple-ported [[register file]], that is, two registers can be read, and a third written at the same time.  An [[arithmetic and logic unit]] performs calculations, usually addition, logical negation, a right shift, and logical AND.  It often performs other functions, as well.  There may also be a [[memory address register]] and a [[memory data register]], used to access the main [[computer storage]].  Together, these elements form an "[[execution unit]]".  Most modern [[Central processing unit|CPUs]] have several execution units.  Even simple computers usually have one unit to read and write memory, and another to execute user code.  These elements could often be brought together as a single chip. This chip came in a fixed width which would form a 'slice' through the execution unit. These were known as '[[bit slicing|bit slice]]' chips. The [[AMD Am2900]] family is one of the best known examples of bit slice elements.  The parts of the execution units, and the execution units themselves are interconnected by a bundle of wires called a [[Computer bus|bus]].  Programmers develop microprograms.  The basic tools are software: A [[microassembler]] allows a programmer to define the table of bits symbolically.  A [[simulator]] program executes the bits in the same way as the electronics (hopefully), and allows much more freedom to debug the microprogram.  After the microprogram is finalized, and extensively tested, it is sometimes used as the input to a computer program that constructs logic to produce the same data. This program is similar to those used to optimize a [[programmable logic device|programmable logic array]]. No known computer program can produce optimal logic, but even pretty good logic can vastly reduce the number of transistors from the number required for a ROM control store. This reduces the cost and power used by a CPU.  Microcode can be characterized as '''horizontal''' or '''vertical'''. This refers primarily to whether each microinstruction directly controls CPU elements (horizontal microcode), or requires subsequent decoding by [[combinatorial logic]] before doing so (vertical microcode). Consequently each horizontal microinstruction is wider (contains more bits) and occupies more storage space than a vertical microinstruction.  ===Horizontal microcode=== Horizontal microcode is typically contained in a fairly wide control store; it is not uncommon for each word to be 56 bits or more. On each tick of a sequencer clock a microcode word is read, decoded, and used to control the functional elements which make up the CPU.  In a typical implementation a horizontal microprogram word comprises fairly tightly defined groups of bits. For example, one simple arrangement might be:  {| class="wikitable" |- | register source A || register source B || destination register || [[arithmetic and logic unit]] operation || type of jump || jump address |}  For this type of micromachine to implement a JUMP instruction with the address following the opcode, the microcode might require two clock ticks; the engineer designing it would write microassembler source code looking something like this:      # Any line starting with a number-sign is a comment     # This is just a label, the ordinary way assemblers symbolically represent a      # memory address.  InstructionJUMP:        # To prepare for the next instruction, the instruction-decode microcode has already        # moved the program counter to the memory address register.  This instruction fetches        # the target address of the jump instruction from the memory word following the        # jump opcode, by copying from the memory data register to the memory address register.        # This gives the memory system two clock ticks to fetch the next         # instruction to the memory data register for use by the instruction decode.        # The sequencer instruction "next" means just add 1 to the control word address.     MDR, NONE, MAR,  COPY, NEXT, NONE        # This places the address of the next instruction into the PC.        # This gives the memory system a clock tick to finish the fetch started on the        # previous microinstruction.        # The sequencer instruction is to jump to the start of the instruction decode.     MAR, 1, PC, ADD,  JMP,  InstructionDecode        # The instruction decode is not shown, because it is usually a mess, very particular        # to the exact processor being emulated.  Even this example is simplified.        # Many CPUs have several ways to calculate the address, rather than just fetching        # it from the word following the op-code. Therefore, rather than just one        # jump instruction, those CPUs have a family of related jump instructions.  For each tick it is common to find that only some portions of the CPU are used, with the remaining groups of bits in the microinstruction being no-ops. With careful design of hardware and microcode this property can be exploited to parallelise operations which use different areas of the CPU, for example in the case above the ALU is not required during the first tick so it could potentially be used to complete an earlier arithmetic instruction.  ===Vertical microcode=== In vertical microcode, each microinstruction is encoded—that is, the bit fields may pass through intermediate combinatory logic which in turn generates the actual control signals for internal CPU elements (ALU, registers, etc.). In contrast, with horizontal microcode the bit fields themselves directly produce the control signals. Consequently vertical microcode requires smaller instruction lengths and less storage, but requires more time to decode, resulting in a slower CPU clock.  Some vertical microcodes are just the assembly language of a simple conventional computer that is emulating a more complex computer. Another form of vertical microcode has two fields: {| class="wikitable" |- | field select || field value |}  The "field select" selects which part of the CPU will be controlled by this word of the control store. The "field value" actually controls that part of the CPU. With this type of microcode, a designer explicitly chooses to make a slower CPU to save money by reducing the unused bits in the control store; however, the reduced complexity may increase the CPU's clock frequency, which lessens the effect of an increased number of cycles per instruction.  As transistors became cheaper, horizontal microcode came to dominate the design of CPUs using microcode, with vertical microcode no longer being used.  ==Writable control stores== A few computers were built using "writable microcode" — rather than storing the microcode in ROM or hard-wired logic, the microcode was stored in a RAM called a ''Writable Control Store'' or ''WCS''. Such a computer is sometimes called a ''Writable Instruction Set Computer'' or ''WISC''.<ref> [http://www.ece.cmu.edu/~koopman/forth/rochester_87.pdf "Writable instruction set, stack oriented computers: The WISC Concept"] article by Philip Koopman Jr. 1987 </ref> Many of these machines were experimental laboratory prototypes, such as the WISC CPU/16<ref> [http://www.ece.cmu.edu/~koopman/stack_computers/sec4_2.html "Architecture of the WISC CPU/16"] by Phil Koopman 1989 </ref> and the RTX 32P.<ref> [http://www.ece.cmu.edu/~koopman/stack_computers/sec5_3.html "Architecture of the RTX 32P"] by Philip Koopman 1989 </ref>  There were also commercial machines that used writable microcode, such as early [[Xerox PARC|Xerox]] workstations, the [[Digital Equipment Corporation|DEC]] [[VAX]] 8800 ("Nautilus") family, the [[Symbolics]] L- and G-machines, and a number of [[IBM]] [[System/370]] implementations. Some DEC [[PDP-10]] machines stored their microcode in SRAM chips (about 80 bits wide x 2 Kwords), which was typically loaded on power-on through some other front-end CPU.<ref>http://pdp10.nocrew.org/cpu/kl10-ucode.txt</ref>  Many more machines offered user-programmable writable control stores as an option (including the [[Hewlett-Packard|HP]] [[HP 2100|2100]], DEC [[PDP-11|PDP-11/60]] and [[Varian Data Machines]] V-70 series [[minicomputer]]s). The [[Mentec PDP-11#M11| Mentec M11]] and [[Mentec PDP-11#M1 | Mentec M1]] stored its microcode in SRAM chips, loaded on power-on through another CPU. The [[Data General Eclipse MV/8000]] ("Eagle") had a SRAM writable control store, loaded on power-on through another CPU.<ref>{{cite web|author=Mark Smotherman|title=CPSC 330 / The Soul of a New Machine|url=http://www.cs.clemson.edu/~mark/330/eagle.html|quote=4096 x 75-bit SRAM writeable control store: 74-bit microinstruction with 1 parity bit (18 fields)}}</ref>  WCS offered several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs could provide. User-programmable WCS allowed the user to optimize the machine for specific purposes.  Some CPU designs compile the instruction set to a writable [[RAM]] or [[Flash memory|FLASH]] inside the CPU (such as the [[Rekursiv]] processor and the [[Imsys]] [[Cjip]]),<ref>{{cite web|url=http://cpushack.net/CPU/cpu7.html |title=Great Microprocessors of the Past and Present (V 13.4.0) |publisher=Cpushack.net |date= |accessdate=2010-04-26}}</ref> or an FPGA ([[reconfigurable computing]]).   A CPU that uses microcode generally takes several clock cycles to execute a single instruction, one clock cycle for each step in the microprogram for that instruction. Some [[Complex instruction set computer|CISC]] processors include instructions that can take a very long time to execute. Such variations interfere with both [[interrupt]] [[latency (engineering)|latency]] and, what is far more important in modern systems, [[pipelining]].  Several Intel CPUs in the [[x86]] architecture family have writable microcode.<ref> [http://www.intel.com/Assets/PDF/manual/253668.pdf "Intel(R) 64 and IA-32 Architectures Software Developer’s Manual", Volume 3A: System Programming Guide, Part 1], chapter 9.11: "Microcode update facilities", December 2009. </ref> This has allowed bugs in the [[Intel Core 2]] microcode and Intel [[Xeon]] microcode to be fixed in software, rather than requiring the entire chip to be replaced. Such fixes can be installed by Linux,<ref>[http://urbanmyth.org/microcode/ "Intel Microcode Update Utility for Linux"]</ref> [[FreeBSD]],<ref>{{cite web|url=http://www.freebsd.org/cgi/cvsweb.cgi/ports/sysutils/devcpu/ |title=ports/sysutils/devcpu/ |publisher=Freebsd.org |date=2008-09-23 |accessdate=2010-04-26}}</ref> Microsoft Windows,<ref>[http://support.microsoft.com/kb/936357 "A microcode reliability update is available that improves the reliability of systems that use Intel processors"]</ref> or the motherboard BIOS.<ref>[http://www.intel.com/support/motherboards/server/sb/cs-021619.htm "BIOS Update required when Missing Microcode message is seen during POST"]</ref>  ==Microcode versus VLIW and RISC==  The design trend toward heavily microcoded processors with complex instructions began in the early 1960s and continued until roughly the mid-1980s. At that point the [[RISC]] design philosophy started becoming more prominent. This included the points:  *Programming has largely moved away from assembly level, so it's no longer worthwhile to provide complex instructions for productivity reasons. *Simpler instruction sets allow direct execution by hardware, avoiding the performance penalty of microcoded execution. *Analysis shows complex instructions are rarely used, hence the machine resources devoted to them are largely wasted. *The machine resources devoted to rarely-used complex instructions are better used for expediting performance of simpler, commonly-used instructions. *Complex microcoded instructions may require many clock cycles which vary, and are difficult to [[pipeline (computing)|pipeline]] for increased performance.  It should be mentioned that there are counter-points as well: * The complex instructions in heavily microcoded implementations may not take much extra machine resources, except for microcode space. For instance, the same ALU is often used to calculate an effective address as well as computing the result from the actual operands (e.g. the original [[Z80]], [[8086]], and others). * The simpler non-RISC instructions (i.e. involving direct memory [[operand]]s) are frequently used by modern compilers. Even immediate to stack (i.e. memory result) arithmetic operations are commonly employed. Although such memory operations, often with varying length encodings, are more difficult to pipeline, it is still fully feasible to do so - clearly exemplified by the [[i486]], [[AMD K5]], [[Cyrix 6x86]], etc. * Non-RISC instructions inherently perform more work per instruction (on average), and are also normally highly encoded, so they enable smaller overall size of the same program, and thus better use of limited cache memories. * Modern CISC/RISC implementations, e.g. [[x86]] designs, decode instructions into dynamically buffered [[micro-operation]]s with instruction encodings similar to traditional fixed microcode. Ordinary static microcode is used as hardware assistance for complex multistep operations such as auto-repeating instructions and for [[transcendental function]]s in the [[floating point unit]]; it is also used for special purpose instructions (such as [[CPUID]]) and internal control and configuration purposes. * The simpler instructions in CISC architectures are also directly executed in hardware in modern implementations.  Many [[RISC]] and [[Very long instruction word|VLIW]] processors are designed to execute every instruction (as long as it is in the cache) in a single cycle. This is very similar to the way CPUs with microcode execute one microinstruction per cycle. [[Very long instruction word|VLIW]] processors have instructions that behave similarly to very wide horizontal microcode, although typically without such fine-grained control over the hardware as provided by microcode. RISC instructions are sometimes similar to the narrow vertical microcode.  Microcoding remains popular in application-specific processors such as [[network processors]].  ==See also== *[[Firmware]] *[[Control unit]] *[[Finite-state machine]] (FSM) *[[Microsequencer]] *[[Microassembler]] *[[Control store]] *[[Execution unit]] *[[Arithmetic logic unit]] (ALU) *[[Floating-point unit]] (FPU) *[[Instruction pipeline]] *[[Superscalar]] *[[Microarchitecture]] *[[CPU design]]  ==References== {{reflist}} *{{cite journal | author=Smith, Richard E. | title=A Historical Overview of Computer Architecture | journal=Annals of the History of Computing | year=1988 | volume=10 | issue=4 | pages=277&ndash;303 | url=http://doi.ieeecomputersociety.org/10.1109/MAHC.1988.10039 | accessdate=2006-06-21  | doi = 10.1109/MAHC.1988.10039}} *{{Cite paper | author=Smotherman, Mark | title=A Brief History of Microprogramming | year=2005 | url=http://www.cs.clemson.edu/~mark/uprog.html | accessdate=2006-07-30}} *{{cite journal | author=[[Maurice Wilkes|Wilkes, M.V.]] | title=The Genesis of Microprogramming | journal=Annals of the History of Computing | year=1986 | volume=8 | issue=2 | pages=116&ndash;126 | url=http://doi.ieeecomputersociety.org/10.1109/MAHC.1986.10035 | accessdate=2006-08-07  | doi = 10.1109/MAHC.1986.10035}} *{{cite journal | author=[[Maurice Wilkes|Wilkes, M.V.]], and [[John Bentley Stringer|Stringer, J. B.]] | title=Microprogramming and the Design of the Control  Circuits in an Electronic Digital Computer | journal=Proc. Cambridge Phil. Soc | volume=49 | issue= pt. 2 | month=April | year=1953 | pages=230–238 | url=http://research.microsoft.com/~gbell/Computer_Structures_Principles_and_Examples/csp0174.htm | accessdate=2006-08-23 | doi=10.1017/S0305004100028322}} *{{cite book | author=Husson, S.S | title=Microprogramming Principles and Practices | publisher=Prentice-Hall |  year=1970 | isbn=0-13-581454-5}}  == Further reading == *  Tucker, S. G., [http://domino.research.ibm.com/tchjr/journalindex.nsf/a3807c5b4823c53f85256561006324be/758c1e6a8a3e5d0285256bfa00685a2f?OpenDocument "Microprogram control for SYSTEM/360"] ''IBM Systems Journal'', Volume 6, Number 4, pp.&nbsp;222-241 (1967)  ==External links== *{{cite web|url=http://www.mikrocodesimulator.de/index_eng.php |title=Mikrocodesimulator MikroSim 2010 |publisher=0/1-SimWare |date= |accessdate=2010-10-03}} *[http://c2.com/cgi/wiki?WritableInstructionSetComputer Writable Instruction Set Computer] *[http://www.research.ibm.com/journal/rd/102/ibmrd1002F.pdf Capacitor Read-only Store] *[http://www-03.ibm.com/ibm/history/exhibits/attic3/attic3_016.html Transformer Read-only Store]  [[Category:Instruction processing]] [[Category:Firmware]] [[Category:Central processing unit]]  {{CPU technologies}}  [[bs:Mikroprogram]] [[ca:Microcodi]] [[de:Mikrocode]] [[es:Microcódigo]] [[fa:ریزبرنامه‌سازی]] [[fr:Microprogrammation]] [[ko:마이크로코드]] [[hr:Mikroprogramiranje]] [[it:Microprogrammazione]] [[ml:മൈക്രോകോഡ്]] [[nl:Microcode]] [[ja:マイクロプログラム方式]] [[pl:Mikroprogram]] [[ru:Микрокод]] [[simple:Microcode]] [[sr:Микропрограм]] [[sh:Mikroprogramiranje]] [[fi:Mikro-ohjelma]] [[sv:Mikroprogram]] [[tr:Mikroprogramlama]] [[uk:Мікрокод]] [[zh:微程序]]
{{Unreferenced|date=December 2009}} '''[[Probability]] and [[statistics]]''' are two related but separate [[academic discipline]]s. [[Statistical analysis]] often uses [[probability distribution]]s, and the two topics are often studied together. However, [[probability theory]] contains much that is of mostly of [[mathematics|mathematical]] interest and not directly relevant to statistics. Moreover, many topics in statistics are independent of probability theory.  ==See also== *[[List of probability topics]] *[[List of statistical topics]] *[[Notation in probability and statistics]]  ==External links== *[http://wiki.stat.ucla.edu/socr/index.php/EBook Probability and Statistics EBook] *[http://www.cs.sunysb.edu/~skiena/jaialai/excerpts/node12.html Probability versus Statistics]  {{DEFAULTSORT:Probability And Statistics}} [[Category:Probability and statistics| ]]   {{Notstub}}  [[ar:علم الاحتمالات والإحصائيات]] [[eo:Probablo kaj statistiko]]
{{refimprove|date=February 2011}} {{Expert-subject|Computer science|date=January 2009}} {{Programming paradigms}} A '''programming [[paradigm]]''' is a fundamental style of [[computer programming]]. There are four main paradigms :  [[object-oriented programming|object-oriented]], [[imperative programming|imperative]],  [[functional programming|functional]] and [[logic programming]].<ref>[http://people.cs.aau.dk/~normark/prog3-03/html/notes/paradigms_themes-paradigm-overview-section.html Overview of the four main programming paradigms]</ref> In addition, [[aspect-oriented programming]] aims specifically to increase modularity by allowing the separation of cross-cutting concerns.<ref>"Aspect-Oriented Programming" "Kiczales, G.; Lamping, J; Mehdhekar, A; Maeda, C; Lopes, C. V.; Loingtier, J; Irwin, J. Proceedings of the European Conference on Object-Oriented Programming (ECOOP), Springer-Verlag LNCS 1241. June 1997."</ref>  == Overview == A ''programming model'' is an abstraction of a computer system. For example, the "[[von Neumann model]]" is a model used in traditional sequential computers. For [[parallel computing]], there are many possible models typically reflecting different ways processors can be interconnected. The most common are based on shared memory, distributed memory with message passing, or a hybrid of the two.  A [[programming language]] can support [[multi-paradigm programming language|multiple paradigms]]. For example, programs written in [[C  ]] or [[Object Pascal]] can be purely [[procedural programming|procedural]], or purely [[object-oriented programming|object-oriented]], or contain elements of both paradigms. Software designers and programmers decide how to use those paradigm elements.  In object-oriented programming, programmers can think of a program as a collection of interacting objects, while in [[functional programming]] a program can be thought of as a sequence of stateless function evaluations. When programming computers or systems with many processors, [[process-oriented programming]] allows programmers to think about applications as sets of concurrent processes acting upon logically shared [[data structure]]s.  Just as different groups in [[software engineering]] advocate different ''methodologies'', different [[programming language]]s advocate different ''programming paradigms''. Some languages are designed to support one particular paradigm ([[Smalltalk]] supports object-oriented programming, [[Haskell (programming language)|Haskell]] supports functional programming), while other programming languages support multiple paradigms (such as [[Object Pascal]], C  , [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[Visual Basic]], [[Common Lisp]], [[Scheme (programming language)|Scheme]], [[Perl]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], [[Oz (programming language)|Oz]] and [[F Sharp (programming language)|F#]]).  Many programming paradigms are as well known for what techniques they ''forbid'' as for what they enable. For instance, pure functional programming disallows the use of [[side-effect (computer science)|side-effects]], while [[structured programming]] disallows the use of the [[goto]] statement. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles.<ref name="rubin87goto">Frank Rubin published a criticism of Dijkstra's letter in the March 1987 CACM where it appeared under the title '' 'GOTO Considered Harmful' Considered Harmful''. {{cite journal | author = Frank Rubin | year = 1987 | month = March | url = http://www.ecn.purdue.edu/ParaMount/papers/rubin87goto.pdf |format=PDF| title = 'GOTO Considered Harmful' Considered Harmful | journal = Communications of the ACM | volume = 30 | issue = 3 | pages = 195–196 | doi = 10.1145/214748.315722}}</ref> Avoiding certain techniques can make it easier to prove theorems about a program's correctness&mdash;or simply to understand its behavior.  == Multi-paradigm programming language == {{see also|List of multi-paradigm programming languages}} A ''multi-paradigm programming language'' is a [[programming language]] that supports more than one programming paradigm. As [[Leda (programming language)|Leda]] designer [[Timothy Budd]] puts it: "The idea of a multiparadigm language is to provide a framework in which programmers can work in a variety of styles, freely intermixing constructs from different paradigms." The design goal of such languages is to allow programmers to use the best tool for a job, admitting that no one paradigm solves all  problems in the easiest or most efficient way.  One of the most renowned examples is [[C sharp (programming language)]]. Another is [[Oz (programming language)|Oz]], which has subsets that are [[logic programming|logic]] (Oz descends from logic programming), a [[functional programming|functional]], an [[object-oriented programming language|object-oriented]], a [[dataflow#Concurrency|dataflow concurrent]], and other language paradigms. Oz was designed over a ten-year period to combine in a harmonious way concepts that are traditionally associated with different programming paradigms. A programming paradigm provides for the programmer the means and structure for the execution of a program.  == History ==  Initially, computers were [[hard-wired]] and then later programmed using [[binary code]] that represented control sequences fed to the computer [[central processing unit]] (CPU). This was difficult and error-prone. The transfer from hardwiring to machine code represents the implementation of the [[Von Neumann architecture|Von Neumann machine]]. A hardwired system reflects a machine with a transfer function for the data between input and output. The function is created by the wiring and components between the two. A later machine incorporates the [[Stored-program computer|stored program]] concept exemplified by a Von Neumann machine where data and program are both stored in the same machine. In the Von Neumann machine each program represents a different hardwired machine, with the program defining the transfer function for data.  Programs written in binary are said to be written in [[machine code]], which is a very [[low-level programming language|low-level]] programming paradigm. To make programming easier, [[assembly language]]s were developed. These replaced machine code functions with mnemonics and [[memory address]]es with symbolic labels. Assembly language programming is considered a low-level paradigm although it is a 'second generation' paradigm. Even assembly languages of the [[1960s]] actually supported library COPY and quite sophisticated conditional macro generation and pre-processing capabilities. They also supported modular programming features such as CALL ([[subroutine]]s), external variables and common sections (globals), enabling significant code re-use and isolation from hardware specifics via use of logical operators as READ/WRITE/GET/PUT. Assembly was, and still is, used for time critical systems and frequently in [[embedded system]]s.  The next advance was the development of ''[[procedural language]]s''. These [[third generation language|third-generation]] languages (the first described as [[high-level programming language|high-level languages]]) use vocabulary related to the problem being solved. For example, * [[C (programming language)|C]] - developed circa 1970 at [[Bell Labs]] * [[COBOL]] (Common Business Oriented Language) - uses terms like [[computer file|file]], [[move (command)|move]] and [[copy (command)|copy]]. * [[FORTRAN]] (FORmula TRANslation) - using [[mathematical]] language terminology, it was developed mainly for scientific and engineering problems. * [[ALGOL]] (ALGOrithmic Language) - focused on being an appropriate language to define [[algorithm]]s, while using mathematical language terminology and targeting scientific and engineering problems just like FORTRAN. * [[PL/I]] (Programming Language One) - a hybrid commercial/scientific general purpose language supporting [[pointer (computer programming)|pointer]]s. * [[BASIC]] (Beginners All purpose Symbolic Instruction Code) - was developed to enable more people to write programs.  All these languages follow the procedural paradigm. That is, they describe, step by step, exactly the procedure that should, according to the particular programmer at least, be followed to solve a specific problem. The [[efficacy]] and [[algorithmic efficiency|efficiency]] of any such solution are both therefore entirely subjective and highly dependent on that programmer's experience, inventiveness and ability.  Later, ''[[object-oriented language]]s'' (like [[Simula]], [[Smalltalk]], [[Eiffel (programming language)|Eiffel]] and [[Java (programming language)|Java]]) were created. In these languages, [[data]], and methods of manipulating the data, are kept as a single unit called an [[object (computer science)|object]]. The only way that a user can access the data is via the object's 'methods' (subroutines). Because of this, the internal workings of an object may be changed without affecting any code that uses the object. There is still some [[object-oriented programming#Criticism|controversy]] by notable programmers such as [[Alexander Stepanov]], [[Richard Stallman]]<ref>{{cite web|url=http://groups.google.com/group/comp.emacs.xemacs/browse_thread/thread/d0af257a2837640c/37f251537fafbb03?lnk=st&q=%22Richard Stallman%22 oop&rnum=5&hl=en#37f251537fafbb03|title=Mode inheritance, cloning, hooks & OOP (Google Groups Discussion)}}</ref> and others, concerning the efficacy of the [[object-oriented programming|OOP]] paradigm versus the procedural paradigm. The necessity of every object to have associative methods leads some skeptics to associate OOP with [[software bloat]].  [[polymorphism in object-oriented programming|Polymorphism]] was developed as one attempt to resolve this dilemma.  Since object-oriented programming is considered a paradigm, not a language, it is possible to create even an object-oriented assembler language. [[High Level Assembly]] (HLA) is an example of this that fully supports advanced data types and object-oriented assembly language programming - despite its early origins. Thus, differing programming paradigms can be thought of as more like 'motivational [[meme]]s' of their advocates - rather than necessarily representing progress from one level to the next. Precise comparisons of the efficacy of competing paradigms are frequently made more difficult because of new and differing terminology applied to similar (but not identical) entities and processes together with numerous implementation distinctions across languages.  Within imperative programming, an alternative to the computer-centered hierarchy of structured programming is [[literate programming]], which structures programs instead as a human-centered web, as in a [[hypertext]] essay – documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.  Independent of the [[imperative programming|imperative]] branch based on procedural languages, ''[[declarative programming]]'' paradigms were developed. In these languages the computer is told what the problem is, not how to solve the problem - the program is structured as a collection of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. The archetypical example of a declarative language is the [[fourth generation language]] [[SQL]], as well as the family of [[functional languages]] and [[logic programming]].  ''[[Functional programming]]'' is a subset of declarative programming. Programs written using this paradigm use [[subroutine|function]]s, blocks of code intended to behave like [[function (mathematics)|mathematical functions]]. Functional languages discourage changes in the value of variables through [[assignment (computer science)|assignment]], making a great deal of use of [[recursion (computer science)|recursion]] instead.  The ''[[logic programming]]'' paradigm views computation as [[automated reasoning]] over a corpus of knowledge. Facts about the [[problem domain]] are expressed as logic formulae, and programs are executed by applying [[inference rule]]s over them until an answer to the problem is found, or the collection of formulae is proved inconsistent.  == See also == * [[Architecture description language]] * [[Comparison of programming paradigms]] * [[Domain-specific language]] * [[Mindset]] * [[Modeling language]] * [[Paradigm]] * [[Programming domain]] * [[Turing completeness]]  == References == <references />  == External links == {{Programming language lists}} * [http://www.infocheese.com/programmingparadigms.html Programming paradigms] * [http://www.info.ucl.ac.be/~pvr/paradigms.html Classification of the principal programming paradigms]  {{Navboxes | state = collapsed | list  = {{Navboxes | title = software engineering | state = collapsed | list  = {{Computer language}} {{Software engineering}} {{Computer science}} }} {{Navboxes | title = programming | state = collapsed | list  = {{Programming language generations}} {{Major programming languages}} {{Programming language}} }} }}  {{DEFAULTSORT:Programming paradigm}} [[Category:Programming paradigms| ]] [[Category:Programming language classification]] [[Category:Programming language topics]]  [[az:Kateqoriya:Proqramlaşdırma_paradiqmaları]] [[ar:نمط برمجة]] [[bg:Парадигма на програмиране]] [[bs:Programska paradigma]] [[ca:Paradigma de programació]] [[cs:Programovací paradigma]] [[da:Programmeringsparadigme]] [[de:Programmierparadigma]] [[et:Programmeerimise paradigma]] [[el:Προγραμματιστικό παράδειγμα]] [[es:Paradigma de programación]] [[fa:پارادایم برنامه‌نویسی]] [[fr:Paradigme (programmation)]] [[gl:Paradigma de programación]] [[ko:프로그래밍 패러다임]] [[id:Paradigma pemrograman]] [[it:Paradigma di programmazione]] [[he:פרדיגמת תכנות]] [[lv:Programmēšanas paradigma]] [[hu:Programozási paradigma]] [[mk:Програмерска парадигма]] [[ms:Paradigma pengaturcaraan]] [[nl:Programmeerparadigma]] [[ja:プログラミングパラダイム]] [[no:Programmeringsparadigme]] [[pl:Paradygmat programowania]] [[pt:Paradigma de programação]] [[ro:Paradigmă de programare]] [[ru:Парадигма программирования]] [[sq:Paradigma programore]] [[sh:Programska paradigma]] [[fi:Ohjelmointiparadigma]] [[sv:Programmeringsparadigm]] [[ta:நிரலாக்க கருத்தோட்டம்]] [[tr:Programlama paradigması]] [[uk:Парадигма програмування]] [[ur:اطار برمجہ]] [[vi:Mẫu hình lập trình]] [[zh:编程范型]]

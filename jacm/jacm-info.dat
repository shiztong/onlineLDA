0 "a randomized polynomialtime algorithm for approximating the volume ofa convex" "a randomized polynomialtime algorithm for approximating the volume ofa convex body k in ndimensional euclidean space is presented theproof of correctness of the algorithm relies on recent theory ofrapidly mixing markov chains and isoperimetric inequalities to showthat a certain random walk can be used to sample nearly uniformly fromwithin k"
1 "the problem of determining shortest paths through a weighted planar" "the problem of determining shortest paths through a weighted planar polygonal subdivision with n vertices is considered distances are measured according to a weighted euclidean metric the length of a path is defined to be the weighted sum of euclidean lengths of the subpaths within each region an algorithm that constructs a restricted shortest path map with respect to a given source point is presented the output is a partitioning of each edge of the subdivion into intervals of egr optimality allowing an egroptimal path to be traced from the source to any query point along any edge the algorithm runs in worstcase time oes and requires oe space where e is the number of events in our algorithm and s is the time it takes to run a numerical search procedure in the worst case e is bounded above by on and we give an ohgrn lower bound but it is likeky that e will be much smaller in practice we also show that s is bounded by onl where l is the precision of the problem instance including the number of bits in the userspecified tolerance egr again the value of s should be smaller in practice the algorithm applies the continuous dijkstra paradigm and exploits the fact that shortest paths obey snells law of refraction at region boundaries a local optimaly property of shortest paths that is well known from the analogous optics model the algorithm generalizes to the multisource case to compute voronoi diagrams"
2 "randomized optimal algorithms to find a partition of the plane" "randomized optimal algorithms to find a partition of the plane induced by a set of algebraic segments of a bounded degree and a set of linear chains of a bounded degree are given this paper also provides a new technique for clipping called virtual clipping whose overhead per window w depends logarithmically on the number if intersections between the borders of w and the input segments in contrast the overhead of the conventional clipping technique depends linearly on this number of intersections as an application of virtual clipping a new simple and efficient algorithm for plannar point location is given"
3 "in many computing applications there are several equivalent algorithms capable" "in many computing applications there are several equivalent algorithms capable of performing a particular task and no one is the most efficient under all statistical distributions of the data in such contexts a good heuristic is to take a sample of the database and use it to guess which procedure is likely to be the most efficient this paper defines the very general notion of a differentiable query problem and shows that the ideal sample size for guessing the optimal choice of algorithm is on for all differential problems involving approximately n executing steps"
4 "a simple characterization of independent database schemes is proved an" "a simple characterization of independent database schemes is proved an algorithm is given for translating a tableau t posed as a query on a representative instance to a union of tableaux that is equivalent to t but can be applied directly to database relations the algorithm may take exponential time in the size of t and the database scheme and it is applicable only to independent database schemes if t is a just a projection of a representative instance then the algorithm has a simpler form which is still exponential in the worst case and is polynomial in some cases"
5 "an algorithm is presented for generating a succinct encoding of" "an algorithm is presented for generating a succinct encoding of all pairs shortest path information in a directed planar graph g with realvalued edge costs but no negative cycles the algorithm runs in opn time where n is the number of vertices in g and p is the minimum cardinality of a subset of the faces that cover all vertices taken over all planar embeddings of g the algorithm is based on a decomposition of the graph into opn outerplanar subgraphs satisfying certain separator properties lineartime algorithms are presented for various subproblems including that of finding an appropriate embedding of g and a corresponding faceonvertex covering of cardinality op and of generating all pairs shortest path information in a directed outerplannar graph"
6 "the class of horn clause sets in propositional logic is" "the class of horn clause sets in propositional logic is extended to a larger class for which the satisfiability problem can still be solved by unit resolution in linear time it is shown that to every arborescence there corresponds a family of extended horn sets where ordinary horn sets correspond to stars with a root at the center these results derive from a theorem of chandresekaran that characterizes when an integer solution of a system of inequalities can be found by rounding a real solution in a certain way a lineartime procedure is provided for identifying hidden extended horn sets extended horn but for complementation of variables that correspond to a specified arborescence finally a way to interpret extended horn sets in applications is suggested"
7 "a technologyindependent framework is established for measuring the switching energy" "a technologyindependent framework is established for measuring the switching energy consumed by very large scale integrated vlsi circuits techniques are developed for analyzing functional energy consumption and for designing energyefficient vlsi circuits a wire or gate in a circuit uses switching energy when it changes state from to or vice versa this paper develops the uniswitch model usm of energy consumption which measures the differences between pairs of states of an embedded circuit"
8 "certain problems related to the length of cycles and paths" "certain problems related to the length of cycles and paths modulo a given integer are studied lineartime algorithms are presented that determine whether all cycles in an undirected graph are of length p mod q and whether all paths between two specified nodes are of length p mod q for fixed integers pq these results are compared to those for directed graphs"
9 "let s be a set f ssr a bivariate function" "let s be a set f ssr a bivariate function and fxs the maximum value of fxy over all elements ys we say that f is decomposable with respect with the maximum if fxs max fxsfxsfxsk for any decomposition s mgriiksi computing the maximum minimum value of a decomposable function is inherent in many problems of computational geometry and robotics in this paper a general technique is presented for updating the maximum minimum value of a decomposable function as elements are inserted into and deleted from the set s our result holds for a semionline model of dynamization when an element is inserted we are told how long it will stay applications of this technique include efficient algorithms for dynamically computing the diameter or closest pair of a set of points minimum separation among a set of rectangles smallest distance between a set of points and a set of hyperplanes and largest or smallest area perimeter retangles determined by a set of points these problems are fundamental to application areas such as robotics vlsi masking and optimization"
10 "a logic simulator can prove the correctness of a digital" "a logic simulator can prove the correctness of a digital circuit if it can be shown that only circuits fulfilling the system specification will produce a particular response to a sequence of simulation commandsthis style of verification has advantages over the other proof methods in being readily automated and requiring less attention on the part of the user to the lowlevel details of the design it has advantages over other approaches to simulation in providing more reliable results often at a comparable cost this paper presents the theoretical foundations of several related approaches to circuit verification based on logic simulation these approaches exploit the threevalued modeling capability found in most logic simulators where the thirdvalue x indicates a signal with unknown digital value although the circuit verification problem is nphard as measured in the size of the circuit description several techniques can reduce the simulation complexity to a manageable level for many practical circuits"
11 "the general problem of parallel concurrent processing is investigated from" "the general problem of parallel concurrent processing is investigated from a queuing theoretic point of view as a basic simple model consider infinitely many processors that can work simultaneously and a stream of arriving jobs each carrying a processing time requirement upon arrival a job is allocated to a processor and starts being executed unless it is blocked by another one already in the system indeed any job can be randomly blocked by any preceding one in the sense that it cannot start being processed before the one that blocks it leaves after execution the job leaves the system the arrival times the processing times and the blocking structures of the jobs form a stationary and ergodic sequence the random precedence constraints capture the essential operational characteristic of parallel processing and allow a unified treatment of concurrent processing systems from such diverse areas as parallel computation database concurrency control queuing networks flexible manufacturing systems the above basic model includes the gg and gg queuing systems as special extreme cases although there is an infinite number of processors the precedence constraints induce a queuing phenomenon which depending on the loading conditions can lead to stability or instability of the system in this paper the condition for stability of the system is first precisely specified the asymptotic behavior at large times of the quantities associated with the performance of the system is then studied and the degree of parallelism expressed as the asymptotic average number of processors that work concurrently is computed finally various design and simulation aspects concerning parallel processing systems are considered and the case of finite number of processors is discussed the results proved for the basic model are then extended to cover more complex and realistic parallel processing systems where each job has a random internal structure of subtasks to be executed according to some internal precedence constriants"
12 "an algebraic framework for the study of recursion has been" "an algebraic framework for the study of recursion has been developed for immediate linear recursion a horn clause is represented by a relational algebra operator it is shown that the set of all such operators forms a closed semiring in this formalism query answering corresponds to solving a linear equation for the first time the query answer is able to be expressed in an explicit algebraic form within an algebraic structure the manipulative power thus afforded has several implications on the implementation of recursive query processing algorithms several possible decompositions of a given operator are presented that improve the performance of the algorithms as well as several transformations that give the ability to take into account any selections or projections that are present in a givin query in addition it is shown that mutual linear recursion can also be studied within a closed semiring by using relation vectors and operator matrices regarding nonlinear recursion it is first shown that horn clauses always give rise to multilinear recursion which can always be reduced to bilinear recursion bilinear recursion is then shown to form a nonassociative closed semiring finally several sufficient and necessaryandsufficient conditions for bilinear recursion to be equivalent to a linear one of a specific form are given one of the sufficient conditions is derived by embedding to bilinear recursion in an algebra"
14 "let km denote the smallest number with the property that" "let km denote the smallest number with the property that every mstate finite automaton can be built as a neural net using km or fewer neurons a counting argument shows that km is at least ohgrm log m and a construction shows that km is at most om the counting argument and the construction allow neural nets with arbitrarily complex local structure and thus may require neurons that themselves amount to complicated networks mild and in practical situations almost necessary constraints on the local structure of the network give again by a counting argument and a construction lower and upper bounds for km that are both linear in m"
15 "autoepistemic logic is one of the principal modes of nonmonotonic" "autoepistemic logic is one of the principal modes of nonmonotonic reasoning it unifies several other modes of nonmonotonic reasoning and has important application in logic programming in the paper a theory of autoepistemic logic is developed this paper starts with a brief survey of some of the previously known results then the nature of nonmonotonicity is studied by investigating how membership of autoepistemic statements in autoepistemic theories depends on the underlying objective theory a notion similar to settheoretic forcing is introduced expansions of autoepistemic theories are also investigated expansions serve as sets of consequences of an autoepistemic theory and they can also be used to define semantics for logic programs with negation theories that have expansions are characterized and a normal form that allows the description of all expansions of a theory is introduced our results imply algorithms to determine whether a theory has a unique expansion sufficient conditions stratification that imply existence of a unique expansion are discussed the definition of stratified theories is extended and under some additional assumptions efficient algorithms for testing whether a theory is stratified are proposed the theorem characterizing expansions is applied to two classes of theories ktheories and aeprograms in each case simple hypergraph characterization of expansions of theories from each of these classes is given finally connections with stable model semantics for logic programs with negation is discussed in particular it is proven that the problem of existence of stable models is npcomplete"
16 "an improved and general approach to connectedcomponent labeling of images" "an improved and general approach to connectedcomponent labeling of images is presented the algorithm presented in this paper processes images in predetermined order which means that the processing order depends only on the image representation scheme and not on specific properties of the image the algorithm handles a wide variety of image representation schemes rasters run lengths quadrees bintrees etc how to adapt the standard unionfind algorithm to permit reuse of temporary labels is shown this is done using a technique called age balancing in which when two labels are merged the older label becomes the father of the younger label this technique can be made to coexist with the more conventional rule of weight balancing in which the label with more descendants becomes the father of the label with fewer descendants various image scanning orders are examined and classified it is also shown that when the algorithm is specialized to a pixel array scanned in raster order the total processing time is linear in the number of pixels the lineartime processing time follows from a special property of the unionfind algorithm which may be of independent interest this property states that under certain restrictions on the input unionfind runs in time linear in the number of find and union operations under these restrictions lineartime performance can be achieved without resorting to the more complicated gabowtarjan algorithm for disjoint set union"
17 "text compression is often done using a fixed previously formed" "text compression is often done using a fixed previously formed dictionary code book that expresses which substrings of the text can be replaced by code words there always exists an optimal solution for textencoding problem due to the long processing times of the various optimal algorithms several heuristics have been proposed in the literature in this paper the worstcase compression gains obtained by the longest match and the greedy heuristics for various types of dictionaries is studied for general dictionaries the performance of the heuristics can be almost the weakest possible in practice however the dictionaries have usually properties that lead to a spaceoptimal or nearspaceoptimal coding result with the heuristics"
18 "tree pattern matching is a fundamental operation that is used" "tree pattern matching is a fundamental operation that is used in a number of programming tasks such as mechanical theorem proving term rewriting symbolic computation and nonprocedural programming languages in this paper we present new sequential algorithms for nonlinear pattern matching in trees our algorithm improves upon know tree pattern matching algorithms in important aspects such as time performance ease of integration with several reduction strategies and ability to avoid unnecessary computation steps on match attempts that fail the expected time complexity of our algorithm is linear in the sum of the sizes of the two trees"
19 "the problem of generating random uniformly distributed binary trees is" "the problem of generating random uniformly distributed binary trees is considered a closed formula that counts the number of trees having a left subtee with k nodes kn is found by inverting the formula random trees with n nodes are generated according to the appropriate probability distribution determining the number of nodes in the left and right subtrees that can be generated recursively the procedure is shown to run in time on occupying an extra space in the order of on"
20 "in this paper it is shown that the method of" "in this paper it is shown that the method of matings due to andrews and bibel can be extended to firstorder languages with equality a decidable version of eunification called rigid eunification is introduced and it is shown that the method of equational matings remains complete when used in conjunction with rigid eunification checking that a family of mated sets is an equational mating is equivalent to the following restricted kind of eunification problem given eareimid in a family of n finite sets of equations and suivi midin a set of n pairs of terms is there a substitution q such that treating each set qei as a set of ground equations ie holding the variables in qei rigid qui and qvi are provably equal from qei for in equivalently is there a substitution q such that qui and qvi can be shown congruent from qei by the congruence closure method for in a substitution q solving the above problem is called a rigid ear unifier of s and a pair ears such that s has some rigid ear unifier is called an equational premating it is show that deciding whether a pair ears is an equational premating is an npcomplete problem"
21 "given a regular expression r of length p and a" "given a regular expression r of length p and a word a of length n the membership problem is to determine if a is in the language denoted by r an opnlgn time algorithm is presented that is based on a lgn speedup of the standard opn time simulation of rs nonderministic finite automaton on a using a combination of the nodelisting and fourrussians paradigms this result places a new worstcase upper bound on regular expression pattern matching moreover in practice the method provides an implementation that is faster than existing software for small regular expressions"
22 "in this paper it is shown that there is an" "in this paper it is shown that there is an algorithm that given by finite set e of ground equations produces a reduced canonical rewriting system r equivalent to e in polynomial time this algorithm based on congruence closure performs simplification steps guided by a total simplification ordering on ground terms and it runs in time on"
23 "this paper studies the problem of perfectly secure communication in" "this paper studies the problem of perfectly secure communication in general network in which processors and communication lines may be faulty lower bounds are obtained on the connectivity required for successful secure communication efficient algorithms are obtained that operate with this connectivity and rely on no complexitytheoretic assumptions these are the first algorithms for secure communication in a general network to simultaneously achieve the three goals of perfect secrecy perfect resiliency and worstcase time linear in the diameter of the network"
24 "consider the problem of generating bitmaps from character shapes given" "consider the problem of generating bitmaps from character shapes given as outlines the obvious scanconversion process does not produce acceptable results unless important features such as stem widths are carefully controlled during the scanconversion process this paper describes a method for automatically extracting the necessary feature information and generating highquality bitmaps without resorting to hand editing almost all of the work is done in a preprocessing step the result of which is an intermediate form that can be quickly converted into bitmaps once the font size and device resolution are known a heuristically defined system of linear equations describes how the ideal outlines should be distorted in order to produce the best possible results when scan converted in a straightforward manner the lovasz basis reduction algorithm then reduces the system of equations to a form that makes it easy to find an approximate solution subject to the constraint that some variables must be integers the heuristic information is of such a general nature that it applies equally well to roman fonts and japanese kanji"
25 "the minimum consistent dfa problem is that of finding a" "the minimum consistent dfa problem is that of finding a dfa with as few states as possible that is consistent with a given sample a finite collection of words each labeled as to whether the dfa found should accept or reject assuming that p np it is shown that for any constant k no polynomialtime algorithm can be guaranteed to find a consistent dfa with fewer than optk states where opt is the number of states in the minimum state dfa consistent with the sample this result holds even if the alphabet is of constant size two and if the algorithm is allowed to produce an nfa a regular expression or a regular grammar that is consistent with the sample a similar nonapproximability result is presented for the problem of finding small consistent linear grammars for the case of finding minimum consistent dfas when the alphabet is not of constant size but instead is allowed to vary with the problem specification the slightly stronger lower bound on approximability of opt egrlog logopt is shown for any egr"
26 "the edinburgh logical framework lf provides a means to define" "the edinburgh logical framework lf provides a means to define or present logics it is based on a general treatment of syntax rules and proofs by means of a typed lgrcalculus with dependent types syntax is treated in a style similar to but more general than martinlofs system of arities the treatment of rules and proofs focuses on his notion of a judgment logics are represented in lf via a new principle the judgments as types principle whereby each judgment is identified with the type of its proofs this allows for a smooth treatment of discharge and variable occurence conditions and leads to a uniform treatment of rules and proofs whereby rules are viewed as proofs of higherorder judgments and proof checking is reduced to type checking the practical benefit of our treatment of formal systems is that logicindependent tools such as proof editors and proof checkers can be constructed"
27 "a readonce formula is a boolean formula in which each" "a readonce formula is a boolean formula in which each variable occurs at most once such formulas are also called mgrformulas or boolean trees this paper treats the problem of exactly identifying an unknown readonce formula using specific kinds of queries the main results are a polynomialtime algorithm for exact identification of monotone readonce formulas using only membership queries and a polynomialtime algorithm for exact identification of general readonce formulas using equivalence and membership queries a protocol based on the notion of a minimally adequate teacher the results of the authors improve on valiants previous results for readonce formulas it is also shown that no polynomialtime algorithm using only membership queries or only equivalence queries can exactly identify all readonce formulas"
28 "in practice almost all dynamic systems require decisions to be" "in practice almost all dynamic systems require decisions to be made online without full knowledge of their future impact on the system a general model for the processing of sequences of tasks is introduced and a general online decision algorithm is developed it is shown that for an important class of special cases this algorithm is optimal among all online algorithms specifically a task system sd for processing sequences of tasks consists of a set s of states and a cost matrix d where di j is the cost of changing from state i to state j we assume that d satisfies the triangle inequality and all diagonal entries are the cost of processing a given task depends on the state of the system a schedule for a sequence t t tk of tasks is a sequence s ssk of states where si is the state in which ti is processed the cost of a schedule is the sum of all task processing costs and the state transition costs incurred an online scheduling algorithm is one that chooses si only knowing ttti such an algorithm is wcompetitive if on any input task sequence its cost is within an additive constant of w times the optimal offline schedule cost the competitive ratio ws d is the infimum w for which there is a wcompetitive online scheduling algorithm for sd it is shown that ws d s for every task system in which d is symmetric and ws d os for every task system finally randomized online scheduling algorithms are introduced it is shown that for the uniform task system in which dij for all ij the expected competitive ratio wsd ologs"
29 "nonoblivious hashing where information gathered from unsuccessful probes is used" "nonoblivious hashing where information gathered from unsuccessful probes is used to modify subsequent probe strategy is introduced and used to obtain the following results for static lookup on full tables an o time worstcase scheme that uses only logarithmic additional memory and no memory when the domain size is linear in the table size which improves upon previously linear space requirements an almost sure otime probabilistic worstcase scheme which uses no additional memory and which improves upon previously logarithmic time requirements enhancements to hashing and are solved for multikey recors where search can be performed under any key in time o these schemes also permit properties such as nearest neighbor and rank to be determined in logarithmic time"
30 "the efficiency of datalink protocols for reliable transmission of a" "the efficiency of datalink protocols for reliable transmission of a sequence of messages over nonfifo physical channels is discussed the transmission has to be online ie a message cannot be accessed by the transmitting station before the preceding message has been received three resources are considered the number of packets that have to be sent the number of headers and the amount of space required by the protocol three lower bounds are proved first the space required by any protocol for delivering n messages that uses less than n headers cannot be bounded by any function of n second the number of packets that have to be sent by any protocol that uses a fixed number of headers in order to deliver a message is linear in the number of packets that are delayed on the channel at the time the message is sent finally the notion of a probabilistic physical channel in which a packet can be delayed on the channel with probability q is introduced an exponential lower bound with overwhelming probability is proved on the number of packets that have to be sent by any datalink protocol using a fixed number of headers when it is implemented over a probabilistic physical channel"
31 "an investigation of interactive proof systems ipss where the verifier" "an investigation of interactive proof systems ipss where the verifier is a way probabilistic finite state automaton pfa is initiated in this model it is shown ipss in which the verifier uses private randomization are strictly more powerful than ipss in which the random choices of the verifier are made public to the prover ipss in which the verifier uses public randomization are strictly more powerful than pfas alone that is without a prover every language which can be accepted by some deterministic turing machine in exponential time can be accepted by some ips additional results concern two other classes of verifiers pfas that halt in polynomial expected time and way probabilistic pushdown automata that halt in polynomial time in particular ipss with verifiers in the latter class are as powerful as ipss where verifiers are polynomialtime probabilistic turing machines in a companion paper zero knowledge ipss with pfa verifiers are investigated"
32 "the zero knowledge properties of interactive proof systems ipss are" "the zero knowledge properties of interactive proof systems ipss are studied in the case that the verifier is a way probabilistic finite state automaton pfa the following results are proved there is a language l such that l has an ips with pfa verifiers but l has no zero knowledge ips with pfa verifiers consider the class of pfas that are sweeping and that halt in polynomial expected time there is a language l such that l has a zero knowledge ips with respect to this class of verifiers and l cannot be recognized by any verifier in the class on its own a new definition of zero knowledge is introduced this definition captures a concept of zero knowledge for ipss that are used for language recognition"
33 "a new algebraic technique for the construction of interactive proof" "a new algebraic technique for the construction of interactive proof systems is presented our technique is used to prove that every language in the polynomialtime hierarchy has an interactive proof system this technique played a pivotal role in the recent proofs that ip pspace and that mip nexp"
34 "in this paper it is proven that when both randomization" "in this paper it is proven that when both randomization and interaction are allowed the proofs that can be verified in polynomial time are exactly those proofs that can be generated with polynomial space"
35 "lund et al have proved that ph is contained in" "lund et al have proved that ph is contained in ip shamir improved this technique and proved that pspace ip in this note a slightly simplified version of shamirs proof is presented using degree reductions instead of simple qbfs"
36 "in a distributed system node failures network delays and other" "in a distributed system node failures network delays and other unpredictable occurences can result in orphan computationssubcomputations that continue to run but whose results are no longer needed several algorithms have been proposed to prevent such computations from seeing inconsistent states of the shared data in this paper two such orphan management algorithms are analyzed the first is an algorithm implemented in the argus distributedcomputing system at mit and the second is an algorithm proposed at carnegiemellon the algorithms are described formally and complete proofs of their correctness are given the proofs show that the fundamental concepts underlying the two algorithms are very similar in that each can be regarded as an implementation of the same highlevel algorithm by exploiting properties of information flow within transaction management systems the algorithms ensure that orphans only see states of the shared data that they could also see if they were not orphans when the algorithms are used in combination with any correct concurrency control algorithm they guarantee that all computations orphan as well as nonorphan see consistent states of the shared data"
37 "the debruijn graph bn is the state diagram for an" "the debruijn graph bn is the state diagram for an nstage binary shift register it has n vertices and n edges in this papers it is shown that bn can be built by appropriately wiring together ie connecting together with extra edges many isomorphic copies of a fixed graph which is called a building block for bn the efficiency of such a building block is refined as the fraction of the edges of bn which are present in the copies of the building block it is then shown among other things that for any agr there exists a graph g which is a building block for bn of efficiency agr for all sufficiently large n these results are illustrated by describing how a special hierarchical family of building blocks has been used to construct a very large viterbi decoder whose floorplan is the graph b which will be used on nasas galileo mission"
38 "a framework for efficient dataflow analyses of logic programs is" "a framework for efficient dataflow analyses of logic programs is investigated a number of problems arise in this context aliasing effects can make analysis computationally expensive for sequential logic programming languages synchronization issues can complicate the analysis of parallel logic programming languages and finiteness restrictions to guarantee termination can limit the expressive power of such analyses our main result is to give a simple characterization of a family of flow analyses where these issues can be ignored without compromising soundness this results in algorithms that are simple to verify and implement and efficient in execution based on this approach we describe an efficient algorithm for flow analysis of sequential logic programs extend this approach to handle parallel executions and finally describe how infinite chains in the analysis domain can be accommodated without compromising termination"
39 "a highlevel knowledgebased approach for deriving a family of protocols" "a highlevel knowledgebased approach for deriving a family of protocols for the sequence transmission problem is presented the protocols of aho et al the alternating bit protocol and stennings protocol are all instances of one knowledgebased protocol that is derived the derivation in this paper leads to transparent and uniform correctness proofs for all these protocols"
40 "in bernhart and kainen conjectured that graphs of fixed genus" "in bernhart and kainen conjectured that graphs of fixed genus g have unbounded pagenumber in this paper it is proven that genus g graphs can be embedded in og pages thus disproving the conjecture an ohgrg lower bound is also derived the first algorithm in the literature for embedding an arbitrary graph in a book with a nontrivial upper bound on the number of pages is presented first the algorithm computes the genus g of a graph using the algorithm of filotti miller reif which is polynomialtime for fixed genus second it applies an optimaltime algorithm for obtaining an ogpage book embedding separate book embedding algorithms are given for the cases of graphs embedded in orientable and nonorientable surfaces an important aspect of the construction is a new decomposition theorem of independent interest for a graph embedded on a surface book embedding has application in several areas two of which are directly related to the results obtained faulttolerant vlsi and complexity theory"
41 "this paper presents an efficient algorithm to solve one of" "this paper presents an efficient algorithm to solve one of the task allocation problems task assignment in an heterogeneous multiple processors system is investigated the cost function is formulated in order to measure the intertask communication and processing costs in an uncapacited network a formulation of the problem in terms of the minimization of a submodular quadratic pseudoboolean function with assignment constraints is then presented the use of a branchandbound algorithm using a lagrangean relaxation of these constraints is proposed the lower bound is the value of an approximate solution to the lagrangean dual problem a zeroduality gap that is a saddle point is characterized by checking the consistency of a pseudoboolean equation a solution is found for largescale problems eg processors tasks and task communications or processors tasks and task communications excellent experimental results were obtained which are due to the weak frequency of a duality gap and the efficient characterization of the zerogap for practical purposes this is achieved in linear time moreover from the saddle point it is possible to derive the optimal task assignment"
42 "dynamic programming solutions to a number of different recurrence equations" "dynamic programming solutions to a number of different recurrence equations for sequence comparison and for rna secondary structure prediction are considered these recurrences are defined over a number of points that is quadratic in the input size however only a sparse set matters for the result efficient algorithms for these problems are given when the weight functions used in the recurrences are taken to be linear the time complexity of the algorithms depends almost linearly on the number of points that need to be considered when the problems are sparse this results in a substantial speedup over known algorithms"
43 "dynamic programming solutions to two recurrence equations used to compute" "dynamic programming solutions to two recurrence equations used to compute a sequence alignment from a set of matching fragments between two strings and to predict rna secondary structure are considered these recurrences are defined over a number of points that is quadratic in the input size however only a sparse set matters for the result efficient algorithms are given for solving these problems when the cost of a gap in the alignment or a loop in the secondary structure is taken as a convex or concave function of the gap or loop length the time complexity of our algorithms depends almost linearly on the number of points that need to be considered when the problems are sparse this results in a substantial speedup over known algorithms"
44 "the problem of testing membership in aperiodic or groupfree transformation" "the problem of testing membership in aperiodic or groupfree transformation monoids is the natural counterpart to the wellstudied membership problem in permutation groups the class a of all finite aperiodic monoids and the class g of all finite groups are two examples of varieties the fundamental complexity units in terms of which finite monoids are classified the collection of all varieties v forms an infinite lattice under the inclusion ordering with the subfamily of varieties that are contained in a forming an infinite sublattice for each v a the associated problem membv of testing membership in transformation monoids that belong to v is considered remarkably the computational complexity of each such problem turns out to look familiar moreover only five possibilities occur as v ranges over the whole aperiodic sublattice with one family of nphard exceptions whose exact status is still unresolved any such membv is either pspacecomplete npcomplete pcomplete or in ac these results thus uncover yet another surprisingly tight link between the theory of monoids and computational complexity theory"
45 "traditional work in inductive inference has been to model a" "traditional work in inductive inference has been to model a learner receiving data about a function f and trying to learn the function the data is usually just the values f f the scenario is modeled so that the learner is also allowed to ask questions about the data eg khgr khgr fkhgr an important parameter is the language that the lerner may use to formulate queries we show that for most languages a learner can learn more by asking questions than by passively receiving data mathematical tools used include the solution to hilberts tenth problem the decidability of presuburger arithmetic and ohgrautomata"
46 "methods are given for automatically verifying temporal properties of concurrent" "methods are given for automatically verifying temporal properties of concurrent systems containing an arbitrary number of finitestate processes that communicate using ccs actions two models of systems are considered systems in the first model consist of a unique control process and an arbitrary number of user processes with identical definitions for this model a decision procedure to check whether all the executions of a process satisfy a given specification is presented this algorithm runs in time double exponential in the sizes of the control and the user process definitions it is also proven that it is decidable whether all the fair executions of a process satisfy a given specification the second model is a special case of the first in this model all the processes have identical definitions for this model an efficient decision procedure is presented that checks if every execution of a process satisfies a given temporal logic specification this algorithm runs in time polynomial in the size of the process definition it is shown how to verify certain global properties such as mutual exclusion and absence of deadlocks finally it is shown how these decision procedures can be used to reason about certain systems with a communication network"
47 "it is proven that monotone circuits computing the perfect matching" "it is proven that monotone circuits computing the perfect matching function on nvertex graphs require ohgrn depth this implies an exponential gap between the depth of monotone and nonmonotone circuits"
48 "the main contribution of this work is an on log" "the main contribution of this work is an on log n ktime algorithm for computing all k intersections among n line segments in the plane this time complexity is easily shown to be optimal within the same asymptotic cost our algorithm can also construct the subdivision of the plane defined by the segments and compute which segment if any lies right above or below each intersection and each endpoint the algorithm has been implemented and performs very well the storage requirement is on the order of n k in the worst case but it is considerably lower in practice to analyze the complexity of the algorithm an amortization argument based on a new combinatorial theorem on line arrangements is used"
49 "a deterministic olog ntime algorithm for the problem of routing" "a deterministic olog ntime algorithm for the problem of routing an aribitrary permutation on an nprocessor boundeddegree network with bounded buffers is presented unlike all previous deterministic solutions to this problem our routing scheme does not reduce the routing problem to sorting and does not use the sorting network of ajtai et al consequently the constant in the run time of our routing scheme is substantially smaller and the network topology is significantly simpler"
50 "a domainindependent formula of firstorder predicate calculus is a formula" "a domainindependent formula of firstorder predicate calculus is a formula whose evaluation in a given interpretation does not change when we add a new constant to the interpretation domain the formulas used to express queries integrity constraints or deductive rules in the database field that have an intuitive meaning are domain independent that is the reason why this class is of great interest in practice unfortunately this class is not decidable and the problem is to characterize new subclasses as large as possible which are decidable a syntactic characterization of a class of formulas the evaluable formulas which are proved to be domain independent are provided this class is defined only for functionfree formulas it is also proved that the class of evaluable formulas contains the other classes of syntactically characterized domainindependent formulas usually found in the literature namely rangeseparable formulas and rangerestricted formulas finally it is shown that the expressive power of evaluable formulas is the same as that of domainindependent formulas that is each domainindependent formula admits an equivalent evaluable one an important advantage of this characterization is that to check if a formula is evaluable it is not necessary to transform it to a normal form as is the case for rangerestricted formulas"
51 "there is a population explosion among the logical systems used" "there is a population explosion among the logical systems used in computing science examples include firstorder logic equational logic hornclause logic higherorder logic infinitary logic dynamic logic intuitionistic logic ordersorted logic and temporal logic moreover there is a tendency for each theorem prover to have its own idiosyncratic logical system the concept of institution is introduced to formalize the informal notion of logical system the major requirement is that there is a satisfaction relation between models and sentences that is consistent under change of notation institutions enable abstracting away from syntactic and semantic detail when working on language structure inthelarge for example we can define language features for building large logical system this applies to both specification languages and programming languages institutions also have applications to such areas as database theory and the semantics of artificial and natural languages a first main result of this paper says that any institution such that signatures which define notation can be glued together also allows gluing together theories which are just collections of sentences over a fixed signature a second main result considers when theory structuring is preserved by institution morphisms a third main result gives conditions under which it is sound to use a theorem prover for one institution on theories from another a fourth main result shows how to extend institutions so that their theories may include in addition to the original sentences various kinds of constraint that are useful for defining abstract data types including both data and hierarchy constraints further results show how to define institutions that allow sentences and constraints from two or more institutions all our general results apply to such duplex and multiplex institutions"
52 "in this paper a process algebra that incorporates explicit representations" "in this paper a process algebra that incorporates explicit representations of successful termination deadlock and divergence is introduced and its semantic theory is analyzed both an operational and a denotational semantics for the language is given and it is shown that they agree the operational theory is based upon a suitable adaptation of the notion of bisimulation preorder the denotational semantics for the language is given in terms of the initial continuous algebra that satisfies a set of equations e cie it is shown that cie is fully abstract with respect to our choice of behavioral preorder several results of independent interest are obtained namely the finite approximability of the behavioral preorder and a partial completeness result for the set of equations e with respect to the preorder"
53 "in a closed separable queuing network model of a computer" "in a closed separable queuing network model of a computer system the number of customer classes is an input parameter the number of classes and the class compositions are assumptions regarding the characteristics of the systems workload often the number of customer classes and their associated device demands are unknown or are unmeasurable parameters of the system however when the system is viewed as having a single composite customer class the aggregate singleclass parameters are more easily obtainable this paper addresses the error made when constructing a singleclass model of a multiclass system it is shown that the singleclass model pessimistically bounds the performance of the multiclass system thus given a multiclass system the corresponding singleclass model can be constructed with the assurance that the actual system performance is better than that given by the singleclass model in the worst case it is shown that the throughput given by the singleclass model underestimates the actual multiclass throughput by at most also lower bounds are provided for the number of necessary customer classes given observed device utilizations this information is useful to clustering analysis techniques as well as to analysts who must obtain classspecific device demands"
54 "a digital signature scheme is presented which is based on" "a digital signature scheme is presented which is based on the existence of any trapdoor permutation the scheme is secure in the strongest possible natural sense namely it is secure against existential forgery under adaptive chosen message attack"
55 "tight bounds are proved for sort merge insert gcd of" "tight bounds are proved for sort merge insert gcd of integers gcd of polynomials and rational functions over a finite inputs domain in a random access machine with arithmetic operations direct and indirect addressing unlimited power for answering yesno questions branching and tables with bounded size these bounds are also true even if additions subtractions multiplications and divisions of elements by elements of the field are not counted in a random access machine with finitely many constants and a bounded number of types of instructions it is proved that the complexity of a function over a countable infinite domain is equal to the complexity of the function in a sufficiently large finite subdomain"
56 "this paper is concerned with a game on graphs called" "this paper is concerned with a game on graphs called graph searching the object of this game is to clear all edges of a contaminated graph clearing is achieved by moving searchers a kind of token along the edges of the graph according to clearing rules certain search strategies cause edges that have been cleared to become contaminated again megiddo et al conjectured that every graph can be searched using a minimum number of searchers without this recontamination occurring that is without clearing any edge twice in this paper this conjecture is proved this places the graphsearching problem in np completing the proof by megiddo et al that the graphsearching problem is npcomplete furthermore by eliminating the need to consider recontamination this result simplifies the analysis of searcher requirements with respect to other properties of graphs"
57 "a new polynomial time decidable fragment of first order logic" "a new polynomial time decidable fragment of first order logic is identified and a general method for using polynomial time inference procedures in knowledge representation systems is presented the results shown in this paper indicate that a nonstandard taxonomic syntax is essential in constructing natural and powerful polynomial time inference procedures the central role of taxonomic syntax in the polynomial time inference procedures provides technical support for the oftenexpressed intuition that knowledge is better represented in terms of taxonomic relationships than classical first order formulas to use the procedures in a knowledge representation system a socratic proof system is defined which is complete for first order inference and which can be used as a semiautomated interface to a first order knowledge base"
58 "a procedure is given for recognizing sets of inference rules" "a procedure is given for recognizing sets of inference rules that generate polynomial time decidable inference relations the procedure can automatically recognize the tractability of the inference rules underlying congruence closure the recognition of tractability for that particular rule set constitutes mechanical verification of a theorem originally proved independently by kozen and shostak the procedure is algorithmic rather than heuristic and the class of automatically recognizable tractable rule sets can be precisely characterized a series of examples of rule sets whose tractability is nontrivial yet machine recognizable is also given the technical framework developed here is viewed as a first step toward a general theory of tractable inference relations"
59 "this paper analytically studies the performance of a synchronous conservative" "this paper analytically studies the performance of a synchronous conservative parallel discreteevent simulation protocol the class of models considered simulates activity in a physical domain and possesses a limited ability to predict future behavior using a stochastic model it is shown that as the volume of simulation activity in the model increases relative to a fixed architecture the complexity of the average perevent overhead due to synchronization event list manipulation lookahead calculations and processor idle time approaches the complexity of the average perevent overhead of a serial simulation sometimes rapidly the method is therefore within a constant factor of optimal the result holds for the worst case fullyconnected communication topology where an event in any other portion of the domain can cause an event in any other protion of the domain our analysis demonstrates that on large problemsthose for which parallel processing is ideally suited there is often enough parallel workload so that processors are not usually idle it also demonstrated the viability of the method empirically showing how good performance is achieved on large problems using a thirtytwo node intel ipsc distributed memory multiprocessor"
60 "time and knowledge are studied in synchronous and asynchronous distributed" "time and knowledge are studied in synchronous and asynchronous distributed systems a large class of problems that can be solved using logical clocks as if they were perfectly synchronized clocks is formally characterized for the same class of problems a broadcast primitive that can be used as if it achieves common knowledge is also proposed thus logical clocks and the broadcast primitive simplify the task of designing and verifying distributed algorithms the designer can assume that processors have access to perfectly synchronized clocks and the ability to achieve common knowledge"
61 "efficient ways of analyzing families of graphs that are generated" "efficient ways of analyzing families of graphs that are generated by a certain type of contextfree graph grammars are considered these graph grammars are called cellular graph grammars they generate the same graph families as hyperedge replacement systems but are defined in a way that supports complexity analysis a characteristic called finiteness of graph properties are defined and combinatorial algorithms are presented for deciding whether a graph language generated by a given cellular graph grammar contains a graph with a given finite graph property structural parameters are introduced that bound the complexity of the decision procedure and special cases for which the decision can be made in polynomial time are discussed extensions to graph grammars that are not contextfree are also given our results provide explicit and efficient combinatorial algorithms where so far only the existence of algorithms has been shown or the best known algorithms are highly inefficient"
63 "the concurrency control cc scheme employed can profoundly affect the" "the concurrency control cc scheme employed can profoundly affect the performance of transactionprocessing systems in this paper a simple unified approximate analysis methodology to model the effect on system performance of data contention under different cc schemes and for different system structures is developed this paper concentrates on modeling data contention and then as others have done in other papers the solutions of the data contention model are coupled with a standard hardware resource contention model through an iteration the methodology goes beyond previously published methods for analyzing cc schemes in terms of the generality of cc schemes and system structures that are handled the methodology is applied to analyze the performance of centralized transaction processing systems using various optimistic and pessimistictype cc schemes and for both fixedlength and variablelength transactions the accuracy of the analysis is demonstrated by comparison with simulations it is also shown how the methodology can be applied to analyze the performance of distributed transactionprocessing systems with replicated data"
64 "this paper introduces a general formulation of atomic snapshot memory" "this paper introduces a general formulation of atomic snapshot memory a shared memory partitioned into words written updated by individual processes or instantaneously read scanned in its entirety this paper presents three waitfree implementations of atomic snapshot memory the first implementation in this paper uses unbounded integer fields in these registers and is particularly easy to understand the second implementation uses bounded registers its correctness proof follows the ideas of the unbounded implementation both constructions implement a singlewriter snapshot memory in which each word may be updated by only one process from singlewriter nreader registers the third algorithm implements a multiwriter snapshot memory from atomic nwriter nreader registers again echoing key ideas from the earlier constructions all operations require thgrn reads and writes to the component shared"
65 "we consider logic programs with a single recursive rules whose" "we consider logic programs with a single recursive rules whose righthand side consists of binary relations forming a chain we give a complete characterization of all programs of this form that are computable in nc assuming that p our proof uses ideas from automata and language theory and the combinatorics of strings"
66 "what should it mean for an agent to know or" "what should it mean for an agent to know or believe an assertion is true with probability different papers give different answers choosing to use quite different probability spaces when computing the probability that an agent assigns to an event we show that each choice can be understood in terms of a betting game this betting game itself can be understood in terms of three types of adversaries influencing three different aspects of the game the first selects the outcome of all nondeterministic choices in the system the second represents the knowledge of the agents opponent in the betting game this is the key place the papers mentioned above differ and the third is needed in asynchronous systems to choose the time the bet is placed we illustrate the need for considering all three types of adversaries with a number of examples given a class of adversaries we show how to assign probability spaces to agents in a way most appropriate for that class where most appropriate is made precise in terms of this betting game we conclude by showing how different assignments of probability spaces corresponding to different opponents yield different levels of guarantees in probabilistic coordinated attack"
67 "many nonmonotonic formalism including default logic logic programming with stable" "many nonmonotonic formalism including default logic logic programming with stable models and autoepistemic logic can be represented faithfully by means of modal nonmonotonic logics in the family proposed by mcdermott and doyle in this paper properties of logics in this family are thoroughly investigated we present several results on characterization of expansions these results are applicable to a wide class of nonmonotonic modal logics using these characterization results algorithms for computing expansions for finite theories are developed perhaps the most important finding of this paper is that the structure of the family of modal nonmonotonic logics is much simpler than that of the family of underlying modal monotonic logics namely it is often the case that different monotonic modal logics collapse to the same nonmonotonic system we exhibit four families of logics whose nonmonotonic variants coincide kd twsw nwk and wdwb these nonmonotonic logics naturally represent logics related to commonsense reasoning and knowledge representation such as autoepistemic logic reflexive autoepistemic logic default logic and truth maintenance with negation"
68 "we consider a processor shared mm queue that can accommodate" "we consider a processor shared mm queue that can accommodate at most a finite number k of customers using singular perturbation techniques we construct asymptotic approximations to the distribution of a customers sojourn time we assume that k is large and treat several different cases of the model parameters and also treat different time scales extensive numerical comparisons are used to back up our asymptotic formulas"
69 "the fishspear priority queue algorithm is presented and analyzed fishspear" "the fishspear priority queue algorithm is presented and analyzed fishspear is comparable to the usual heap algorithm in its worstcase running time and its relative performance is much better in many common situations fishspear also differs from the heap method in that it can be implemented efficiently using sequential storage such as stacks or tapes making it potentially attractive for implementation of very large queues on paged memory systems"
70 "in this paper we prove the intractability of learning several" "in this paper we prove the intractability of learning several classes of boolean functions in the distributionfree model also called the probably approximately correct or pac model of learning from examples these results are representation independent in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses our methods reduce the problems of cracking a number of wellknown publickey cryptosystems to the learning problems we prove that a polynomialtime learning algorithm for boolean formulae deterministic finite automata or constantdepth threshold circuits would have dramatic consequences for cryptography and number theory in particular such an algorithm could be used to break the rsa cryptosystem factor blum integers composite numbers equivalent to modulo and detect quadratic residues the results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing the techniques used demonstrate an interesting duality between learning and cryptography we also apply our results to obtain strong intractability results for approximating a generalization of graph coloring"
71 "we introduce a temporal logic for the specification of realtime" "we introduce a temporal logic for the specification of realtime systems our logic tptl employs a novel quantifier construct for referencing time the freeze quantifier binds a variable to the time of the local temporal context tptl is both a natural language for specification and a suitable formalism for verification we present a tableaubased decision procedure and a modelchecking algorithm for tptl several generalizations of tptl are shown to be highly undecidable"
72 "a spanning tree in a graph is the smallest connected" "a spanning tree in a graph is the smallest connected spanning subgraph given a graph how does one find the smallest ie least number of edges connected spanning subgraph connectivity refers to both edge and vertex connectivity if not specified unfortunately the problem is known to be nphard we consider the problem of finding a better approximation to the smallest connected subgraph by an efficient algorithm for edge connectivity our algorithm guarantees a solution that is no more than times the optimal for vertex connectivity our algorithm guarantees a solution that is no more than times the optimal the previous best approximation factor is for each of these problems the new algorithms and their analyses depend upon a structure called a carving of a graph which is of independent interest we show that approximating the optimal solution to within an additive constant is nphard as well we also consider the case where the graph has edge weights for this case we show that an approximation factor of is possible in polynomial time for finding a kedge connected spanning subgraph this improves an approximation factor of for k due to frederickson and jaja and extends it for any k with an increased running time though"
73 "we describe the application of proof orderingsa technique for reasoning" "we describe the application of proof orderingsa technique for reasoning about inference systemsto various rewritebased theoremproving methods including refinements of the standard knuthbendix completion procedure based on critical pair criteria huets procedure for rewriting modulo a congruence ordered completion a refutationally complete extension of standard completion and a proof by consistency procedure for proving inductive theorems"
74 "a model that captures communication on asynchronous unidirectional rings is" "a model that captures communication on asynchronous unidirectional rings is formalized our model incorporates both probabilistic and nondeterministic features and is strictly more powerful than a purely probabilistic model using this model a collection of tools are developed that facilitate studying lower bounds on the expected communication complexity of monte carlo algorithms for language recognition problems on anonymous asynchronous unidirectional rings the tools are used to establish tight lower bounds on the expected bit complexity of the solitude verification problem that asymptotically match upper bounds for this problem the bounds demonstrate that for this problem the expected bit complexity depends subtly on the processors knowledge of the size of the ring and on whether or not processordetectable termination is required"
75 "we present a construction of a singlewriter multiplereader atomic register" "we present a construction of a singlewriter multiplereader atomic register from singlewriter singlereader atomic registers the complexity of our construction is asymptotically optimal om mn shared singlewriter singlereader safe bits are required to construct a singlewriter mreader nbit atomic register"
76 "we carry out an analysis of typability of terms in" "we carry out an analysis of typability of terms in ml our main result is that this problem is dexptimehard where by dexptime we mean dtimen this together with the known exponentialtime algorithm that solves the problem yields the dexptimecompleteness result this settles an open problem of p kanellakis and j c mitchell part of our analysis is an algebraic characterization of ml typability in terms of a restricted form of semiunification which we identify as acyclic semiunification we prove that ml typability and acyclic semiunification can be reduced to each other in polynomial time we believe this result is of independent interest"
77 "for any fixed dimension d the linear programming problem with" "for any fixed dimension d the linear programming problem with n inequality constraints can be solved on a probabilistic crcw pram with on processors almost surely in constant time the algorithm always finds the correct solution with ndlogd processors the probability that the algorithm will not finish within odlogd time tends to zero exponentially with n"
78 "the concepts of binary constraint satisfaction problems can be naturally" "the concepts of binary constraint satisfaction problems can be naturally generalized to the relation algebras of tarski the concept of pathconsistency plays a central role algorithms for pathconsistency can be implemented on matrices of relations and on matrices of elements from a relation algebra we give an example of a by matrix of infinite relations on which on iterative local pathconsistency algorithm terminates we give a class of examples over a fixed finite algebra on which all iterative local algorithms whether parallel or sequential must take quadratic time specific relation algebras arising from interval constraint problems are also studied the interval algebra the point algebra and the containment algebra"
79 "the problem of coloring a graph with the minimum number" "the problem of coloring a graph with the minimum number of colors is well known to be nphard even restricted to kcolorable graphs for constant k this paper explores the approximation problem of coloring kcolorable graphs with as few additional colors as possible in polynomial time with special focus on the case of k the previous best upper bound on the number of colors needed for coloring colorable nvertex graphs in polynomial time was onlogn colors by berger and rompel improving a bound of on colors by wigderson this paper presents an algorithm to color any colorable graph with onpolylog n colors thus breaking an on ogr barrier the algorithm given here is based on examining secondorder neighborhoods of vertices rather than just immediate neighborhoods of vertices as in previous approaches we extend our results to improve the worstcase bounds for coloring kcolorable graphs for constant k as well"
80 "we investigate the descriptive succinctness of three fundamental notions for" "we investigate the descriptive succinctness of three fundamental notions for modeling concurrency nondeterminism and pure parallelism the two facets of alternation and bounded cooperative concurrency whereby a system configuration consists of a bounded number of cooperating states our results are couched in the general framework of finitestate automata but hold for appropriate versions of most concurrent models of computation such as petri nets statecharts or finitestate versions of concurrent programming languages we exhibit exhaustive sets of upper and lower bounds on the relative succinctness of these features over sgr and sgrohgr establishing that each of the three features represents an exponential saving in succinctness of the representation in a manner that is independent of the other two and additive with respect to them of the three bounded concurrency is the strongest representing a similar exponential saving even when substituted for each of the others for example we prove exponential upper and lower bounds on the simulation of deterministic concurrent automata by afas and tripleexponential bounds on the simulation of alternating concurrent automata by dfas"
81 "this is the second in a series of papers on" "this is the second in a series of papers on the inherent power of bounded cooperative concurrency whereby an automaton can be in some bounded number of states that cooperate in accepting the input in this paper we consider pushdown automata we are interested in differences in power of expression and in exponential or higher discrepancies in succinctness between variants of pdas that incorporate nondeterminism e pure parallelism a and bounded cooperative concurrency c technically the results are proved for cooperating pushdown automata with cooperating states but they hold for appropriate versions of most concurrent models of computation we exhibit exhaustive sets of upper and lower bounds on the relative succinctness of these features for three classes of languages deterministic contextfree regular and finite for example we show that c represents exponential savings in succinctness in all cases except when both e and a are present ie except for alternating automata and that e and a represent unlimited savings in succinctness in all cases"
82 "we present new procedures for inferring the structure of a" "we present new procedures for inferring the structure of a finitestate automaton fsa from its inputoutput behavior using access to the automaton to perform experiments our procedures use a new representation for finite automata based on the notion of equivalence between tests we call the number of such equivalence classes the diversity of the automaton the diversity may be as small as the logarithm of the number of states of the automaton for the special class of permutation automata we describe an inference procedure that runs in time polynomial in the diversity and logdgr where dgr is a given upper bound on the probability that our procedure returns an incorrect result since our procedure uses randomization to perform experiments there is a certain controllable chance that it will return an erroneous result we also discuss techniques for handling more general automata we present evidence for the practical efficiency of our approach for example our procedure is able to infer the structure of an automaton based on rubiks cube which has approximately states in about minutes on a dec microvax this automaton is many orders of magnitude larger than possible with previous techniques which would require time proportional at least to the number of global states note that in this example only a small fraction of the global states were even visited finally we present a new procedure for inferring automata of a special type in which the global state is composed of a vector of binary local state variables all of which are observable or visible to the experimenter our inference procedure runs provably in time polynomial in the size of this vector which happens to be the diversity of the automaton even though the global state space may be exponentially larger the procedure plans and executes experiments on the unknown automaton we show that the number of input symbols given to the automaton during this process is to within a constant factor the best possible"
84 "we consider the following problem given a collection of strings" "we consider the following problem given a collection of strings s sm find the shortest string s such that each si appears as a substring a consecutive block of s although this problem is known to be nphard a simple greedy procedure appears to do quite well and is routinely used in dna sequencing and data compression practice namely repeatedly merge the pair of distinct strings with maximum overlap until only one string remains let n denote the length of the optimal superstring a common conjecture states that the above greedy procedure produces a superstring of length on in fact n yet the only previous nontrivial bound known for any polynomialtime algorithm is a recent on log n result we show that the greedy algorithm does in fact achieve a constant factor approximation proving an upper bound of n furthermore we present a simple modified version of the greedy algorithm that we show produces a superstring of length at most n we also show the superstring problem to be maxsnphard which implies that a polynomialtime approximation scheme for this problem is unlikely"
85 "three new decomposition methods are developed for the exact analysis" "three new decomposition methods are developed for the exact analysis of stochastic multifacility blocking models of the productform type the first is a basic decomposition algorithm that reduces the analysis of blocking probabilities to that of two separate subsystems the second is a generalized msubsystem decomposition method the third is a more elaborate and efficient incremental decomposition technique all of the algorithms exploit the sparsity of locality that can be found in the demand matrix of a system by reducing the analysis to that of a set of subsystems the overall dimensionality of the problem is diminished and the computational requirements are reduced significantly this enables the efficient computation of blocking probabilities in large systems several numerical examples are provided to illustrate the computational savings that can be realized"
86 "one of the most important performance measures for computer system" "one of the most important performance measures for computer system designers is system availability most often markov models are used in representing systems for dependabilityavailability analysis due to complex interactions between components and complex repair policies the markov model often has an irregular structure and closedform solutions are extremely difficult to obtain also a realistic system model often has an unmanageably large state space and it quickly becomes impractical to even generate the entire transition rate matrix in this paper we present a methodology that can i bound the system steady state availability and at the same time ii drastically reduce the state space of the model that must be solved the bounding algorithm is iterative and generates a part of the transition matrix at each step at each step tighter bounds on system availability are obtained the algorithm also allows the size of the submodel to be solved at each step to be chosen so as to accommodate memory limitations this general bounding methodology provides an efficient way to evaluate dependability models with very large state spaces without ever generating the entire transition rate matrix"
87 "text compression methods can be divided into two classes symbolwise" "text compression methods can be divided into two classes symbolwise and parsing symbolwise methods assign codes to individual symbols while parsing methods assign codes to groups of consecutive symbols phrases the set of phrases available to a parsing method is referred to as a dictionary the vast majority of parsing methods in the literature use greedy parsing including nearly all variations of the popular zivlempel methods when greedy parsing is used the coder processes a string from left to right at each step encoding as many symbols as possible with a phrase from the dictionary this parsing strategy is not optimal but an optimal method cannot guarantee a bounded coding delay an important problem in compression research has been to establish the relationship between symbolwise methods and parsing methods this paper extends prior work that shows that there are symbolwise methods that simulate a subset of greedy parsing methods we provide a more general algorithm that takes any nonadaptive greedy parsing method and constructs a symbolwise method that achieves exactly the same compression combined with the existence of symbolwise equivalents for two of the most significant adaptive parsing methods this result gives added weight to the idea that research aimed at increasing compression should concentrate on symbolwise methods while parsing methods should be chosen for speed or temporary storage considerations"
88 "the time complexity of waitfree algorithms in normal executions where" "the time complexity of waitfree algorithms in normal executions where no failures occur and processes operate at approximately the same speed is considered a lower bound of log n on the time complexity of any waitfree algorithm that achieves approximate agreement among n processes is proved in contrast there exists a nonwaitfree algorithm that solves this problem in constant time this implies an ohgrlog n time separation between the waitfree and nonwaitfree computation models on the positive side we present an olog n time waitfree approximate agreement algorithm the complexity of this algorithm is within a small constant of the lower bound"
89 "this paper investigates the computational complexity of planning the motion" "this paper investigates the computational complexity of planning the motion of a body b in d or d space so as to avoid collision with moving obstacles of known easily computed trajectories dynamic movement problems are of fundamental importance to robotics but their computational complexity has not previously been investigated we provide evidence that the d dynamic movement problem is intractable even if b has only a constant number of degrees of freedom of movement in particular we prove the problem is pspacehard if b is given a velocity modulus bound on its movements and is nphard even if b has no velocity modulus bound where in both cases b has degrees of freedom to prove these results we use a unique method of simulation of a turing machine that uses time to encode configurations whereas previous lower bound proofs in robotic motion planning used the system position to encode configurations and so required unbounded number of degrees of freedom we also investigate a natural class of dynamic problems that we call asteroid avoidance problems b the object we wish to move is a convex polyhedron that is free to move by translation with bounded velocity modulus and the polyhedral obstacles have known translational trajectories but cannot rotate this problem has many applications to robot automobile and aircraft collision avoidance our main positive results are polynomial time algorithms for the d asteroid avoidance problem where b is a moving polygon and we assume a constant number of obstacles as well as single exponential time or polynomial space algorithms for the d asteroid avoidance problem where b is a convex polyhedron and there are arbitrarily many obstacles our techniques for solving these asteroid avoidance problems use normal path arguments which are an intereting generalization of techniques previously used to solve static shortest path problems we also give some additional positive results for various other dynamic movers problems and in particular give polynomial time algorithms for the case in which b has no velocity bounds and the movements of obstacles are algebraic in spacetime"
91 "one important facet of commonsense reasoning is the ability to" "one important facet of commonsense reasoning is the ability to draw default conclusions about the state of the world so that one can for example assume that a given bird flies in the absence of information to the contrary a deficiency in the circumscriptive approach to commonsense reasoning has been its difficulties in producing default that tweety blutto using ordinary circumscription or conclude by default that a particular bird flies if some birds are known not to fly in this paper we introduce a new form of circumscription based on homomorphisms between models that remedies these two problems and still retains the major desirable properties of traditional forms of circumscription"
92 "in this paper we study quantitative as well as qualitative" "in this paper we study quantitative as well as qualitative properties of forkjoin queuing networks with blocking fjqnbs specifically we prove results regarding the equivalence of the behavior of a fjqnb and that of its duals and a strongly connected marked graph in addition we obtain general conditions that must be satisfied by the service times to guarantee the existence of a longterm throughput and its independence on the initial configuration we also establish conditions under which the reverse of a fjqnb has the same throughput as the original network by combining the equivalence result for duals and the reversibility result we establish a symmetry property for the throughput of a fjqnb last we establish that the throughput is a concave function of the buffer sizes and the initial marking provided that the service times are mutually independent random variables belonging to the class of pert distributions that includes the erlang distributions this last result coupled with the symmetry property can be used to identify the initial configuration that maximizes the longterm throughput in closed seriesparallel networks"
93 "this paper considers the problem of representing stacks with catenation" "this paper considers the problem of representing stacks with catenation so that any stack old or new is available for access or update operations this problem arises in the implementation of listbased and functional programming languages a solution is proposed requiring constant time and space for each stack operation except catenation which requires olog log k time and space here k is the number of stack operations done before the catenation all the resource bounds are amortized over the sequence of operations"
94 "we present a practical algorithm for finding minimumlength paths between" "we present a practical algorithm for finding minimumlength paths between points in the euclidean plane with not necessarily convex polygonal obstacles prior to this work the best known algorithm for finding the shortest path between two points in the plane required ohgrn log n time and on space where n denotes the number of obstacle edges assuming that a triangulation or a voronoi diagram for the obstacle space is provided with the input if is not either one can be precomputed in on log n time we present an okn time algorithm where k denotes the number of islands connected components in the obstacle space the algorithm uses only on space and given a source point s produces an o n size data structure such that the distance between s and any other point x in the plane x is not necessarily an obstacle vertex or a point on an obstacle edge can be computed in o time the algorithm can also be used to compute shortest paths for the movement of a disk so that optimal movement for arbitrary objects can be computed to the accuracy of enclosing them with the smallest possible disk"
95 "we derive a singleexponential time upper bound for finding the" "we derive a singleexponential time upper bound for finding the shortest path between two points in dimensional euclidean space with nonnecessarily convex polyhedral obstacles prior to this work the best known algorithm required doubleexponential time given that the problem is known to be pspacehard the bound we present is essentially the best in the worstcase sense that can reasonably be expected"
96 "many fundamental multiprocessor coordination problems can be expressed as counting" "many fundamental multiprocessor coordination problems can be expressed as counting problems processes must cooperate to assign successive values from a given range such as addresses in memory or destinations on an interconnection network conventional solutions to these problems perform poorly because of synchronization bottlenecks and high memory contentionmotivated by observations on the behavior of sorting networks we offer a new approach to solving such problems by introducing counting networks a new class of networks that can be used to count we give two counting network constructions one of depth log n plus log n using n log plus log n gates and a second of depth log n using n log n gates these networks avoid the sequential bottlenecks inherent to earlier solutions and substantially lower the memory contentionfinally to show that counting networks are not merely mathematical creatures we provide experimental evidence that they outperform conventional synchronization techniques under a variety of circumstances"
97 "given an offline sequence s of n setmanipulation operations we" "given an offline sequence s of n setmanipulation operations we investigate the parallel complexity of evaluating s ie finding the response to every operation in s and returning the resulting set we show that the problem of evaluating s is in nc for various combinations of common setmanipulation operations once we establish membership in nc or if membership in nc is obvious we develop techniques for improving the time andor processor complexity"
98 "the problem of verifiable secret sharing vss is the following" "the problem of verifiable secret sharing vss is the following a dealer who may be honest or cheating can share a secret s among n t players where t players at most are cheaters the sharing process will cause the dealer to commit himself to a secret s if the dealer is honest then during the sharing process the set of dishonest players will have no information about s when the secret is reconstructed at a later time all honest players will reconstruct s the solution that is given is a constant round protocol with polynomial time local computations and polynomial message size the protocol assumes private communication lines between every two participants and a broadcast channel the protocol achieves the desired properties with an exponentially small probability of errora new tool called information checking which provides authentication and is not based on any unproven assumptions is introduced and may have wide application elsewherefor the case in which it is known that the dealer is honest a simple constant round protocol is proposed without assuming broadcasta weak version of secret sharing is defined weak secret sharing wss wss has the same properties as vss for the sharing process but during reconstruction if the dealer is dishonest then he might obstruct the reconstruction of s a protocol for wss is also introduced this protocol has an exponentially small probability of error wss is an essential building block for vss for certain applications the much simpler wss protocol sufficeall protocols introduced in this paper are secure in the information theoretic sense"
99 "although many closed multiclass queuing networks have a productform solution" "although many closed multiclass queuing networks have a productform solution evaluating their performance measures remains nontrivial due to the presence of a normalization constant we propose the application of monte carlo summation in order to determine the normalization constant throughputs and gradients of throughputs a class of importancesampling functions leads to a decomposition approach where separate singleclass problems are first solved in a setup module and then the original problem is solved by aggregating the singleclass solutions in an execution model we also consider monte carlo methods for evaluating performance measures based on integral representations of the normalization constant a theory for optimal importance sampling is developed computational examples are given that illustrate that the monte carlo methods are robust over a wide range of networks and can rapidly solve networks that cannot be handled by the techniques in the existing literature"
100 "bottomup evaluation of deductive database programs has the advantage that" "bottomup evaluation of deductive database programs has the advantage that it avoids repeated computations by storing all intermediate results and replacing recomputation by table lookup however in general storing all intermediate results for the duration of a computation wastes space in this paper we propose an evaluation scheme that avoids recomputation yet for a fairly general class of programs at any given time stores only a small subset of the facts generated the results constitute a significant first step in compiletime garbage collection for bottomup evaluation of deductive database programs"
101 "though the declarative semantics of both explicit and nonmonotonic negation" "though the declarative semantics of both explicit and nonmonotonic negation in logic programs has been studied extensively relatively little work has been done on computation and implementation of these semantics in this paper we study three different approaches to computing stable models of logic programs based on mixed integer linear programming methods for automated deduction introduced by r jeroslow we subsequently discuss the relative efficiency of these algorithms the results of experiments with a prototype compiler implemented by us tend to confirm our theoretical discussion in contrast to resolution the mixed integer programming methodology is both fully declarative and handles reuse of old computations gracefullywe also introduce compare implement and experiment with linear constraints corresponding to four semantics for explicit negation in logic programs the fourvalued annotated semantics blair and subrahmanian the gelfondlifschitz semantics the overdetermined models grant and subrahmanian the gelfondlifschitz semantics the overdetermined models grant and subrahmanian and the classical logic semantics gelfond and lifschitz argue for simultaneous use of two modes of negation in logic programs classical and nonmonotonic so we give algorithms for computing answer sets for such logic programs too"
102 "a class of modularly stratified logic programs is defined modular" "a class of modularly stratified logic programs is defined modular stratification generalizes stratification and local stratification while allowing programs that are not expressible as stratified programs for modularly stratified programs the wellfounded semantics coincides with the stable model semantics and makes every ground literal true or false modularly stratified programs are weakly stratified but the converse is false unlike some weakly stratified programs modularly stratified programs can be evaluated in a subgoalata time fashion an extension of topdown methods with memoing that handles this broader class of programs is presented a technique for rewriting a modularly stratified program for bottomup evaluation is demonstrated and extended to include magicset techniques the rewritten program when evaluated bottomup gives correct answers according to the wellfounded semantics but much more efficiently than computing the complete wellfounded model a onetoone correspondence between steps of the extended topdown method and steps during the bottomup evaluation of the magicrewritten program is exhibited demonstrating that the complexity of the two methods is the same extensions of modular stratification to other operators such as setgrouping and aggregation which have traditionally been stratified to prevent semantic difficulties are discussed"
103 "efficient distributionfree learning of boolean formulas from positive and negative" "efficient distributionfree learning of boolean formulas from positive and negative examples is considered it is shown that classes of formulas that are efficiently learnable from only positive examples or only negative examples have certain closure properties a new substitution technique is used to show that in the distributionfree case learning dnf disjunctive normal form formulas is no harder than learning monotone dnf we prove that monomials cannot be efficiently learned from negative examples alone even if the negative examples are uniformly distributed it is also shown that if the examples are drawn from uniform distributions then the class of dnf in which each variable occurs at most once is efficiently weakly learnable ie individual examples are correctly classified with a probability larger than p where p is a polynomial in the relevant parameters of the learning problem we then show an equivalence between the notion of weak learning and the notion of group learning where a group of examples of polynomial size either all positive or all negative must be correctly classified with high probability"
104 "abduction is an important form of nonmonotonic reasoning allowing one" "abduction is an important form of nonmonotonic reasoning allowing one to find explanations for certain symptoms or manifestations when the application domain is described by a logical theory we speak about logicbased abduction candidates for abductive explanations are usually subjected to minimality criteria such as subsetminimality minimal cardinality minimal weight or minimality under prioritization of individual hypotheses this paper presents a comprehensive complexity analysis of relevant decision and search problems related to abduction on propositional theories our results indicate that abduction is harder than deduction in particular we show that with the most basic forms of abduction the relevant decision problems are complete for complexity classes at the second level of the polynomial hierarchy while the use of prioritization raises the complexity to the third level in certain cases"
105 "we introduce a new subclass of allens interval algebra we" "we introduce a new subclass of allens interval algebra we call ordhorn subclass which is a strict superset of the pointisable subclass we prove that reasoning in the ordhorn subclass is a polynomialtime problem and show that the pathconsistency method is sufficient for deciding satisfiability further using an extensive machinegenerated case analysis we show that the ordhorn subclass is a maximal tractable subclass of the full algebra assuming p np in fact it is the unique greatest tractable subclass amongst the subclasses that contain all basic relations"
107 "network throughput can be increased by allowing multipath adaptive routing" "network throughput can be increased by allowing multipath adaptive routing adaptive routing allows more freedom in the paths taken by messages spreading load over physical channels more evenly the flexibility of adaptive routing introduces new possibilities of deadlock previous deadlock avoidance schemes in kary ncubes require an exponential number of virtual channels we describe a family of deadlockfree routing algorithms called planaradaptive routing algorithms that require only a constant number of virtual channels independent of networks size and dimension planaradaptive routing algorithms reduce the complexity of deadlock prevention by reducing the number of choices at each routing step in the faultfree case planaradaptive networks are guaranteed to be deadlockfree in the presence of network faults the planaradaptive router can be extended with misrouting to produce a working network which remains provably deadlock free and is provably livelock free in addition planaradaptive networks can simultaneously support both inorder and adaptive outoforder packet deliveryplanaradaptive routing is of practical significance it provides the simplest known support for deadlockfree adaptive routing in kary ncubes of more than two dimensions with k restricting adaptivity reduces the hardware complexity improving router speed or allowing additional performanceenhancing network features the structure of planaradaptive routers is amenable to efficient implementationsimulation studies show that planaradaptive routers can increase the robustness of network throughput for nonuniform communication patterns planaradaptive routers outperform deterministic routers with equal hardware resources further adding virtual lanes to planaradaptive routers increases this advantage comparisons with fully adaptive routers show that planaradaptive routers limited adaptive routers can give superior performance these results indicate the best way to allocate router resources to combine adaptivity and virtual lanesplanaradaptive routers are a special case of limited adaptivity routers we define a class of adaptive routers with f degrees of routing freedom this class termed fflat adaptive routers allows a direct costperformance tradeoff between implementation cost speed and silicon area and routing freedom channel utilization for a network of a particular dimension the cost of adaptivity grows linearly with the routing freedom however the rate of growth is a much larger constant for highdimensional networks all of the properties proven for planaradaptive routers such as deadlock and livelock freedom also apply to fflat adaptive routers"
108 "emulators that translate algorithms from the sharedmemory model to two" "emulators that translate algorithms from the sharedmemory model to two different messagepassing models are presented both are achieved by implementing a waitfree atomic singlewriter multireader register in unreliable asynchronous networks the two messagepassing models considered are a complete network with processor failures and an arbitrary network with dynamic link failuresthese results make it possible to view the sharedmemory model as a higherlevel language for designing algorithms in asynchronous distributed systems any waitfree algorithm based on atomic singlewriter multireader registers can be automatically emulated in messagepassing systems provided that at least a majority of the processors are not faulty and remain connected the overhead introduced by these emulations is polynomial in the number of processors in the systemimmediate new results are obtained by applying the emulators to known sharedmemory algorithms these include among others protocols to solve the following problems in the messagepassing model in the presence of processor or link failures multiwriter multireader registers concurrent timestamp systems lexclusion atomic snapshots randomized consensus and implementation of data structures"
109 "this paper gives two simple efficient distributed algorithms one for" "this paper gives two simple efficient distributed algorithms one for keeping clocks in a network synchronized and one for allowing new processors to join the network with their clocks synchronized assuming a faulttolerant authentication protocol the algorithms tolerate both link and processor failures of any type the algorithm for maintaining synchronization works for arbitrary networks rather than just completely connected networks and tolerates any number of processor or communication link faults as long as the correct processors remain connected by faultfree paths it thus represents an improvement over other clock synchronization algorithms such as those of lamport and melliar smith and welch and lynch although unlike them it does require an authentication protocol to handle byzantine faults our algorithm for allowing new processors to join requires that more than half the processors be correct a requirement that is provably necessary"
110 "a simple waitfree construction of writer multireader multivalued atomic variable" "a simple waitfree construction of writer multireader multivalued atomic variable from multireader regular variables is presented in this paper a key point of the construction is the use of an elegant forwarding technique to overcome the newold inversion property inherent in regular variablesanother construction using a different forwarding technique is also given this technique is a refinement of one proposed in the literatureformal correctness proofs for both the constructions are short and easy to follow"
111 "in this paper we derive bounds on the speedup and" "in this paper we derive bounds on the speedup and efficiency of applications that schedule tasks on a set of parallel processors we assume that the application runs an algorithm that consists of n iterations and before starting its ist iteration a processor must wait for data ie synchronize calculated in the ith iteration by a subset of the other processors of the system processing times and interconnections between iterations are modeled by random variables with possibly deterministic distributions scientific applications consisting of iterations of recursive equations are examples of such applications that can be modeled within this formulation we consider the efficiency of applications and show that although efficiency decreases with an increase in the number of processors it has a nonzero limit when the number of processors increases to infinity we obtain a lower bound for the efficiency by solving an equation that depends on the distribution of task service times and the expected number of tasks needed to be synchronized we also show that the lower bound is approached if the topology of the processor graph is ldquo spreadout a notion we define in the paper"
112 "in the concurrent language ccs two programs are considered the" "in the concurrent language ccs two programs are considered the same if they are bisimilar several years and many researchers have demonstrated that the theory of bisimulation is mathematically appealing and useful in practice however bisimulation makes too many distinctions between programs we consider the problem of adding operations to ccs to make bisimulation fully abstract we define the class of gsos operations generalizing the style and technical advantages of ccs operations we characterize gsos congruence in as a bisimulationlike relation called readysimulation bisimulation is strictly finer than ready simulation and hence not a congruence for any gsos language"
113 "a program correctness checker is an algorithm for checking the" "a program correctness checker is an algorithm for checking the output of a computation that is given a program and an instance on which the program is run the checker certifies whether the output of the program on that instance is correct this paper defines the concept of a program checker it designs program checkers for a few specific and carefully chosen problems in the class fp of functions computable in polynomial time problems in fp for which checkers are presented in this paper include sorting matrix rank and gcd it also applies methods of modern cryptography especially the idea of a probabilistic interactive proof to the design of program checkers for group theoretic computationstwo structural theorems are proven here one is a characterization of problems that can be checked the other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable all problems in the class are checkable"
114 "we investigate logical formalization of the effects of actions in" "we investigate logical formalization of the effects of actions in the situation calculus we propose a formal criterion against which to evaluate theories of deterministic actions we show how the criterion provides us a formal foundation upon which to tackle the frame problem as well as its variant in the context of concurrent actions our main technical contributions are in formulating a wide class of monotonic causal theories that satisfy the criterion and showing that each such theory can be reformulated succinctly in circumscription"
115 "we present a randomized lineartime algorithm to find a minimum" "we present a randomized lineartime algorithm to find a minimum spanning tree in a connected graph with edge weights the algorithm uses random sampling in combination with a recently discovered lineartime algorithm for verifying a minimum spanning tree our computational model is a unitcost randomaccess machine with the restriction that the only operations allowed on edge weights are binary comparisons"
116 "the magic rewriting focuses on relevant data but suffers from" "the magic rewriting focuses on relevant data but suffers from additional rules predicates and tuples that are generated in search for the relevant data reducing the arity of predicates can cut down the number of such rules predicates and tuples by an exponential factor in this paper we consider a subclass of linear singleidb programs and show that the magic rewriting can be decomposed in such a way that it is applied to only programs having smaller arities and fewer recursive rules without losing the binding capacity the decomposed rewriting is shown to be much more efficient than the standard one and amenable to distributed and parallel environments the considered subclass significantly generalizes recursions previously proposed for efficient implementation the decomposed rewriting and the standard generalized magic rewriting are extended to multibinding queries in such a way that data relevant to one binding is not necessarily considered as relevant to other bindings the work in this paper shows the use of tuple id as an important technique in optimizing logic programs"
117 "we study the communication complexity of asynchronous distributed algorithms such" "we study the communication complexity of asynchronous distributed algorithms such algorithms can generate excessively many messages in the worst case nevertheless we show that under certain probabilistic assumptions the expected number of messages generated per time unit is bounded by a polynomial function of the number of processors under a very general model of distributed computation furthermore for constantdegree processor graphs the expected number of generated messages is only ont where n is the number of processors and t is the running time we conclude that under our model any asynchronous algorithm with good time complexity will also have good communication complexity on the average"
118 "in this paper we consider problems and complexity classes definable" "in this paper we consider problems and complexity classes definable by interdependent queries to an oracle in np how the queries depend on each other is specified by a directed graph g we first study the class of problems where g is a general dag and show that this class coincides with d p we then consider the class where g is a tree our main result states that this class is identical to pnpolog n the class of problems solvable in polynomial time with a logarithmic number of queries to an oracle in np this result has interesting applications in the fields of modal logic and artificial intelligence in particular we show that the following problems are all pnpolog n complete validitychecking of formulas in carnaps modal logic checking whether a formula is almost surely valid over finite structures in modal logics k t and s a problem recently considered by halpern and kapron and checking whether a formula belongs to the stable set of beliefs generated by a propositional theorywe generalize the case of dags to the case where g is a general possibly cyclic directed graph of nporacle queries and show that this class corresponds to pp we show that such graphs are easily expressible in autoepistemic logic finally we generalize our complexity results to higher classes of the polynomialtime hierarchy"
119 "three temporal logics are introduced that induce on labeled transition" "three temporal logics are introduced that induce on labeled transition systems the same identifications as branching bisimulation a behavioral equivalence that aims at ignoring invisible transitions while preserving the branching structure of systems the first logic is an extension of hennessymilner logic with an until operator the second one is another extension of hennessymilner logic which exploits the power of backward modalities the third logic is ctl without the nexttime operator a relevant sideeffect of the last characterization is that it sets a bridge between the state and actionbased approaches to the semantics of concurrent systems"
120 "this paper gives an algorithm for solving linear programming problems" "this paper gives an algorithm for solving linear programming problems for a problem with n constraints and d variables the algorithm requires an expectedodnlognoddoodnlogn arithmetic operations asn the constant factors do not depend on d also an algorithm is given for integer linear programming let bound the number of bits required to specify the rational numbers defining an input constraint or the objective function vector let n and d be as before then the algorithm requires expected oddnddnl nnlnn dod lnn operations on numbers with do bits as n where the constant factors do not depend on d orto other convex programming problems for example an algorithm for finding the smallest sphere enclosing a set of n points in edhas the same time bound"
121 "this paper presents algorithms for routing channels with l layers" "this paper presents algorithms for routing channels with l layers for the unit vertical overlap model we describe a twolayer channel routing algorithm that uses at most dod tracks to route twoterminal net problems and dod tracks to route multiterminal nets we also show that dwlog d tracks are required to route twoterminal net problems in the worst case even if arbitrary vertical overlap is allowed we generalize the algorithm to unrestricted multilayer routing and use only dlo dl tracks for twoterminal net problems within within odl tracks of optimaland d lo dl tracks for multiterminal net problems within a factor ofl ltimes optimal we demonstrate the generality of our routing strategy by showing that it can be used to duplicate some of the best previous upper bounds for other models twolayer manhattan routing and two and threelayer knockknee routing of twoterminal twosided nets and gives a new upper bound for rotuing with degree diagonal wires"
122 "a new algorithm is developed for calculating normalization constants partition" "a new algorithm is developed for calculating normalization constants partition functions and moments of productform steadystate distributions of closed queuing networks and related models the essential idea is to numerically invert the generating function of the normalization constant and related generating functions appearing in expressions for the moments it is known that the generating function of the normalization constant often has a remarkably simple form but numerical inversion evidently has not been considered before for pdimensional transforms as occur with queuing networks having p closed chains the algorithm recursively performs p onedimensional inversions the required computation grows exponentially in the dimension but the dimension can often be reduced by exploiting conditional decomposition based on special structure for large populations the inversion algorithm is made more efficient by computing large sums using euler summation the inversion algorithm also has a very low storage requirement a key ingredient in the inversion algorithm is scaling an effective static scaling is developed for multichain closed queuing networks with only singleserver and optionally infiniteserver queues an important feature of the inversion algorithm is a selfcontained accuracy check which allows the results to be verified in the absence of alternative algorithms"
123 "we prove that the work function algorithm for the kserver" "we prove that the work function algorithm for the kserver problem has a competitive ratio at most k manasse et al conjectured that the competitive ratio for the kserver problem is exactly k it is trivially at least k previously the bestknown upper bound was exponential in k our proof involves three crucial ingredients a quasiconvexity property of work functions a duality lemma that uses quasiconvexity to characterize the configuration that achieve maximum increase of the work function and a potential function that exploits the duality lemma"
124 "implementation of programming language interpreters proving theorem of the form" "implementation of programming language interpreters proving theorem of the form ab implementation of abstract data types and program optimization are all problems that can be reduced to the problem of finding a normal form for an expression with respect to a finite set of equations in chew proposed an elegant congruence closure based simplifier ccns for computing with regular systems which stores the history of it computations in a compact data structure in verma and ramakrishnan showed that it can also be used for noetherian systems with no overlapsin this paper we develop a general theory of using ccns for computing normal forms and present several applications our results are more powerful and widely applicable than earlier work we present an independent set of postulates and prove that ccns can be used for any system that satisfies them this proof is based on the notion of strong closure we then show that ccns can be used for consistent convergent systems and for various kinds of priority rewrite systems this is the first time that the applicability of ccns has been shown for priority systems finally we present a new and simpler translation scheme for converting convergent systems into effectively nonoverlapping convergent priority systems such a translation scheme has been proposed earlier but we show that it is incorrectbecause ccns requires some strong properties of the given system our demonstration of its wide applicability is both difficult and surprising the tension between demands imposed by ccns and our efforts to satisfy them gives our work much general significance our results are partly achieved through the idea of effectively simulating bad systems by almostequivalent good ones partly through our theory that substantially weakens the demands and partly through the design of a powerful and unifying reduction proof method"
125 "this paper deals with the problem of maintaining a distributed" "this paper deals with the problem of maintaining a distributed directory server that enables us to keep track of mobile users in a distributed network the paper introduces the graphtheoretic concept of regional matching and demonstrates how finding a regional matching with certain parameters enables efficient tracking the communication overhead of our tracking mechanism is within a polylogarithmic factor of the lower bound"
126 "we consider the sequence transmission problem that is the problem" "we consider the sequence transmission problem that is the problem of transmitting an infinite sequence of messages xxx over a channel that can both lose and reorder packets we define performance measures ideal transmission cost and recovery cost for protocols that solve the sequence transmission problem ideal transmission cost measures the number of packets needed to deliver xn when the channel is behaving ideally and recovery cost measures how long it takes in terms of number of messages delivered for the ideal transmission cost to take hold once the channel begins behaving ideally we also define lookahead which measures the number of messages the sender can be ahead of the receiver in the protocol we show that any protocol with constant recovery cost and lookahead requires linear ideal transmission cost we describe a protocol plin that has ideal transmission cost n recovery cost and lookahead"
127 "the spectral method is the best currently known technique to" "the spectral method is the best currently known technique to prove lower bounds on expansion ramanujan graphs which have asymptotically optimal second eigenvalue are the bestknown explicit expanders the spectral method yielded a lower bound of k on the expansion of linearsized subsets of kregular ramanujan graphs we improve the lower bound on the expansion of ramanujan graphs to approximately k moreover we construct a family of kregular graphs with asymptotically optimal second eigenvalue and linear expansion equal to k this shows that k is the best bound one can obtain using the second eigenvalue method we also show an upper bound of roughly k on the average degree of linearsized induced subgraphs of ramanujan graphs this compares positively with the classical bound k as a byproduct we obtain improved results on random walks on expanders and construct selection networks respectively extrovert graphs of smaller size respectively degree than was previously known"
129 "since konoliges translation of default logic into strongly grounded autoepistemic" "since konoliges translation of default logic into strongly grounded autoepistemic logic several other variants of moores original autoepistemic logic that embody default logic have been studied all these logics differ significantly from moores autoepistemic logic standard ael in that expansions are subject to additional groundednessconditions hence the question naturally arises whether default logic can be translated into standard ael at all we show that a modular translation is not possible however we are able to construct a faithful polynomialtime translation from default logic into standard ael which is nonmodular our translation exploits the selfreferentiality of ael it uses as an important intermediate step an embedding of mareks and truszczynskis nonmonotonic logic n into standard ael it follows from our results that the expressive power of standard ael is strictly greater than that of default logic"
130 "we propose a novel formalism called frame logic abbr flogic" "we propose a novel formalism called frame logic abbr flogic that accounts in a clean and declarative fashion for most of the structural aspects of objectoriented and framebased languages these features include object identity complex objects inheritance polymorphic types query methods encapsulation and others in a sense flogic stands in the same relationship to the objectoriented paradigm as classical predicate calculus stands to relational programming flogic has a modeltheoretic semantics and a sound and complete resolutionbased proof theory a small number of fundamental concepts that come from objectoriented programming have direct representation in flogic other secondary aspects of this paradigm are easily modeled as well the paper also discusses semantic issues pertaining to programming with a deductive objectoriented language based on a subset of flogic"
131 "we determine the complexity of testing whether a finite state" "we determine the complexity of testing whether a finite state sequential or concurrent probabilistic program satisfies its specification expressed in lineartime temporal logic for sequential programs we present an algorithm that runs in time linear in the program and exponential in the specification and also show that the problem is in pspace matching the known lower bound for concurrent programs we show that the problem can be solved in time polynomial in the program and doubly exponential in the specification and prove that it is complete for double exponential time we also address these questions for specifications described by ohgr automata or formulas in extended temporal logic"
132 "given a pattern string we describe a way to preprocess" "given a pattern string we describe a way to preprocess it we design a crcwpram constant time optimal parallel algorithm for finding all occurrences of the preprocessed pattern in any given text"
133 "we present an algorithm for sorting efficiently with parallel twolevel" "we present an algorithm for sorting efficiently with parallel twolevel memories our main result is an elegant easytoimplement optimal deterministic algorithm for external sorting with d disk drives this result answers in the affirmative the open problem posed by vitter and shriver of whether an optimal algorithm exists that is deterministic our measure of performance is the number of parallel inputoutput io operations in which each of the d disks can simultaneously transfer a block of b contiguous records we assume that internal memory can hold m records our algorithm sorts n records in the optimal bound of thgrn bd lognb logmb deterministically and thus improves upon vitter and shrivers optimal randomized algorithm as well as the wellknown deterministic but nonoptimal technique of disk striping it is also practical to implement"
134 "constraint networks have been shown to be useful in formulating" "constraint networks have been shown to be useful in formulating such diverse problems as scene labeling natural language parsing and temporal reasoning given a constraint network we often wish to i find a solution that satisfies the constraints and ii find the corresponding minimal network where the constraints are as explicit as possible both tasks are known to be npcomplete in the general case task is usually solved using a backtracking algorithm and task ii is often solved only approximately by enforcing various levels of local consistency in this paper we identify a property of binary constraint called row convexity and show its usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee that a network is both minimal and globally consistent globally consistent networks have the property that a solution can be found without backtracking we show that one can test for the row convexity property efficiently and we show by examining applications of constraint networks discussed in the literature that our results are useful in practice thus we identify a class of binary constraint networks for which we can solve both tasks i and ii efficiently finally we generalize the results for binary constraint networks to networks with nonbinary constraints"
135 "we consider multiprocessing systems where processes make independent poisson distributed" "we consider multiprocessing systems where processes make independent poisson distributed resource requests with mean arrival time we assume that resources are not released it is shown that the expected deadlock time is never less than no matter how many processes and resources are in the system also the expected number of processes blocked by deadlock time is onehalf more than half the number of initially active processes we obtain expressions for system statistics such as expected deadlock time expected total processing time and system efficiency in terms of abel sums we derive asymptotic expressions for these statistics in the case of systems with many processes and the case of systems with a fixed number of processes in the latter generalizations of the ramanujan qfunction arise we use singularity analysis to obtain asymptotics of coefficients of generalized qfunctions"
136 "the existence of nash equilibria in noncooperative flow control in" "the existence of nash equilibria in noncooperative flow control in a general productform network shared by k users is investigated the performance objective of each user is to maximize its average throughput subject to an upper bound on its average timedelay previous attempts to study existence of equilibria for this flow control model were not successful partly because the timedelay constraints couple the strategy spaces of the individual users in a way that does not allow the application of standard equilibrium existence theorems from the game theory literature to overcome this difficulty a more general approach to study the existence of nash equilibria for decentralized control schemes is introduced this approach is based on directly proving the existence of a fixed point of the best reply correspondence of the underlying game for the investigated flow control model the best reply correspondence is shown to be a function implicitly defined by means of k interdependent linear programs employing an appropriate definition for continuity of the set of optimal solutions of parameterized linear programs it is shown that under appropriate conditions the best reply function is continuous brouwers theorem implies then that the best reply function has a fixed point"
137 "two deterministic routing networks are presented the pruned butterfly and" "two deterministic routing networks are presented the pruned butterfly and the sorting fattree both networks are areauniversal that is they can simulate any other routing network fitting in similar area with polylogarithmic slowdown previous areauniversal networks were either for the offline problem where the message set to be routed is known in advance and substantial precomputation is permitted or involved randomization yielding results that hold only with high probability the two networks introduced here are the first that are simultaneously deterministic and online and they use two substantially different routing techniques the performance of their routing algorithms depends on the difficulty of the problem instance which is measured by a quantity lgr known as the load factor the pruned butterfly runs in time olgrlogn is the number of possible sources and destinations for messages and lgr is assumed to be polynomial in n the sorting fattree algorithm runs in o lgr log n log n time for a restricted class of message sets including partial permutations other results of this work include a flexible circuit that is areatime optimal across a range of different input sizes and an areatime lower bound for routers based on wirelength arguments"
138 "consider a switching component in a packetswitching network where messages" "consider a switching component in a packetswitching network where messages from several incoming channels arrive and are routed to appropriate outgoing ports according to a service policy one requirement in the design of such a system is to determine the buffer storage necessary at the input of each channel and the policy for serving these buffers that will prevent buffer overflow and the corresponding loss of messages in this paper a class of buffer service policies called least time to reach bound ltrb is introduced that guarantees no overflow and for which the buffer size required at each input channel is independent of the number of channels and their relative speeds further the storage requirement is only twice the maximal length of a message in all cases and as a consequence the class is shown to be optimal in the sense that any nonoverflowing policy requires at least as much storage as ltrb"
139 "we propose that the phenomenon of local state may be" "we propose that the phenomenon of local state may be understood in terms of stracheys concept of parametric ie uniform polymorphism the intuitive basis for our proposal is the following analogy a nonlocal procedure is independent of locallydeclared variables in the same way that a parametrically polymorphic function is independent of types to which it is instantiateda connection between parametricity and representational abstraction was first suggested by jc reynolds reynolds used logical relations to formalize this connection in languages with type variables and userdefined types we use relational parametricity to construct a model for an algollike language in which interactions between local and nonlocal entities satisfy certain relational criteria reasoning about local variables essentially involved proving properties of polymorphic functions the new model supports straightforward validations of all the test equivalences that have been proposed in the literature for localvariable semantics and encompasses standard methods of reasoning about data representations it is not known whether our techniques yield fully abstract semantics a model based on partial equivalence relations on the natural numbers is also briefly examined"
140 "a technique is developed for establishing lower bounds on the" "a technique is developed for establishing lower bounds on the computational complexity of certain natural problems the results have the form of timespace tradeoff and exhibit the power of nondeterminism in particular a form of the clique problem is defined and it is proved that a nondeterministic logspace turing machine solves the problem in linear time but no deterministic machine in a very general use of this term with sequentialaccess input tape and work space nsgr solves the problem in time ntgr if sgr tgr"
141 "this paper is concerned with the properties of nonlinear equations" "this paper is concerned with the properties of nonlinear equations associated with the scheweitzerbard sb approximate mean value analysis mva heuristic for closed productform queuing networks three forms of nonlinear sb approximate mva equations in multiclass networks are distinguished schweitzer minimal and the nearly decoupled forms the approximate mva equations have enabled us to a derive bounds on the approximate throughput b prove the existence and uniqueness of the sb throughput solution and the convergence of the sb approximation algorithm for a wide class of monotonic singleclass networks c establish the existence of the sb solution for multiclass monotonic networks and d prove the asymptotic ie as the number of customers of each class tends to uniqueness of the sb throughput solution and e the convergence of the gradient projection and the primaldual algorithms to solve the asymptotic versions of the minimal the schweitzer and the nearly decoupled forms of mva equations for multiclass networks with single server and infinite server nodes the convergence is established by showing that the approximate mva equations are the gradient vector of a convex function and by using results from convex programming and the convex duality theory"
142 "a parallel algorithm for computing the connected components of undirected" "a parallel algorithm for computing the connected components of undirected graphs is presented shared memory computation models are assumed for a graph of e edges and n nodes the time complexity of the algorithm is ogr ep n log np logn with p processors the algorithm can be further refined to yield time complexity ogrhe n pp n log np logn p logn where he n p is very close to ogre these results show that linear speedup can be obtained for up to p elogn processors when e n log n linear speedup can still be achieved with up to p n egr processors egr for graphs satisfying e n logn our results can be further improved if a more efficient integer sorting algorithm is available"
143 "in this paper the shortestpath problem in networks in which" "in this paper the shortestpath problem in networks in which the delay or weight of the edges changes with time according to arbitrary functions is considered algorithms for finding the shortest path and minimum delay under various waiting constraints are presented and the properties of the derived path are investigated it is shown that if departure time from the source node is unrestricted then a shortest path can be found that is simple and achieves a delay as short as the most unrestricted path in the case of restricted transit it is shown that there exist cases in which the minimum delay is finite but the path that achieves it is infinite"
144 "the patricia trie is a simple modification of a regular" "the patricia trie is a simple modification of a regular trie by eliminating unary branching nodes the patricia achieves better performance than regular tries however the question is how much on the average is the patricia better this paper offers a thorough answer to this question by considering some statistics of the number of nodes examined in a successful search and an unsuccessful search in the patricia tries it is shown that for the patricia containing n records the average of the successful search length sn asymptotically becomes h ln n o and the variance of sn is either var sn c ln n for an asymmetric patricia or var sn for a symmetric patricia where h is the entropy of the alphabet over which the patricia is built and c is an explicit constant higher moments of sn are also assessed the number of nodes examined in an unsuccessful search un is studied only for binary symmetric patricia tries we prove that the mth moment of the unsuccessful search length eumn satisfies limn eumnlogmn and the variance of un is var un these results suggest that patricia tries are very well balanced trees in the sense that a random shape of patriciatries resembles the shape of complete trees that are ultimately balanced trees"
145 "probabilistic algorithms are presented for testing the result of the" "probabilistic algorithms are presented for testing the result of the product of two nbit integers in on bit operations and for testing the result of the product of two polynomials of degree n over any integral domain in n on algebraic operations with the error probability oln egr for any egr the last algorithm does not depend on the constants of the underlying domain"
146 "methods are presented for finding reductions between the computations of" "methods are presented for finding reductions between the computations of certain arithmetic functions that preserve asymptotic boolean complexities circuit depth or size they can be used to show for example that all nonlinear algebraic functions are as difficult as integer multiplication with respect to circuit size as a consequence any lower or upper bound eg on log n log log n for one of them applies to the whole class it is also shown that with respect to depth and size simultaneously multiplication is reducible to any nonlinear and division to any nonpolynomial algebraic function"
147 "the minimumcost flow problem is given a network with n" "the minimumcost flow problem is given a network with n vertices and m edges find a maximum flow of minimum cost many network problems are easily reducible to this problem a polynomialtime algorithm for the problem has been known for some time but only recently a strongly polynomial algorithm was discovered in this paper an onm n log nlog n algorithm is designed the previous best algorithm due to fujishige and orlin had an omm nlognlogn time bound thus for dense graphs an improvement of two orders of magnitude is obtained the algorithm in this paper is based on fujishiges algorithm which is based on tardoss algorithm fujishiges algorithm consists of up to m iterations each consisting of om log n steps each step solves a single source shortest path problem with nonnegative edge lengths this algorithm is modified in order to make an improved analysis possible the new algorithm may still consist of up to m iterations and an iteration may still consist of up to omlogn steps but it can still be shown that the total number of steps is bounded by onlogn the improvement is due to a new technique that relates the time spent to the progress achieved"
149 "computational efficiency is a central concern in the design of" "computational efficiency is a central concern in the design of knowledge representation systems in order to obtain efficient systems it has been suggested that one should limit the form of the statements in the knowledge base or use an incomplete inference mechanism the former approach is often too restrictive for practical applications whereas the latter leads to uncertainty about exactly what can and cannot be inferred from the knowledge base we present a third alternative in which knowledge given in a general representation language is translated compiled into a tractable formallowing for efficient subsequent query answeringwe show how propositional logical theories can be compiled into horn theories that approximate the original information the approximations bound the original theory from below and above in terms of logical strength the procedures are extended to other tractable languages for example binary clauses and to the firstorder case finally we demonstrate the generality of our approach by compiling concept descriptions in a general framebased language into a tractable form"
150 "we introduce the concept of unreliable failure detectors and study" "we introduce the concept of unreliable failure detectors and study how they can be used to solve consensus in asynchronous systems with crash failures we characterise unreliable failure detectors in terms of two propertiescompleteness and accuracy we show that consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes and determine which ones can be used to solve consensus despite any number of crashes and which ones require a majority of correct processes we prove that consensus and atomic broadcast are reducible to each other in asynchronous systems with crash failures thus the above results also apply to atomic broadcast a companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving consensus chandra et al"
151 "the contribution of this paper is twofold first a connection" "the contribution of this paper is twofold first a connection is established between approximating the size of the largest clique in a graph and multiprover interactive proofs second an efficient multiprover interactive proof for np languages is constructed where the verifier uses very few random bits and communication bits last the connection between cliques and efficient multiprover interaction proofs is shown to yield hardness results on the complexity of approximating the size of the largest clique in a graphof independent interest is our proof of correctness for the multilinearity test of functions"
152 "we present optimal algorithms for sorting on parallel crew and" "we present optimal algorithms for sorting on parallel crew and erew versions of the pointer machine model intuitively one can view our methods as being based on a parallel mergesort using linked lists rather than arrays the usual parallel data structure we also show how to exploit the locality of our approach to solve the set expression evaluation problem a problem with applications to database querying and logicprogramming in olog n time using on processors interestingly this is an asymptotic improvement over what seems possible using previous techniques"
153 "categorical combinators curien hardin yokouchi and more recently lgrsgrcalculus abadi" "categorical combinators curien hardin yokouchi and more recently lgrsgrcalculus abadi hardin and levy have been introduced to provide an explicit treatment of substitutions in the lgrcalculus we reintroduce here the ingredients of these calculi in a selfcontained and stepwise way with a special emphasis on confluence properties the main new results of the paper with respect to curien hardin abadi and hardin and levy are the following we present a confluent weak calculus of substitutions where no variable clashes can be feared we solve a conjecture raised in abadi lgrsgrcalculus is not confluent it is confluent on ground terms only this unfortunate result is repaired by presenting a confluent version of lgrsgrcalculus named the lgrenvcaldulus in hardin and levy called here the confluent lgrsgrcalculus"
154 "this paper introduces a new distributed data object called resource" "this paper introduces a new distributed data object called resource controller that provides an abstraction for managing the consumption of a global resource in a distributed system examples of resources that may be managed by such an object include number of messages sent number of nodes participating in the protocol and total cpu time consumed the resource controller object is accessed through a procedure that can be invoked at any node in the network before consuming a unit of resource at some node the controlled algorithm should invoke the procedure at this node requesting a permit or a rejection the key characteristics of the resource controller object are the constraints that it imposes on the global resource consumption an m wcontroller guarantees that the total number of permits granted is at most m it also ensures that if a request is rejected then at least mw permits are eventually granted even if no more requests are made after the rejected one in this paper we describe several message and spaceefficient implementations of the resource controller object in particular we present an m wcontroller whose message complexity is on logn logmw where n is the total number of nodes this is in contrast to the onm message complexity of a fully centralized controller which maintains a global counter of the number of granted permits at some distinguished node and relays all the requests to the node"
155 "sld resolution with negation as finite failure sldnf reflects the" "sld resolution with negation as finite failure sldnf reflects the procedural interpretation of predicate calculus as a programming language and forms the computational basis for prolog systems despite its advantages for stackbased memory management sldnf is often not appropriate for query evaluation for three reasons a it may not terminate due to infinite positive recursion b it may be terminate due to infinite recursion through negation and c it may repeatedly evaluate the same literal in a rule body leading to unacceptable performance we address all three problems for goaloriented query evaluation of general logic programs by presenting tabled evaluation with delaying called slg resolution it has three distinctive features i slg resolution is a partial deduction procedure consisting of seven fundamental transformations a query is transformed step by step into a set of answers the use of transformations separates logical issues of query evaluation from procedural ones slg allows an arbitrary computation rule for selecting a literal from a rule body and an arbitrary control strategy for selecting transformations to apply ii slg resolution is sound and search space complete with respect to the wellfounded partial model for all nonfloundering queries and preserves all threevalued stable models to evaluate a query under differenc threevalued stable models slg resolution can be enhanced by further processing of the answers of subgoals relevant to a query iii slg resolution avoids both positive and negative loops and always terminates for programs with the boundedtermsize property it has a polynomial time data complexity for wellfounded negation of functionfree programs through a delaying mechanism for handling ground negative literals involved in loops slg resolution avoids the repetition of any of its derivation steps restricted forms of slg resolution are identified for definite locally stratified and modularly stratified programs shedding light on the role each transformation plays"
156 "let mmn be the minimum number of comparatorsneeded in a" "let mmn be the minimum number of comparatorsneeded in a comparator network that merges m elements xxcdotsxm and n elements yy cdotsyn where nm batchers oddeven merge yields the following upper bound mmn mnlog mon in particular mnnnlog non we prove the following lower bound that matches the upper bound above asymptotically as nm mmn mnlog mom in particular mnnnlog non our proof technique extends to give similarily tight lower bounds for the size of monotone boolean circuits for merging and for the size of switching networks capable of realizing the set of permutations that arise from merging"
157 "the ability of the strongest parallel random access machine model" "the ability of the strongest parallel random access machine model wram is investigated in this model different processors may simultaneously try to write into the same cell of the common memory it has been shown that a parallel ram without this option pram even with arbitrarily many processors can almost never achieve sublogarithmic time on the contrary every function with a small domain like binary values in case of boolean functions can be computed by a wram in constant time the machine makes fast table lookups using its simultaneous write ability the main result of this paper implies that in general this is the only way to perform such fast computations and that a domain of small size is necessary otherwise simultaneous writes do not give an advantage functions with large domains for which any change of one of the n arguments also changes the result are considered and a logarithmic lower time bound for wrams is proved this bound can be achieved by machines that do not perform simultaneous writes a simple example of such a function is the sum of n natural numbers"
158 "new improved algorithms are proposed for regulating access to a" "new improved algorithms are proposed for regulating access to a multipleaccess channel a common channel shared by many geographically distributed computing stations a conflict of multiplicity n occurs when n stations transmit simultaneously to the channel as a result all stations receive feedback indicating whether n is or if n the transmission succeeds whereas if n all the transmissions fail algorithms are presented and analyzed that allow the conflicting stations to compute a stochastic estimate n of n cooperatively at small cost as a function of the feedback elicited during its execution an algorithm to resolve a conflict among two or more stations controls the retransmissions of the conflicting stations so that each eventually transmits singly to the channel combining one of our estimation algorithms with a tree algorithm of capetanakis hayes and tsybakov and mikhailov then leads to a hybrid algorithm for conflict resolution several efficient combinations are possible the most efficient of which resolves conflicts about percent faster on average than any of the comparable algorithms reported to date"
159 "it has been observed that for some database schemes users" "it has been observed that for some database schemes users may have difficulties retrieving correct information even for simple queries the problem occurs when some implicit piece of information defined on some subset of a relation scheme is not explicitly represented in the database state in this situation users may be required to know how the state and the constraints interact before they can retrieve the information correctly in this paper the formal notion of embeddedcompleteness is proposed and it is shown that schemes with this property avoid the problem described above a polynomialtime algorithm is given to test whether a database scheme is independent and embeddedcomplete under the assumption of independence it is shown that embeddedcomplete schemes allow efficient computation of optimal relational algebra expressions equivalent to the xtotal projection for any set of attributes x"
160 "in this paper concurrent dynamic logic cdl is introduced as" "in this paper concurrent dynamic logic cdl is introduced as an extension of dynamic logic tailored toward handling concurrent programs properties of cdl are discussed both on the propositional and firstorder level and the extension is shown to possess most of the desirable properties of dl its relationships with the mgrcalculus game logic dl with recursive procedures and ptime are further explored revealing natural connections between concurrency recursion and alternation"
161 "the existence of minimal degrees is investigated for several polynomial" "the existence of minimal degrees is investigated for several polynomial reducibilities it is shown that no set has minimal degree with respect to polynomial manyone or turing reducibility this extends a result of ladner in which only recursive sets are considered a polynomial reducibility ht is defined this reducibility is a strengthening of polynomial turing reducibility and its properties relate to the p np question for this new reducibility a set of minimal degree is constructed under the assumption that p np however the set constructed is nonrecursive and it is shown that no recursive set is of minimal ht degree"
162 "a new quantitative approach to the problem of reconfiguring a" "a new quantitative approach to the problem of reconfiguring a degradable multimodule system is presented the approach is concerned with both assigning some modules for computation and arranging others for reliability conventionally a faulttolerant system performs reconfiguration only upon a subsystem failure since there exists an inherent tradeoff between the computation capacity and fault tolerance of a multimodule computing system the conventional approach is a passive action and does not yield a configuration that provides an optimal compromise for the tradeoff by using the expected total reward as the optimal criterion the need and existence of an active reconfiguration strategy in which the system reconfigures itself on the basis of not only the occurrence of a failure but also the progression of the mission are shown following the problem formulation some important properties of an optimal reconfiguration strategy which specify i the times at which the system should undergo reconfiguration and ii the configurations to which the system should change are investigated then the optimal reconfiguration problem is converted to integer nonlinear knapsack and fractional programming problems the algorithms for solving these problems and a demonstrative example are given extensions of the optimal reconfiguration problem are also discussed"
163 "this paper investigates the complexity of multioperand residue addition and" "this paper investigates the complexity of multioperand residue addition and multiplication implemented by associative table lookup processing the complexity measure used is the size of the associative memory that is the number of matching words in memory this measure largely depends on the residue recurrencies or multiplicities in the addition and multiplication tables module m the major effort in this work is to evaluate the recurrencies in simultaneous multioperand residue addition and multiplication the evaluation is simple in case of addition mod m and also in multiplication mod m if m is prime to treat the more difficult case of m nonprime a recursive procedure was developed for computing the operand multiplication recurrencies mod m the basis of this technique is the precedence relationships associated with a tree representation of the factors of m it is then shown that the general doperand multiplication mod m d and m nonprime can be reduced to the operand case by isomorphic transformation computation results of operand residue arithmetic operations are provided applications to rns arithmetic implementation are discussed"
164 "probabilistic reasoning suffers from nphard implementations in particular the amount" "probabilistic reasoning suffers from nphard implementations in particular the amount of probabilistic information necessary to the computations is often overwhelming for example the size of conditional probability tables in bayesian networks has long been a limiting factor in the general use of these networkswe present a new approach for manipulating the probabilistic information given this approach avoids being overwhelmed by essentially compressing the information using approximation functions called linear potential functions we can potentially reduce the information from a combinatorial amount to roughly linear in the number of random variable assigments furthermore we can compute these functions through closed form equations as it turns out our approximation method is quite general and may be applied to other data compression problems"
165 "software protection is one of the most important issues concerning" "software protection is one of the most important issues concerning computer practice there exist many heuristics and adhoc methods for protection but the problem as a whole has not received the theoretical treatment it deserves in this paper we provide theoretical treatment of software protection we reduce the problem of software protection to the problem of efficient simulation on oblivious rama machine is oblivious if thhe sequence in which it accesses memory locations is equivalent for any two inputs with the same running time for example an oblivious turing machine is one for which the movement of the heads on the tapes is identical for each computation thus the movement is independent of the actual input what is the slowdown in the running time of a machine if it is required to be oblivious in pippenger and fischer showed how a twotape oblivious turing machine can simulate online a onetape turing machine with a logarithmic slowdown in the running time we show an analogous result for the randomaccess machine ram model of computation in particular we show how to do an online simulation of an arbitrary ram by a probabilistic oblivious ram with a polylogaithmic slowdown in the running time on the other hand we show that a logarithmic slowdown is a lower bound"
167 "we present a lineartime algorithm to decide for any fixed" "we present a lineartime algorithm to decide for any fixed deterministic contextfree language l and input string w whether wis a suffix of some string in l in contrast to a previously published technique the decision procedure may be extended to produce syntactic structures parses without an increase in time complexity we also show how this algorithm may be applied to pocess incorrect input in linear time"
169 "this paper present a new approach to finding minimum cuts" "this paper present a new approach to finding minimum cuts in undirected graphs the fundamental principle is simple the edges in a graphs minimum cut form an extremely small fraction of the graphs edges using this idea we give a randomized strongly polynomial algorithm that finds the minimum cut in an arbitrarily weighted undirected graph with high probability the algorithm runs in onlogn time a significant improvement over the previous omn time bounds based on maximum flows it is simple and intuitive and uses no complex data structures our algorithm can be parallelized to run in rnc with n processors this gives the first proof that the minimum cut problem can be solved in rnc the algorithm does more than find a single minimum cut it finds all of themwith minor modifications our algorithm solves two other problems of interest our algorithm finds all cuts with value within a multiplicative factor of agr of the minimum cuts in expected onagr time or in rnc with nagr processors the problem of finding a minimum multiway cut of graph into r pieces is solved in expected onr time or in rnc with nr processors the trace of the algorithms execution on these two problems forms a new compact data structure for representing all small cuts and all multiway cuts in a graph this data structure can be efficiently transformed into the more standard cactus representing for minimum cuts"
170 "productform queuing network models have been widely used to model" "productform queuing network models have been widely used to model systems with shared resources such as computer systems both centralized and distributed communication networks and flexible manufacturing systems closed multichain productform networks are inherently more difficult to analyze than open networks due to the effect of normalization results in workload characterization for closed networks in the literature are often for networks having special structures and only specific performance measures have been consideredin this article we drive certain properties insensitivity of conditional state probability distributions and fractionallinearity of markov reward functions for a broad class of closed multichain productform networks these properties are derived using the most basic flow balance conditions of productform networks then we show how these basic properties can be applied in obtaining error bounds when similar customers are clustered together to speed up computation"
171 "the exponent of periodicity is an important factor in estimates" "the exponent of periodicity is an important factor in estimates of complexity of wordunification algorithms we prove that the exponent of periodicity of a minimal solution of a word equation is of order d where d is the length of the equation we also give a lower bound d so our upper bound is almost optimal and exponentially better than the original bound dd consequently our result implies an exponential improvement of known upper bounds on complexity of wordunification algorithms"
173 "sharing data between multiple asynchronous userseach of which can atomically" "sharing data between multiple asynchronous userseach of which can atomically read and write the datais a feature that may help to increase the amount of parallelism in distributed systems an algorithm implementing this feature is presented the main construction of an nuser atomic variable directly from singlewriter singlereader atomic variables uses o n control bits and on accesses per readwrite running in o parallel time"
174 "caching and prefetching are important mechanisms for speeding up access" "caching and prefetching are important mechanisms for speeding up access time to data on secondary storage recent work in competitive online algorithms has uncovered several promising new algorithms for caching in this paper we apply a form of the competitive philosophy for the first time to the problem of prefetching to develop an optimal universal prefetcher in terms of fault rate with particular applications to largescale databases and hypertext systems our prediction algorithms with particular applications to largescale databases and hypertext systems our prediction algorithms for prefetching are novel in that they are based on data compression techniques that are both theoretically optimal and good in practice intuitively in order to compress data effectively you have to be able to predict future data well and thus good data compressors should be able to predict well for purposes of prefetching we show for powerful models such as markov sources and mthe order markov sources that the page fault rate incurred by our prefetching algorithms are optimal in the limit for almost all sequences of page requests"
175 "balancing networks originally introduced by aspnes et al proceedings of" "balancing networks originally introduced by aspnes et al proceedings of the rd annual acm symposium on theory of computing pp may represent a new class of distributed lowcontention data structures suitable for solving many fundamental multiprocessor coordination problems that can be expressed as balancing problems in this work we present a mathematical study of the combinatorial structure of balancing networks and a variety of its applicationsour study identifies important combinatorial transfer parameters of balancing networks in turn necessary and sufficient combinatorial conditions are established expressed in terms of transfer parameters which precisely characterize many important and well studied classes of balancing networks such as counting networks and smoothing networks we propose these combinatorial conditions to be balancing analogs of the well known zeroone principle holding for sorting networkswithin the combinatorial framework we develop our first application is in deriving combinatorial conditions involving the transfer parameters which precisely delimit the boundary between counting networks and sorting networks"
176 "we investigate the query complexity of exact learning in the" "we investigate the query complexity of exact learning in the membership and proper equivalence query model we give a complete characterization of concept classes that are learnable with a polynomial number of polynomial sized queries in this model we give applications of this characterization including results on learning a natural subclass of dnf formulas and on learning with membership queries alone query complexity has previously been used to prove lower bounds on the time complexity of exact learning we show a new relationship between query complexity and time complexity in exact learning if any honest class is exactly and properly learnable with polynomial query complexity but not learnable in polynomial time then p np in particular we show that an honest class is exactly polynomialquery learnable if and only if it is learnable using an oracle for ggrp"
177 "we present a general theory for the use of negative" "we present a general theory for the use of negative premises in the rules of transition system specifications tsss we formulate a criterion that should be satisfied by a tss in order to be meaningful that is to unequivocally define a transition relation we also provide powerful techniques for proving that a tss satisfies this criterion meanwhile constructing this transition relation both the criterion and the techniques originate from logic programming van gelder et al gelfond and lifschitz to which tsss are close in an appendix we provide an extensive comparison between themas in groote we show that the bisimulation relation induced by a tss is a congruence provided that it is in ntyftntyxtformat and can be proved meaningful using our techniques we also considerably extend the conservativity theorems of groote and groote and vaandrager as a running example we study the combined addition of priorities and abstraction to basic process algebra bpa under some reasonable conditions we show that this tss is indeed meaningful which could not be shown by other methods bloom et al groote finally we provide a sound and complete axiomatization for this example"
178 "we present algorithms for efficient searching of regular expressions on" "we present algorithms for efficient searching of regular expressions on preprocessed text using a patricia tree as a logical model for the index we obtain searching algorithms that run in logarithmic expected time in the size of the text for a wide subclass of regular expressions and in sublinear expected time for any regular expression this is the first such algorithm to be found with this complexity"
179 "recurrent neural networks that are trained to behave like deterministic" "recurrent neural networks that are trained to behave like deterministic finitestate automata dfas can show deteriorating performance when tested on long strings this deteriorating performance can be attributed to the instability of the internal representation of the learned dfa states the use of a sigmoidel discriminant function together with the recurrent structure contribute to this instability we prove that a simple algorithm can construct secondorder recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal dfa state representations are stable that is the constructed network correctly classifies strings of arbitrary length the algorithm is based on encoding strengths of weights directly into the neural network we derive a relationship between the weight strength and the number of dfa states for robust string classification for a dfa with n state and minput alphabet symbols the constructive algorithm generates a programmed neural network with on neurons and omn weights we compare our algorithm to other methods proposed in the literature"
180 "this paper studies the problem of dedicating routes to connections" "this paper studies the problem of dedicating routes to connections in optical networks in optical networks the vast bandwidth available in an optical fiber is utilized by partitioning it into several channels each at a different optical wavelength a connection between two nodes is assigned a specific wavelength with the constraint that no two connections sharing a link in the network can be assigned the same wavelength this paper considers optical networks with and without switches and different types of routing in these networks it presents optimal or nearoptimal constructions of optical networks in these cases and algorithms for routing connections specifically permutation routing for the networks constructed here"
181 "in this paper a new algorithm for performing quantifier elimination" "in this paper a new algorithm for performing quantifier elimination from first order formulas over real closed fields in given this algorithm improves the complexity of the asymptotically fastest algorithm for this problem known to this data a new feature of this algorithm is that the role of the algebraic part the dependence on the degrees of the imput polynomials and the combinatorial part the dependence on the number of polynomials are sparated another new feature is that the degrees of the polynomials in the equivalent quantifierfree formula that is output are independent of the number of input polynomials as special cases of this algorithm new and improved algorithms for deciding a sentence in the first order theory over real closed fields and also for solving the existential problem in the first order theory over real closed fields are obtained"
182 "we analyze the optimization effect of the magic sets rewriting" "we analyze the optimization effect of the magic sets rewriting technique for datalog queries and present some supplementary or alternative techniques that avoid many shortcomings of the basic technique given a magic sets rewritten query the set of facts generated for the original nonmagic predicates by the seminaive bottomup evaluation is characterized precisely it is shown thatbecause of the additional magic factsmagic sets processing may result in generating an order of magnitude more facts than the straightforward naive evaluation a refinement of magic sets in factorized magic sets is defined these magic sets retain most of the efficiency of original magic sets in regards to the number of nonmagic facts generated and have the property that a lineartime bound with respect to seminaive evaluation is guaranteed in all cases an alternative technique for magic sets called envelopes which has several desirable properties over magic sets is introduced envelope predicates are never recursive with the original predicates thus envelopes can be computed as a preprocessing task envelopes also allow the utilization of multiple sideways information passing strategies sips for a rule an envelopetransformed program may be readorned according to another choice of sips and reoptimized by magic sets or envelopes thus making possible an optimization effect that cannot be achieved by magic sets based on a particular choice of sips"
183 "a graphical representation of quantifierfree predicate calculus formulas in negation" "a graphical representation of quantifierfree predicate calculus formulas in negation normal form and a new rule of inference that employs this representation are introduced the new rule path resolution is an amalgamation of resolution and prawitz analysis the goal in the design of path resolution is to retain some of the advantages of both prawitz analysis and resolution methods and yet to avoid to some extent their disadvantages path resolution allows prawitz analysis of an arbitrary subgraph of the graph representing a formula if such a subgraph is not large enough to demonstrate a contradiction a path resolvent of the subgraph may be generated with respect to the entire graph this generalizes the notions of large inference present in hyperresolution clashresolution ncresolution and urresolution a class of subgraphs is described for which deletion of some of the links resolved upon preserves the spanning property"
184 "one of the basic geometric operations involves determining whether a" "one of the basic geometric operations involves determining whether a pair of convex objects intersect this problem is well understood in a model of computation in which the objects are given as input and their intersection is returned as output for many applications however it may be assumed that the objects already exist within the computer and that the only output desired is a single piece of data giving a common point if the objects intersect or reporting no intersection if they are disjoint for this problem none of the previous lower bounds are valid and algorithms are proposed requiring sublinear time for their solution in two and three dimensions"
185 "this paper is concerned with the question of the decidability" "this paper is concerned with the question of the decidability and the complexity of the decision problem for certain fragments of the theory of free term algebras the existential fragment of the theory of term algebras is shown to be decidable through the presentation of a nondeterministic algorithm which given a quantifierfree formula p constructs a solution for p if it has one and indicates failure if there are no solutions it is shown that the decision problem is in np by proving that if a quantifierfree formula p has a solution then there is one that can be represented as a dag in space at most cubic in the length of p the decision problem is shown to be complete for np by reducing sat to that problem thus it is established that the existential fragment of the theory of pure list structures in the language of nil cons car cdr subexpression is npcomplete it is further shown that even a slightly more expressive fragment of the theory of term algebras the one that allows bounded universal quantifiers is undecidable"
186 "in our model a graph describes a net of processes" "in our model a graph describes a net of processes communicating through ports and at the same time its computation history consisting of a partial ordering of events standalone evolution of processes is specified by contextfree productions from productions and a basic synchronization mechanism a set of contextsensitive rewriting rules that models the evolution of processes connected to the same ports can be derived a computation is a sequence of graphs obtained by successive rewritings the result of a finite computation is its last graph whereas the result of an infinite computation is the limit infinite graph defined through a completion technique based on metric spaces a result characterizes a concurrent computation since it abstracts from any particular interleaving of concurrent events while in the meantime providing information about termination partial or complete deadlocks and fairness not every result is acceptable however but only the computations that produce a result no longer rewritable are successful infinite successful computations are shown to coincide with weakly fair computations and a scheduler yielding all and only such computations is defined"
187 "a collection of n balls in d dimensions forms a" "a collection of n balls in d dimensions forms a kply system if no point in the space is covered by more than k balls we show that for every kply system ggr there is a sphere s that intersects at most okdnd balls of ggr and divides the remainder of ggr into two parts those in the interior and those in the exterior of the sphere s respectively so that the larger part contains at most dn balls this bound of o kdnd is the best possible in both n and k we also present a simple randomized algorithm to find such a sphere in on time our result implies that every knearest neighbor graphs of n points in d dimensions has a separator of size okdnd in conjunction with a result of koebe that every triangulated planar graph is isomorphic to the intersection graph of a diskpacking our result not only gives a new geometric proof of the planar separator theorem of lipton and tarjan but also generalizes it to higher dimensions the separator algorithm can be used for point location and geometric divide and conquer in a fixed dimensional space"
188 "we establish a general connection between fixpoint logic and complexity" "we establish a general connection between fixpoint logic and complexity on one side we have fixpoint logic parameterized by the choices of storder operators inflationary or noninflationary and iteration constructs deterministic nondeterministic or alternating on the other side we have the complexity classes between p and exptime our parameterized fixpoint logics capture the complexity classes p np pspace and exptime but equally is achieved only over ordered structures there is however an inherent mismatch between complexity and logicwhile computational devices work on encodings of problems logic is applied directly to the underlying mathematical structures to overcome this mismatch we use a theory of relational complexity which bridges the gap between standard complexity and fixpoint logic on one hand we show that questions about containments among standard complexity classes can be translated to questions about containments among relational complexity classes on the other hand the expressive power of fixpoint logic can be precisely characterized in terms of relational complexity classes this tight threeway relationship among fixpoint logics relational complexity and standard complexity yields in a uniform way logical analogs to all containments among the complexity classes p np pspace and exptime the logical formulation shows that some of the most tantalizing questions in complexity theory boil down to a single question the relative power of inflationary vs noninflationary storder operators"
189 "computing the natural join of a set of relations is" "computing the natural join of a set of relations is an important operation in relational database systems the ordering of joins determines to a large extent the computation time of the join since the number of possible orderings could be very large query optimizers first reduce the search space by using various heuristics and then try to select an optimal ordering of joins avoiding cartesian products is a common heuristic for reducing the search space but it cannot guarantee optimal ordering in its search space because the cheapest cartesianproductfree cpf for short ordering could be significantly worse than an optimal noncpf ordering by a factor of an arbitrarily large number in this paper we use programs consisting of joins semijoins and projections for computing the join of some relations and we introduce a novel algorithm that derives programs from cpf orderings of joins we show that there exists a cpf ordering from which our algorithm derives a program whose cost is within a constant factor of the cost of an optimal ordering thus our result demonstrates the effectiveness of avoiding cartesian products as a heuristic for restricting the search space of orderings of joins"
190 "a basic task in distributed computation is the maintenance at" "a basic task in distributed computation is the maintenance at each processor of the network of a current and accurate copy of a common database a primary example is the maintenance for routing and other purposes of a record of the current topology of the system such a database must be updated in the wake of locally generated changes to its contents due to previous disconnections of parts of the network a maintenance protocol may need to update processors holding widely varying versions of the database we provide a deterministic protocol for this problem with only polylogarithmic overhead in both time and communication complexities previous deterministic solutions required polynomial overhead in at least one of these measures"
191 "we introduce a general framework for constraint satisfaction and optimization" "we introduce a general framework for constraint satisfaction and optimization where classical csps fuzzy csps weighted csps partial constraint satisfaction and others can be easily cast the framework is based on a semiring structure where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain and the two semiring operations and x model constraint projection and combination respectively local consistency algorithms as usually used for classical csps can be exploited in this general framework as well provided that certain conditions on the semiring operations are satisfied we then show how this framework can be used to model both old and new constraint solving and optimization schemes thus allowing one to both formally justify many informally taken choices in existing schemes and to prove that local consistency techniques can be used also in newly defined schemes"
192 "we show that a turing machine with two singlehead onedimensional" "we show that a turing machine with two singlehead onedimensional tapes cannot recognize the set"
193 "objectoriented applications of database systems require database transformations involoving nonstandard" "objectoriented applications of database systems require database transformations involoving nonstandard functionalities such as set manipulation and object creation that is the introduction of new domain elements to deal with thse functionalities abiteboul and kanellakis introduced the determinate transformations as a generalization of the standard domainpreserving transformations the obvious extensions of complete standard database programming languages however are not complete for the determinate transformations to remedy this mismatch the constructive transformations are proposed it is shown that the constructive transformations are precisely the transformations that can be expressed in said extensions of complete standard languages thereto a close correspondence between object creation and the construction of hereditarily finite sets is establisheda restricted version of the main completeness result for the case where only list manipulations are involved is also presented"
194 "this paper addresses and answers a fundamental question about resolution" "this paper addresses and answers a fundamental question about resolution informally what is gained with respect to the search for a proof by performing a single resolution step it is first shown that any unsatisfiable formula may be decomposed into regular formulas provable in linear time by resolution a relevant resolution step strictly reduces at least one of the formulas in the decomposition while an irrelevant one does not contribute to the proof in any way the relevance of this insight into the nature of resolution and of the unsatisfiability problem for the development of proof strategies and for complexity considerations are briefly discussedthe decomposition also provides a technique for establishing completeness proofs for refinements of resolution as a first application connectiongraph resolution is shown to be strongly complete this settles a problem that remained open for two decades despite many proff attempts the result is relevant for theorem proving because without strong completeness a connection graph resolution prover might run into an infinite loop even on the ground level"
195 "consider an array of processing elements pes connected by a" "consider an array of processing elements pes connected by a dimensional grid network and holding at most one operand of an expression in each pe suppose that each pe is allowed in any one parallel step to receive one item of data from any of its four immediate neighbors and to transmit one datum as well how can an associative operator such as addition combine all the operands using as little time for communciation as possible an expression using such a single operator is termed a uniform expression when the total number of communication links used is the measure of goodness this problem becomes a steiner tree problem in the manhattan distance metric when the measure is minimizing the parallel time to completion a method for solving this problem is given which is optimal to within an additive constant of two timesteps the method has applications when the operands are matrices spread over an array of pes as well some lower bounds for this problem in more general networks are also proven"
196 "in this paper we develop a framework for computing upper" "in this paper we develop a framework for computing upper and lower bounds of an exponential form for a large class of single resource systems with markov additive inputs specifically the bounds are on quantities such as backlog queue length and response time explicit or computable expressions for our bounds are given in the context of queuing theory and numerical comparisons with other bounds and exact results are presented the paper concludes with two applications to admission control in multimedia systems"
197 "we investigate a problem arising in the computeraided design of" "we investigate a problem arising in the computeraided design of cars planes ships trains and other motor vehicles and machines refine a mesh of curved polygons which approximates the surface of a workpiece into quadrilaterals so that the resulting mesh is suitable for a numerical analysis this mesh refinement problem turns out to be strongly nphard in commercial cad systems this problem is usually solved using a gree dy approach however these algorithms leave the user a lot of patchwork to do afterwards we introduce a new global approach which is based on network obtain an undirected graph with upper and lower capacities on the edges and some additional node constraints we reduce this problem to a sequence of bidirected flwo problems or equivalently to bmatching problems for the first time network flow techniques are applied to a mesh refinement problem this approach avoids the local traps of greedy approaches and yields solutions that require significantly less additional patchwork"
198 "we analyze algorithms that predict a binary value by combining" "we analyze algorithms that predict a binary value by combining the predictions of several prediction strategies called experts our analysis is for worstcase situations ie we make no assumptions about the way the sequence of bits to be predicted is generated we measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence where the expectation is taken with respect to the randomization in the predictins we show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert and we give efficient algorithms that achieve this our upper and lower bounds have matching leading constants in most cases we then show how this leads to certain kinds of pattern recognitionlearning algorithms with performance bounds that improve on the best results currently know in this context we also compare our analysis to the case in which log loss is used instead of the expected number of mistakes"
200 "strictness analysis is an important technique for optimization of lazy" "strictness analysis is an important technique for optimization of lazy functional languages it is well known that all strictness analysis methods are incomplete ie fail to report some strictness properties in this paper we provide a precise and formal characterization of the loss of information that leads to this incompletenss specifically we establish the following characterization theorem for mycrofts strictness analysis method and a generalization of this method called eeanalysis that reasons about exhaustive evaluation in nonflat domains mycrofts method will deduce a strictness property for program p iff the property is independent of any constant appearing in any evaluation of p to prove this we specify a small set of equations called eaxioms that capture the information loss in mycrofts method and develop a new proof technique called erewriting erewriting extends the standard notion of rewriting to permit the use of reductions using eaxioms interspersed with standard reduction steps eaxioms are a syntactic characterization of information loss and erewriting provides and algorithmindependent proof technique for characterizing the power of analysis methods it can be used to answer questions on completeness and incompleteness of mycrofts method on certain natural classes of programs finally the techniques developed in this paper provide a general principle for establishing similar results for other analysis methods such as those based on abstract interpretation as a demonstration of the generality of our technique we give a characterization theorem for another variation of mycrofts method called ddanalysis"
201 "many combinatorial search problems can be expressed as constraint satisfaction" "many combinatorial search problems can be expressed as constraint satisfaction problems and this class of problems is known to be npcomplete in general in this paper we investigate the subclasses that arise from restricting the possible constraint types we first show that any set of constraints that does not give rise to an npcomplete class of problems must satisfy a certain type of algebraic closure condition we then investigate all the different possible forms of this algebraic closure property and establish which of these are sufficient to ensure tractability as examples we show that all known classes of tractable constraints over finite domains can be characterized by such an algebraic closure property finally we describe a simple computational procedure that can be used to determine the closure properties of a given set of constraints this procedure involves solving a particular constraint satisfaction problem which we call an indicator problem"
202 "constraint networks are a simple representation and reasoning framework with" "constraint networks are a simple representation and reasoning framework with diverse applications in this paper we identify two new complementary properties on the restrictiveness of the constraints in a networkconstraint tightness and constraint loosenessand we show their usefulness for estimating the level of local consistency needed to ensure global consistency and for estimating the level of local consistency present in a network in particular we present a sufficient condition based on constraint tightness and the level of local consistency that guarantees that a solution can be found in a backtrackfree manner the condition can be useful in applications where a knowledge base will be queried over and over and the preprocessing costs can be amortized over many queries we also present a sufficient condition for local consistency based on constraint looseness that is straightforward and inexpensive to determine the condition can be used to estimate the level of local consistency of a network this in turn can be used in deciding whether it would be useful to preprocess the network before a backtracking search and in deciding which local consistency conditions if any still need to be enforced if we want to ensure that a solution can be found in a backtrackfree manner two definitions of local consistency are employed in characterizing the conditions the traditional variablebased notion and a recently introduced definition of local consistency called relational consistency"
203 "given a convex polytope p with n faces in r" "given a convex polytope p with n faces in r points stp and a parameter e we present an algorithm that constructs a path on p from s to t whose length is at most edp st where dpst is the length of the shortest path between s and t on p the algorithm runs in onlog e e time and is relatively simple the running time is one if we only want the approximate shortest path distance and not the path itself we also present an extension of the algorithm that computes approximate shortest path distances from a given source point on p to all vertices of p"
204 "we present an algorithm for finding the minimum cut of" "we present an algorithm for finding the minimum cut of an undirected edgeweighted graph it is simple in every respect it has a short and compact description is easy to implement and has a surprisingly simple proof of correctness its runtime matches that of the fastest algorithm known the runtime analysis is straightforward in contrast to nearly all approaches so far the algorithm uses no flow techniques roughly speaking the algorithm consists of about v nearly identical phases each of which is a maximum adjacency search"
205 "the problem of implementing a shared object of one type" "the problem of implementing a shared object of one type from shared objects of other types has been extensively researched recent focus has mostly been on waitfree implementations which permit every process to complete its operations on implemented objects regardless of the speeds of other processes it is known that shared objects of different types have differing abilities to support waitfree implementations it is therefore natural to want to arrange types in a hierarchy that reflects their relative abilities to support waitfree implementations in this paper we formally define robustness and other desirable properties of hierarchies roughly speaking a hierarchy is robust if each type is stronger than any combination of lower level types we study two specific hierarchies one that we call hrm in which the level of a type is based on the ability of an unbounded number of objects of that type and another hierarchy that we call hr in which a types level is based on the ability of a fixed number of objects of that type we prove that resource bounded hierarchies such as hr and its variants are not robust we also establish the unique importance of hrm every nontrivial robust hierarchy if one exists is necessarily a coarsening of hrm"
206 "learnability in valiants pac learning model has been shown to" "learnability in valiants pac learning model has been shown to be strongly related to the existence of uniform laws of large numbers these laws define a distributionfree convergence property of means to expectations uniformly over classes of random variables classes of realvalued functions enjoying such a property are also known as uniform glivenkocantelli classes in this paper we prove through a generalization of sauers lemma that may be interesting in its own right a new characterization of uniform glivenkocantelli classes our characterization yields dudley gine and zinns previous characterization as a corollary furthermore it is the first based on a gine and zinns previous characterization as a corollary furthermore it is the first based on a simple combinatorial quantity generalizing the vapnikchervonenkis dimension we apply this result to obtain the weakest combinatorial condition known to imply pac learnability in the statistical regression or agnostic framework furthermore we find a characterization of learnability in the probabilistic concept model solving an open problem posed by kearns and schapire these results show that the accuracy parameter plays a crucial role in determining the effective complexity of the learners hypothesis class"
207 "inspired by the success of the distributed computing community in" "inspired by the success of the distributed computing community in apply logics of knowledge and time to reasoning about distributed protocols we aim for a similarly powerful and highlevel abstraction when reasoning about control problems involving uncertainty this paper concentrates on robot motion planning with uncertainty in both control and sensing a problem that has already been well studied within the robotics community first a new and natural problem in this domain is defined does there exists a sound and complete termination condition for a motion given initial and goal locations if yes how to construct it then we define a highlevel language a logic of time and knowledge which we use to reason about termination conditions and to state general conditions for the existence of sound and complete termination conditions in a broad domain finally we show that sound termination conditions that are optimal in a precise sense provide a natural example of knowledgebased programs with multiple implementations"
208 "we provide data strutures that maintain a graph as edges" "we provide data strutures that maintain a graph as edges are inserted and deleted and keep track of the following properties with the following times minimum spanning forests graph connectivity graph edge connectivity and bipartiteness in timeon per change edge connectivity in time on per change edge connectivity in time on agrn per change kedge connectivity for constant k in time onlogn per changevertex connectivity and vertex connectivity in the on per change and vertex connectivity in time onagrn per change further results speed up the insertion times to match the bounds of known partially dynamic algorithmsall our algorithms are based on a new technique that transforms an algorithm for sparse graphs into one that will work on any graph which we call sparsification"
209 "we introduce a new framework for the study of reasoning" "we introduce a new framework for the study of reasoning the learning in order to reason approach developed here views learning as an integral part of the inference process and suggests that learning and reasoning should be studied togetherthe learning to reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it in this framework the intelligent agent is given access to its favorite learning interface and is also given a grace period in with it can interact with this interface and construct a representation kb of the world w the reasoning performance is measured only after this period when the agent is presented with queries agr from some query language relevant to the world and has to answer whether w implies agrthe approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the world since the agent interacts with the world when construction its knowledge representation it can choose a representation that is useful for the task at hand moreover we can now make explicit the dependence of the reasoning performance on the environment the agent interacts withwe show how previous results from learning theory and reasoning fit into this framwork and illustrate the usefulness of the learning to reason approach by exhibiting new results that are not possible in the traditional setting first we give learning to reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms when represented as a traditional formulabased knowledge base second we exhibit a learning to reason algorithm for a class of propositional languages that is not know to be learnable in the traditional sense"
210 "we study the extent to which complex hardware can speed" "we study the extent to which complex hardware can speed up routing specifically we consider the following questions how much does adaptive routing improve over oblivious routing how much does randomness help how does it help if each node can have a large number of neighbors what benefit is available if a node can send packets to several neighbors within a single time step some of these features require complex networking hardware and it is thus important to investigate whether the performance justifies the investment by varying these hardware parameters we obtain a hierarchy of time bounds for worstcase permutation routing"
211 "some parallel algorithms have the property that as they are" "some parallel algorithms have the property that as they are allowed to take more time the total work that they do is reduced this paper describes several algorithms with this property these algorithms solve important problems on directed graphs including breadthfirst search topological sort strong connectivity and and the single source shorest path problem all of the algorithms run on the erew pram model of parallel computer except the algorithm for strong connectivity which runs on the probabilistic erew pram"
212 "most complexity measures for concurrent algorithms for asynchronous sharedmemory architectures" "most complexity measures for concurrent algorithms for asynchronous sharedmemory architectures focus on process steps and memory consumption in practice however performance of multiprocessor algorithms is heavily influenced by contention the extent to which processess access the same location at the same time nevertheless even though contention is one of the principal considerations affecting the performance of real algorithms on real multiprocessors there are no formal tools for analyzing the contention of asynchronous sharedmemory algorithmsthis paper introduces the first formal complexity model for contention in sharedmemory multiprocessors we focus on the standard multiprocessor architecture in which n asynchronous processes communicate by applying read write and readmodifywrite operations to a shared memory to illustrate the utility of our model we use it to derive two kinds of results lower bounds on contention for wellknown basic problems such as agreement and mutual exclusion and tradeoffs between the length of the critical path maximal number of accesses to shared variables performed by a single process in executing the algorithm and contention for these algorithms furthermore we give the first formal contention analysis of a variety of counting networks a class of concurrent data structures inplementing shared counters experiments indicate that certain counting networks outperform conventional singlevariable counters at high levels of contention our analysis provides the first formal model explaining this phenomenon"
214 "we review the field of resultchecking discussing simple checkers and" "we review the field of resultchecking discussing simple checkers and selfcorrectors we argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and enhanced reliability we consider how to modify traditional checking methodologies to make them more appropriate for use in realtime realnumber computer systems in particular we suggest that checkers should be allowed to use stored randomness that is that they should be allowed to generate preprocess and store random bits prior to runtime and then to use this information repeatedly in a series of runtime checks in a case study of checking a general realnumber linear transformation eg a fourier transform we present a simple checker which uses stored randomness and a selfcorrector which is particularly efficient if stored randomness is employed"
215 "we introduce a method to describe systems and their components" "we introduce a method to describe systems and their components by functional specification techniques we define notions of interface and interaction refinement for interactive systems and their components these notions of refinement allow us to change both the syntactic the number of channels and sorts of messages at the channels and the semantic interface causality flow between messages and interaction granularity of an interactive system component we prove that these notions of refinement are compositional with respect to sequential and parallel composition of system components communication feedback and recursive declarations of system components according to these proofs refinements of networks can be accomplished in a modular way by refining their compponents we generalize the notions of refinement to refining contexts finally full abstraction for specifications is defined and compositionality with respect to this abstraction is shown too"
216 "the expressive power of firstorder query languages with several classes" "the expressive power of firstorder query languages with several classes of equality and inequality constraints is studied in this paper we settle the conjecture that recursive queries such as parity test and transitive closure cannot be expressed in the relational calculus augmented with polynomial inequality constraints over the reals furthermore noting that relational queries exhibit several forms of genericity we establish a number of collapse results of the following form the class of generic boolean queries expressible in the relational calculus augmented with a given class of constraints coincides with the class of queries expressible in the relational calculus with or without an order relation we prove such results for both the natural and activedomain semantics as a consequence the relational calculus augmented with polynomial inequalities expresses the same classes of generic boolean queries under both the natural and activedomain semanticsin the course of proving these results for the activedomin semantics we establish ramseytype theorems saying that any query involving certain kinds of constraints coincides with a constraintfree query on databases whose elements come from a certain infinite subset of the domain to prove the collapse results for the natural semantics we make use of techniques from nonstandard analysis and from the model theory of ordered structures"
217 "this paper presents and proves correct a distributed algorithm that" "this paper presents and proves correct a distributed algorithm that implements a sequentially consistent collection of shared readupdate objects this algorithm is a generalization of one used in the orca shared object system the algorithm caches objects in the local memory of processors according to application needs each read operation accesses a single copy of the object while each update accesses all copies the algorithm uses broadcast communication when it sends messages to replicated copies of an object and it uses pointtopoint communication when a message is sent to a single copy and when a reply is returned copies of all objects are kept consistent using a strategy based on sequence numbers for broadcaststhe algorithm is presented in two layers the lower layer uses the given broadcast and pointtopoint communication services plus sequence numbers to provide a new communication service called a context multicast channel the higher layer uses a context multicast channel to manage the object replication in a consistent fashion both layers and their combination are described and verified formally using the io automation model for asynchronous concurrent systems"
218 "we give a new characterization of np the class np" "we give a new characterization of np the class np contains exactly those languages l for which membership proofs a proof that an input x is in l can be verified probabilistically in polynomial time using logarithmic number of random bits and by reading sublogarithmic number of bits from the proofwe discuss implications of this characterization specifically we show that approximating clique and independent set even in a very weak sense is nphard"
219 "a finite automatonthe socalled neuromaton realized by a finite discrete" "a finite automatonthe socalled neuromaton realized by a finite discrete recurrent neural network working in parallel computation mode is considered both the size of neuromata ie the number of neurons and their descriptional complexity ie the number of bits in the neuromaton representation are studied it is proved that a constraint time delay of the neuromaton output does not play a role within a polynomial descriptional complexity it is shown that any regular language given by a regular expression of length n is recognized by a neuromaton with thgrn neurons further it is proved that this network size is in the worst case optimal on the other hand generally there is not an equivalent polynomial length regular expression for a given neuromaton then two specialized constructions of neural acceptors of the optimal descriptional complexity thgrn for a single nbit string recognition are described they both require on neurons and either on connections with constant weights or on edges with weights of the on hopfield condition stating when a regular language is a hopfield language is formulated a construction of a hopfield neuromaton is presented for a regular language satisfying the hopfield condition the class of hopfield languages is shown to be closed under union intersection concatenation and complement and it is not closed under iteration finally the problem whether a regular language given by a neuromaton or by a hopfield acceptor is nonempty is proved to be pspacecomplete as a consequence the same result for a neuromaton equivalence problem is achieved"
220 "an n m tordisperser is a bipartite multigraph gv w" "an n m tordisperser is a bipartite multigraph gv w e with v n and w m having the following expansion property any subset of v having at least t vertices has a neighbor set of size at least m for any pair of constants xgr lgr xgr lgr any sufficiently large n and for any t logn m log nlgr we give an explicit elementary construction of an n m tordisperser such that the outdegree of any vertex in v is at most polylogarithmic in n using this with known applications of ordispersers yields several results first our construction implies that the complexity class strongrp defined by sipser equals rp second for any fixed eegr we give the first polynomialtime simulation of rp algorithms using the output of any eegr minimally random source for any integral r such a source accepts a single request for an rbit string and generates the string according to a distribution that assigns probability at most reegr to any string it is minimally random in the sense that any weaker source is insufficient to do a blackbox polynomialtime simulation of rp algorithms"
221 "we show that quick hitting set generators can replace quick" "we show that quick hitting set generators can replace quick pseudorandom generators to derandomize any probabilistic twosided error algorithms up to now quick hitting set generators have been known as the general and uniform derandomization method for probabilistic onesided error algorithms while quick pseudorandom generators as the generators as the general and uniform method to derandomize probabilistic twosided error algorithmsour method is based on a deterministic algorithm that given a boolean circuit c and given access to a hitting set generator constructs a discrepancy set for c the main novelty is that the discrepancy set depends on c so the new derandomization method is not uniform ie not obliviousthe algorithm works in time exponential in kpn where k is the price of the hitting set generator and p is a polynomial function in the size of c we thus prove that if a logarithmic price quick hitting set generator exists then bpp p"
222 "we consider the problem faced by a robot that must" "we consider the problem faced by a robot that must explore and learn an unknown room with obstacles in it we seek algorithms that achieve a bounded ratio of the worstcase distance traversed in order to see all visible points of the environment thus creating a map divided by the optimum distance needed to verify the map if we had it in the beginning the situation is complicated by the fact that the latter offline problem the problem of optimally verifying a map is nphard although we show that there is no such competitive algorithm for general obstacle courses we give a competitive algorithm for the case of a polygonal room with a bounded number of obstacles in it we restrict ourselves to the rectilinear case where each side of the obstacles and the room is parallel to one of the coordinates and the robot must also move either parallel or perpendicular to the sides in a subsequent paper we will discuss the extension to polygons of general shapeswe also discuss the offline problem for simple rectilinear polygons and find an optimal solution in the l metric in polynomial time in the case where the entry and the exit are different points"
223 "we consider the problem of coloring kcolorable graphs with the" "we consider the problem of coloring kcolorable graphs with the fewest possible colors we present a randomized polynomial time algorithm that colors a colorable graph on n vertices with minodgr log dgr log n on log n colors where dgr is the maximum degree of any vertex besides giving the best known approximation ratio in terms of n this marks the first nontrivial approximation result as a function of the maximum degree dgr this result can be generalized to kcolorable graphs to obtain a coloring using minodgrk log dgr log n on k log n colors our results are inspired by the recent work of goemans and williamson who used an algorithm for semidefinite optimization problems which generalize linear programs to obtain improved approximations for the max cut and max sat problems an intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the lovsz thgr function we show lower bounds on the gap between the optimum solution of our semidefinite program and the actual chromatic number by duality this also demonstrates interesting new facts about the thgrfunction"
224 "recent developments in analyzing molecular structures and representing solid models" "recent developments in analyzing molecular structures and representing solid models using simplicial complexes have further enhanced the need for computing structural information about simplicial complexes in r this paper develops basic techniques required to manipulate and analyze structures of complexes in ra new approach to analyze simplicial complexes in euclidean space r is described first methods from topology are used to analyze triangulated manifolds in r then it is shown that these methods can in fact be applied to arbitrary simplicial complexes in r after simulating the process of thickening a complex to a manifold homotopic to it as a consequence considerable structural information about the complex can be determined and certain discrete problems solved as well for example it is shown how to determine homology groups as well as concrete representations of their generators for a given complex in r"
225 "in this paper we present randomized algorithms over binary search" "in this paper we present randomized algorithms over binary search trees such that a the insertion of a set of keys in any fixed order into an initially empty tree always produces a random binary search tree b the deletion of any key from a random binary search tree results in a random binary search tree c the random choices made by the algorithms are based upon the sizes of the subtrees of the tree this implies that we can support accesses by rank without additional storage requirements or modification of the data structures and d the cost of any elementary operation measured as the number of visited nodes is the same as the expected cost of its standard deterministic counterpart hence all search and update operations have guaranteed expected cost olog n but now irrespective of any assumption on the input distribution"
226 "consider an online scheduling problem in which a set of" "consider an online scheduling problem in which a set of abstract processes are competing for the use of a number of resources further assume that it is either prohibitively expensive or impossible for any two of the processes to directly communicate with one another if several processes simultaneously attempt to allocate a particular resource as may be expected to occur since the processes cannot easily coordinate their allocations then none succeed in such a framework it is a challenge to design efficient contention resolution protocolstwo recentlyproposed approaches to the problem of pram emulation give rise to scheduling problems of the above kind in one approach the resources in this case the shared memory cells are duplicated and distributed randomly we analyze a simple and efficient deterministic algorithm for accessing some subset of the duplicated resources in the other approach we analyze how quickly we can access the given nonduplicated resource using a simple randomized strategy we obtain precise bounds on the performance of both strategies we anticipate that our results with find other applications"
227 "this paper examines numerical issues in computing solutions to networks" "this paper examines numerical issues in computing solutions to networks of stochastic automata it is wellknown that when the matrices that represent the automata contain only constant values the cost of performing the operation basic to all iterative solution methods that of matrixvector multiply is given by rni n nii n ni where ni is the number of states in the ith automaton and n is the number of automata in the network we introduce the concept of a generalized tensor product and prove a number of lemmas concerning this product the result of these lemmas allows us to show that this relatively small number of operations is sufficient in many practical cases of interest in which the automata contain functional and not simply constant transitions furthermore we show how the automata should be ordered to achieve this"
228 "we examine a class of collective coinflipping games that arises" "we examine a class of collective coinflipping games that arises from randomized distributed algorithms with halting failures in these games a sequence of local coin flips is generated which must be combined to form a single global coin flip an adversary monitors the game and may attempt to bias its outcome by hiding the result of up to t local coin flips we show that to guarantee at most constant bias ohgrt local coins are needed even if a the local coins can have arbitrary distributions and ranges b the adversary is required to decide immediately wheter to hide or reveal each local coin and c the game can detect which local coins have been hidden if the adversary is permitted to control the outcome of the coin except for cases whose probability is polynomial in t ohgrt logt local coins are needed combining this fact with an extended version of the wellknown fischerlynchpaterson impossibility proof of deterministic consensus we show that given an adaptive adversary any tresilient asynchronous consensus protocol requires ohgrtlogt local coin flips in any model that can be simulated deterministically using atomic registers this gives the first nontrivial lower bound on the total work required by waitfree consensus and is tight to within logarithmic factors"
229 "waitfree implementations of shared objects tolerate the failure of processes" "waitfree implementations of shared objects tolerate the failure of processes but not the failure of base objects from which they are implemented we consider the problem of implementing shared objects that tolerate the failure of both processes and base objectswe identify two classes of object failures responsive and nonresponsive with responsive failures a faulty object responds to every operation but its responses may be incorrect with nonresponsive failures a faulty object may also hang without responding in each class we define crash omission and arbitrary modes of failurewe show that all responsive failure modes can be tolerated more precisely for all responsive failure modes f object types t and t ohgr we show how to implement a shared object of type t which is ttolerant for f such an object remains correct and waitfree even if up to t base objects fail according to f in contrast to responsive failures we show that even the most benign nonresponsive failure mode cannot be tolerated we also show that randomization can be used to circumvent this impossibility resultgraceful degradation is a desirable property of faulttolerant implementations the implemented object never fails more severely than the base objects it is derived from even if all the base objects fail for several failure modes we show wheter this property can be achieved and if so how"
230 "we show that every language in np has a probablistic" "we show that every language in np has a probablistic verifier that checks membership proofs for it using logarithmic number of random bits and by examining a constant number of bits in the proof if a string is in the language then there exists a proof such that the verifier accepts with probability ie for every choice of its random string for strings not in the language the verifier rejects every provided proof with probability at least our result builds upon and improves a recent result of arora and safra whose verifiers examine a nonconstant number of bits in the proof though this number is a very slowly growing function of the input lengthas a consequence we prove that no max snphard problem has a polynomial time approximation scheme unless np p the class max snp was defined by papadimitriou and yannakakis and hard problems for this class include vertex cover maximum satisfiability maximum cut metric tsp steiner trees and shortest superstring we also improve upon the clique hardness results of feige et al and arora and safra and show that there exists a positive egr such that approximating the maximum clique size in an nvertex graph to within a factor of negr is nphard"
231 "directorybased coherence protocols in sharedmemory multiprocessors are so complex that" "directorybased coherence protocols in sharedmemory multiprocessors are so complex that verification techniques based on automated procedures are required to establish their correctness state enumeration approaches are wellsuited to the verification of cache protocols but they face the problem of state space explosion leading to unacceptable verification time and memory consumption even for small system configurations one way to manage this complexity and make the verification feasible is to map the system model to verify onto a symbolic state model ssm since the number of symbolic states is considerably less than the number of system states an exhaustive state search becomes possible even for largescale sytems and complex protocolsin this paper we develop the concepts and notations to verifiy some properties of a directorybased protocol designed for nonfifo interconnection networks we compare the verification of the protocol with ssm and with the stanford mur a verification tool enumerating system states we show that ssm is much more efficient in terms of verification time and memory consumption and therefore holds that promise of verifying much more complex protocols a unique feature of ssm is that it verifies protocols for any system size and therefore provides reliable verification results in one run of the tool"
232 "a database query is finite if its result consists of" "a database query is finite if its result consists of a finite sets tuples for queries formulated as sets of pure horn rules the problem of determining finiteness is in general undecidablein this paper we consider superfinitenessa stronger kind of finiteness which applies to horn queries whose function symbols are replaced by the abstraction of infinite relations with finiteness constraints abbr fcs we show that superfiniteness is not only decidable but also axiomatizable and the axiomatization yields an effective decision procedure although there are finite queries that are not superfinite we demonstrate that superfinite queries represent an interesting and nontrivial subclass within the class of all finite queriesthe we turn to the issue of inference of finiteness constraintsan important practical problem that is instrumental in deciding if a query is evaluable by a bottomup algorithm although it is not known whether fcentailment is decidable for sets of functionfree horn rules we show that superentailment a stronger form of entailment is decidable we also show how a decision procedure for superentailment can be used to enhance tests for query finiteness"
233 "given a collection f of subsets of s n set" "given a collection f of subsets of s n set cover is the problem of selecting as few as possible subsets from f such that their union covers s and max kcover is the problem of selecting k subsets from f such that their union has maximum cardinality both these problems are nphard we prove that o ln n is a threshold below which set cover cannot be approximated efficiently unless np has slightly superpolynomial time algorithms this closes the gap up to loworder terms between the ratio of approximation achievable by the greedy alogorithm which is o ln n and provious results of lund and yanakakis that showed hardness of approximation within a ratio of log nsime ln n for max kcover we show an approximation threshold of eup to loworder terms under assumption that pnp"
234 "in this paper we consider the question of determining whether" "in this paper we consider the question of determining whether a function f has property p or is egrfar from any function with property p a property testing algorithm is given a sample of the value of f on instances drawn according to some distribution in some cases it is also allowed to query f on instances of its choice we study this question for different properties and establish some connections to problems in learning theory and approximationin particular we focus our attention on testing graph properties given access to a graph g in the form of being able to query whether an edge exists or not between a pair of vertices we devise algorithms to test whether the underlying graph has properties such as being bipartite kcolorable or having a pclique clique of density p with respect to the vertex set our graph property testing algorithms are probabilistic and make assertions that are correct with high probability while making a number of queries that is independent of the size of the graph moreover the property testing algorithms can be used to efficiently ie in time linear in the number of vertices construct partitions of the graph that correspond to the property being tested if it holds for the input graph"
235 "this paper describes an onlogn deterministic algorithm and an on" "this paper describes an onlogn deterministic algorithm and an on las vegas algorithm for testing whether two given trivalent graphs on n vertices are isomorphic in fact the algorithms construct the set of all isomorphisms between two such graphs presenting in particular generators for the group of all automorphisms of a trivalent graph the algorithms are based upon the original polynomialtime solution to these problems by luks but they introduce numerous speedups these include improved permutationgroup algorithms that exploit the structure of the underlying groups a remarkable property of the las vegas algorithm is that it computes the set of all isomorphisms between two trivalent graphs for the cost of computing only those isomorphisms that map a specified edge to a specified edge"
237 "the desirability of acyclic database schemes is well argued in" "the desirability of acyclic database schemes is well argued in and for schemas described by multivalued dependencies acyclicity means that the dependencies do not split each others lefthand sides and do not form intersection anomalies in a recent work it is argued that realworld database schemes always meet the former requirement and in it is shown that any given realworld scheme can be made to satisfy also the latter requirement after being properly extended however the method of elimination of intersection anomalies proposed in is intrinsically nondeterministican undesirable property for a design tool in the present work it is shown that this nondeterminism does not however affect the final result of the design process in addition we present an efficient deterministic algorithm which is equivalent to the nondeterministic process of along the way a study of intersection anomalies which is interesting in its own right is performed"
238 "given a finite set of texts s w wk over" "given a finite set of texts s w wk over some fixed finite alphabet sgr a complete inverted file for s is an abstract data type that provides the functions findw which returns the longest prefix of w that occurs as a subword of a word in s freqw which returns the number of times w occurs in s and locationsw which returns the set of positions where w occurs in s a data structure that implements a complete inverted file for s that occupies linear space and can be built in linear time using the uniformcost ram model is given using this data structure the time for each of the above query functions is optimal to accomplish this techniques from the theory of finite automata and the work on suffix trees are used to build a deterministic finite automaton that recognizes the set of all subwords of the set s this automaton is then annotated with additional information and compacted to facilitate the desired query functions the result is a data structure that is smaller and more flexible than the suffix tree"
239 "in this paper we develop a new data structure for" "in this paper we develop a new data structure for implementing heaps priority queues our structure fibonacci heaps abbreviated fheaps extends the binomial queues proposed by vuillemin and studied further by brown fheaps support arbitrary deletion from an nitem heap in olog n amortized time and all other standard heap operations in o amortized time using fheaps we are able to obtain improved running times for several network optimization algorithms in particular we obtain the following worstcase bounds where n is the number of vertices and m the number of edges in the problem graph on log n m for the singlesource shortest path problem with nonnegative edge lengths improved from omlogm nn onlog n nm for the allpairs shortest path problem improved from onm logmnn onlog n nm for the assignment problem weighted bipartite matching improved from onmlogmnn ombgrm n for the minimum spanning tree problem improved from omlog logmn n where bgrm n min i uharl login mn note that bgrm n logn if m n of these results the improved bound for minimum spanning trees is the most striking although all the results give asymptotic improvements for graphs of appropriate densities"
240 "presented are several algorithms whose operations are governed by a" "presented are several algorithms whose operations are governed by a principle of failure functions when searching for an extremal value within a sequence it suffices to consider only the subsequence of items each of which is the first possible improvement of its predecessor these algorithms are more efficient than their more traditional counterparts"
241 "we present a simple efficient and unified solution to the" "we present a simple efficient and unified solution to the problems of synchronizing initializing and integrating clocks for systems with different types of failures crash omission and arbitrary failures with and without message authentication this is the first known solution that achieves optimal accuracythe accuracy of synchronized clocks with respect to real time is as good as that specified for the underlying hardware clocks the solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy"
242 "an algorithm for computing the power series solution of a" "an algorithm for computing the power series solution of a system of linear equations with components that are dense univariate polynomials over a field is described and analyzed a method for converting the power series solution to rational form is derived theoretical and experimental cost estimates are obtained and used to identify classes of problems for which the power series method outperforms modular methods finally it is shown that the power series method also provides an effective mechanism for solving the problem in which the coefficients of the polynomials are from the ring of integers"
243 "in this paper catastrophic behavior found in computer systems is" "in this paper catastrophic behavior found in computer systems is investigated deterministic catastrophe theory is introduced first then it is shown how the theory can be applied in a stochastic framework which is useful for understanding computer system performance models computer system models that exhibit stochastic cusp catastrophe behavior are then analyzed these models include slotted aloha multiprogramming in computer systems and buffer flow control in computer networks"
244 "a rigorous extension of the recent perturbation analysis approach to" "a rigorous extension of the recent perturbation analysis approach to more general discrete event systems is given first a general class of systems and performance measures is defined and some basic reprsentational and linearity properties are derived next a sample gradient of performance with respect to a parameter of the system is defined then for certain parameters of such systems an infinitesimal perturbation analysis algorithm is derived it is proved that this algorithm gives exact values for the sample gradients of performance with respect to the parameters by observing only one sample path of the deds however the sample gradient may or may not be a good estimate of the gradient of the performance measure this point is elaborated in the body of the paper the computational complexity of this algorithm is bound to be linear in the number of events these results offer the potential for very efficient calculation of the gradientsa fact that can be used for designoperation of computer systems communication networks manufacturing systems and many other realworld systems particularly since restrictive assumptions eg exponential distributions are not required of the system"
245 "if c is a class of sets and a is" "if c is a class of sets and a is not in c then an infinite set h is a proper hard core for a with respect to c if h a and for every c egr c such that c a c cap h is finite it is shown that if c is a countable class of sets of strings that is closed under finite union and finite variation then every infinite set not in c has a proper hard core with respect to c in addition the density of such generalized complexity cores is studied"
247 "we present a polynomial time approximation scheme for euclidean tsp" "we present a polynomial time approximation scheme for euclidean tsp in fixed dimensions for every fixed c and given any n nodes in r a randomized version of the scheme finds a capproximation to the optimum traveling salesman tour in onlog noc time when the nodes are in r d the running time increases to onlog nod cd for every fixed c d the running time is n polylogn that is nearly linear in n the algorithmm can be derandomized but this increases the running time by a factor ond the previous best approximation algorithm for the problem due to christofides achieves a aproximation in polynomial timewe also give similar approximation schemes for some other nphard euclidean problems minimum steiner tree ktsp and kmst the running times of the algorithm for ktsp and kmst involve an additional multiplicative factor k the previous best approximation algorithms for all these problems achieved a constantfactor approximation we also give efficient approximation schemes for euclidean mincost matching a problem that can be solved exactly in polynomial timeall our algorithms also work with almost no modification when distance is measured using any geometric norm such as ell p for p or other minkowski norms they also have simple parallel ie nc implementations"
248 "we introduce a new approach to the maximum flow problem" "we introduce a new approach to the maximum flow problem this approach is based on assigning arc lengths based on the residual flow value and the residual arc capacities our approach leads to an ominn mm logn m log u time bound for a network with n vertices m arcs and integral arc capacities in the range u this is a fundamental improvement over the previous time bounds we also improve bounds for the gomoryhu tree problem the parametric flow problem and the approximate st cut problem"
249 "we demonstrate the power of object identities oids as a" "we demonstrate the power of object identities oids as a database query language primitive we develop an objectbased data model whose structural part generalizes most of the known complexobject data models cyclicity is allowed in both its schemas and instances our main contribution is the operational part of the data model the query language iql which uses oids for three critical purposes to represent datastructures with sharing and cycles to manipulate sets and to express any computable database query iql can be type checked can be evaluated bottomup and naturally generalizes most popular rulebased languages the model can also be extended to incorporate type inheritance without changes to iql finally we investigate an analogous valuebased data model whose structural part is founded on regular infinte trees and whose operational part is iql"
250 "the waitefree hierarchy provides a classification of multiprocessor synchronization primitives" "the waitefree hierarchy provides a classification of multiprocessor synchronization primitives based on the values of n for which there are deterministic waitfree implementations of nprocess consensus using instances of these objects and readwrite registers in a randomized waitfree setting this classification is degenerate since nprocess consensus can be solved using only on readwrite registersin this paper we propose a classification of synchronization primitives based on the space complexity of randomized solutions to nprocess consensus a historyless object such as a readwrite register a swap register or a testset register is an object whose state depends only on the lost nontrivial operation thate was applied to it we show that using historyless objects ohgrn object instances are necessary to solve nprocess consensus this lower bound holds even if the objects have unbounded size and the termination requirement is nondeterministic solo termination a property strictly weaker than randomized waitfreedomwe then use this result to related the randomized space complexity of basic multiprocessor synchronization primitives such as shared counters fetch add registers and compareswap registers viewed collectively our results imply that there is a separation based on space complexity for synchronization primitives in randomized computation and that this separation differs from that implied by the deterministic waitfree hierarchy"
251 "we present an efficient algorithm for paclearning a very general" "we present an efficient algorithm for paclearning a very general class of geometric concepts over r d for fixed d more specifically let t be any set of s halfspaces let x x xd be an arbitrary point in r d with each tt we associate a boolean indicator function itx which is if and only if x is in the halfspace t the concept class cds that we study consists of all concepts formed by any boolean function over it its for ti t this class is much more general than any geometric concept class known to be paclearnable our results can be extended easily to learn efficiently any boolean combination of a polynomial number of concepts selected from any concept class c over r given that the vcdimension of c has dependence only on d and there is a polynomial time algorithm to determine if there is a concept from c consistent with a given set of labeled examples we also present a statistical query version of our algorithm that can tolerate random classification noise finally we present a generalization of the standard egrnet result of haussler and welzl and apply it to give an alternative noisetolerant algorithm for d based on geometric subdivisions"
252 "consider a set of s of n data points in" "consider a set of s of n data points in real ddimensional space rd where distances are measured using any minkowski metric in nearest neighbor searching we preprocess s into a data structure so that given any query point q rd is the closest point of s to q can be reported quickly given any positive real egr data point p is a egrapproximate nearest neighbor of q if its distance from q is within a factor of egr of the distance to the true nearest neighbor we show that it is possible to preprocess a set of n points in rd in odn log n time and odn space so that given a query point q rd and egr a egrapproximate nearest neighbor of q can be computed in ocd egr log n time where cd egrd ded is a factor depending only on dimension and egr in general we show that given an integer k egrapproximations to the k nearest neighbors of q can be computed in additional okd log n time"
253 "the following problems that arise in the computation of electrostatic" "the following problems that arise in the computation of electrostatic forces and in the boundary element method are considered given two convex interiordisjoint polyhedra in space endowed with a volume charge density which is a polynomial in the cartesian coordinates of r compute the coulomb force acting on them given two interiordisjoint polygons in space endowed with a surface charge density which is polynomial in the cartesian coordinates of r compute the normal component of the coulomb force acting on them for both problems adaptive gaussian approximation algorithms are given which for n gaussian points in time on achieve absolute error ocn for a constant c such a result improves upon previously known best asymptotic bounds this result is achieved by blending techniques from integral geometry computational geometry and numerical analysis in particular integral geometry is used in order to represent the forces as integrals whose kernal is free from singularities"
254 "publicly accessible databases are an indispensable resource for retrieving uptodate" "publicly accessible databases are an indispensable resource for retrieving uptodate information but they also pose a significant risk to the privacy of the user since a curious database operator can follow the users queries and infer what the user is after indeed in cases where the users intentions are to be kept secret users are often cautious about accessing the database it can be shown that when accessing a single database to completely guarantee the privacy of the user the whole database should be downloaded namely n bits should be communicated where n is the number of bits in the databasein this work we investigate whether by replicating the database more efficient solutions to the private retrieval problem can be obtained we describe schemes that enable a user to access k replicated copies of a database k and privately retrieve information stored in the database this means that each individual server holding a replicated copy of the database gets no information on the identity of the item retrieved by the user our schemes use the replication to gain substantial saving in particular we present a twoserver scheme with communication complexity on"
255 "in this paper we study the problem of learning in" "in this paper we study the problem of learning in the presence of classification noise in the probabilistic learning model of valiant and its variants in order to identify the class of robust learning algorithms in the most general way we formalize a new but related model of learning from statistical queries intuitively in this model a learning algorithm is forbidden to examine individual examples of the unknown target function but is given acess to an oracle providing estimates of probabilities over the sample space of random examplesone of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in valiants model with a noise rate approaching the informationtheoretic barrier of we then demonstrate the generality of the statistical query model showing that practically every class learnable in valiants model and its variants can also be learned in the new model and thus can be learned in the presence of noise a notable exception to this statement is the class of parity functions which we prove is not learnable from statistical queries and for which no noisetolerant algorithm is known"
256 "we propose inference systems for binary relations that satisfy composition" "we propose inference systems for binary relations that satisfy composition laws such as transitivity our inference mechanisms are based on standard techniques from term rewriting and represent a refinement of chaining methods as they are used in the context of resolutiontype theorem proving we establish the refutational completeness of these calculi and prove that our methods are compatible with the usual simplification techniques employed in refutational theorem provers such as subsumption or tautology deletion various optimizations of the basic chaining calculus will be discussed for theories with equality and for total orderings a key to the practicality of chaining methods is the extent to which socalled variable chaining can be avoided we demonstrate that rewrite techniques considerably restrict variable chaining and that further restrictions are possible if the transitive relation under consideration satisfies additional properties such as symmetry but we also show that variable chaining cannot be completely avoided in general"
257 "a class of parallel algorithms for evaluating game trees is" "a class of parallel algorithms for evaluating game trees is presented these algorithms parallelize a standard sequential algorithm for evaluating andor trees and the agrbgr pruning procedure for evaluating minmax trees it is shown that uniformly on all instances of uniform andor trees the parallel andor tree algorithm achieves an asymptotic linear speedup using a polynomial number of processors in the height of the tree the analysis of linear speedup using more than a linear number of processors is due to j harting a numerical lower bound rigorously establishes a good speedup for the uniform andor trees with parameters that are typical in practice the performance of the parallel agrbgr algorithm on bestordered minmax trees is analyzed"
258 "genomes frequently evolve by reversals rgrij that transform a gene" "genomes frequently evolve by reversals rgrij that transform a gene order pgr pgripgri pgrjpgrj pgrn into pgr pgr ipgrj pgripgrj pgrn reversal distance between permutations pgr and sgris the minimum number of reversals to transform pgr into agr analysis of genome rearrangements in molecular biology started in the late s when dobzhansky and sturtevant published a milestone paper presenting a rearrangement scenario with inversions between the species of drosophilia analysis of genomes evolving by inversions leads to a combinatorial problem of sorting by reversals studied in detail recently we study sorting of signed permutations by reversals a problem that adequately models rearrangements in a small genomes like chloroplast or mitochondrial dna the previously suggested approximation algorithms for sorting signed permutations by reversals compute the reversal distance between permutations with an astonishing accuracy for both simulated and biological data we prove a duality theorem explaining this intriguing performance and show that there exists a hidden parameter that allows one to compute the reversal distance between signed permutations in polynomial time"
259 "this paper introduces compressed certificates for planarity biconnectivity and triconnectivity" "this paper introduces compressed certificates for planarity biconnectivity and triconnectivity in planar graphs and proves many structural properties of certificates in planar graphs as an application of our compressed certificates we develop efficient dynamic planar algorithms in particular we consider the following three operations on a planar graph g i insert an edge if the resultant graph remains planar ii delete an edge and iii test whether an edge could be added to the graph without violating planarity we show how to support each of the above operations in on time where n is the number of vertices in the graph the bound for tests and deletions is worstcase while the bound for insertions is amortized this is the first algorithm for this problem with sublinear running time and it affirmatively answers a question posed in epstein et al we use our compressed certificates for biconnectivity and triconnectivity to maintain the biconnected and triconnected components of a dynamic planar graph the time bounds are the same on worstcase time per edge deletion on amortized time per edge insertion and on amortized time per edge insertion and onworstcase time to check whether two vertices are either biconnected or triconnected"
260 "this paper analyzes a recently published algorithm for page replacement" "this paper analyzes a recently published algorithm for page replacement in hierarchical paged memory systems oneil et al the algorithm is called the lruk method and reduces to the wellknown lru least recently used method for k previous work oneil et al weikum et al johnson and shasha has shown the effectiveness for k by simulation especially in the most common case of k the basic idea in lruk is to keep track of the times of the last k references to memory pages and to use this statistical information to rankorder the pages as to their expected future behavior based on this the page replacement policy decision is made which memoryresident page to replace when a newly accessed page must be read into memory in the current paper we prove under the assumptions of the independent reference model that lruk is optimal specifically we show given the times of the up to k most recent references to each disk page no other algorithm a making decisions to keep pages in a memory buffer holding n pages based on this infomation can improve on the expected number of ios to access pages over the lruk algorithm using a memory buffer holding n pages the proof uses the bayesian formula to relate the space of actual page probabilities of the model to the space of observable page numbers on which the replacement decision is acutally made"
261 "this paper has two agendas one is to develop the" "this paper has two agendas one is to develop the foundations of roundoff in computation the other is to describe an algorithm for deciding feasibility for polynomial systems of equations and inequalities together with its complexity analysis and its roundoff properties each role reinforces the other"
262 "we consider the problem of deciding whether a polygonal knot" "we consider the problem of deciding whether a polygonal knot in dimensional euclidean space is unknotted ie capable of being continuously deformed without selfintersection so that it lies in a plane we show that this problem unknotting problem is in np we also consider the problem splitting problem of determining whether two or more such polygons can be split or continuously deformed without selfintersection so that they occupy both sides of a plane without intersecting it we show that it also is in np finally we show that the problem of determining the genus of a polygonal knot a generalization of the problem of determining whether it is unknotted is in pspace we also give exponential worstcase running time bounds for deterministic algorithms to solve each of these problems these algorithms are based on the use of normal surfaces and decision procedures due to w haken with recent extensions by w jaco and j l tollefson"
263 "a number of current technologies allow for the determination of" "a number of current technologies allow for the determination of interatomic distance information in structures such as proteins and rna thus the reconstruction of a threedimensional set of points using information about its interpoint distances has become a task of basic importance in determining molecular structure the distance measurements one obtains from techniques such as nmr are typically sparse and errorprone greatly complicating the reconstruction task many of these errors result in distance measurements that can be safely assumed to lie within certain fixed tolerances but a number of sources of systematic error in these experiments lead to inaccuracies in the data that are very hard to quantify in effect one must treat certain entries of the measured distance matrix as being arbitrarily corruptedthe existence of arbitrary errors leads to an interesting sort of errorcorrection problemhow many corrupted entries in a distance matrix can be efficiently corrected to produce a consistent threedimensional structure for the case of an n n matrix in which every entry is specified we provide a randomized algorithm running in time on log n that enumerates all structures consistent with at most egrn errors per row with high probability in the case of randomly located errors we can correct errors of the same density in a sparse matrixone in which only a bgr fraction of the entries in each row are given for any constant bgrgt"
264 "we introduce a new textindexing data structure the string btree" "we introduce a new textindexing data structure the string btree that can be seen as a link between some traditional externalmemory and stringmatching data structures in a short phrase it is a combination of btrees and patricia tries for internalnode indices that is made more effective by adding extra pointers to speed up search and update operations consequently the string btree overcomes the theoretical limitations of inverted files btrees prefix btrees suffix arrays compacted tries and suffix trees string btrees have the same worstcase performance as btrees but they manage unboundedlength strings and perform much more powerful search operations such as the ones supported by suffix trees string btrees are also effective in main memory ram model because they improve the online suffix tree search on a dynamic set of strings they also can be successfully applied to database indexing and software duplication"
265 "many highlevel parallel programming languages allow for finegrained parallelism as" "many highlevel parallel programming languages allow for finegrained parallelism as in the popular worktime framework for parallel algorithm design programs written in such languages can express the full parallelism in the program without specifying the mapping of program tasks to processors a common concern in executing such programs is to schedule tasks to processors dynamically so as to minimize not only the execution time but also the amount of space memory needed without careful scheduling the parallel execution on p processors can use a factor of p or larger more space than a sequential implementation of the same programthis paper first identifies a class of parallel schedules that are provably efficient in both time and space for any computation with w units of work and critical path length d and for any sequential schedule that takes space s we provide a parallel schedule that takes fewer than wp d steps on p processors and requires less than s pdotd space this matches the lower bound that we show and significantly improves upon the best previous bound of sdotp spaces for the common case where ds the paper then describes a scheduler for implementing highlevel languages with nested parallelism that generates schedules in this class during program execution as the structure of the computation is revealed the scheduler keeps track of the active tasks allocates the tasks to the processors and performs the necessary task synchronization the scheduler is itself a parallel algorithm and incurs at most a constant factor overhead in time and space even when the scheduling granularity is individual units of work the algorithm is the first efficient solution to the scheduling problem discussed here even if space considerations are ignored"
266 "an on algorithm for recognizing planar graphs that do not" "an on algorithm for recognizing planar graphs that do not contain induced odd cycles of length greater than odd holes is presented a planar graph with this property satisfies the requirement that its maximum clique size equal the minimum number of colors required for the graph graphs all of whose induced subgraphs satisfy the latter property are perfect as defined by berge the algorithm presented is based on decomposing these graphs into essentially two special classes of inseparable component graphs that are easy to recognize they are i planar comparability graphs and ii planar line graphs of those planar bipartite graphs whose maximum degrees are no greater than composition schemes for generating planar perfect graphs from those basic components are also provided this decomposition algorithm can also be adapted to solve the corresponding maximum independent set and minimum coloring problems finally the pathparity problem on planar perfect graphs is considered"
267 "we focus on a rich axiomatization for actions in the" "we focus on a rich axiomatization for actions in the situation calculus that includes among other features a solution to the frame problem for deterministic actions our work is foundational in nature directed at simplifying the entailment problem for these axioms specifically we make four contributions to the metatheory of situation calculus axiomatizations of dynamical systems we prove that the abovementioned axiomatization for actions has a relative satisfiability property the full axiomatization is satisfiable iff the axioms for the initial state are we define the concept of regression relative to these axioms and prove a soundness and completeness theorem for a regressionbased approach to the entailment problem for a wide class of queries our formalization of the situation calculus requires certain foundational axioms specifying the domain of situations these include an induction axiom whose presence complicates human and automated reasoning in the situation calculus we characterize various classes of sentences whose proofs do not require induction and in some cases some of the other foundational axioms we prove that the logic programming language golog never requires any of the foundational axioms for the evaluation of programs"
268 "the singlesource shortest paths problem sssp is one of the" "the singlesource shortest paths problem sssp is one of the classic problems in algorithmic graph theory given a positively weighted graph g with a source vertex s find the shortest path from s to all other vertices in the graphsince all theoretical developments in sssp for general directed and undirected graphs have been based on dijkstras algorithm visiting the vertices in order of increasing distance from s thus any implementation of dijkstras algorithm sorts the vertices according to their distances from s however we do not know how to sort in linear time here a deterministic linear time and linear space algorithm is presented for the undirected single source shortest paths problem with positive integer weights the algorithm avoids the sorting bottleneck by building a hierarchical bucketing structure identifying vertex pairs that may be visited in any order"
269 "the approximate string matching problem is to find all locations" "the approximate string matching problem is to find all locations at which a query of lengthm matches a substring of a text of length n with korfewer differences simple and practical bitvector algorithms have been designed for this problem most notably the one used in agrep these algorithms compute a bit representation of the current stateset of the kdifference automaton for the query and asymptotically run in either onmw or onm log sgrw time where w is the word size of the machine eg or in practice and sgr is the size of the pattern alphabet here we present an algorithm of comparable simplicity that requires only onmw time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem thus the algorithms performance is independent of k and it is found to be more efficient than the previous results for many choices of k and smallm moreover because the algorithm is not dependent on k it can be used to rapidly compute blocks of the dynamic programming matrix as in the russians algorithm of wu et al this gives rise to an oknw expectedtime algorithm for the case where m may be arbitrarily large in practice this new algorithm that computes a region of the dynamic progr amming dp matrx w entries at a time using the basic algorithm as a subroutine is significantly faster than our previous russians algorithm that computes the same region or entries at a time using table lookup this performance improvement yields a code that is either superior or competitive with all existing algorithms except for some filtration algorithms that are superior when km is sufficiently small"
270 "the structural treebased mapping algorithm is an efficient and popular" "the structural treebased mapping algorithm is an efficient and popular technique for technology mapping in order to make good use of this mapping technique in ftga design it is desirable to design fpga logic modules based on boolan functions which can be represented by a tree of gates ie seriesparallel or sp functions thakur and wong a b studied this issue and they demonstrated the advantages of designing logic modules as universal sp functions that is sp functions which can implement all sp functions with a certain number of inputs the number of variables in the universal function corresponds to the number of inputs to the fpga module so it is desirable to have as few variables as possible in the constructed functions the universal sp functions presented in thakur and wong a b were designed manually recently there is an algorithm that can generate these functions automatically young and wong but the number of variables in the generated functions grows exponentially in this paper we present an algorithm to generate for each n a universal sp function fn for implementing all sp functions with n inputs or less the number of variables in fn is less than n and the constructions are the smallest possible when n is small n we also derived a nontrival lower bound on the sizes of the optimal universal sp functions ohgrn log n"
271 "weighted voting is used as the basis for a replication" "weighted voting is used as the basis for a replication technique for directories this technique affords arbitrarily high data availability as well as high concurrency efficient algorithms are presented for all of the standard directory operations a structural property of the replicated directory that permits the construction of an efficient algorithm for deletion is proven simulation results are presented and the system is modeled and analyzed the analysis agrees well with the simulation and the space and time performance are shown to be good for all configurations of the system"
272 "a new summation formula based on the orthogonal property of" "a new summation formula based on the orthogonal property of walsh functions is devised using this formula the kdimensional discrepancy of the generalized feedback shift register gfsr pseudorandom numbers is derived the relation between the discrepancy and kdistribution of gfsr sequences is also obtained finally the definition of optimal gpsr pseudorandom number generators is introduced"
273 "algorithms are given that compute maximum flows in planar directed" "algorithms are given that compute maximum flows in planar directed networks either in olog n parallel time using on processors or olog n parallel time using on processors the resource consumption of these algorithms is dominated by the cost of finding the value of a maximum flow when such a value is given or when the computation is on an undirected network the bound is olog n time using on processors no efficient parallel algorithm is known for the maximum flow problem in general networks"
274 "an algorithm is presented to compute the residue of a" "an algorithm is presented to compute the residue of a polynomial over a finite field of degree n modulo a polynomial of degree olog n in on algebraic operations this algorithm can be implemented on a turing machine the implementation is based on turing machine procedure that divides a polynomial of degree n by a sparse polynomial with k nonzero coefficients in okn steps this algorithm can be adapted to compute the residue of a number of length n modulo a number of length olog n in on bit operations"
275 "striking progress has been made recently in obtaining expressions for" "striking progress has been made recently in obtaining expressions for the sojourn time distribution function stdf of a job at a cserver firstcome firstserve fcfs center in a closed productform queuing network these results have more recently been extended and expressions have been obtained for the joint distribution function df of the sojourn times of a job at a sequence of singleserver fcfs centers lying on an overtakefree path however these formulas present considerable computational problems in the case of large closed queuing networks in this paper asymptotic techniques developed by mitra and mckenna for the calculation of the partition function of large productform closed queuing networks are applied to the sojourn time problem asymptotic expansions are obtained for the stdf of a job at cserver fcfs center in closed productform queuing networks similar expansions are obtained for the joint df of the sojourn times of a job at a sequence of single server fcfs centers lying on an overtakefree path in addition integral expressions are obtained for the stdf of a job at a single server fcps center in a closed productform queuing network in which all the centers are load independent these integral expressions also yield useful asymptotic expansions finally integral expressions are also obtained for the joint df of the sojourn times of a job at the centers of an overtakefree path in such a network"
276 "in connection with the least fixed point operator the following" "in connection with the least fixed point operator the following question was raised suppose that a firstorder formula pp is semantically monotone in a predicate symbol p on finite structures is pp necessarily equivalent on finite structures to a firstorder formula with only positive occurrences of p in this paper this question is answered negatively moreover the counterexample naturally gives a uniform sequence of constantdepth polynomialsize monotone boolean circuits that is not equivalent to any however nonuniform sequence of constantdepth polynomialsize positive boolean circuits"
277 "according to the definition of satisfaction of boolean dependencies theorem" "according to the definition of satisfaction of boolean dependencies theorem is not true for boolean dependencies with negation a positive boolean dependency is built using the boolean connectives cuwed cuvee and nrarr a general boolean dependency with negation may use also the boolean connective actually the definition of satisfaction is not meaningful for boolean dependencies with negation since many are never satisfied we show how the definition of satisfaction should be changed in order to make boolean dependencies with negation meaningful and correct the error we associate with each relation r a set agrr of truth assignments as follows for each pair of distinct tuples of r the set agrr contains the truth assignment that maps an attribute a to true if the two tuples are equal on a and to false if the two tuples have different values for a a boolean dependency sgr is satisfied by a relation r if sgr ie the corresponding boolean formula satisfies every truth assignment of agrr the original definition given in the paper is equivalent to having agrr also include the truth assignment that is generated by pairs in which both tuples are really the same tuple of r that is to having agrr also always include the truth assignment tgr mapping all attributes to true under that definition however many boolean dependencies with negation are never satisfied and hence are meaningless more precisely according to the original definition a boolean dependency is satisfied by"
278 "evolution can be mathematically modelled by a stochastic process that" "evolution can be mathematically modelled by a stochastic process that operates on the dna of species such models are based on the established theory that the dna sequences or genomes of all extant species have been derived from the genome of the common ancestor of all species by a process of random mutation and natural selectiona stochastic model of evolution can be used to construct phylogenies or evolutionary trees for a set of species maximum likelihood estimation mle methods seek the evolutionary tree which is most likely to have produced the dna under consideration while these methods are intellectually satisfying they have not been widely accepted because of their computational intractabilityin this paper we address the intractability of mle methods as follows we introduce a metric on stochastic process models of evolution we show that this metric is meaningful by proving that in order for any algorithm to distinguish between two stochastic models that are close according to this metric it needs to be given many observations we complement this result with a simple and efficient algorithm for inverting the stochastic process of evolution that is for building a tree from observations on twostate characters we will use the same techniques in a subsequent paper to solve the problem for multistate characters and hence for building a tree from dna sequence data the tree we build is provably close in our metric to the tree generating the data and gets closer as more observations become availablethough there have been many heuristics suggested for the problem of finding good approximations to the most likely tree our algorithm is the first one with a guaranteed convergence rate and further this rate is within a polynomial of the lowerbound rate we establish ours is also the first polynomialtime algorithm that is proven to converge at all to the correct tree"
279 "we present a primality proving algorithma probablistic primality test that" "we present a primality proving algorithma probablistic primality test that produces short certificates of primality on prime inputs we prove that the test runs in expected polynomial time for all but a vanishingly small fraction of the primes as a corollary we obtain an algorithm for generating large certified primes with distribution statistically close to uniform under the conjecture that the gap between consecutive primes is bounded by some polynomial in their size the test is shown to run in expected polynomial time for all primes yielding a las vegas primality testour test is based on a new methodology for applying group theory to the problem of prime certification and the application of this methodology using groups generated by elliptic curves over finite fieldswe note that our methodology and methods have been subsequently used and improved upon most notably in the primality proving algorithm of adleman and huang using hyperelliptic curves and in practical primality provers using elliptic curves"
280 "the pairing heap is well regarded as an efficient data" "the pairing heap is well regarded as an efficient data structure for implementing priority queue operations it is included in the gnu c library strikingly simple in design the pairing heap data structure nonetheless seems difficult to analyze belonging to the genre of selfadjusting data structures with its design originating as a selfadjusting analogue of the fibonacci heap it has been previously conjectured that the pairing heap provides constrant amortized time decreasekey operations and experimental studies have supported this conjecture this paper demonstrates contrary to conjecture that the pairing heap requires more than constant amortized time to perform decreasekey operations moreover new experimental findings are presented that reveal detectable growth in the amortized cost of the decreasekey operationsecond a unifying framework is developed that includes both pairing heaps and fibonacci heaps the parameter of interest in this framework is the storage capacity available in the nodes of the data structure for auxiliary balance information fields in this respect fibonacci heaps require log log n bits per node when n items are present this is shown to be asymptotically optimal for data structures that achieve the same asymptotic performance bounds as fibonacci heaps and fall within this framework"
281 "this paper solves a longstanding open problem in fully dynamic" "this paper solves a longstanding open problem in fully dynamic algorithms we present the first fully dynamic algorithms that maintain connectivity bipartiteness and approximate minimum spanning trees in polylogarithmic time per edge insertion or deletion the algorithms are designed using a new dynamic technique that combines a novel graph decomposition with randomization they are lasvegas type randomized algorithms which use simple data structures and have a small constant factorlet n denote the number of nodes in the graph for a sequence of ohgrm operations where m is the number of edges in the initial graph the expected time for p updates is op log n througout the paper the logarithms are based for connectivity and bipartiteness the worstcase time for one query is o log nlog log n for the kedge witness problem does the removal of k given edges disconnect the graph the expected time for p updates is op log n and the expected time for q queries is oqk log n given a graph with k different weights the minimum spanning tree can be maintained during a sequence of p updates in expected time opk log n this implies an algorithm to maintain a egrapproximation of the minimum spanning tree in expected time op log n loguegr for p updates where the weights of the edges are between and u"
282 "we introduce a search problem called mutual search where k" "we introduce a search problem called mutual search where k agents arbitrarily distributed over n sites are required to locate one another by posing queries of the form anybody at site i we ask for the least number of queries that is necessary and sufficient for the case of two agents using deterministic protocols we obtain the following worstcase results in an oblivious setting where all preplanned queries are executed there is no savings n queries are required and are sufficient in a nonoblivious setting we can exploit the paradigm of no news is also news to obtain significant savings in the synchronous case n queries are required in the asynchronous case n queries suffice and a fortiori n queries are required for on agents using a synchronous deterministic protocol less than n queries suffice there is a simple randomized protocol for two agents with worstcase expected n queries and all radomized protocols require at least n worstcase expected queries the graphtheoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest"
283 "in this paper we give a new algorithm for quantifier" "in this paper we give a new algorithm for quantifier elimination in the first order theory of real closed fields that improves the complexity of the best known algorithm for this problem till now unlike previously known algorithms basu et al renegar heintz et al the combinatorial part of the complexity the part depending on the number of polynomials in the input of this new algorithm is independent of the number of free variables moreover under the assumption that each polynomial in the input depends only on a constant number of the free variables the algebraic part of the complexitythe part depending on the degrees of the input polynomials can also be made independent of the number of free variables this new feature of our algorithm allow us to obtain a new algorithm for a variant of the quantifier elimination problem we give an almost optimal algorithm for this new problem which we call the uniform quantifier elimination problemusing tthe uniform quantifier elimination algorithm we give an algorithm for solving a problem arising in the field of constraint databases with real polynomial constraints we give an algorithm for converting a query with natural domain semantics to an equivalent one with active domain semantics a nonconstructive version of this result was proved in benedikt et al very recently a constructive proof was also given independently in benedikt and libkin however complexity issues were not considered and no algorithm with a reasonable complexity bound was known for this latter problem till nowwe also point out interesting logical consequences of this algorithmic result concerning the expressive power of a constraint query language over the reals this leads to simpler and constructive proofs for these inexpressibility results than the ones known beforemoreover our improved algorithm for performing quantifier elimination immediately leads to improved algorithms for several problems for which quantifier elimination is a basic step for example the problem of computing the closure of a given semialgebraic set"
284 "the problem of finding the circular attributes in an grammar" "the problem of finding the circular attributes in an grammar is considered two algorithms are proposed the first is polynomial but yields conservative results while the second is exact but is potentially expontial it is also shown that finding the circular attributes is harder than testing circularity"
285 "with a system of parallel coordinates objects in rn can" "with a system of parallel coordinates objects in rn can be represented with planar graphs ie planar diagrams for arbitrary n in r embedded in the projective plane parallel coordinates induce a point line duality this yields a new duality between bounded and unbounded convex sets and hstars a generalization of hyperbolas as well as a duality between convex union convex merge and intersection from these results algorithms for constructing the intersection and convex merge of convex polygons in on time and the convex hull on the plane in olog n for realtime and on log n worstcase construction where n is the total number of points are derived by virtue of the duality these algorithms also apply to polygons whose edges are a certain class of convex curves these planar constructions are studied prior to exploring generalizations to ndimensions the needed results on parallel coordinates are given first"
286 "we describe an efficient purely functional implementation of deques with" "we describe an efficient purely functional implementation of deques with catenation in addition to being an intriguing problem in its own right finding a purely functional implementation of catenable deques is required to add certain sophisticated programming constructs to functional programming languages our solution has a worstcase running time of o for each push pop inject eject and catenation the best previously known solution has an ologk time bound for the kth deque operation our solution is not only faster but simpler a key idea used in our result is an algorithmic technique related to the redundant digital representations used to avoid carry propagation in binary counting"
287 "the network structure of a hyperlinked environment can be a" "the network structure of a hyperlinked environment can be a rich source of information about the content of the environment provided we have effective means for understanding it we develop a set of algorithmic tools for extracting information from the link structures of such environments and report on experiments that demonstrate their effectiveness in a variety of context on the world wide web the central issue we address within our framework is the distillation of broad search topics through the discovery of authorative information sources on such topics we propose and test an algorithmic formulation of the notion of authority based on the relationship between a set of relevant authoritative pages and the set of hub pages that join them together in the link structure our formulation has connections to the eigenvectors of certain matrices associated with the link graph these connections in turn motivate additional heuristrics for linkbased analysis"
288 "in a timestamping system processors repeatedly choose timestamps so that" "in a timestamping system processors repeatedly choose timestamps so that the order of the timestamps obtained reflects the realtime order in which they were requested concurrent timestamping systems permit requests by multiple processors to be issued concurrently in bounded timestamping systems the sizes of the timestamps and the size and number of shared variables are bounded an algorithm is waitfree if there exists an a priori bound on the number of steps a processor must take in order to make progress independent of the action or inaction of other processors letting n denote the number of procesors we construct a simple waitfree bounded concurrent timestamping system requiring on steps accesses to shared memory for a processor to read the current timestamps and determine the order among them and on steps to generate a timestamp independent of the actions of the other processors in addition we introduce and implement the traceable use abstraction a new primitive providing inventory control over values introduced by processors in the course of an algorithm execution this abstraction has proved to be of great value in converting unbounded algorithms to bounded ones attiya and rachman dwork et al"
289 "consider the set h of all linear or affine transformations" "consider the set h of all linear or affine transformations between two vector spaces over a finite field f we study how good h is as a class of hash functions namely we consider hashing a set s of size n into a range having the same cardinality n by a randomly chosen function from h and look at the expected size of the largest hash bucket h is a universal class of hash functions for any finite field but with respect to our measure different fields behave differentlyif the finite field f has n elements then there is a bad set s f of size n with expected maximal bucket size hn if n is a perfect square then there is even a bad set with largest bucket size always at least n this is worst possible since with respect to a universal class of hash functions every set of size n has expected largest bucket size below n if however we consider the field of two elements then we get much better bounds the best previously known upper bound on the expected size of the largest bucket for this class was o log n we reduce this upper bound to olog n log logn note that this is not far from the guarantee for a random function there the average largest bucket would be thgrlog n log log nin the course of our proof we develop a tool which may be of independent interest suppose we have a subset s of a vector space d over z and consider a random linear mapping of d to a smaller vector space r if the cardinality of s is larger than cegrrlogr then with probability egr the image of s will cover all elements in the range"
290 "in this paper we prove various results about pac learning" "in this paper we prove various results about pac learning in the presence of malicious noise our main interest is the sample size behavior of learning algorithms we prove the first nontrivial sample complexity lower bound in this model by showing that order of egrdgr ddgr up to logarithmic factors examples are necessary for pac learning any target class of valued functions of vc dimension d where egr is the desired accuracy and eegr egr egr dgr the malicious noise rate it is well known that any nontrivial target class cannot be pac learned with accuracy egr and malicious noise rate eegr egr egr this irrespective to sample complexity we also show that this result cannot be significantly improved in general by presenting efficient learning algorithms for the class of all subsets of d elements and the class of unions of at most d intervals on the real line this is especialy interesting as we can also show that the popular minimum disagreement strategy needs samples of size d egrdgr hence is not optimal with respect to sample size we then discuss the use of randomized hypotheses for these the bound egr egr on the noise rate is no longer true and is replaced by egr egr in fact we present a generic algorithm using randomized hypotheses that can tolerate noise rates slightly larger than egr egr while using samples of size degr as in the noisefree case again one observes a quadratic powerlaw in this case degrdgr dgr egr egr eegr as dgr goes to zero we show upper and lower bounds of this order"
291 "this paper studies the problem of efficiently schedulling fully strict" "this paper studies the problem of efficiently schedulling fully strict ie wellstructured multithreaded computations on parallel computers a popular and practical method of scheduling this kind of dynamic mimdstyle computation is work stealing in which processors needing work steal computational threads from other processors in this paper we give the first provably good workstealing scheduler for multithreaded computations with dependenciesspecifically our analysis shows that the expected time to execute a fully strict computation on p processors using our workstealing scheduler is tp ot where t is the minimum serial execution time of the multithreaded computation and t is the minimum execution time with an infinite number of processors moreover the space required by the execution is at most sp where s is the minimum serial space requirement we also show that the expected total communication of the algorithm is at most opt ndsmax where smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent this communication bound justifies the folk wisdom that workstealing schedulers are more communication efficient than their worksharing counterparts all three of these bounds are existentially optimal to within a constant factor"
292 "we develop principles and rules for achieving secrecy properties in" "we develop principles and rules for achieving secrecy properties in security protocols our approach is based on traditional classification techniques and extends those techniques to handle concurrent processes that use sharedkey cryptography the rules have the form of typing rules for a basic concurrent language with cryptographic primitives the spi calculus they guarantee that if a protocol typechecks then it does not leak its secret inputs"
293 "the main result of this paper is a general technique" "the main result of this paper is a general technique for determining lower bounds on the communication complexity of problems on various distributed computer networks this general technique is derived by simulating the general network by a linear array and then using a lower bound on the communication complexity of the problem on the linear array applications of this technique yield optimal bounds on the communication complexity of merging ranking uniqueness and triangledetection problems on a ring of processors nontrivial nearoptimal lower bounds on the communication complexity of distinctness merging and ranking on meshes and complete binary trees are also derived"
294 "the kserver problem is a generalization of the paging problems" "the kserver problem is a generalization of the paging problems and is the most studied problem in the area of competive online problems the harmonic algorithm is a very natural and simple randomized algorithm for the kserver problem we give a simple proof that the harmonic kserver algorithm is competitive the competitive ratio we prove is the best currently known fo the algorithm the harmonic algorithm is memoryless and timeefficient this is the only such algorithm known to be competitive for the kserver problem"
295 "we identify and study a natural and frequently occurring subclass" "we identify and study a natural and frequently occurring subclass of concurrent read exclusive write parallel random access machines crewprams called concurrent read owner write or crowprams these are machines in which each global memory location is assigned a unique owner processor which is the only processor allowed to write into it considering the difficulties that would be involved in physically realizinga full crewpram model and demonstrate its stability under several definitional changes second we precisely characterize the power of the crowpram by showing that the class of languages recognizable by it in time olog n and implicity with a polynomial number of processors is exactly the class logdcfl of languages log space reducible to deterministic contextfree languages third using the same basic machinery we show that the recognition problem for deterministic contextfree languages can be solved quickly on a deterministic auxilliary pushdown automation having random access to its input tape a log n space work tape and pushdown store of small maximum height for example time on egr is achievable with pushdown height olog n these result extend and unify work of von braunmhl cook mehlhorn and verbeek klein and reif and rytter"
296 "we significantly improve known time bounds for solving the minimum" "we significantly improve known time bounds for solving the minimum cut problem on undirected graphs we use a semiduality between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques we give a randomized monte carlo algorithm that finds a minimum cut in an medge nvertex graph with high probability in om log n time we also give a simpler randomized algorithm that finds all minimum cuts with high probability in om log n time this variant has an optimal rnc parallelization both variants improve on the previous best time bound of on log n other applications of the treepacking approach are new nearly tight bounds on the number of nearminimum cuts a graph may have and a new data structure for representing them in a spaceefficient manner"
297 "existential secondorder logic eso and monadic secondorder logicmso have attracted" "existential secondorder logic eso and monadic secondorder logicmso have attracted much interest in logic and computer science eso is a much expressive logic over successor structures than mso however little was known about the relationship between msoand syntatic fragments of eso we shed light on this issue by completely characterizing this relationship for the prefix classes of eso over strings ie finite successor structures moreover we determine the complexity of model checking over strings for all esoprefix classes let eso q denote the prefix class containing all sentences of the shape rq where r is a list of predicate variables q is a firstorder predicate qualifier from the prefix set q and is quantifierfree we show that eso and eso are the maximal standard esoprefix classes contained in mso thus expressing only regular languages we further prove the following dichotomy theorem an eso prefixclass either expresses only regular languages and is thus in mso or it expresses some npcomplete languages we also give a precise characterization of those esoprefix classes that are equivalent to mso over strings and of the esoprefix classes which are closed under complementation on strings"
298 "shortest paths computations constitute one of the most fundamental network" "shortest paths computations constitute one of the most fundamental network problems nonetheless known parallel shortestpaths algorithms are generally inefficient they perform significantly more work product of time and processors than their sequential counterparts this gap known in the literature as the transitive closure bottleneck poses a longstanding open problem our main result is an omnes mne work polylogtime randomized algorithm that computes paths within o polylog n of shortest from s source nodes to all other nodes in weighted undirected networks with n nodes and m edges for any fixed egr this work bound nearly matches the odsm sequential time in contrast previous polylogtime algorithms required nearly minod nod m work even when s and previous nearlinear work algorithms required nearon time we also present faster sequential algorithms that provide good approximate distances only between distant vertices we obtain an omsnne time algorithm that computes paths of weight opolylog n dist owmax polylog n where dist is the corresponding distance and wmax is the maximum edge weight our chief instrument which is of independent interest are efficient constructions of sparse hop sets a degrhop set of a network gve is a set e of new weighted edges such that mimimumweight dedge paths in vee have weight within egr of the respective distances in g we construct hop sets of size one where egropolylog n and dopolylog n"
299 "in a linearlytyped functional language one can define functions that" "in a linearlytyped functional language one can define functions that consume their arguments in the process of computing their results this is reminiscent of state transformations in imperative languages where execition of an assignment statement alters the contents of the store we explore this connection by translating two variations on algol into a purely functional language with polymorphic linear types on the one hand the translations lead to a semantic analysis of algollike programs in terms of a model of the linear language on the other hand they demonstrate that a linearlytyped functional language can be at least as expressive as algol"
300 "we initiate a graphtheoretic study of privacy in distributed environments" "we initiate a graphtheoretic study of privacy in distributed environments with mobile eavesdroppers bugs for two privacy tasksdistributed database maintenance and message transmissiona computationally unbounded adversary plays an eavesdrpping game coordinating the moment of the bugs among the sites to learn the current memory contents many different adversaries are considered motivated by differences in eavesdropping technologies we characterize the feasibility of the two privacy tasks combinatorially construct protocols for the feasible cases and analyze their computational complexity"
301 "a crashing network protocol is an asynchronous protocol whose memory" "a crashing network protocol is an asynchronous protocol whose memory does not survive crashes we show that a crashing network protocol that works over unreliable links can be driven to arbitrary global states where each node is in a state reached in some possibly different execution and each link has an arbitrary mixture of packets sent in possibly different executions our theorem considerably generalizes an earlier result due to fekete et al which states that there is no correct crashing data link protocol for example we prove that there is no correct crashing protocol for token passing and for many other resource allocation protocols such as kexclusion and the drinking and dining philosophers problems we further characterize the reachable states caused by crash failures using reliable nonfifo and reliable fifo links we show that with reliable nonfifo links any acyclic subset of nodes and links can be driven to arbitrary states we show that with reliable fifo links only nodes can be driven to arbitrary states overall we show a strict hierarchy in terms of the set of states reachable by crash failures in the three link models"
302 "we present a deterministic algorithm that computes stconnectivity in undirected" "we present a deterministic algorithm that computes stconnectivity in undirected graphs using olog n space this improves the previous o logn bound of nisan et al"
303 "translating linear temporal logic formulas to automata has proven to" "translating linear temporal logic formulas to automata has proven to be an effective approach for implementing lineartime modelchecking and for obtaining many extensions and improvements to this verification method on the other hand for branching temporal logic automatatheoretic techniques have long been thought to introduce an exponential penalty making them essentially useless for modelchecking recently bernholtz and grumberg have shown that this exponential penalty can be avoided though they did not match the linear complexity of nonautomatatheoretic algorithms in this paper we show that alternating tree automata are the key to a comprehensive automatatheoretic framework for branching temporal logics not only can they be used to obtain optimal decision procedures as was shown by muller et al but as we show here they also make it possible to derive optimal modelchecking algorithms moreover the simple combinatorial structure that emerges from the automatatheoretic approach opens up new possibilities for the implementation of branchingtime model checking and has enabled us to derive improved space complexity bounds for this longstanding problem"
304 "completeness is an ideal although uncommon feature of abstract interpretations" "completeness is an ideal although uncommon feature of abstract interpretations formalizing the intuition that relatively to the properties encoded by the underlying abstract domains there is no loss of information accumulated in abstract computations thus complete abstract interpretations can be rightly understood as optimal we deal with both pointwise completeness involving generic semantic operations and least fixpoint completeness completeness and fixpoint completeness are shown to be properties that depend on the underlying abstract domains only our primary goal is then to solve the problem of making abstract interpretations complete by minimally extending or restricting the underlying abstract domains under the weak and reasonable hypothesis of dealing with continuous semantic operations we provide constructive characterizations for the least complete extensions and the greatest complete restrictions of abstract domains as far as fixpoint completeness is concerned for merely monotone semantic operators the greatest restrictions of abstract domains are constructively characterized while it is shown that the existence of least extensions of abstract domains cannot be in general guaranteed even under strong hypotheses these methodologies which in finite settings give rise to effective algorithms provide advanced formal tools for manipulating and comparing abstract interpretations useful both in static program analysis and in semantics design a number of examples illustrating these techniques are given"
305 "multivariate resultants generalize the sylvester resultant of two polynomials and" "multivariate resultants generalize the sylvester resultant of two polynomials and characterize the solvability of a polynomial system they also reduce the computation of all common roots to a problem in linear algebra we propose a determinantal formula for the sparse resultant of an arbitrary system of n polynomials in n variables this resultant generalizes the classical one and has significantly lower degree for polynomials that are sparse in the sense that their mixed volume is lower than their bzout number our algorithm uses a mixed polyhedral subdivision of the minkowski sum of the newton polytopes in order to construct a newton matrix its determinant is a nonzero multiple of the sparse resultant and the latter equals the gcd of at most n such determinants this construction implies a restricted version of an effective sparse nullstellensatz for an arbitrary specialization of the coefficients there are two methods that use one extra variable and yield the sparse resultant this is the first algorithm to handle the general case with complexity polynomial in the resultant degree and simply exponential in n we conjecture its extension to producing an exact rational expression for the sparse resultant"
306 "the need for computationally efficient decisionmaking techniques together with the" "the need for computationally efficient decisionmaking techniques together with the desire to simplify the processes of knowledge acquisition and agent specification have led various researchers in artificial intelligence to examine qualitative decision tools however the adequacy of such tools is not clear this paper investigates the foundations of maximin minmax regret and competitive ratio three central qualitative decision criteria by characterizing those behaviors that could result from their use this characterizaton provides two important insights under what conditions can we employ an agent model based on these basic qualitative decision criteria and how rational are these decision procedures for the competitive ratio criterion in particular this latter issue is of central importance to our understanding of current work on online algorithms our main result is a constructive representation theorem that uses two choice axioms to characterize maximin minmax regret and competitive ratio"
307 "classically several properties and relations of words such as being" "classically several properties and relations of words such as being a power of the same word can be expressed by using word equations this paper is devoted to a general study of the expressive power of word equations as main results we prove theorems which allow us to show that certain properties of words are not expressible as components of solutions of word equations in particular the primitiveness and the equal length are such properties as well as being any word over a proper subalphabet"
308 "we study the learnability of multiplicity automata in angluins exact" "we study the learnability of multiplicity automata in angluins exact learning model and we investigate its applications our starting point is a known theorem from automata theory relating the number of states in a minimal multiplicity automaton for a function to the rank of its hankel matrix with this theorem in hand we present a new simple algorithm for learning multiplicity automata with improved time and query complexity and we prove the learnability of various concept classes these include among others the class of disjoint dnf and more generally satisfyo dnf the class of polynomials over finite fields the class of boundeddegree polynomials over infinite fields the class of xor of terms certain classes of boxes in high dimensions in addition we obtain the best query complexity for several classes known to be learnable by other methods such as decision trees and polynomials over gf while multiplicity automata are shown to be useful to prove the learnability of some subclasses of dnf formulae and various other classes we study the limitations of this method we prove that this method cannot be used to resolve the learnability of some other open problems such as the learnability of general dnf formulas or even kterm dnf for k ohgrlog n or satisfys dnf formulas for s ohgr these results are proven by exhibiting functions in the above classes that require multiplicity automata with superpolynomial number of states"
309 "we investigate parametric polymorphism in messagebased concurrent programming focusing on" "we investigate parametric polymorphism in messagebased concurrent programming focusing on behavioral equivalences in a typed process calculus analogous to the polymorphic lambdacalculus of girard and reynolds polymorphism constrains the power of observers by preventing them from directly manipulating data values whose types are abstract leading to notions of equivalence much coarser than the standard untyped ones we study the nature of these constraints through simple examples of concurrent abstract data types and develop basic theoretical machinery for establishing bisimilarity of polymorphic processes we also observe some surprising interactions between polymorphism and aliasing drawing examples from both the polymorphic picalculus and ml"
310 "we rework parts of the classical relational theory when the" "we rework parts of the classical relational theory when the underlying domain is a structure with some interpreted operations that can be used in queries we identify parts of the classical theory that go through as before when interpreted structure is present parts that go through only for classes of nicely behaved structures and parts that only arise in the interpreted case the first category include a number of results on language equivalence and expressive power characterizations for the activedomain semantics for a variety of logics under this semantics quantifiers range over elements of a relational database the main kind of results we prove here are generic collapse results for generic queries adding operations beyond order does not give us extra power the second category includes results on the natural semantics under which quantifiers range over the entire interpreted structure we prove for a variety of structures naturalactive collapse results showing that using unrestricted quantification does not give us any extra power moreover for a variety of structures including the real field we give a set of algorithms for eliminating unbounded quantifications in favor of bounded ones furthermore we extend these collapse results to a new class of higherorder logics that mix unbounded and bounded quantification we give a set of normal forms for these logics under special conditions on the interpreted structures as a byproduct we obtain an elementary proof of the fact that parity test is not definable in the relational calculus with polynomial inequality constraints we also give examples of structures with nice modeltheoretic properties over which the naturalactive collapse fails"
311 "we present a novel divideandconquer paradigm for approximating nphard graph" "we present a novel divideandconquer paradigm for approximating nphard graph optimization problems the paradigm models graph optimization problems that satisfy two properties first a divideandconquer approach is applicable second a fractional spreading metric is computable in polynomial time the spreading metric assigns lengths to either edges or vertices of the input graph such that all subgraphs for which the optimization problem is nontrivial have large diameters in addition the spreading metric provides a lower bound t on the cost of solving the optimization problem we present a polynomial time approximation algorithm for problems modeled by our paradigm whose approximation factor is omin log t log log t log k log log k where k denotes the number of interesting vertices in the problem instance and is at most the number of vertices we present seven problems that can be formulated to fit the paradigm for all these problems our algorithm improves previous results the problems are linear arrangement embedding a graph in a ddimensional mesh interval graph completion minimizing storagetime product subset feedback sets in directed graphs and multicuts in circular networks symmetric multicuts in directed networks balanced partitions and pseparators for small values of p in directed graphs"
312 "we introduce resource augmentation as a method for analyzing online" "we introduce resource augmentation as a method for analyzing online scheduling problems in resource augmentation analysis the online scheduler is given more resources say faster processors or more processors than the adversary we apply this analysis to two wellknown online scheduling problems the classic uniprocessor cpu scheduling problem ri pmtnsgr fi and the besteffort firm realtime scheduling problem ri pmtn sgr wi ui it is known that there are no constant competitive nonclairvoyant online algorithms for these problems we show that there are simple online scheduling algorithms for these problems that are constant competitive if the online scheduler is equipped with a slightly faster processor than the adversary thus a moderate increase in processor speed effectively gives the online scheduler the power of clairvoyance furthermore the online scheduler can be constant competitive on all inputs that are not closely correlated with processor speed we also show that the performance of an online scheduler is besteffort real time scheduling can be significantly improved if the system is designed in such a way that the laxity of every job is proportional to its length"
313 "controlled stochastic systems occur in science engineering manufacturing social sciences" "controlled stochastic systems occur in science engineering manufacturing social sciences and many other cntexts if the systems is modeled as a markov decision process mdp and will run ad infinitum the optimal control policy can be computed in polynomial time using linear programming the problems considered here assume that the time that the process will run is finite and based on the size of the input there are mny factors that compound the complexity of computing the optimal policy for instance there are many factors that compound the complexity of this computation for instance if the controller does not have complete information about the state of the system or if the system is represented in some very succint manner the optimal policy is provably not computable in time polynomial in the size of the input we analyze the computational complexity of evaluating policies and of determining whether a sufficiently good policy exists for a mdp based on a number of confounding factors including the observability of the system state the succinctness of the representation the type of policy even the number of actions relative to the number of states in almost every case we show that the decision problem is complete for some known complexity class some of these results are familiar from work by papadimitriou and tsitsiklis and others but some such as our plcompleteness proofs are surprising we include proofs of completeness for natural problems in the as yet littlestudied classes nppp"
314 "motivated by a growing need to understand the computational potential" "motivated by a growing need to understand the computational potential of quantum devices we suggest an approach to the relevant issues via quantum logic and its model theory by isolating such notions as quantum parallelism and interference within a modeltheoretic setting quite divorced from their customary physical trappings we seek to lay bare their logical underpinnings and possible computational ramifications in the first part of the paper a brief account of the relevant model theory is given and some new results are derived in the second part we model the simplest classical gate namely the ngate propose a quantization scheme which translates between classical and quantum models and from which emerges a logical interpretation of the notion of quantum parallelism and apply it to the classical ngate model a class of physical instantiations of the resulting quantum ngate model is also briefly discussed"
315 "the objective pursued in this paper is twofold the first" "the objective pursued in this paper is twofold the first part addresses the following combinatorial problem is it possible to construct an infinite sequence over n letters where each letter is distributed as evenly as possible and appears with a given rate the second objective of the paper is to use this construction in the framework of optimal routing in queuing networks we show under rather general assumptions that the optimal deterministic routing in stochastic event graphs is such a sequence"
316 "the narrowing relation over terms constitutes the basis of the" "the narrowing relation over terms constitutes the basis of the most important operational semantics of languages that integrate functional and logic programming paradigms it also plays an important role in the definition of some algorithms of unification modulo equational theories that are defined by confluent term rewriting systems due to the inefficiency of simple narrowing many refined narrowing strategies have been proposed in the last decade this paper presents a new narrowing strategy that is optimal in several respects for this purpose we propose a notion of a needed narrowing step that for inductively sequential rewrite systems extends the huet and lvy notion of a needed reduction step we define a strategy based on this notion that computes only needed narrowing steps our strategy is sound and complete for a large class of rewrite systems is optimal with respect to the cost measure that counts the number of distinct steps of a derivation computes only incomparable and disjoint unifiers and is efficiently implemented by unification"
317 "many combinatorial search problems can be expressed as constraint satisfaction" "many combinatorial search problems can be expressed as constraint satisfaction problems this class of problems is known to be nphard in general but a number of restricted constraint classes have been identified which ensure tractability this paper presents the first general results on combining tractable constraint classes to obtain larger more general tractable classes we give examples to show that many known examples of tractable constraint classes from a wide variety of different contexts can be constructed from simpler tractable classes using a general method we also construct several new tractable classes that have not previously been identified"
318 "an architecture is described for designing systems that acquire and" "an architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized or socalled commonsense knowledge its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases the architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases the main claims are that i the basic learning and deduction tasks are provably tractable and ii tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed among the issues that learning offers to resolve are robustness to inconsistencies robustness to incomplete information and resolving among alternatives attributeefficient learning algorithms which allow learning from few examples in large dimensional systems are fundamental to the approach underpinning the overall architecture is a new principled approach to manipulating relations in learning systems this approach of independently quantified arguments allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion"
319 "a silver is a tetrahedon whose four vertices lie close" "a silver is a tetrahedon whose four vertices lie close to a plane and whose orthogonal projection to that plane is a convex quadrilateral with no short edge silvers are notoriously common in dimensional delaunay triangulations even for wellspaced point sets we show that if the delaunay triangulation has the ratio property introduced in miller et al then there is an assignment of weights so the weighted delaunay traingulation contains no silvers we also give an algorithm to compute such a weight assignment"
320 "we demonstrate an ohgrpnp lower bound on the averagecase running" "we demonstrate an ohgrpnp lower bound on the averagecase running time uniform distribution of ppass shellsort this is the first nontrivial general lower bound for averagecase shellsort"
321 "we prove tight bounds on the time needed to solve" "we prove tight bounds on the time needed to solve kset agreement in this problem each processor starts with an arbitrary input value taken from a fixed set and halts after choosing an output value in every execution at most k distinct output values may be chosen and every processors output value must be some processors input value we analyze this problem in a synchronous messagepassing model where processors fail by crashing we prove a lower bound of fk degree of coordination required and the number of faults tolerated even in idealized models like the synchronous model the proof of this result is interesting because it is the first to apply topological techniques to the synchronous model"
322 "we consider comparator networks m that are used repeatedly while" "we consider comparator networks m that are used repeatedly while the output produced by m is not sorted it is fed again into m sorting algorithms working in this way are called periodic the number of parallel steps performed during a single run of m is called its period the sorting time of m is the total number of parallel steps that are necessary to sort in the worst case periodic sorting networks have the advantage that they need little hardware control logic wiring area and that they are adaptive we are interested in comparator networks of a constant period due to their potential applications in hardware design previously very little was known on such networks the fastest solutions required time on egr where the depth was roughly egr we introduce a general method called periodification scheme that converts automatically an arbitrary sorting network that sorts n items in time tn and that has layout area a n into a sorting network that has period sorts n tn items in time otn log n and has layout area oan tn in particular applying this scheme to batchers algorithms we get practical period comparator networks that sort in time ologn for theoretical interest one may use the aks netork resulting in a period comparator network with runtime ologn"
323 "we study integrated prefetching and caching problems following the work" "we study integrated prefetching and caching problems following the work of cao et al and kimbrel and karlin cao et al and kimbrel and karlin gave approximation algorithms for minimizing the total elapsed time in single and parallel disk settings the total elapsed time is the sum of the processor stall times and the length of the request sequence to be servedwe show that an optimum prefetchingcaching schedule for a single disk problem can be computed in polynomial time thereby settling an open question by kimbrel and karlin for the parallel disk problem we give an approximation algorithm for minimizing stall time the solution uses a few extra memory blocks in cache stall time is an important and harder to approximate measure for this problem all of our algorithms are based on a new approach which involves formulating the prefetchingcaching problems as linear programs"
324 "the suffix tree of a string is the fundamental data" "the suffix tree of a string is the fundamental data structure of combinatorial pattern matching we present a recursive technique for building suffix trees that yields optimal algorithms in different computational models sorting is an inherent bottleneck in building suffix trees and our algorithms match the sorting lower bound specifically we present the following results weiner who introduced the data structure gave an optimal ntime algorithm for building the suffix tree of an ncharacter string drawn from a constantsize alphabet in the comparison model there is a trivial ogrn log ntime lower bound based on sorting and weiners algorithm matches this bound for integer alphabets the fastest known algorithm is the on log ntime comparisonbased algorithm but no superlinear lower bound is known closing this gap is the main open question in stringology we settle this open problem by giving a linear time reduction to sorting for building suffix trees since sorting is a lowerbound for building suffix trees this algorithm is timeoptimal in every alphabet mode in particular for an alphabet consisting of integers in a polynomial range we get the first known lineartime algorithm all previously known algorithms for building suffix trees exhibit a marked absence of locality of reference and thus they tend to elicit many page faults ios when indexing very long strings they are therefore unsuitable for building suffix trees in secondary storage devices where ios dominate the overall computational cost we give a lineario reduction to sorting for suffix tree construction since sorting is a trivial iolower bound for building suffix trees our algorithm is iooptimal"
325 "a simple variant of a priority queue called a soft" "a simple variant of a priority queue called a soft heap is introduced the data structure supports the usual operations insert delete meld and findmin its novelty is to beat the logarithmic bound on the complexity of a heap in a comparisonbased model to break this informationtheoretic barrier the entropy of the data structure is reduced by artifically raising the values of certain keys given any mixed sequence of n operations a soft heap with error rate egr for any egr ensures that at any time at most egrn of its items have their keys raised the amortized complexity of each operation is constant except for insert which takes log egrtime the soft heap is optimal for any value of egr in a comparisonbased model the data structure is purely pointerbased no arrays are move items across the data structure not individually as is customary but in groups in a datastructuring equivalent of car pooling keys must be raised as a result in order to preserve the heap ordering of the data structure the soft heap can be used to compute exact or approximate medians and percentiles optimally it is also useful for approximate sorting and for computing minimum spanning trees of general graphs"
326 "a deterministic algorithm for computing a minimum spanning tree of" "a deterministic algorithm for computing a minimum spanning tree of a connected graph is presented its running time is m agrm n where agr is the classical functional inverse of ackermanns function and n respectively m is the number of vertices respectively edges the algorithm is comparisonbased it uses pointers not arrays and it makes no numeric assumptions on the edge costs"
327 "we study contention resolution in a multipleaccess channel such as" "we study contention resolution in a multipleaccess channel such as the ethernet channel in the model that we consider n users generate messages for the channel according to a probability distribution raghavan and upfal have given a protocol in which the expected delay time to get serviced of every message is olog n when messages are generated according to a bernoulli distribution with generation rate up to about our main results are the following protocols a one in which the expected average message delay is o when messages are generated according to a bernoulli distribution with a generation rate smaller than e and b one in which the expected delay of any message is o for an analogous model in which users are synchronized ie they agree about the time there are potentially an infinite number of users and messages are generated according to a poisson distribution with generation rate up to e each message constitutes a new userto achieve a we first show how to simulate b using n synchronized users and then show how to build the synchronization into the protocol"
328 "we give a data structure that allows arbitrary insertions and" "we give a data structure that allows arbitrary insertions and deletions on a planar point set p and supports basic queries on the convex hull of p such as membership and tangentfinding updates take ologegrn amori tzed time and queries take o log n time each where n is the maximum size of p and egr is any fixed positive constant for some advanced queries such as bridgefinding both our bounds increase to ologn the only previous fully dynamic solution was by overmars and van leeuwen from and required ologn time per update and olog n time per query"
329 "we consider packet routing when packets are injected continuously into" "we consider packet routing when packets are injected continuously into a network we develop an adversarial theory of queuing aimed at addressing some of the restrictions inherent in probabilistic analysis and queuing theory based on timeinvariant stochastic generation we examine the stability of queuing networks and policies when the arrival process is adversarial and provide some preliminary results in this direction our approach sheds light on various queuing policies in simple networks and paves the way for a systematic study of queuing with few or no probabilistic assumptions"
330 "in this paper we analyze the behavior of packetswitched communication" "in this paper we analyze the behavior of packetswitched communication networks in which packets arrive dynamically at the nodes and are routed in discrete time steps across the edges we focus on a basic adversarial model of packet arrival and path determination for which the timeaveraged arrival rate of packets requiring the use of any edge is limited to be less than this model can reflect the behavior of connectionoriented networks with transient connections such as atm networks as well as connectionless networks such as the internet we concentrate on greedy also known as workconserving contentionresolution protocols a crucial issue that arises in such a setting is that of stabilitywill the number of packets in the system remain bounded as the system runs for an arbitrarily long period of time we study the universal stability of network ie stability under all greedy protocols and universal stability of protocols ie stability in all networks once the stability of a system is granted we focus on the two main parameters that characterize its performance maximum queue size required and maximum endtoend delay experienced by any packet among other things we show i there exist simple greedy protocols that are stable for all networks ii there exist other commonly used protocols such as fifo and networks such as arrays and hypercubes that are not stable iii the nnode ring is stable for all greedy routing protocols with maximum queuesize and packet delay that is linear in n iv there exists a simple distributed randomized greedy protocol that is stable for all networks and requires only polynomial queue size and polynomial delay our results resolve several questions posed by borodin et al and provide the first examples of i a protocol that is stable for all networks and ii a protocol that is not stable for all networks"
331 "we define order locality to be a property of clauses" "we define order locality to be a property of clauses relative to a term ordering this property generalizes the subformula property for proofs where the terms appearing in proofs can be bounded under the given ordering by terms appearing in the goal clause we show that when a clause set is order local then the complexity of its ground entailment problem is a function of its structure eg full versus horn clauses and the ordering used we prove that in many cases order locality is equivalent to a clause set being saturated under ordered resolution this provides a means of using standard resolution theorem provers for testing order locality and transforming nonlocal clause sets into local ones we have used the saturate system to automatically establish complexity bounds for a number of nontrival entailment problems relative to complexity classes which include polynomial and exponential time and conp"
332 "in the context of meshlike parallel processing computers for i" "in the context of meshlike parallel processing computers for i approximating continuous space and ii analog simulation of the motion of objects and waves in continuous space the present paper is concerned with which meshlike interconnection of processors might be particularly suitable for the task and why processor interconnection schemes based on nearest neighbor connections in geometric lattices are presented along with motivation then two major threads are exploded regarding which lattices would be good the regular lattices for their symmetry and other properties in common with continuous space and the wellknown root lattices for being in a sense the lattices required for physically natural basic algorithms for motion the main theorem of the present paper implies that thewellknown lattice an is the regular lattice having the maximum number of nearest neighbors among the ndimensional regular lattices it is noted that the only ndimensional lattices that are both regular and root are an and zn zn is the lattice of ncubes the remainder of the paper specifies other desirable properties of an including other ways it is superior to zn for our purposes"
333 "the widthof a resolution proof is defined to be the" "the widthof a resolution proof is defined to be the maximal number of literals in any clause of the proof in this paper we relate proof width to proof length size in both general resolution and its treelike variant the following consequences of these relations reveal width as a crucial resource of resolution proofs in one direction the relations allow us to give simple unified proofs for almost all known exponential lower bounds on size of resolution proofs as well as several interesting new ones they all follow from width lower bounds and we show how these follow from natural expansion property of clauses of the input tautology in the other direction the widthsize relations naturally suggest a simple dynamic programming procedure for automated theorem provingone which simply searches for small width proofs this relation guarantees that the runnuing time and thus the size of the produced proof is at most quasipolynomial in the smallest treelike proof this algorithm is never much worse than any of the recursive automated provers such as dll used in practice in contrast we present a family of tautologies on which it is exponentially faster"
334 "this paper presents new theorems to analyze divideandconquer recurrences which" "this paper presents new theorems to analyze divideandconquer recurrences which improve other similar ones in several aspects in particular these theorems provide more information free us almost completely from technicalities like floors and ceilings and cover a wider set of toll functions and weight distributions stochastic recurrences included"
335 "we consider the problem of scheduling unrelated parallel machines subject" "we consider the problem of scheduling unrelated parallel machines subject to release dates so as to minimize the total weighted completion time of jobs the main contribution of this paper is a provably good convex quadratic programming relaxation of strongly polynomial size for this problem the best previously known approximation algorithms are based on lp relaxations in time or intervalindexed variables those lp relaxations however suffer from a huge number of variables as a result of the convex quadratic programming approach we can give a very simple and easy to analyze approximation algorithm which can be further improved to performance guarantee in the absence of release dates we also consider preemptive scheduling problems and derive approximation algorithms and results on the power of preemption which improve upon the best previously known results for these settings finally for the special case of two machines we introduce a more sophisticated semidefinite programming relaxation and apply the random hyperplane technique introduced by goemans and williamson for the maxcut problem this leads to an improved approximation"
336 "we study an online problem that is motivated by the" "we study an online problem that is motivated by the networking problem of dynamically adjusting of acknowledgments in the transmission control protocol tcp we provide a theoretical model for this problem in which the goal is to send acks at a time that minimize a linear combination of the cost for the number of acknowledgments sent and the cost for the additional latency introduced by delaying acknowledgments to study the usefulness of applying packet arrival time prediction to this problem we assume there is an oracle that provides the algorithm with the times of the next l arrivals for some l we give two different objective functions for measuring the cost of a solution each with its own measure of latency cost for each objective function we first give an ontime dynamic programming algorithm for optimally solving the offline problem then we describe an online algorithm that greedily acknowledges exactly when the cost for an acknowledgment is less than the latency cost incurred by not acknowledging we show that for this algorithm there is a sequence of n packet arrivals for which it is ohgr competitive for the first objective function competitive for the second function for l and competitivefor the second function for l next we present a second online algorithm which is a slight modification of the first and we prove that it is competitive for both objective functions for all l we also give lower bounds on the competitive ratio for any deterministic online algorithm these results show that for each objective function at least one of our algorithms is optimal finally we give some initial empirical results using arrival sequences from real network traffic where we compare the two methods used in tcp for acknowledgment delay with our two online algorithms in all cases we examine performance with l and l"
337 "we present approximation algorithms for the metric uncapacitated facility location" "we present approximation algorithms for the metric uncapacitated facility location problem and the metric kmedian problem achieving guarantees of and respectively the distinguishing feature of our algorithms is their low running time om logm and om logml log n respectively where n and m are the total number of vertices and edges in the underlying complete bipartite graph on cities and facilities the main algorithmic ideas are a new extension of the primaldual schema and the use of lagrangian relaxation to derive approximation algorithms"
338 "this paper resolves a longstanding open problem on whether the" "this paper resolves a longstanding open problem on whether the concurrent write capability of parallel random access machine pram is essential for solving fundamental graph problems like connected components and minimum spanning trees in ologn time specifically we present a new algorithm to solve these problems in ologn time using a linear number of processors on the exclusiveread exclusivewrite pram the logarithmic time bound is actually optimal since it is well known that even computing the or of nbit requires ohgrlog n time on the exclusivewrite pram the efficiency achieved by the new algorithm is based on a new schedule which can exploit a high degree of parallelism"
339 "we prove a sufficient condition for the stability of dynamic" "we prove a sufficient condition for the stability of dynamic packet routing algorithms our approach reduces the problem of steady state analysis to the easier and better understood question of static routing we show that certain high probability and worst case bounds on the quasistatic finite past performance of a routing algorithm imply bounds on the performance of the dynamic version of that algorithm our technique is particularly useful in analyzing routing on networks with bounded buffers where complicated dependices make standard queuing techniques inapplicable we present several applications of our approach in all cases we start from a known static algorithm and modify it to fit our framework in particular we give the first dynamic algorithms for routing on a butterfly or twodimensional mesh with bounded buffers both the injection rate for which the algorithm is stable and the expected time a packet spends in the system are optimal up to constant factors our approach is also applicable to the recently introduced adversarial input model"
341 "the burrowswheeler transform also known as blocksorting is at the" "the burrowswheeler transform also known as blocksorting is at the base of compression algorithms that are the state of the art in lossless data compression in this paper we analyze two algorithms that use this technique the first one is the original algorithm described by burrows and wheeler which despite its simplicity outperforms the gzip compressor the second one uses an additional runlength encoding step to improve compression we prove that the compression ratio of both algorithms can be bounded in terms of the kth order empirical entropy of the input string for any k we make no assumptions on the input and we obtain bounds which hold in the worst case that is for every possible input string all previous results for blocksorting algorithms were concerned with the average compression ratio and have been established assuming that the input comes from a finiteorder markov source"
343 "we show that a type system based on the intuitionistic" "we show that a type system based on the intuitionistic modal logic s provides an expressive framework for specifying and analyzing computation stages in the context of typed lgrcalculi and functional languages we directly demonstrate the sense in which our lsquaree calculus captures staging and also give a conservative embeddng of nielson and nielsons twolevel functional language in our functional language miniml square thus proving that bindingtime correctness is equivalent to modal correctness on this fragment in addition minimlsquare can also express immediate evaluation and sharing of code across multiple stages thus supporting runtime code generation as well as partial evaluation"
344 "this paper presents a new approach to the analysis of" "this paper presents a new approach to the analysis of hashing with linear probing for nonuniformly distributed hashed keys the use of urn models is avoided instead some facts about empirical processes which are well known in statistics are used in particular an asymptotic formula for the expected probe length for both a successful and an unsuccessful search is obtained the accuracy of the approximation is confirmed by simulation"
345 "parallel communication algorithms and networks are central to largescale parallel" "parallel communication algorithms and networks are central to largescale parallel computing and also data communications this paper identifies adverse sourcedestination traffic patterns and proposes a scheme for obtaining relief by means of randomized routing of packets on simple extensions of the wellknown omega networks valiant and aleliunas have demonstrated randomized algorithms for a certain context which we call nonrenewal that complete the communication task in time olog n with overwhelming probability where n is the number of sources and destinations our scheme has advantages because it uses switches of fixed degree requires no scheduling and for the nonrenewal context is as good in proven performance the main advantage of our scheme comes when we consider the renewal context in which packets are generated at the sources continually and asynchronously our algorithm extends naturally from the nonrenewal context in the analysis in the renewal context we first explicitly identify the maximum traffic intensities in the internal links of the extended omega networks over all sourcedestination traffic specifications that satisfy loose bounds second the benefits of randomization on the stability of the network are identified third exact results for certain restricted models for sources and transmission and approximate analytic results for quite general models are derived for the mean delays these results show that in the stable regime the maximum mean time from source to destination is asymptotically proportional to log n numerical results are presented"
346 "a new onepass algorithm for constructing dynamic huffman codes is" "a new onepass algorithm for constructing dynamic huffman codes is introduced and analyzed we also analyze the onepass algorithm due to faller gallager and knuth in each algorithm both the sender and the receiver maintain equivalent dynamically varying huffman trees and the coding is done in real time we show that the number of bits used by the new algorithm to encode a message containing t letters is t bits more than that used by the conventional twopass huffman scheme independent of the alphabet size this is best possible in the worst case for any onepass huffman method tight upper and lower bounds are derived empirical tests show that the encodings produced by the new algorithm are shorter than those of the other onepass algorithm and except for long messages are shorter than those of the twopass method the new algorithm is well suited for online encodingdecoding in data networks and for tile compression"
347 "an orthogonal query that asks to aggregate the set of" "an orthogonal query that asks to aggregate the set of records in kdimensional box regions is studied and it is shown that space onlog nlog log nk makes possible a combined time complexity ologkn for retrievals insertions and deletions"
348 "byzantine generals protocols enable processes to broadcast messages reliably in" "byzantine generals protocols enable processes to broadcast messages reliably in the presence of faulty processes these protocols are run in a system that consists of n processes t of which are faulty the protocols are conducted in synchronous rounds of message exchange it is shown that in the absence of eavesdropping without using cryptography for any egr and t n egr there is a randomized protocol with olog n expected number of rounds if cryptographic methods are allowed then for egr and t n egr there is a randomized protocol with olog n expected number of rounds this is an improvement on the lower bound of t rounds required for deterministic protocols and on a previous result of tlog n expected number of rounds for randomized noncryptographic protocols"
349 "t parsons originally proposed and studied the following pursuitevasion problem" "t parsons originally proposed and studied the following pursuitevasion problem on graphs members of a team of searchers traverse the edges of a graph g in pursuit of a fugitive who moves along the edges of the graph with complete knowledge of the locations of the pursuers what is the smallest number sg of searchers that will suffice for guaranteeing capture of the fugitive it is shown that determining whether sg k for a given integer k is npcomplete for general graphs but can be solved in linear time for trees we also provide a structural characterization of those graphs g with sg k for k"
350 "in this paper a new method is presented for i" "in this paper a new method is presented for i determining an optimal retry policy and ii using retry for fault characterization which is defined as classification of the fault type and determination of fault durations first an optimal retry policy is derived for a given fault characteristic which determines the maximum allowable retry durations so as to minimize the total task completion time then the combined fault characterization and retry decision in which the characteristic of a fault is estimated simultaneously with the determination of the optimal retry policy are carried out two solution approaches are developed one is based on point estimation and the other on bayes sequential decision analysis numerical examples are presented in which all the durations associated with faults ie active benign and interfailure durations have monotone hazard rate functions eg exponential weibull and gamma distributions these are standard distributions commonly used for modeling and analyses of faults"
351 "a large class of relational database update transactions is investigated" "a large class of relational database update transactions is investigated with respect to equivalence and optimization the transactions are straightline programs with inserts deletes and modifications using simple selection conditions several basic results are obtained it is shown that transaction equivalence can be decided in polynomial time a number of optimality criteria for transactions are then proposed as well as two normal forms polynomialtime algorithms for transaction optimization and normalization are exhibited also an intuitively appealing system of axioms for proving transaction equivalence is introduced finally a simple natural subclass of transactions called strongly acyclic is shown to have particularly desirable properties"
352 "reliable concurrent processing of transactions in a database system is" "reliable concurrent processing of transactions in a database system is examined since serializability the conventional concurrency control correctness criterion is not adequate in the presence of common failures another theory of correctness is proposed involving the concepts of commit serializability recoverability and resiliency"
353 "conjunctive queries are generalized so that inequality comparisons can be" "conjunctive queries are generalized so that inequality comparisons can be made between elements of the query algorithms for containment and equivalence of such inequality queries are given under the assumption that the data domains are dense and totally ordered in general containment does not imply the existence of homomorphisms containment mappings but the homomorphism property does exist for subclasses of inequality queries a minimization algorithm is defined using the equivalence algorithm it is first shown that the constants appearing in a query can be divided into essential and nonessential subgroups the minimum query can be nondeterministically guessed using only the essential constants of the original query"
354 "the following problem is studied how and to what extent" "the following problem is studied how and to what extent can the retrieval speed of external hashing be improved by storing a small amount of extra information in internal storage several algorithms that guarantee retrieval in one access are developed and analyzed in the first part of the paper a restricted class of algorithms is studied and a lower bound on the amount of extra storage is derived an algorithm that achieves this bound up to a constant difference is also given in the second part of the paper a number of restrictions are relaxed and several more practical algorithms are developed and analyzed the last one in particular is very simple and efficient allowing retrieval in one access using only a fixed number of bits of extra internal storage per bucket the amount of extra internal storage depends on several factors but it is typically very small only a fraction of a bit per record stored the cost of inserting a record is also analyzed and found to be low taking all factors into account this algorithm is highly competitive for applications requiring very fast retrieval"
355 "the first part of the paper shows that previous theoretical" "the first part of the paper shows that previous theoretical work on the semantics of probabilistic programs kozen and on the correctness of performance annotated programs ramshaw can be used to automate the averagecase analysis of simple programs containing assignments conditionals and loops a performance compiler has been developed using this theoretical foundation the compiler is described and it is shown that special cases of symbolic simplifications of formulas play a major role in rendering the system usable the performance compiler generates a system of recurrence equations derived from a given program whose efficiency one wishes to analyze this generation is always possible but the problem of solving the resulting equations may be complex the second part of the paper presents an original method that generalizes the previous approach and is applicable to functional programs that make use of recursion and complex data structures several examples are presented including an analysis of binary tree sort a key feature of the analysis of such programs is that distributions on complex data structures are represented using attributed probabilistic grammars"
356 "the concept of partial synchrony in a distributed system is" "the concept of partial synchrony in a distributed system is introduced partial synchrony lies between the cases of a synchronous system and an asynchronous system in a synchronous system there is a known fixed upper bound dgr on the time required for a message to be sent from one processor to another and a known fixed upper bound phgr on the relative speeds of different processors in an asynchronous system no fixed upper bounds dgr and phgr exist in one version of partial synchrony fixed bounds dgr and phgr exist but they are not known a priori the problem is to design protocols that work correctly in the partially synchronous system regardless of the actual values of the bounds dgr and phgr in another version of partial synchrony the bounds are known but are only guaranteed to hold starting at some unknown time t and protocols must be designed to work correctly regardless of when time t occurs faulttolerant consensus protocols are given for various cases of partial synchrony and various fault models lower bounds that show in most cases that our protocols are optimal with respect to the number of faults tolerated are also given our consensus protocols for partially synchronous processors use new protocols for faulttolerant distributed clocks that allow partially synchronous processors to reach some approximately common notion of time"
357 "since about much research has been done on thue systems" "since about much research has been done on thue systems that have properties that ensure viable and efficient computation the strongest of these is the churchrosser property which states that two equivalent strings can each be brought to a unique canonical form by a sequence of lengthreducing rules in this paper three ways in which formal languages can be defined by thue systems with this property are studied and some general results about the three families of languages so determined are studied"
358 "considered is the question of whether topdown prologlike evaluation of" "considered is the question of whether topdown prologlike evaluation of a set of logical rules can be guaranteed to terminate the nail system is designed to process programs consisting of logical rules and to select for each fragment of the program the best from among many possible strategies for its evaluation in the context of such a system it is essential that termination tests be fast thus the uniqueness property of logical rules is introduced this property is satisfied by many of the common examples of rules and is easily recognized for rules with this property a set of inequalities whose satisfaction is sufficient for termination of the rules can be generated in polynomial time then a polynomial test for satisfaction of constraints generated by this process is given"
359 "the protection state of a system is defined by the" "the protection state of a system is defined by the privileges possessed by subjects at a given moment operations that change this state are themselves authorized by the current state this poses a design problem in constructing the initial state so that all derivable states conform to a particular policy it also raises an analysis problem of characterizing the protection states derivable from a given initial state a protection model provides a framework for both design and analysis design generality and tractable analysis are inherently conflicting goals analysis is particularly difficult if creation of subjects is permitted the schematic protection model resolves this conflict by classifying subjects and objects into protection types the privileges possessed by a subject consist of a typedetermined part specified by a static protection scheme and a dynamic part consisting of tickets capabilities it is shown that analysis is tractable for this model provided certain restrictions are imposed on subject creation a scheme authorizes creation of subjects via a binary relation on subject types our principal constraint is that this relation be acyclic excepting loops that authorize a subject to create subjects of its own type our assumptions admit a variety of useful systems"
360 "two mathematical models dealing with optimal placement of directories on" "two mathematical models dealing with optimal placement of directories on disk devices are analyzed storage addresses on the disk are approximated by points in the interval requests for information on the disk are represented by a sequence of file names to process a request a readwrite head is first moved to a directory kept on the disk that specifies the address of the file and then a head is moved to the specified address the addresses are assumed to be independent and uniform on in the first model we consider a system of two heads separated by a fixed distance d and a directory situated at x in the second model we consider a system consisting of one head and n directories at x x xn for both models we study the problem of finding those values of the parameters that minimize the expected head motion to process a request in statistical equilibrium"
361 "the churchrosser theorem is a celebrated metamathematical result on the" "the churchrosser theorem is a celebrated metamathematical result on the lambda calculus we describe a formalization and proof of the churchrosser theorem that was carried out with the boyermoore theorem prover the proof presented in this paper is based on that of tait and martinlf the mechanical proof illustrates the effective use of the boyermoore theorem prover in proof checking difficult metamathematical proofs"
362 "the computational complexity of constructing the imbeddings of a given" "the computational complexity of constructing the imbeddings of a given graph into surfaces of different genus is not well understood in this paper topological methods and a reduction to linear matroid parity are used to develop a polynomialtime algorithm to find a maximumgenus cellular imbedding this seems to be the first imbedding algorithm for which the running time is not exponential in the genus of the imbedding surface"
363 "efficient decomposition algorithms for the weighted maximum independent set minimum" "efficient decomposition algorithms for the weighted maximum independent set minimum coloring and minimum clique cover problems on planar perfect graphs are presented these planar graphs can also be characterized by the absence of induced odd cycles of length greater than odd holes the algorithm in this paper is based on decomposing these graphs into essentially two special classes of inseparable component graphs whose optimization problems are easy to solve finding the solutions for these components and combining them to form a solution for the original graph these two classes are i planar comparability graphs and ii planar line graphs of those planar bipartite graphs whose maximum degrees are no greater than three the same techniques can be applied to other classes of perfect graphs provided that efficient algorithms are available for their inseparable component graphs"
364 "a probability distribution mgr on allows perfect packing if n" "a probability distribution mgr on allows perfect packing if n items of size x xn independent and identically distributed according to mgr can be packed in unit size bins in such a way that the expected wasted space is on a large class of distributions that allow perfect packing is exhibited as a corollary the intervals a b for which the uniform distribution on a b allows perfect packing are determined"
365 "binary exponential backoff is a randomized protocol for regulating transmissions" "binary exponential backoff is a randomized protocol for regulating transmissions on a multipleaccess broadcast channel ethernet a localarea network is built upon this protocol the fundamental theoretical issue is stability does the backlog of packets awaiting transmission remain bounded in time provided the rates of new packet arrivals are small enough it is assumed n stations share the channel each having an infinite buffer where packets accumulate while the station attempts to transmit the first from the buffer here it is established that binary exponential backoff is stable if the sum of the arrival rates is sufficiently small detailed results are obtained on which rates lead to stability when n stations share the channel in passing several other results are derived bearing on the efficiency of the conflict resolution process simulation results are reported that in particular indicate alternative retransmission protocols can significantly improve performance"
366 "the problem of realizing an idealized parallel architecture on a" "the problem of realizing an idealized parallel architecture on a possibly faultladen physical architecture is studied our formulation performs the mapping in the light of the algorithm that one wants to implement on the idealized architecture a version of the mapping algorithm suggested by the diogenes methodology for designing faulttolerant vlsi processor arrays is settled definitely two quality metrics for mappings are considered the first embodying an idealized notion of average delay which relates to power consumption and the second being the length of the longest run of wire for the averagedelay measure four algorithms that optimally assign the m vertices of the embedded graph to the n faultfree processors that have been fabricated are presented the most general algorithm makes no assumptions about the structure of the array or the physical format of the processors it runs in time om n m the other algorithms assume that the processors are laid out in such a way that interprocessor distances obey the triangle equality they run in times ranging from time o maxm n m log min m n m for certain array structures including linear arrays to time omaxm n m for a narrow class of array structures including pyramid arrays for the maxwirerun cost measure it is shown that the problem of finding costoptimal vertextoprocessor assignments is npcomplete however an algorithm is presented that yields in time om n m vertextoprocessor assignments that are within a factor of of optimal they are optimal when the input graphembedding is outplanar this algorithm can easily be converted to one that yields in time om n m vertextoprocessor assignments that are within a factor of of optimal finally an algorithm that yields optimal assignments when the interprocessor distances obey the triangle equality is presented this algorithm operates in time om n m logm n m log m where m is the largest interprocessor distance"
367 "let m be a parallel ram with p processors and" "let m be a parallel ram with p processors and arithmetic operations addition and subtraction recognizing l nn in t steps inputs for m are given integer by integer not bit by bit then l can be recognized by a sequential linear search algorithm lsa in onlogn t logp steps thus many ndimensional restrictions of npcomplete problems binary programming traveling salesman problem etc and even that of the uniquely optimum traveling salesman problem which is dgrpcomplete can be solved in polynomial time by an lsa this result generalizes the construction of a polynomial lsa for the ndimensional restriction of the knapsack problem previously shown by the author and destroys the hope of proving nonpolynomial lower bounds on lsas for any problem that can be recognized by a pram as above with polyn processors in polyn time"
368 "recent advances in graph theory and graph algorithms dramatically alter" "recent advances in graph theory and graph algorithms dramatically alter the traditional view of concrete complexity theory in which a decision problem is generally shown to be in p by producing an efficient algorithm to solve an optimization version of the problem nonconstructive tools are now available for classifying problems as decidable in polynomial time by guaranteeing only the existence of polynomialtime decision algorithms in this paper these new methods are employed to prove membership in p for a number of problems whose complexities are not otherwise known powerful consequences of these techniques are pointed out and their utility is illustrated a type of partially ordered set that supports this general approach is defined and explored"
369 "for online randomaccess machines under logarithmic cost the simple task" "for online randomaccess machines under logarithmic cost the simple task of storing arbitrary binary inputs has nonlinear complexity even if all kinds of powerful internal operations are admitted and reading of storage locations is free of charge just successively changing the storage contents for properly storing arbitrary nbit inputs requires an average cost of order n logn"
370 "in this paper a very simple model of parallel computation" "in this paper a very simple model of parallel computation is considered and the question of how restricting the flow of data to be one way compares with twoway flow is studied it is shown that the oneway version is surprisingly very powerful in that it can solve problems that seemingly require twoway communication whether or not oneway communication is strictly weaker than twoway is an open problem although the conjecture in this paper is in the positive it is shown however that proving this conjecture is at least as hard as some wellknown open problems in complexity theory"
372 "the busy period of order n for a subnetwork which" "the busy period of order n for a subnetwork which for large n describes heavy traffic periods of that subnetwork is described for queuing networks the mean duration of such busy periods and efficient algorithms for computing these quantities are determined"
373 "modular integer exponentiation given a e and m compute ae" "modular integer exponentiation given a e and m compute ae mod m is a fundamental problem in algebraic complexity for which no efficient parallel algorithm is known two closely related problems are modular polynomial exponentiation given ax e and mx compute axe mod mx and polynomial exponentiation given ax e and t compute the coefficient of xt in axe it is shown that these latter two problems are in nc when ax and mx are polynomials over a finite field whose characteristic is polynomial in the input size"
374 "a new algorithm for the hierarchical aggregation of singularly perturbed" "a new algorithm for the hierarchical aggregation of singularly perturbed finitestate markov processes is derived the approach taken bridges the gap between conceptually simple results for a relatively restricted class of processes and the significantly more complex results for the general case the critical role played by almost transient states is exposed resulting in a straightforward algorithm for the construction of a sequence of aggregate generators associated with various time scales these generators together provide a uniform asymptotic approximation of the original probability transition function"
375 "a productform queuing network with multiple open and closed chains" "a productform queuing network with multiple open and closed chains is considered some of the closed chains which have a single customer each require allocation of resources in the network so as to maximize a weighted throughput performance criterion chains with more than one customer can be decomposed into many chains of one customer each it is proved that an optimal allocation of resources lies on a vertex extreme points of the set of feasible allocations this considerably reduces the search space for an optimal allocation applications of this result in distributed computing are discussed"
376 "algorithms on multivariate polynomials represented by straightline programs are developed" "algorithms on multivariate polynomials represented by straightline programs are developed first it is shown that most algebraic algorithms can be probabilistically applied to data that are given by a straightline computation testing such rational numeric data for zero for instance is facilitated by random evaluations modulo random prime numbers then auxiliary algorithms that determine the coefficients of a multivariate polynomial in a single variable are constructed the first main result is an algorithm that produces the greatest common divisor of the input polynomials all in straightline representation the second result shows how to find a straightline program for the reduced numerator and denominator from one for the corresponding rational function both the algorithm for that construction and the greatest common divisor algorithm are in random polynomial time for the usual coefficient fields and output a straightline program which with controllably high probability correctly determines the requested answer the running times are polynomial functions in the binary input size the input degrees as unary numbers and the logarithm of the inverse of the failure probability the algorithm for straightline programs for the numerators and denominators of rational functions implies that every degreebounded rational function can be computed fast in parallel that is in polynomial size and polylogarithmic depth"
377 "manysorted unification is considered that is unification in the manysorted" "manysorted unification is considered that is unification in the manysorted free algebras of terms where variables as well as the domains and ranges of functions are restricted to certain subsets of the universe given as a potentially infinite hierarchy of sorts it is shown that complete and minimal sets of unifiers may not always exist for manysorted unification conditions for sort hierarchies that are equivalent for the existence of these sets with one finitely many or infinitely many elements are presented it is also proved that being a foreststructured sort hierarchy is a necessary and sufficient criterion for the robinson unification theorem to hold for manysorted unification an algorithm for manysorted unification is given"
378 "the random heuristic search algorithm called simulated annealing is considered" "the random heuristic search algorithm called simulated annealing is considered for the problem of finding the maximum cardinality matching in a graph it is shown that neither a basic form of the algorithm nor any other algorithm in a fairly large related class of algorithms can find maximum cardinality matchings such that the average time required grows as a polynomial in the number of nodes of the graph in contrast it is also shown for arbitrary graphs that a degenerate form of the basic annealing algorithm obtained by letting temperature be a suitably chosen constant produces matchings with nearly maximum cardinality in polynomial average time"
379 "exponential lower bounds on the complexity of computing the clique" "exponential lower bounds on the complexity of computing the clique functions in the boolean decisiontree model are proved for onetimeonly branching programs large polynomial lower bounds are proved for kclique functions if k is fixed and exponential lower bounds if k increases with n finally the hierarchy of the classes bpdp of all sequences of boolean functions that may be computed by dtimes only branching programs of polynomial size is introduced it is shown constructively that bpp is a proper subset of bpp"
380 "the minimumweight perfect matching problem for complete graphs of n" "the minimumweight perfect matching problem for complete graphs of n vertices with edge weights satisfying the triangle inequality is considered for each nonnegative integer k logn and for any perfect matching algorithm that runs in tn time and has an error bound of fn times the optimal weight an omaxn tkntime heuristic algorithm with an error bound of k f kn is given by the selection of k as appropriate functions of n heuristics that have better running times andor error bounds than existing ones are derived"
381 "for every choice of positive integers c and k such" "for every choice of positive integers c and k such that k and ck there is a positive number egr such that with probability tending to as n tends to a randomly chosen family of cn clauses of size k over n variables is unsatisfiable but every resolution proof of its unsatisfiability must generate at least egrn clauses"
382 "this work describes a large number of constructions for sorting" "this work describes a large number of constructions for sorting n integers in the range m for n m n for the standard vlsi bit model among other results we attain vlsi sorter constructions that are within a constant factor of optimal size for all m and almost all running times t a fundamentally new merging network for sorting numbers in a bit model new organizational approaches for optimal tuning of merging networks and the proper management of data flow"
383 "the problem of connecting a set of terminals that lie" "the problem of connecting a set of terminals that lie on the sides of a rectangle to minimize the total area is discussed an on algorithm is presented to solve this problem when the set of n terminals is initially sorted the strategy in this paper is to reduce the problem to several problems such that no matter what instance is started with at least one of these problems can be solved optimally by a greedy method"
384 "many problems can be modeled as singleserver queues with impatient" "many problems can be modeled as singleserver queues with impatient customers an example is that of the transmission of voice packets over a packetswitched network if the voice packets do not reach their destination within a certain time interval of their transmission they are useless to the receiver and considered lost it is therefore desirable to schedule the customers such that the fraction of customers served within their respective deadlines is maximized for this measure of performance it is shown that the shortest time to extinction ste policy is optimal for a class of continuous and discrete time nonpreemptive mg queues that do not allow unforced idle times when unforced idle times are allowed the best policies belong to the class of shortest time to extinction with inserted idle time stei policies an stei policy requires that the customer closest to his or her deadline be scheduled whenever it schedules a customer it also has the choice of inserting idle times while the queue is nonempty it is also shown that the ste policy is optimal for the discrete time gd queue where all customers receive one unit of service the paper concludes with a comparison of the expected customer loss using an ste policy with that of the firstcome firstserved fcfs scheduling policy for one specific queue"
385 "suppose we want to eliminate the local go to statements" "suppose we want to eliminate the local go to statements of a pascal program by replacing them with multilevel loop exit statements the standard ground rules for eliminating go tos require that we preserve the flow graph of the program but they allow us to completely rewrite the control structures that glue together the programs atomic tests and actions the go tos can be eliminated from a program under those ground rules if and only if the flow graph of that program has the graphtheoretic property named reducibility this paper considers a stricter set of ground rules introduced by peterson kasami and tokura which demand that we preserve the programs original control structures as well as its flow graph while we eliminate its go tos in particular we are allowed to delete the go to statements and the labels that they jump to and to insert various exit statements and labeled repeatendloop pairs for them to jump out of but we are forbidden to change the rest of the program text in any way the critical issue that determines whether go tos can be eliminated under these stricter rules turns out to be the static order of the atomic tests and actions in the program text this static order can be encoded in the programs flow graph by augmenting it with extra edges it can then be shown that the reducibility of a programs augmented flow graph augmenting edges and all is a necessary and sufficient condition for the eliminability of go tos from that program under the stricter rules"
386 "the computational capabilities of a system of n indistinguishable anonymous" "the computational capabilities of a system of n indistinguishable anonymous processors arranged on a ring in the synchronous and asynchronous models of distributed computation are analyzed a precise characterization of the functions that can be computed in this setting is given it is shown that any of these functions can be computed in on messages in the asynchronous model this is also proved to be a lower bound for such elementary functions as and sum and orientation in the synchronous model any computable function can be computed in on log n messages a ring can be oriented and start synchronized within the same bounds the main contribution of this paper is a new technique for proving lower bounds in the synchronous model with this technique tight lower bounds of thgrn log n for particular n are proved for xor sum orientation and start synchronization the technique is based on a stringproducing mechanism from formal language theory first introduced by thue to study squarefree words two methods for generalizing the synchronous lower bounds to arbitrary ring sizes are presented"
387 "knowledge compilation has been emerging recently as a new direction" "knowledge compilation has been emerging recently as a new direction of research for dealing with the computational intractability of general propositional reasoning according to this approach the reasoning process is split into two phases an offline compilation phase and an online queryanswering phase in the offline phase the propositional theory is compiled into some target language which is typically a tractable one in the online phase the compiled target is used to efficiently answer a potentially exponential number of queries the main motivation behind knowledge compilation is to push as much of the computational overhead as possible into the offline phase in order to amortize that overhead over all online queries another motivation behind compilation is to produce very simple online reasoning systems which can be embedded costeffectively into primitive computational platforms such as those found in consumer electronicsone of the key aspects of any compilation approach is the target language into which the propositional theory is compiled previous target languages included horn theories prime implicatesimplicants and ordered binary decision diagrams obdds we propose in this paper a new target compilation language known as decomposable negation normal form dnnf and present a number of its properties that make it of interest to the broad community specifically we show that dnnf is universal supports a rich set of polynomialtime logical operations is more spaceefficient than obdds and is very simple as far as its structure and algorithms are concerned moreover we present an algorithm for converting any propositional theory in clausal form into a dnnf and show that if the clausal form has a bounded treewidth then its dnnf compilation has a linear size and can be computed in linear time treewidth is a graphtheoretic parameter that measures the connectivity of the clausal form we also propose two techniques for approximating the dnnf compilation of a theory when the size of such compilation is too large to be practical one of the techniques generates a sound but incomplete compilation while the other generates a complete but unsound compilation together these approximations bound the exact compilation from below and above in terms of their ability to answer clausal entailment queries finally we show that the class of polynomialtime dnnf operations is rich enough to support relatively complex ai applications by proposing a specific framework for compiling modelbased diagnosis systems"
388 "we introduce a new approach to modeling uncertainty based on" "we introduce a new approach to modeling uncertainty based on plausibility measures this approach is easily seen to generalize other approaches to modeling uncertainty such as probability measures belief functions and possibility measures we focus on one application of plausibility measures in this paper default reasoning in recent years a number of different semantics for defaults have been proposed such as preferential structures egrsemantics possibilistic structures and kgrrankings that have been shown to be characterized by the same set of axioms known as the klm properties while this was viewed as a surprise we show here that it is almost inevitable in the framework of plausibility measures we can give a necessary condition for the klm axioms to be sound and an additional condition necessary and sufficient to ensure that the klm axioms are complete this additional condition is so weak that it is almost always met whenever the axioms are sound in particular it is easily seen to hold for all the proposals made in the literature"
389 "problems of statistical inference involve the adjustment of sample observations" "problems of statistical inference involve the adjustment of sample observations so they fit some a priori rank requirements or order constraints in such problems the objective is to minimize the deviation cost function that depends on the distance between the observed value and the modify value in markov random field problems there is also a pairwise relationship between the objects the objective in markov random field problem is to minimize the sum of the deviation cost function and a penalty function that grows with the distance between the values of related pairsseparation functionwe discuss markov random fields problems in the context of a representative applicationthe image segmentation problem in this problem the goal is to modify color shades assigned to pixels of an image so that the penalty function consisting of one term due to the deviation from the initial color shade and a second term that penalizes differences in assigned values to neighboring pixels is minimized we present here an algorithm that solves the problem in polynomial time when the deviation function is convex and separation function is linear and in strongly polynomial time when the deviation cost function is linear quadratic or piecewise linear convex with few pieces where quotefew quote means a number exponential in a polynomial function of the number of variables and constraints the complexity of the algorithm for a problem on n pixels or variables m adjacency relations or constraints and range of variable values colors u is otnm n log u where tnm is the complexity of solving the minimum s t cut problem on a graph with n nodes and m arcs furthermore other algorithms are shown to solve the problem with convex deviation and convex separation in running time omn log n log nu and the problem with nonconvex deviation and convex separation in running time otnu mu the nonconvex separation problem is nphard even for fixed value of ufor the family of problems with convex deviation functions and linear separation function the algorithm described here runs in polynomial time which is demonstrated to be fastest possible"
390 "we describe efficient techniques for a number of parties to" "we describe efficient techniques for a number of parties to jointly generate an rsa key at the end of the protocol an rsa modulus n pq is publicly known none of the parties know the factorization of n in addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption our protocols are efficient in computation and communication all results are presented in the honest but curious scenario passive adversary"
391 "deterministic fully dynamic graph algorithms are presented for connectivity minimum" "deterministic fully dynamic graph algorithms are presented for connectivity minimum spanning tree edge connectivity and biconnectivity assuming that we start with no edges in a graph with n vertices the amortized operation costs are olog n for connectivity o log n for minimum spanning forest edge connectivity and olog n biconnectivity"
392 "this paper presents a combinatorial polynomialtime algorithm for minimizing submodular" "this paper presents a combinatorial polynomialtime algorithm for minimizing submodular functions answering an open question posed in by grtschel lovsz and schrijver the algorithm employs a scaling scheme that uses a flow in the complete directed graph on the underlying set with each arc capacity equal to the scaled parameter the resulting algorithm runs in time bounded by a polynomial in the size of the underlying set and the length of the largest absolute function value the paper also presents a strongly polynomial version in which the number of steps is bounded by a polynomial in the size of the underlying set independent of the function values"
393 "we examine the number of queries to input variables that" "we examine the number of queries to input variables that a quantum algorithm requires to compute boolean functions on n in the blackbox model we show that the exponential quantum speedup obtained for partial functions ie problems involving a promise on the input by deutsch and jozsa simon and shor cannot be obtained for any total function if a quantum algorithm computes some total boolean function f with small error probability using t blackbox queries then there is a classical deterministic algorithm that computes f exactly with ots queries we also give asymptotically tight characterizations of t for all symmetric f in the exact zeroerror and boundederror settings finally we give new precise bounds for and or and parity our results are a quantum extension of the socalled polynomial method which has been successfully applied in classical complexity theory and also a quantum extension of results by nisan about a polynomial relationship between randomized and deterministic decision tree complexity"
394 "we prove optimal up to an arbitrary inapproximability results for" "we prove optimal up to an arbitrary inapproximability results for maxe ksat for k maximizing the number of satisfied linear equations in an overdetermined system of linear equations modulo a prime p and set splitting as a consequence of these results we get improved lower bounds for the efficient approximability of many optimization problems studied previously in particular for maxesat maxcut maxdicut and vertex cover"
395 "we introduce a new approach to constructing extractors extractors are" "we introduce a new approach to constructing extractors extractors are algorithms that transform a quoteweakly randomquote distribution into an almost uniform distribution explicit constructions of extractors have a variety of important applications and tend to be very difficult to obtainwe demonstrate an unsuspected connection between extractors and pseudorandom generators in fact we show that every pseudorandom generator of a certain kind is an extractora pseudorandom generator construction due to impagliazzo and wigderson once reinterpreted via our connection is already an extractor that beats most known constructions and solves an important open question we also show that using the simpler nisanwigderson generator and standard errorcorrecting codes one can build even better extractors with the additional advantage that both the construction and the analysis are simple and admit a short selfcontained description"
396 "we study adding aggregate operators such as summing up elements" "we study adding aggregate operators such as summing up elements of a column of a relation to logics with counting mechanisms the primary motivation comes from database applications where aggregate operators are present in all real life query languages unlike other features of query languages aggregates are not adequately captured by the existing logical formalisms consequently all previous approaches to analyzing the expressive power of aggregation were only capable of producing partial results depending on the allowed class of aggregate and arithmetic operationswe consider a powerful counting logic and extend it with the set of all aggregate operators we show that the resulting logic satisfies analogs of hanfs and gaifmans theorems meaning that it can only express local properties we consider a database query language that expresses all the standard aggregates found in commercial query languages and show how it can be translated into the aggregate logic thereby providing a number of expressivity bounds that do not depend on a particular class of arithmetic functions and that subsume all those previously known we consider a restricted aggregate logic that gives us a tighter capture of database languages and also use it to show that some questions on expressivity of aggregation cannot be answered without resolving some deep problems in complexity theory"
397 "research on multimedia information retrieval mir has recently witnessed a" "research on multimedia information retrieval mir has recently witnessed a booming interest a prominent feature of this research trend is its simultaneous but independent materialization within several fields of computer science the resulting richness of paradigms methods and systems may on the long run result in a fragmentation of efforts and slow down progress the primary goal of this study is to promote an integration of methods and techniques for mir by contributing a conceptual model that encompasses in a unified and coherent perspective the many efforts that are being produced under the label of mir the model offers a retrieval capability that spans two media text and images but also several dimensions form content and structure in this way it reconciles similaritybased methods with semanticsbased ones providing the guidelines for the design of systems that are able to provide a generalized multimedia retrieval service in which the existing forms of retrieval not only coexist but can be combined in any desired manner the model is formulated in terms of a fuzzy description logic which plays a twofold role it directly models semanticsbased retrieval and it offers an ideal framework for the integration of the multimedia and multidimensional aspects of retrieval mentioned above the model also accounts for relevance feedback in both text and image retrieval integrating known techniques for taking into account user judgments the implementation of the model is addressed by presenting a decomposition technique that reduces query evaluation to the processing of simpler requests each of which can be solved by means of widely known methods for text and image retrieval and semantic processing a prototype for multidimensional image retrieval is presented that shows this decomposition technique at work in a significant case"
398 "we consider the problems of containment equivalence satisfiability and queryreachability" "we consider the problems of containment equivalence satisfiability and queryreachability for datalog programs with negation these problems are important for optimizing datalog programs we show that both queryreachability and satisfiability are decidable for programs with stratified negation provided that negation is applied only to edb predicates or that all edb predicates are unary in the latter case we show that equivalence is also decidable the algorithms we present can also be used to push constraints from a given query to the edb predicates in showing our decidability results we describe a powerful tool the querytree which is used for several optimization problems for datalog programs finally we show that satisfiability is undecidable for datalog programs with unary idb predicates stratified negation and the interpreted predicate"
399 "we present an algorithm for implementing binary operations of any" "we present an algorithm for implementing binary operations of any type from unary loadlinked ll and storeconditional sc operations the performance of the algorithm is evaluated according to its sensitivity measuring the distance between operations in the graph induced by conflicts which guarantees that they do not influence the step complexity of each other the sensitivity of our implementation is ologast n where n is the number of processors in the system that is operations that are logast n apart in the graph induced by conflicts do not delay each other constant sensitivity is achieved for operations used to implement heaps and arraybased linked listswe also prove that there is a problem which can be solved in o steps using binary llsc operations but requires olog logast n operations if only unary llsc operations are used this indicates a nonconstant gap between unary and binary llsc operations"
400 "we start with a mathematical definition of a real interval" "we start with a mathematical definition of a real interval as a closed connected set of reals interval arithmetic operations addition subtraction multiplication and division are likewise defined mathematically and we provide algorithms for computing these operations assuming exact real arithmetic next we define interval arithmetic operations on intervals with ieee floating point endpoints to be sound and optimal approximations of the real interval operations and we show that the ieee standards specification of operations involving the signed infinities signed zeros and the exactinexact flag are such as to make a correct and optimal implementation more efficient from the resulting theorems we derive data that are sufficiently detailed to convert directly to a program for efficiently implementing the interval operations finally we extend these results to the case of general intervals which are defined as connected sets of reals that are not necessarily closed"
401 "we present a general framework for solving resource allocation and" "we present a general framework for solving resource allocation and scheduling problems given a resource of fixed size we present algorithms that approximate the maximum throughput or the minimum loss by a constant factor our approximation factors apply to many problems among which are i realtime scheduling of jobs on parallel machines ii bandwidth allocation for sessions between two endpoints iii general caching iv dynamic storage allocation and v bandwidth allocation on optical line and ring topologies for some of these problems we provide the first constant factor approximation algorithm our algorithms are simple and efficient and are based on the localratio technique we note that they can equivalently be interpreted within the primaldual schema"
402 "the idea of preprocessing part of the input of a" "the idea of preprocessing part of the input of a problem in order to improve efficiency has been employed by several researchers in several areas of computer science in this article we show sufficient conditions to prove that an intractable problem cannot be efficiently solved even allowing an exponentially long preprocessing phase the generality of such conditions is shown by applying them to various problems coming from different fields while the results may seem to discourage the use of compilation we present some evidence that such negative results are useful in practice"
403 "we provide two algorithms for computing the volume of the" "we provide two algorithms for computing the volume of the convex polytope x n ax b for a mn b n the computational complexity of both algorithms is essentially described by nm which makes them especially attractive for large n and relatively small m when the other methods with omn complexity fail the methodology which differs from previous existing methods uses a laplace transform technique that is well suited to the halfspace representation of"
404 "recent trends in information management involve the periodic transcription of" "recent trends in information management involve the periodic transcription of data onto secondary devices in a networked environment and the proper scheduling of these transcriptions is critical for efficient data management to assist in the scheduling process we are interested in modeling data obsolescence that is the reduction of consistency over time between a relation and its replica the modeling is based on techniques from the field of stochastic processes and provides several stochastic models for content evolution in the base relations of a database taking referential integrity constraints into account these models are general enough to accommodate most of the common scenarios in databases including batch insertions and lifespans both with and without memory as an initial proof of concept of the applicability of our approach we validate the insertion portion of our model framework via experiments with real data feeds we also discuss a set of transcription protocols that make use of the proposed stochastic model"
405 "we introduce the concept of a class of graphs or" "we introduce the concept of a class of graphs or more generally relational structures being locally treedecomposable there are numerous examples of locally treedecomposable classes among them the class of planar graphs and all classes of bounded valence or of bounded treewidth we also consider a slightly more general concept of a class of structures having bounded local treewidthwe show that for each property of structures that is definable in firstorder logic and for each locally treedecomposable class c of structures there is a linear time algorithm deciding whether a given structure a c has property for classes c of bounded local treewidth we show that for every k there is an algorithm solving the same problem in time onk where n is the cardinality of the input structure"
406 "we study extensions of the process algebra axiom system acp" "we study extensions of the process algebra axiom system acp with two recursive operations the binary kleene star which is defined by xy xxy y and the pushdown operation defined by xy xxyx y y in this setting it is easy to represent register machine computation and an equational theory results that is not decidable in order to increase the expressive power abstraction is then added with rooted branching bisimulation equivalence each computable process can be expressed and with rooted bisimilarity each semicomputable process that initially is finitely branching can be expressed moreover with abstraction and a finite number of auxiliary actions these results can be obtained without binary kleene star finally we consider two alternatives for the pushdown operation each of these gives rise to similar results"
407 "in valiant showed that boolean matrix multiplication can be used" "in valiant showed that boolean matrix multiplication can be used for parsing contextfree grammars cfgs yielding the asympotically fastest although not practical cfg parsing algorithm known we prove a dual result any cfg parser with time complexity ogn where g is the size of the grammar and n is the length of the input string can be efficiently converted into an algorithm to multiply m m boolean matrices in time om given that practical substantially subcubic boolean matrix multiplication algorithms have been quite difficult to find we thus explain why there has been little progress in developing practical substantially subcubic general cfg parsers in proving this result we also develop a formalization of the notion of parsing"
408 "we establish that the algorithmic complexity of the minimum spanning" "we establish that the algorithmic complexity of the minimum spanning tree problem is equal to its decisiontree complexity specifically we present a deterministic algorithm to find a minimum spanning tree of a graph with n vertices and m edges that runs in time otmn where t is the minimum number of edgeweight comparisons needed to determine the solution the algorithm is quite simple and can be implemented on a pointer machinealthough our time bound is optimal the exact function describing it is not known at present the current best bounds known for t are t mn m and tmn om mn where is a certain natural inverse of ackermanns functioneven under the assumption that t is superlinear we show that if the input graph is selected from gnm our algorithm runs in linear time with high probability regardless of n m or the permutation of edge weights the analysis uses a new martingale for g nm similar to the edgeexposure martingale for gnp"
409 "we develop a theoretical framework to characterize the hardness of" "we develop a theoretical framework to characterize the hardness of indexing data sets on blockaccess memory devices like hard disks we define an indexing workload by a data set and a set of potential queries for a workload we can construct an indexing scheme which is a collection of fixedsized subsets of the data we identify two measures of efficiency for an indexing scheme on a workload storage redundancy r how many times each item in the data set is stored and access overhead a how many times more blocks than necessary does a query retrievefor many interesting families of workloads there exists a tradeoff between storage redundancy and access overhead given a desired access overhead a there is a minimum redundancy that any indexing scheme must exhibit we prove a lowerbound theorem for deriving the minimum redundancy by applying this theorem we show interesting upper and lower bounds and tradeoffs between a and r in the case of multidimensional range queries and set queries"
410 "structured document databases can be naturally viewed as derivation trees" "structured document databases can be naturally viewed as derivation trees of a contextfree grammar under this view the classical formalism of attribute grammars becomes a formalism for structured document query languages from this perspective we study the expressive power of bags booleanvalued attribute grammars with propositional logic formulas as semantic rules and rags relationvalued attribute grammars with firstorder logic formulas as semantic rules bags can express only unary queries rags can express queries of any arity we first show that the unary queries expressible by bags are precisely those definable in monadic secondorder logic we then show that the queries expressible by rags are precisely those definable by firstorder inductions of linear depth or equivalently those computable in linear time on a parallel machine with polynomially many processors further we show that rags that only use synthesized attributes are strictly weaker than rags that use both synthesized and inherited attributes we show that rags are more expressive than monadic secondorder logic for queries of any arity finally we discuss relational attribute grammars in the context of bags and rags we show that in the case of bags this does not increase the expressive power while different semantics for relational rags capture the complexity classes np conp and up coup"
411 "shared registers are basic objects used as communication mediums in" "shared registers are basic objects used as communication mediums in asynchronous concurrent computation a concurrent timestamp system is a higher typed communication object and has been shown to be a powerful tool to solve many concurrency control problems it has turned out to be possible to construct such higher typed objects from primitive lower typed ones the next step is to find efficient constructions we propose a very efficient waitfree construction of bounded concurrent timestamp systems from writer shared registers this finalizes corrects and extends a preliminary bounded multiwriter construction proposed by the second author in that work partially initiated the current interest in waitfree concurrent objects and introduced a notion of discrete vector clocks in distributed algorithms"
412 "we consider a modified notion of planarity in which two" "we consider a modified notion of planarity in which two nations of a map are considered adjacent when they share any point of their boundaries not necessarily an edge as planarity requires such adjacencies define a map graph we give an np characterization for such graphs derive some consequences regarding sparsity and coloring and survey some algorithmic results"
413 "the johnsonlindenstrauss lemma states that n points in a highdimensional" "the johnsonlindenstrauss lemma states that n points in a highdimensional hilbert space can be embedded with small distortion of the distances into an olog n dimensional space by applying a random linear transformation we show that similar though weaker properties hold for certain random linear transformations over the hamming cube we use these transformations to solve nphard clustering problems in the cube as well as in geometric settingsmore specifically we address the following clustering problem given n points in a larger set eg d endowed with a distance function eg l distance we would like to partition the data set into k disjoint clusters each with a cluster center so as to minimize the sum over all data points of the distance between the point and the center of the cluster containing the point the problem is provably nphard in some highdimensional geometric settings even for k we give polynomialtime approximation schemes for this problem in several settings including the binary cube d with hamming distance and d either with l distance or with l distance or with the square of l distance in all these settings the best previous results were constant factor approximation guaranteeswe note that our problem is similar in flavor to the kmedian problem and the related facility location problem which has been considered in graphtheoretic and fixed dimensional geometric settings where it becomes hard when k is part of the input in contrast we study the problem when k is fixed but the dimension is part of the input"
414 "the problem of finding a center string that is close" "the problem of finding a center string that is close to every given string arises in computational molecular biology and coding theory this problem has two versions the closest string problem and the closest substring problem given a set of strings s s s sn each of length m the closest string problem is to find the smallest d and a string s of length m which is within hamming distance d to each si s this problem comes from coding theory when we are looking for a code not too far away from a given set of codes closest substring problem with an additional input integer l asks for the smallest d and a string s of length l which is within hamming distance d away from a substring of length l of each si this problem is much more elusive than the closest string problem the closest substring problem is formulated from applications in finding conserved regions identifying genetic drug targets and generating genetic probes in molecular biology whether there are efficient approximation algorithms for both problems are major open questions in this area we present two polynomialtime approximation algorithms with approximation ratio for any small to settle both questions"
415 "in this article we define timed regular expressions a formalism" "in this article we define timed regular expressions a formalism for specifying discrete behaviors augmented with timing information and prove that its expressive power is equivalent to the timed automata of alur and dill this result is the timed analogue of kleene theorem and similarly to that result the hard part in the proof is the translation from automata to expressions this result is extended from finite to infinite in the sense of bchi behaviors in addition to these fundamental results we give a clean algebraic framework for two commonly accepted formalisms for timed behaviors timeevent sequences and piecewiseconstant signals"
416 "we view congestion control as a distributed primaldual algorithm carried" "we view congestion control as a distributed primaldual algorithm carried out by sources and links over a network to solve a global optimization problem we describe a multilink multisource model of the tcp vegas congestion control mechanism the model provides a fundamental understanding of delay fairness and loss properties of tcp vegas it implies that vegas stabilizes around a weighted proportionally fair allocation of network capacity when there is sufficient buffering in the network it clarifies the mechanism through which persistent congestion may arise and its consequences and suggests how we might use rem active queue management to prevent it we present simulation results that validate our conclusions"
417 "we consider the problem of routing traffic to optimize the" "we consider the problem of routing traffic to optimize the performance of a congested network we are given a network a rate of traffic between each pair of nodes and a latency function for each edge specifying the time needed to traverse the edge given its congestion the objective is to route traffic such that the sum of all travel timesthe total latencyis minimizedin many settings it may be expensive or impossible to regulate network traffic so as to implement an optimal assignment of routes in the absence of regulation by some central authority we assume that each network user routes its traffic on the minimumlatency path available to it given the network congestion caused by the other users in general such a selfishly motivated assignment of traffic to paths will not minimize the total latency hence this lack of regulation carries the cost of decreased network performancein this article we quantify the degradation in network performance due to unregulated traffic we prove that if the latency of each edge is a linear function of its congestion then the total latency of the routes chosen by selfish network users is at most times the minimum possible total latency subject to the condition that all traffic must be routed we also consider the more general setting in which edge latency functions are assumed only to be continuous and nondecreasing in the edge congestion here the total latency of the routes chosen by unregulated selfish network users may be arbitrarily larger than the minimum possible total latency however we prove that it is no more than the total latency incurred by optimally routing twice as much traffic"
418 "we consider a distributed server system and ask which policy" "we consider a distributed server system and ask which policy should be used for assigning jobs tasks to hosts in our server jobs are not preemptible also the jobs service demand is not known a priori we are particularly concerned with the case where the workload is heavytailed as is characteristic of many empirically measured computer workloads we analyze several natural task assignment policies and propose a new one tags task assignment based on guessing size the tags algorithm is counterintuitive in many respects including load unbalancing nonworkconserving and fairness we find that under heavytailed workloads tags can outperform all task assignment policies known to us by several orders of magnitude with respect to both mean response time and mean slowdown provided the system load is not too high we also introduce a new practical performance metric for distributed servers called server expansion under the server expansion metric tags significantly outperforms all other task assignment policies regardless of system load"
419 "in completely symmetric systems that have homogeneous nodes hosts computers" "in completely symmetric systems that have homogeneous nodes hosts computers or processors with identical arrival processes an optimal static load balancing scheme does not involve the forwarding of jobs among nodes using an appropriate analytic model of a distributed computer system we examine the following three decision schemes for load balancing completely distributed intermediately distributed and completely centralized we show that there is no forwarding of jobs in the completely centralized and completely distributed schemes but that in an intermediately distributed decision scheme mutual forwarding of jobs among nodes is possible leading to degradation in system performance for every decision maker this result appears paradoxical because by adding communication capacity to the system for the sharing of jobs between nodes the overall system performance is degraded we characterize conditions under which such paradoxical behavior occurs and we give examples in which the degradation of performance may increase without bound we show that the degradation reduces and finally disappears in the limit as the intermediately distributed decision scheme tends to a completely distributed one"
420 "we present two new algorithms for solving the all pairs" "we present two new algorithms for solving the all pairs shortest paths apsp problem for weighted directed graphs both algorithms use fast matrix multiplication algorithmsthe first algorithm solves the apsp problem for weighted directed graphs in which the edge weights are integers of small absolute value in n time where satisfies the equation and is the exponent of the multiplication of an n n matrix by an n n matrix currently the best available bounds on obtained by coppersmith imply that the running time of our algorithm is therefore on our algorithm improves on the otiledenc time algorithm where is the usual exponent of matrix multiplication obtained by alon et al whose running time is only known to be onthe second algorithm solves the apsp problem almost exactly for directed graphs with arbitrary nonnegative real weights the algorithm runs in nepsis logw epsis time where epsis is an error parameter and w is the largest edge weight in the graph after the edge weights are scaled so that the smallest nonzero edge weight in the graph is it returns estimates of all the distances in the graph with a stretch of at most epsis corresponding paths can also be found efficiently"
421 "we derive tight bounds on cache misses for evaluation of" "we derive tight bounds on cache misses for evaluation of explicit stencil operators on rectangular grids our lower bound is based on the isoperimetric property of the discrete crosspolytope our upper bound is based on a good surfacetovolume ratio of a parallelepiped spanned by a reduced basis of the interference lattice of a grid measurements show that our algorithm typically reduces the number of cache misses by a factor of three relative to a compiler optimized code we show that stencil calculations on grids whose interference lattices have a short vector feature abnormally high numbers of cache misses we call such grids unfavorable and suggest to avoid these in computations by appropriate padding by direct measurements on a mips r processor we show a good correlation between abnormally high numbers of cache misses and unfavorable threedimensional grids"
422 "the subject of this article is differential compression the algorithmic" "the subject of this article is differential compression the algorithmic task of finding common strings between versions of data and using them to encode one version compactly by describing it as a set of changes from its companion a main goal of this work is to present new differencing algorithms that i operate at a fine granularity the atomic unit of change ii make no assumptions about the format or alignment of input data and iii in practice use linear time use constant space and give good compression we present new algorithms which do not always compress optimally but use considerably less time or space than existing algorithms one new algorithm runs in on time and o space in the worst case where each unit of space contains log n bits as compared to algorithms that run in on time and on space or in on time and o space we introduce two new techniques for differential compression and apply these to give additional algorithms that improve compression and time performance we experimentally explore the properties of our algorithms by running them on actual versioned data finally we present theoretical results that limit the compression power of differencing algorithms that are restricted to making only a single pass over the data"
423 "the article investigates xml document specifications with dtds and integrity" "the article investigates xml document specifications with dtds and integrity constraints such as keys and foreign keys we study the consistency problem of checking whether a given specification is meaningful that is whether there exists an xml document that both conforms to the dtd and satisfies the constraints we show that dtds interact with constraints in a highly intricate way and as a result the consistency problem in general is undecidable when it comes to unary keys and foreign keys the consistency problem is shown to be npcomplete this is done by coding dtds and integrity constraints with linear constraints on the integers we consider the variations of the problem by both restricting and enlarging the class of constraints and identify a number of tractable cases as well as a number of additional npcomplete ones by incorporating negations of constraints we establish complexity bounds on the implication problem which is shown to be conpcomplete for unary keys and foreign keys"
424 "this paper investigates to what extent a purely symbolic approach" "this paper investigates to what extent a purely symbolic approach to decision making under uncertainty is possible in the scope of artificial intelligence contrary to classical approaches to decision theory we try to rank acts without resorting to any numerical representation of utility or uncertainty and without using any scale on which both uncertainty and preference could be mapped our approach is a variant of savages where the setting is finite and the strict preference on acts is a partial order it is shown that although many axioms of savage theory are preserved and despite the intuitive appeal of the ordinal method for constructing a preference over acts the approach is inconsistent with a probabilistic representation of uncertainty the latter leads to the kind of paradoxes encountered in the theory of voting it is shown that the assumption of ordinal invariance enforces a qualitative decision procedure that presupposes a comparative possibility representation of uncertainty originally due to lewis and usual in nonmonotonic reasoning our axiomatic investigation thus provides decisiontheoretic foundations to the preferential inference of lehmann and colleagues however the obtained decision rules are sometimes either not very decisive or may lead to overconfident decisions although their basic principles look sound this paper points out some limitations of purely ordinal approaches to savagelike decision making under uncertainty in perfect analogy with similar difficulties in voting theory"
425 "we consider the possibility of encoding m classical bits into" "we consider the possibility of encoding m classical bits into many fewer n quantum bits qubits so that an arbitrary bit from the original m bits can be recovered with good probability we show that nontrivial quantum codes exist that have no classical counterparts on the other hand we show that quantum encoding cannot save more than a logarithmic additive factor over the best classical encoding the proof is based on an entropy coalescence principle that is obtained by viewing holevos theorem from a new perspectivein the existing implementations of quantum computing qubits are a very expensive resource moreover it is difficult to reinitialize existing bits during the computation in particular reinitialization is impossible in nmr quantum computing which is perhaps the most advanced implementation of quantum computing at the moment this motivates the study of quantum computation with restricted memory and no reinitialization that is of quantum finite automata it was known that there are languages that are recognized by quantum finite automata with sizes exponentially smaller than those of corresponding classical automata here we apply our technique to show the surprising result that there are languages for which quantum finite automata take exponentially more states than those of corresponding classical automata"
426 "this paper argues that for many algorithms and static analysis" "this paper argues that for many algorithms and static analysis algorithms in particular bottomup logic program presentations are clearer and simpler to analyze for both correctness and complexity than classical pseudocode presentations the main technical contribution consists of two theorems which allow in many cases the asymptotic running time of a bottomup logic program to be determined by inspection it is well known that a datalog program runs in onk time where k is the largest number of free variables in any single rule the theorems given here are significantly more refined a variety of algorithms are presented and analyzed as examples"
427 "we show how to use an interactive theorem prover hol" "we show how to use an interactive theorem prover hol together with a model checker spin to prove key properties of distance vector routing protocols we do three case studies correctness of the rip standard a sharp realtime bound on rip stability and preservation of loopfreedom in aodv a distance vector protocol for wireless networks we develop verification techniques suited to routing protocols generally these case studies show significant benefits from automated support in reduced verification workload and assistance in finding new insights and gaps for standard specifications"
428 "some important classical mechanisms considered in microeconomics and game theory" "some important classical mechanisms considered in microeconomics and game theory require the solution of a difficult optimization problem this is true of mechanisms for combinatorial auctions which have in recent years assumed practical importance and in particular of the gold standard for combinatorial auctions the generalized vickrey auction gva traditional analysis of these mechanismsin particular their truth revelation propertiesassumes that the optimization problems are solved precisely in reality these optimization problems can usually be solved only in an approximate fashion we investigate the impact on such mechanisms of replacing exact solutions by approximate ones specifically we look at a particular greedy optimization method we show that the gva payment scheme does not provide for a truth revealing mechanism we introduce another scheme that does guarantee truthfulness for a restricted class of players we demonstrate the latter property by identifying natural properties for combinatorial auctions and showing that for our restricted class of players they imply that truthful strategies are dominant those properties have applicability beyond the specific auction studied"
429 "given a collection of contigs and matepairs the contig scaffolding" "given a collection of contigs and matepairs the contig scaffolding problem is to order and orientate the given contigs in a manner that is consistent with as many matepairs as possible this paper describes an efficient heuristic called the greedypath merging algorithm for solving this problem the method was originally developed as a key component of the compartmentalized assembly strategy developed at celera genomics this interim approach was used at an early stage of the sequencing of the human genome to produce a preliminary assembly based on preliminary whole genome shotgun data produced at celera and preliminary human contigs produced by the human genome project"
430 "in a traditional classification problem we wish to assign one" "in a traditional classification problem we wish to assign one of k labels or classes to each of n objects in a way that is consistent with some observed data that we have about the problem an active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified this issue is one of the principal motivations for the framework of markov random fields and it arises in areas such as image processing biometry and document analysis in its most basic form this style of analysis seeks to find a classification that optimizes a combinatorial function consisting of assignment costsbased on the individual choice of label we make for each objectand separation costsbased on the pair of choices we make for two related objectswe formulate a general classification problem of this type the metric labeling problem we show that it contains as special cases a number of standard classification frameworks including several arising from the theory of markov random fields from the perspective of combinatorial optimization our problem can be viewed as a substantial generalization of the multiway cut problem and equivalent to a type of uncapacitated quadratic assignment problemwe provide the first nontrivial polynomialtime approximation algorithms for a general family of classification problems of this type our main result is an olog k log log kapproximation algorithm for the metric labeling problem with respect to an arbitrary metric on a set of k labels and an arbitrary weighted graph of relationships on a set of objects for the special case in which the labels are endowed with the uniform metricall distances are the sameour methods provide a approximation algorithm"
431 "a new framework for analyzing online bin packing algorithms is" "a new framework for analyzing online bin packing algorithms is presented this framework presents a unified way of explaining the performance of algorithms based on the harmonic approach within this framework it is shown that a new algorithm harmonic has asymptotic performance ratio at most it is also shown that the analysis of harmonic presented in richey is incorrect this is a fundamental logical flaw not an error in calculation or an omitted case the asymptotic performance ratio of harmonic is at least thus harmonic provides the best upper bound for the online bin packing problem to date"
432 "temporal logic comes in two varieties lineartime temporal logic assumes" "temporal logic comes in two varieties lineartime temporal logic assumes implicit universal quantification over all paths that are generated by the execution of a system branchingtime temporal logic allows explicit existential and universal quantification over all paths we introduce a third more general variety of temporal logic alternatingtime temporal logic offers selective quantification over those paths that are possible outcomes of games such as the game in which the system and the environment alternate moves while lineartime and branchingtime logics are natural specification languages for closed systems alternatingtime logics are natural specification languages for open systems for example by preceding the temporal operator eventually with a selective path quantifier we can specify that in the game between the system and the environment the system has a strategy to reach a certain state the problems of receptiveness realizability and controllability can be formulated as modelchecking problems for alternatingtime formulas depending on whether or not we admit arbitrary nesting of selective path quantifiers and temporal operators we obtain the two alternatingtime temporal logics atl and atl astatl and atlast are interpreted over concurrent game structures every state transition of a concurrent game structure results from a choice of moves one for each player the players represent individual components and the environment of an open system concurrent game structures can capture various forms of synchronous composition for open systems and if augmented with fairness constraints also asynchronous composition over structures without fairness constraints the modelchecking complexity of atl is linear in the size of the game structure and length of the formula and the symbolic modelchecking algorithm for ctl extends with few modifications to atl over structures with weakfairness constraints atl model checking requires the solution of pair rabin games and can be done in polynomial time over structures with strongfairness constraints atl model checking requires the solution of games with boolean combinations of bchi conditions and can be done in pspace in the case of atlast the modelchecking problem is closely related to the synthesis problem for lineartime formulas and requires doubly exponential time"
433 "a new computational algorithm called distribution analysis by chain dac" "a new computational algorithm called distribution analysis by chain dac is developed this algorithm computes joint queuelength distributions for productform queuing networks with singleserver fixed rate infinite server and queuedependent service centers joint distributions are essential in problems such as the calculation of availability measures using queuing network models the algorithm is efficient since the cost to evaluate joint queuelength probabilities is of the same order as the number of these probabilities this contrasts with the cost of evaluating these probabilities using previous algorithms the dac algorithm also computes mean queue lengths and throughputs more efficiently than the recently proposed recal and mvac algorithms furthermore the algorithm is numerically stable and its recursion is surprisingly simple"
434 "modular decomposition is a form of graph decomposition that has" "modular decomposition is a form of graph decomposition that has been discovered independently by researchers in graph theory game theory network theory and other areas this paper reduces the time needed to find the modular decomposition of a graph from ohgrn to ogrn together with a new algorithm for transitive orientation given in this leads to fast new algorithms for a number of problems in graph recognition and isomorphism including recognition of comparability graphs and permutation graphs the new algorithm works by inserting each vertex successively into the decomposition tree using ogrn time to insert each vertex"
435 "a unified framework is developed for the study of asynchronous" "a unified framework is developed for the study of asynchronous circuits of both gate and mos type a basic network model consisting of a directed graph and a set of vertex excitation functions is introduced a race analysis model using three values and x is developed for studying state transitions in the network it is shown that the results obtained using this model are equivalent to those using ternary simulation it is also proved that the set of state variables can be reduced to a minimum size set of feedback variables and the analysis still yields both the correct state transitions and output hazard information finally it is shown how the general results above are applicable to both gate and mos circuits"
436 "if a relational database is required to satisfy a set" "if a relational database is required to satisfy a set of integrity constraints then when the database is updated one must ensure that it continues to satisfy the constraints it is desirable not to have to evaluate each constraint after each update a method is described that takes a constraint c and a class of updates and either proves that an update in the class cannot violate c or produces a formula c a complete test that is satisfied before the update if and only if c would continue to be satisfied were the update to occur c is frequently much easier to evaluate than c in addition a formula d a sufficient test is sometimes produced such that if d is satisfied before the update then c would continue to be satisfied were the update to occur the method is proved correct the method is substantially more general than other reported techniques for this problem the method has been implemented and a number of experiments with the implementation are presented"
437 "recursive inference rules arise in recursive definitions in logic programming" "recursive inference rules arise in recursive definitions in logic programming systems and in database systems with recursive query languages let d be a recursive definition of a relation t d is considered minimal if for any predicate p in a recursive rule in d p must appear in a recursive rule in any definition of t it is shown that testing for minimality is in general undecidable however an efficient algorithm for a useful class of recursive rules is presented and it is used to transform a recursive definition to a minimal recursive definition evaluating the minimized definition avoids redundant computation without the overhead of caching intermediate results and runtime checking for duplicate goals"
438 "a new model for dynamic programming and branch and bound" "a new model for dynamic programming and branch and bound algorithms is presented the model views these algorithms as utilizing computationally feasible dominance relations to infer the orderings of application objects thereby implicitly enumerating a finite solution space the formalism is broad enough to apply the computational strategies of dynamic programming and branch and bound to problems with nonassociative objects and can model both oblivious and nonoblivious algorithms as well as parallel algorithms the model is used to classify computations based in part on the types of computationally feasible dominances that they employ it is demonstrated that the model is computationally precise enough to support the derivation of lower bounds on the number of operations required to solve various types of problems"
439 "in this paper efficient algorithms are given for inferring sequences" "in this paper efficient algorithms are given for inferring sequences produced by certain pseudorandom number generators the generators considered are all of the form xn sgrkjl agrjphgrjxo xl xnl mod m in each case we assume that the functions phgrj are known and polynomial time computable but that the coefficients aj and the modulus m are unknown using this general method specific examples of generators having this form the linear congruential method linear congruences with n terms in the recurrence and quadratic congruences are shown to be cryptographically insecure"
440 "let mqn denote the number of multiplications required to compute" "let mqn denote the number of multiplications required to compute the coefficients of the product of two polynomials of degree n over a qelement field by means of bilinear algorithms it is shown that mqn nge n o n in particular if q n lne q we establish the tight bound mq n n qthe technique we use can be applied to analysis of algorithms for multiplication of polynomials modulo a polynomial as well"
441 "repairable computer systems are considered the availability behavior of which" "repairable computer systems are considered the availability behavior of which can be modeled as a homogeneous markov process the randomization method is used to calculate various measures over a finite observation period related to availability modeling of these systems these measures include the distribution of the number of events of a certain type the distribution of the length of time in a set of states and the probability of a nearcoincident fault the method is then extended to calculate performability distributions the method relies on coloring subintervals of the finite observation period based on the particular application and then calculating the measure of interest using these colored intervals"
442 "a number of efficient methods for evaluating firstorder and monadicsecond" "a number of efficient methods for evaluating firstorder and monadicsecond order queries on finite relational structures are based on treedecompositions of structures or queries we systematically study these methodsin the first part of the article we consider arbitrary formulas on treelike structures we generalize a theorem of courcelle by showing that on structures of bounded treewidth a monadic secondorder formula with free first and secondorder variables can be evaluated in time linear in the structure size plus the size of the outputin the second part we study treelike formulas on arbitrary structures we generalize the notions of acyclicity and bounded treewidth from conjunctive queries to arbitrary firstorder formulas in a straightforward way and analyze the complexity of evaluating formulas of these fragments moreover we show that the acyclic and bounded treewidth fragments have the same expressive power as the wellknown guarded fragment and the finitevariable fragments of firstorder logic respectively"
443 "an exponential lower bound on the circuit complexity of deciding" "an exponential lower bound on the circuit complexity of deciding the weak monadic secondorder theory of one successor wss is proved circuits are built from binary operations or input gates which compute arbitrary boolean functions in particular to decide the truth of logical formulas of length at most in this secondorder language requires a circuit containing at least gates so even if each gate were the size of a proton the circuit would not fit in the known universe this result and its proof due to both authors originally appeared in in the phd thesis of the first author in this article the proof is given the result is put in historical perspective and the result is extended to probabilistic circuitsast"
444 "we study a property of correctness of programs written in" "we study a property of correctness of programs written in a sharedmemory parallel language this property is a semantic equivalence between the parallel program and its sequential version that we define we consider some standard parallel imperative language within this language this correctness property follows from the preservation of data dependences by the control flow and the synchronizations our result makes use of the semantics of the sequential version only hence through our result checking the correctness of some parallel program boils down to verifying properties of some sequential program"
445 "we present a model that enables us to analyze the" "we present a model that enables us to analyze the running time of an algorithm on a computer with a memory hierarchy with limited associativity in terms of various cache parameters our cache model an extension of aggarwal and vitters io model enables us to establish useful relationships between the cache complexity and the io complexity of computations as a corollary we obtain cacheefficient algorithms in the singlelevel cache model for fundamental problems like sorting fft and an important subclass of permutations we also analyze the averagecase cache behavior of mergesort show that ignoring associativity concerns could lead to inferior performance and present supporting experimental evidencewe further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage and for dealing with the hitherto unresolved problem of limited associativity"
446 "this contribution proposes a set of criteria that distinguish a" "this contribution proposes a set of criteria that distinguish a grand challenge in science or engineering from the many other kinds of shortterm or longterm research problems that engage the interest of scientists and engineers as an example drawn from computer science it revives an old challenge the construction and application of a verifying compiler that guarantees correctness of a program before running it"
447 "i examine the question of why so few classes of" "i examine the question of why so few classes of quantum algorithms have been discovered i give two possible explanations for this and some thoughts about what lines of research might lead to the discovery of more quantum algorithms"
448 "because many problems of general interest have natural nondeterministic algorithms" "because many problems of general interest have natural nondeterministic algorithms and because computers act deterministically it is important to understand the relationship between deterministic and nondeterministic time specifically it is important to understand how quickly a deterministic computing device can determine the outcome of a nondeterministic calculation so far we have no general techniques that work any better than trying all stepbystep simulations an exponential methodthe most famous question concerning determinism versus nondeterminism is the p np question however this is a different question than what is the relationship and it is possible that significant progress about the relationship can be achieved without answering the pv np question thanks to efficient reductions from turing machine simulation to sat the relationship question can be posed as a question about sat the problem of proving a nontrivial lower bound on the time required to solve sat is just an instance of the larger problem of proving lower bounds for any natural problem it is suggested that a study of generic problems might be a fruitful approach toward insights on such problems"
449 "would physical laws permit the construction of computing machines that" "would physical laws permit the construction of computing machines that are capable of solving some problems much faster than the standard computational model recent evidence suggests that this might be the case in the quantum world but the question is of great interest even in the realm of classical physics in this article we observe that there is fundamental tension between the extended churchturing thesis and the existence of numerous seemingly intractable computational problems arising from classical physics efforts to resolve this incompatibility could both advance our knowledge of the theory of computation as well as serve the needs of scientific computing"
450 "all previously known efficient maximumflow algorithms work by finding augmenting" "all previously known efficient maximumflow algorithms work by finding augmenting paths either one path at a time as in the original ford and fulkerson algorithm or all shortestlength augmenting paths at once using the layered network approach of dinic an alternative method based on the preflow concept of karzanov is introduced a preflow is like a flow except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out the method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths the algorithm and its analysis are simple and intuitive yet the algorithm runs as fast as any other known method on dense graphs achieving an on time bound on an nvertex graph by incorporating the dynamic tree data structure of sleator and tarjan we obtain a version of the algorithm running in onm lognm time on an nvertex medge graph this is as fast as any known method for any graph density and faster on graphs of moderate density the algorithm also admits efficient distributed and parallel implementations a parallel implementation running in onlog n time using n processors and om space is obtained this time bound matches that of the shiloachvishkin algorithm which also uses n processors but requires on space"
451 "this paper gives a fast lineartime algorithm for generating highquality" "this paper gives a fast lineartime algorithm for generating highquality pixel representations of curved lines the results are similar to what is achieved by selecting a circle whose diameter is the desired line width and turning on all pixels covered by the circle as it moves along the desired curve however the circle is replaced by a carefully chosen polygon whose deviations from the circle represent subpixel corrections designed to improve the aesthetic qualities of the rasterized curve for nonsquare pixels equally good results are obtained when an ellipse is used in place of the circle the class of polygons involved is introduced an algorithm for generating them is given and how to construct the set of pixels covered when such a polygon moves along a curve is shown the results are analyzed in terms of a mathematical model for the uniformity and accuracy of line width in the rasterized image"
452 "todays standard model for database concurrency control called serializability theory" "todays standard model for database concurrency control called serializability theory represents executions of transactions as partial orders of operations the theory tells when an execution is serializable that is when the set of operations of a transaction execute atomically with respect to those of other transactions it has been used successfully to prove correctness of most database concurrency control algorithms its most serious limitation is its inability to represent nested computations conveniently this paper presents a more general model that permits nested transactions in this model transactions may execute subtransactions giving rise to treestructured computations a serializability theory is developed for this model which can be used to prove the correctness of concurrency control algorithms for nested transactions and for multilevel database systems the theory is based on an abstract model of computation that allows arbitrary operations and parallel and even nondeterministic programs axioms are presented that express the basic properties that programs that manage or access data need to satisfy we use these axioms to derive proof techniques one new techniquesubstitutionshows the equivalence of two executions by substituting one subcomputation by another usually shallower ie less nested one our proof techniques are illustrated by applying them to several wellknown concurrency control problems"
453 "it is shown that n k o comparisons are necessary" "it is shown that n k o comparisons are necessary on average to find the kth smallest of n numbers k lne n this lower bound matches the behavior of the technique of floyd and rivest to within a lowerorder term n on comparisons on average are shown to be necessary and sufficient to find the maximum and median of a set an upper bound of n on and a lower bound of n on are shown for the maxminmedian problem"
454 "it is shown that the external path length of a" "it is shown that the external path length of a binary tree is closely related to the ratios of means of certain integers and establish the upper bound externalpathlengthn lognd logd where n denotes the number of external nodes in the tree and d is the difference in length between a longest and shortest path then it is proved that this bound is tight up to an on term if dn if dn we contstruct binary trees whose external path length is at least as large as nlog nfnd dlog d where fnd dn"
455 "in g k manacher showed that the fordjohnson sorting algorithm" "in g k manacher showed that the fordjohnson sorting algorithm fja acting on t real numbers can be beaten for an infinite set of values t these values form a partial cover of constant density not close to over an initial sequence of each band running from uk k to ukl this early result depended on showing that the hwanglin merging algorithm hla merging m elements with n m n could be surpassed by cm comparisons where c is an arbitrary small positive constant in this paper it is shown that the fja can be beaten for a set of integers of asymptotic density under the greatly weakened assumption that the hla can be surpassed by only egrlog m comparisons with egr a small positive constant the even weaker assumption that no improvement in the hla exists but that an isolated value to exists for which the fja can be surpassed by only egrlog to comparisons yields the same result only for a set of refractory integers of size about t in the neighborhood of each uk does the fja fail to be beaten all these results depend on a new technique for obtaining optimum sortmerge sequences for bestpossible sorting given a merging method the technique turns out to be amenable to precise asymptotic analysis when the technique is applied using the most powerful known merging algorithm christens the density mentioned above is still but islands of refractory points still remain this time forming sets provably of size thgrlogt in the neighborhood of each uk it is shown that if information theoretic merging were achievable the fja could be beaten for all t u from these results and a few others we adduce evidence in support of our main conjecture that even optimum combinations of optimum merging and fordjohnson sorting will not beat the fja when t uk but will instead produce refractory regions of size thgrlogt in the neighborhood of each lgr"
456 "an information dispersal algorithm ida is developed that breaks a" "an information dispersal algorithm ida is developed that breaks a file f of length l uharl fuharr into n pieces fi l i n each of length uharlfiuharr lm so that every m pieces suffice for reconstructing f dispersal and reconstruction are computationally efficient the sum of the lengths uharlfiuharr is nm l since nm can be chosen to be close to l the ida is space efficient ida has numerous applications to secure and reliable storage of information in computer networks and even on single disks to faulttolerant and efficient transmission of information in networks and to communications between processors in parallel computers for the latter problem provably timeefficient and highly faulttolerant routing on the ncube is achieved using just constant size buffers"
457 "a general framework is presented for rapidly analyzing tree networks" "a general framework is presented for rapidly analyzing tree networks to compute a measure of the centrality or eccentricity of all vertices in the tree several problems which have been previously described in the literature fit this framework some of these problems have no published solution better than performing a separate traversal for each vertex whose eccentricity is calculated the method presented in this paper performs just two traversals and yields the eccentricities of all vertices in the tree natural sufficient conditions for the algorithm to work in linear time on any given problem are stated"
458 "the prefix problem consists of computing all the products xx" "the prefix problem consists of computing all the products xx xj j n given a sequence x x x xn of elements in a semigroup in this paper we completely characterize the sizetime complexity of computing prefixes with boolean networks which are synchronized interconnections of boolean gates and onebit storage devices this complexity crucially depends upon two properties of the underlying semigroup which we call cyclefreedom no cycle of length greater than one in the cayley graph of the semigroup and memoryinduciveness arbitrarily long products of semigroup elements are true functions of all their factors a nontrivial characterization is given of nonmemoryinducive semigroups as those whose recurrent subsemigroup formed by the elements with selfloops in the cayley graph is the direct product of a leftzero semigroup and a rightzero semigroup denoting by s and t size and computation time respectively we have s thgrntlognt for memoryinducive noncyclefree semigroups and s thgrnt for all other semigroups we have t egr ohgrlog n ogrn for all semigroups with the exception of those whose recurrent subsemigroup is a rightzero semigroup for which t egr ohgr ogrn the preceding results are also extended to the vlsi model of computation areatime optimal circuits are obtained for both boundary and nonboundary i o protocols"
459 "inductive inference machines construct programs for total recursive functions given" "inductive inference machines construct programs for total recursive functions given only example values of the functions probabilistic inductive inference machines are defined and for various criteria of successful inference it is asked whether a probabilistic inductive inference machine can infer larger classes of functions if the inference criterion is relaxed to allow inference with probability at least p p as opposed to requiring certainty for the most basic criteria of success ex and bc it is shown that any class of functions that can be inferred from examples with probability exceeding can be inferred deterministically and that for probabilities p there is a discrete hierarchy of inferability parameterized by p the power of probabilistic inference strategies is characterized by equating the classes of probabilistically inferable functions with those classes that can be inferred by teams of inductive inference machines a parallel model of inference or by a third model called frequency inference"
460 "recently a new connection was discovered between the parallel complexity" "recently a new connection was discovered between the parallel complexity class nc and the theory of finite automata in the work of barrington on bounded width branching programs there nonuniform nc was characterized as those languages recognized by a certain nonuniform version of a dfa here we extend this characterization to show that the internal structures of nc and the class of automata are closely related in particular using thriens classification of finite monoids we give new characterizations of the classes ac depthk ac and acc the last being the ac closure of the mod q functions for all constant q we settle some of the open questions in give a new proof that the dotdepth hierarchy of algebraic automata theory is infinite and offer a new framework for understanding the internal structure of nc"
461 "the nature of programming languages that fail to have a" "the nature of programming languages that fail to have a relatively complete proof formalism is discussed first it is shown that such failures may be due to the meagerness of the programming language rather than to the presence of complex control structures as in the cases studied so far the failure of relative completeness is then derived for two languages with a rich control structure using simple simulations of general recursive functions by procedure call mechanisms"
462 "the computational complexity of learning boolean concepts from examples is" "the computational complexity of learning boolean concepts from examples is investigated it is shown for various classes of concept representations that these cannot be learned feasibly in a distributionfree sense unless r np these classes include a disjunctions of two monomials b boolean threshold functions and c boolean formulas in which each variable occurs at most once relationships between learning of heuristics and finding approximate solutions to nphard optimization problems are given"
463 "for any fixed k a remarkably simple singletape turing machine" "for any fixed k a remarkably simple singletape turing machine can simulate k independent counters in real time"
464 "this article presents a class of approximation algorithms that extend" "this article presents a class of approximation algorithms that extend the idea of boundedcomplexity inference inspired by successful constraint propagation algorithms to probabilistic inference and combinatorial optimization the idea is to bound the dimensionality of dependencies created by inference algorithms this yields a parameterized scheme called minibuckets that offers adjustable tradeoff between accuracy and efficiency the minibucket approach to optimization problems such as finding the most probable explanation mpe in bayesian networks generates both an approximate solution and bounds on the solution quality we present empirical results demonstrating successful performance of the proposed approximation scheme for the mpe task both on randomly generated problems and on realistic domains such as medical diagnosis and probabilistic decoding"
465 "we prove the first timespace lower bound tradeoffs for randomized" "we prove the first timespace lower bound tradeoffs for randomized computation of decision problems the bounds hold even in the case that the computation is allowed to have arbitrary probability of error on a small fraction of inputs our techniques are extension of those used by ajtai and by beame jayram and saks that applied to deterministic branching programs our results also give a quantitative improvement over the previous resultsprevious timespace tradeoff results for decision problems can be divided naturally into results for functions with boolean domain that is each input variable is lcubrcubvalued and the case of large domain where each input variable takes on values from a set whose size grows with the number of variablesin the case of boolean domain ajtai exhibited an explicit class of functions and proved that any deterministic boolean branching program or ram using space s on requires superlinear time t to compute them the functional form of the superlinear bound is not given in his paper but optimizing the parameters in his arguments gives t n log log nlog log log n for s on epsis for the same functions considered by ajtai we prove a timespace tradeoff for randomized branching programs with error of the form t n sqrt lognslog log ns in particular for space onepsis this improves the lower bound on time to nsqrt log nlog log nin the large domain case we prove lower bounds of the form t nsqrt logns log log ns for randomized computation of the element distinctness function and lower bounds of the form t n log ns for randomized computation of ajtais hamming closeness problem and of certain functions associated with quadratic forms over large fields"
466 "we present the first complete problem for szk the class" "we present the first complete problem for szk the class of promise problems possessing statistical zeroknowledge proofs against an honest verifier the problem called statistical difference is to decide whether two efficiently samplable distributions are either statistically close or far apart this gives a new characterization of szk that makes no reference to interaction or zero knowledgewe propose the use of complete problems to unify and extend the study of statistical zero knowledge to this end we examine several consequences of our completeness theorem and its proof such asa way to make every honestverifier statistical zeroknowledge proof very communication efficient with the prover sending only one bit to the verifier to achieve soundness error simpler proofs of many of the previously known results about statistical zero knowledge such as the fortnow and aiellohnumstad upper bounds on the complexity of szk and okamotos result that szk is closed under complementstrong closure properties of szk that amount to constructing statistical zeroknowledge proofs for complex assertions built out of simpler assertions already shown to be in szknew results about the various measures of knowledge complexity including a collapse in the hierarchy corresponding to knowledge complexity in the hint sensealgorithms for manipulating the statistical difference between efficiently samplable distributions including transformations that polarize and reverse the statistical relationship between a pair of distributions"
467 "this article provides necessary and sufficient conditions for deadlockfree unicast" "this article provides necessary and sufficient conditions for deadlockfree unicast and multicast routing with the pathbased routing model in interconnection networks that use the wormhole switching technique the theory is developed around three central concepts channel waiting false resource cycles and valid destination sets the first two concepts are suitable extensions to those developed for unicast routing by two authors of this article the third concept has been developed by lin and ni the necessary and sufficient conditions relax the requirements for deadlockfree routing compared to techniques that provide only a sufficient condition these necessary and sufficient conditions can be applied in a straightforward manner to prove deadlock freedom of newly developed adaptive routing algorithms for collective communication which in turn will help in developing efficient and correct routing algorithms the latter point is illustrated by developing two routing algorithms for multicast communication on d mesh architectures the first algorithm uses fewer resources channels than an algorithm proposed in the literature but achieves the same adaptiveness the second algorithm provides fully adaptive routing for both unicast and multicast messages"
468 "the main result of this paper is an v x" "the main result of this paper is an v x e time algorithm for deciding whether a given graph is a circle graph that is the intersection graph of a set of chords on a circle the algorithm utilizes two new graphtheoretic results regarding necessary induced subgraphs of graphs having neither articulation points nor similar pairs of vertices furthermore as a substep of the algorithm it is shown how to find in v x e time a decomposition of a graph into prime graphs thereby improving on a result of cunningham"
469 "using hierarchical definitions one can describe very large graphs in" "using hierarchical definitions one can describe very large graphs in small space the blowup from the length of the hierarchical description to the size of the graph can be as large as exponential if the efficiency of graph algorithms is measured in terms of the length of the hierarchical description rather than in terms of the graph size algorithms that do not exploit the hierarchy become hopelessly inefficient whether the hierarchy can be exploited to speed up the solution of graph problems depends on the hierarchical graph model in the literature hierarchical graph models have been described that allow almost no exploitation of the hierarchy in this paper a hierarchical graph model that permits taking advantage of the hierarchy is presented for this model algorithms are given that test planarity of a hierarchically described graph in linear time in the length of the hierarchical description"
470 "two conflicting goals play a crucial role in the design" "two conflicting goals play a crucial role in the design of routing schemes for communication networks a routing scheme should use paths that are as short as possible for routing messages in the network while keeping the routing information stored in the processors local memory as succinct as possible the efficiency of a routing scheme is measured in terms of its stretch factorthe maximum ratio between the length of a route computed by the scheme and that of a shortest path connecting the same pair of vertices most previous work has concentrated on finding good routing schemes with a small fixed stretch factor for special classes of network topologies in this paper the problem for general networks is studied and the entire range of possible stretch factors is examined the results exhibit a tradeoff between the efficiency of a routing scheme and its space requirements almost tight upper and lower bounds for this tradeoff are presented specifically it is proved that any routing scheme for general nvertex networks that achieves a stretch factor k must use a total of ohgrnk bits of routing information in the networks this lower bound is complemented by a family kk of hierarchical routing schemes for every k l for unitcost general networks which guarantee a stretch factor of ok require storing a total of oknhlogn bits of routing information in the network name the vertices with ologn bit names and use olognbit headers"
471 "the component merging problem is a new graph problem versions" "the component merging problem is a new graph problem versions of this problem appear as bottlenecks in various graph algorithms a new data structure solves this problem efficiently and two special cases of the problem have even more efficient solutions based on other data structures the performance of the data structures is sped up by introducing a new algorithmic tool called packets the algorithms that use these solutions to the component merging problem also exploit new properties of two existing data structures specifically bgrtrees can be used simultaneously as a priority queue and a concatenable queue similarly fheaps support some kinds of split operations with no loss of efficiency an immediate application of the solution to the simplest version of the merging problem is an ogrtm n algorithm for finding minimum spanning trees in undirected graphs without using fheaps where tm n mlogloglogdn the graph has n vertices and m edges and d maxmn packets also improve the fheap minimum spanning tree algorithm giving the fastest algorithm currently known for this problem the efficient solutions to the merging problem and the new observation about fheaps lead to an ogrnt m n nlogn algorithm for finding a maximum weighted matching in general graphs this settles an open problem posed by tarjan p where the weaker bound of onm log nm was conjectured"
472 "binary search trees with costs agr and bgr respectively on" "binary search trees with costs agr and bgr respectively on the left and right edges lopsided search trees are considered the exact shape minimum worstcase cost and minimum average cost of lopsided trees of n internal nodes are determined for nonnegative agr and bgr the costs are both roughly logpn where p is the unique real number in the interval satisfying pagr pbgr search procedures are given that come within a small additive constant of the lower bounds almostoptimum algorithms for the lopsided case of unbounded searching are also obtained some extensions to nonconstant costs are briefly sketched"
473 "using simple protocols it is shown how to achieve consensus" "using simple protocols it is shown how to achieve consensus in constant expected time within a variety of failstop and omission failure models significantly the strongest models considered are completely asynchronous all of the results are based on distributively flipping a coin which is usable by a significant majority of the processors finally a nearly matching lower bound is also given for randomized protocols for consensus"
474 "in this paper the class of acyclic forkjoin queuing networks" "in this paper the class of acyclic forkjoin queuing networks that arise in various applications including parallel processing and flexible manufacturing are studied in such queuing networks a fork describes the simultaneous creation of several new customers which are sent to different queues the corresponding join occurs when the services of all these new customers are completed the evolution equations that govern the behavior of such networks are derived from this the stability conditions are obtained and upper and lower bounds on the network response times are developed these bounds are based on various stochastic ordering principles and on the notion of association of random variables"
475 "optimal ohgrlog nlog log n lower bounds on the time" "optimal ohgrlog nlog log n lower bounds on the time for crcw prams with polynomially bounded numbers of processors or memory cells to compute parity and a number of related problems are proven a strict time hierarchy of explicit boolean functions of n bits on such machines that holds up to ogrlog nlog log n time is also exhibited that is for every time bound t within this range a function is exhibited that can be easily computed using polynomial resources in time t but requires more than polynomial resources to be computed in time t finally it is shown that almost all boolean functions of n bits require log n log log n ohgr time when the number of processors is at most polynomial in n the bounds do not place restrictions on the uniformity of the algorithms nor on the instruction sets of the machines"
476 "lower bounds are proven on the paralleltime complexity of several" "lower bounds are proven on the paralleltime complexity of several basic functions on the most powerful concurrentread concurrentwrite pram with unlimited shared memory and unlimited power of individual processors denoted by priority it is proved that with a number of processors polynomial in n ohgr log n time is needed for addition multiplication or bitwise or of n numbers when each number has n bits hence even the bit complexity ie the time complexity as a function of the total number of bits in the input is logarithmic in this case this improves a beautiful result of meyer auf der heide and wigderson they proved a log n lower bound using ramseytype techniques using ramsey theory it is possible to get an upper bound on the number of bits in the inputs used however for the case of polynomially many processors this upper bound is more than a polynomial in n an ohgr log n lower bound is given for priority with no processors on a function with inputs from namely for the function fx xn sgr nl xlai where a is fixed and xi egr finally by a new efficient simulation of priority by unbounded fanin circuits that with less than exponential number of processors it is proven a priority cannot compute parity in constant time and with no processors ohgrlog n time is needed the simulation technique is of independent interest since it can serve as a general tool to translate circuit lower bounds into pram lower bounds further the lower bounds in and remain valid for probabilistic or nondeterministic concurrentread concurrentwrite prams"
477 "a randomized algorithm that sorts on an n node network" "a randomized algorithm that sorts on an n node network with constant valence in olog n time is given more particularly the algorithm sorts n items on an nnode cubeconnected cycles graph and for some constant k for all large enough agr it terminates within kagr log n time with probability at least nagr"
478 "reaching agreement is a primitive of distributed computing whereas this" "reaching agreement is a primitive of distributed computing whereas this poses no problem in an ideal failurefree environment it imposes certain constraints on the capabilities of an actual system a system is viable only if it permits the existence of consensus protocols tolerant to some number of failures fischer et al have shown that in a completely asynchronous model even one failure cannot be tolerated in this paper their work is extended several critical system parameters including various synchrony conditions are identified and how varying these affects the number of faults that can be tolerated is examined the proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others"
479 "the direct sum of two term rewriting systems is the" "the direct sum of two term rewriting systems is the union of systems having disjoint sets of function symbols it is shown that if two term rewriting systems both have the chruchrosser property then the direct sum of these systems also has this property"
480 "the problem of scheduling a set of n jobs on" "the problem of scheduling a set of n jobs on m identical machines so as to minimize the makespan time is perhaps the most wellstudied problem in the theory of approximation algorithms for nphard optimization problems in this paper the strongest possible type of result for this problem a polynomial approximation scheme is presented more precisely for each egr an algorithm that runs in time onegregr and has relative error at most egr is given in addition more practical algorithms for egr k and egr k which have running times onk log n and onkm log n are presented the techniques of analysis used in proving these results are extremely simple especially in comparison with the baroque weighting techniques used previously the scheme is based on a new approach to constructing approximation algorithms which is called dual approximation algorithms where the aim is to find superoptimal but infeasible solutions and the performance is measured by the degree of infeasibility allowed this notion should find wide applicability in its own right and should be considered for any optimization problem where traditional approximation algorithms have been particularly elusive"
481 "todays concomitant needs for higher computing power and reliability has" "todays concomitant needs for higher computing power and reliability has increased the relevance of multipleprocessor faulttolerant systems multiple functional units improve the raw performance throughput response time etc of the system and as units fail the system may continue to function albeit with degraded performance such systems and other faulttolerant systems are not adequately characterized by separate performance and reliability measures a composite measure for the performance and reliability of a faulttolerant system observed over a finite mission time is analyzed a markov chain model is used for system statespace representation and transient analysis is performed to obtain closedform solutions for the density and moments of the composite measure only failures that cannot be repaired until the end of the mission are modeled the time spent in a specific system configuration is assumed to be large enough to permit the use of a hierarchical model and static measures to quantify the performance of the system in individual configurations for a multipleprocessor system where performance measures are usually associated with and aggregated over many jobs this is tantamount to assuming that the time to process a job is much smaller than the time between failures an extension of the results to general acyclic markov chain models is included"
482 "megiddo introduced a technique for using a parallel algorithm for" "megiddo introduced a technique for using a parallel algorithm for one problem to construct an efficient serial algorithm for a second problem this paper provides a general method that trims a factor of olog n time or more for many applications of this technique"
483 "many realworld applications involve the management of large amounts of" "many realworld applications involve the management of large amounts of timedependent information temporal database systems maintain this information in order to support various sorts of inference eg answering questions involving propositions that are true over some intervals and false over others for any given proposition there are typically many different occasions on which that proposition becomes true and persists for some length of time in this paper these occasions are referred to as time tokens many routine database operations must search through the database for time tokens satisfying certain temporal constraints to expedite these operations this paper describes a set of techniques for organizing temporal information by exploiting the local and global structure inherent in a wide class of temporal reasoning problems the global structure of time is exemplified in conventions for partitioning time according to the calendar and the clock this global structure is used to partition the set of time tokens to facilitate retrieval the local structure of time is exemplified in the causal relationships between events and the dependencies between planned activities this local structure is used as part of a strategy for reducing the computation required during constraint propagation the organizational techniques described in this paper are quite general and have been used to support a variety of powerful inference mechanisms integrating these techniques into an existing temporal database system has increased by an order of magnitude or more in most applications the number of time tokens that can be efficiently"
484 "to construct a short tour through points in the plane" "to construct a short tour through points in the plane the points are sequenced as they appear along a spacefilling curve this heuristic consists essentially of sorting so it is easily coded and requires only o n memory and on log n operations its performance is competitive with that of other fast methods"
485 "a periodic sorting network consists of a sequence of identical" "a periodic sorting network consists of a sequence of identical blocks in this paper the periodic balanced sorting network which consists of log n blocks is introduced each block called a balanced merging block merges elements on the even input lines with those on the odd input lines the periodic balanced sorting network sorts n items in olog n time using nlog n comparators although these bounds are comparable to many existing sorting networks the periodic structure enables a hardware implementation consisting of only one block with the output of the block recycled back as input until the output is sorted an implementation of our network on the shuffle exchange interconnection model in which the direction of the comparators are all identical and fixed is also presented"
486 "an operational approach to database specification is proposed and investigated" "an operational approach to database specification is proposed and investigated valid database states are described as the states resulting from the application of admissible transactions specified by a transactional schema the approach is similar in spirit to the modeling of behavior by methods and encapsulation in objectoriented systems the transactions considered are line programs consisting of insertions deletions and modifications using simple selection conditions the results concern basic properties of transactional schemas as well as the connection with traditional constraint schemas in particular the expressive power of transactional schemas is characterized although it is shown that transactionbased specification and constraintbased specification are incomparable constraints of practical interest that have corresponding transactional schemas are identified the preservation of constraints by transactions is also studied"
487 "the algebras and query languages for nested relations defined thus" "the algebras and query languages for nested relations defined thus far do not allow us to flatten a relation scheme by disregarding the internal representation of data in real life however the degree in which the structure of certain information such as addresses phone numbers etc is taken into account depends on the particular application and may even vary in time therefore an algebra is proposed that does allow us to simplify relations by disregarding the internal structure of a certain class of information this algebra is based on a careful manipulation of attribute names furthermore the key operator in this algebra called copying allows us to deal with various other common queries in a very uniform manner provided these queries are interpreted as operations on classes of semantically equivalent relations rather than individual relations finally it is shown that the proposed algebra is complete in the sense of bancilhon and paredaens"
488 "the design of systolic algorithms is discussed that is algorithms" "the design of systolic algorithms is discussed that is algorithms that may efficiently be executed by a synchronous array of cells that perform local communications only systolic algorithms are designed through techniques developed in the context of sequential programming heuristics are given that guide the programmer in the design of a variety of efficient solutions"
489 "theoretical results are presented on multipass both lefttoright and alternating" "theoretical results are presented on multipass both lefttoright and alternating multisweep and multivisit attribute grammars for each of these a pure type and a simple type are distinguished the pure attribute grammars are defined by nondeterministic attribute evaluators and the simple ones by the corresponding usual deterministic evaluators the time complexity of deciding membership in these classes of attribute grammars is studied in general this is harder for the pure classes than for the simple ones for which it is either polynomial or npcomplete the expressive power of the eight classes is compared by studying the translations they can compute it is shown that sweeps are more powerful than passes and visits are more powerful than sweeps"
490 "the conjecture of fliess concerning commutative contextfree languages is disproved" "the conjecture of fliess concerning commutative contextfree languages is disproved using a counterexample"
491 "a classical algorithm for finding a minimumcost circulation consists of" "a classical algorithm for finding a minimumcost circulation consists of repeatedly finding a residual cycle of negative cost and canceling it by pushing enough flow around the cycle to saturate an arc we show that a judicious choice of cycles for canceling leads to a polynomial bound on the number of iterations in this algorithm this gives a very simple strongly polynomial algorithm that uses no scaling a variant of the algorithm that uses dynamic trees runs in ogrnmlog nminlognc m log n time on a network of n vertices m arcs and arc costs of maximum absolute value c this bound is comparable to those of the fastest previously known algorithms"
492 "a new equivalence between concurrent processes is proposed it generalizes" "a new equivalence between concurrent processes is proposed it generalizes the wellknown bisimulation equivalence to take into account the distributed nature of processes the result is a noninterleaving semantic theory concurrent processes are differentiated from processes that are nondeterministic but sequential the new equivalence together with its observational version is investigated for a subset of the language ccs and various algebraic characterizations are obtained"
493 "much complexitytheoretic work on parallelism has focused on the class" "much complexitytheoretic work on parallelism has focused on the class nc which is defined in terms of logspaceuniform circuits yet puniform circuit complexity is in some ways a more natural setting for studying feasible parallelism in this paper puniform nc punc is characterized in terms of spacebounded auxpdas and alternating turing machines with bounded access to the input the notions of generalpurpose and specialpurpose computation are considered and a generalpurpose parallel computer for punc is presented it is also shown that nc punc if all tally languages in p are in nc this implies that the nc punc question and the nc p question are both instances of the aspacesn aspacetimesn sno question as a corollary it follows that nc punc implies pspace dtimeno"
494 "valiants learnability model is extended to learning classes of concepts" "valiants learnability model is extended to learning classes of concepts defined by regions in euclidean space en the methods in this paper lead to a unified treatment of some of valiants results along with previous results on distributionfree convergence of certain pattern recognition algorithms it is shown that the essential condition for distributionfree learnability is finiteness of the vapnikchervonenkis dimension a simple combinatorial parameter of the class of concepts to be learned using this parameter the complexity and closure properties of learnable classes are analyzed and the necessary and sufficient conditions are provided for feasible learnability"
495 "we present a new approach to inference in bayesian networks" "we present a new approach to inference in bayesian networks which is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial the network polynomial itself is exponential in size but we show how it can be computed efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size the proposed framework for inference subsumes one of the most influential methods for inference in bayesian networks known as the treeclustering or jointree method which provides a deeper understanding of this classical method and lifts its desirable characteristics to a much more general setting we discuss some theoretical and practical implications of this subsumption"
496 "let hn be the height of a random binary search" "let hn be the height of a random binary search tree on n nodes we show that there exist constants and such that ehn ln n ln ln n o we also show that varhn o"
497 "it is shown that all centralized absolute moments everbarhn ehnverbar" "it is shown that all centralized absolute moments everbarhn ehnverbar of the height hn of binary search trees of size n and of the saturation level hn are bounded the methods used rely on the analysis of a retarded differential equation of the form u u with the method can also be extended to prove the same result for the height of mary search trees finally the limiting behaviour of the distribution of the height of binary search trees is precisely determined"
498 "the static single assignment ssa form is a program representation" "the static single assignment ssa form is a program representation used in many optimizing compilers the key step in converting a program to ssa form is called placement many algorithms for placement have been proposed in the literature but the relationships between these algorithms are not well understoodin this article we propose a framework within which we systematically derive i properties of the ssa form and ii placement algorithms this framework is based on a new relation called merge which captures succinctly the structure of a programs control flow graph that is relevant to its ssa form the placement algorithms we derive include most of the ones described in the literature as well as several new ones we also evaluate experimentally the performance of some of these algorithms on the spec benchmarkssome of the algorithms described here are optimal for a single variable however their repeated application is not necessarily optimal for multiple variables we conclude the article by describing such an optimal algorithm based on the transitive reduction of the merge relation for multivariable placement in structured programs the problem for general programs remains open"
499 "a new probabilistic failure model for networks of gates is" "a new probabilistic failure model for networks of gates is formulated although this model has not been used previously it supports the proofs of both the positive and negative results appearing in the literature furthermore with respect to this new model the complexity measures of both size and depth are affected by at most constant multiplicative factors when the set of functions that can be computed by gates is changed from one finite and complete basis to another or when the bound on the failure probability of the gates is changed within the limits allowed by the basis or when the bound on the error probability of the network is changed within the limits allowed by the basis and the failure probability of the gates"
500 "jazayeri j acm oct proposes a simpler construction for use" "jazayeri j acm oct proposes a simpler construction for use in the proof by jazayeri et al commun acm dec that the circularity problem for attribute grammars has inherent exponential time complexity the simplification introduces a flaw that invalidates the proof the flaw can be corrected at the cost of eliminating some of the simplification claimed for the new construction"
501 "pearl has shown that in admissible a treesearching the expected" "pearl has shown that in admissible a treesearching the expected number of nodes expanded is bounded above and below by exponential functions of heuristic error an additional assumption required for the validity of pearls argument is given the assumptions significance and interpretation are discussed"
503 "efficient implementations of dijkstras shortest path algorithm are investigated a" "efficient implementations of dijkstras shortest path algorithm are investigated a new data structure called the radix heap is proposed for use in this algorithm on a network with n vertices m edges and nonnegative integer arc costs bounded by c a onelevel form of radix heap gives a time bound for dijkstras algorithm of om n log c a twolevel form of radix heap gives a bound of om n log clog log c a combination of a radix heap and a previously known data structure called a fibonacci heap gives a bound of om na log c the best previously known bounds are om n log n using fibonacci heaps alone and om log log c using the priority queue structure of van emde boas et al"
504 "a replicated data object is a typed object that is" "a replicated data object is a typed object that is stored redundantly at multiple locations in a distributed system each of the objects operations has a set of quorums which are sets of sites whose cooperation is needed to execute that operation a quorum assignment associates each operation with its set of quorums an operations quorums determine its availability and the constraints governing an objects quorum assignments determine the range of availability properties realizable by replication in this paper the restrictions on quorum assignment imposed by three kinds of atomicity mechanisms found in the literature are analyzed serial schemes in which replication and atomicity are implemented independently at different levels in the system static schemes in which the transaction serialization order is predetermined and hybrid schemes in which the serialization order emerges dynamically the following results are derived although serial schemes place the strongest restrictions on concurrency they place the weakest restrictions on availability although hybrid and static mechanisms place incomparable restrictions on concurrency hybrid mechanisms place weaker restrictions on availability bounding the maximum depth of transaction nesting strengthens restrictions on concurrency for all classes but weakens restrictions on availability for hybrid schemes only concurrency and availability are best considered as dual properties a complete analysis of an atomicity mechanism should take both into account"
505 "this paper presents a proof system for firstorder temporal logic" "this paper presents a proof system for firstorder temporal logic the system extends the nonclausal resolution method for ordinary firstorder logic with equality to handle quantifiers and temporal operators soundness and completeness issues are considered the use of the system for verifying concurrent programs is discussed and variants of the system for other modal logics are also described"
506 "this paper concerns the message complexity of broadcast in arbitrary" "this paper concerns the message complexity of broadcast in arbitrary pointtopoint communication networks broadcast is a task initiated by a single processor that wishes to convey a message to all processors in the network the widely accepted model of communication networks in which each processor initially knows the identity of its neighbors but does not know the entire network topology is assumed although it seems obvious that the number of messages required for broadcast in this model equals the number of links no proof of this basic fact has been given before it is shown that the message complexity of broadcast depends on the exact complexity measure if messages of unbounded length are counted at unit cost then broadcast requires thgruharlvuharr messages where v is the set of processors in the network it is proved that if one counts messages of bounded length then broadcast requires thgruharleuharr messages where e is the set of edges in the network assuming an intermediate model in which each vertex knows the topology of the network in radius rgr from itself matching upper and lower bounds of thgrminuharleuharr uharlvuharrthgrlrgr is proved on the number of messages of bounded length required for broadcast both the upper and lower bounds hold for both synchronous and asynchronous network models the same results hold for the construction of spanning trees and various other global tasks"
508 "the maximum concurrent flow problem mcfp is a multicommodity flow" "the maximum concurrent flow problem mcfp is a multicommodity flow problem in which every pair of entities can send and receive flow concurrently the ratio of the flow supplied between a pair of entities to the predefined demand for that pair is called throughput and must be the same for all pairs of entities for a concurrent flow the mcfp objective is to maximize the throughput subject to the capacity constraints we develop a fully polynomialtime approximation scheme for the mcfp for the case of arbitrary demands and uniform capacity computational results are presented it is shown that the problem of associating costs distances to the edges so as to maximize the minimumcost of routing the concurrent flow is the dual of the mcfp a pathcut type duality theorem to expose the combinatorial structure of the mcfp is also derived our duality theorems are proved constructively for arbitrary demands and uniform capacity using the algorithm applications include packetswitched networks and cluster analysis"
509 "an axiomatic algebraic calculus of modules is given that is" "an axiomatic algebraic calculus of modules is given that is based on the operators combinationunion export renaming and taking the visible signature four different models of module algebra are discussed and compared"
510 "the new class of queuing models called synchronized queuing networks" "the new class of queuing models called synchronized queuing networks is proposed for evaluating the performance of multiprogrammed and multitasked multiprocessor systems where workloads consists of parallel programs of similar structure and where the scheduling discipline is firstcomefirstserve pathwise evolution equations are established for these networks that capture the effects of competition for processors and the precedence constraints governing tasks executions a general expression is deduced for the stability condition of such queuing networks under general statistical assumptions basically the stationarity and the ergodicity of input sequences which yields the maximum program throughput of the multiprocessor system or equivalently the maximum rate at which programs can be executed or submitted the proof is based on the ergodic theory of queues basic integral equations are also derived for the stationary distribution of important performance criteria such as the workload of the queues and program response times an iterative numerical schema that converges to this solution is proposed and various upper and lower bounds on moments are derived using stochastic ordering techniques"
511 "the probabilistic polynomialtime hierarchy bph is the hierarchy generated by" "the probabilistic polynomialtime hierarchy bph is the hierarchy generated by applying the bpoperator to the meyerstockmeyer polynomialtime hierarchy ph where the bpoperator is the natural generalization of the probabilistic complexity class bpp the similarity and difference between the two hierarchies bph and ph is investigated oracles a and b are constructed such that both pha and phb are infinite while bpha is not equal to pha at any level and bphb is identical to phb at every level similar separating and collapsing results in the case that pha is finite having exactly k levels are also considered"
512 "the effects of circumscribing firstorder formulas are explored from a" "the effects of circumscribing firstorder formulas are explored from a computational standpoint first extending work of v lifschitz it is shown that the circumscription of any existential firstorder formula is equivalent to a firstorder formula after this it is established that a set of universal horn clauses has a firstorder circumscription if and only if it is bounded when considered as a logic program thus it is undecidable to tell whether such formulas have firstorder circumscription finally it is shown that there arefirstorder formulas whode circumscription has a conpcomplete modelchecking problem"
513 "unary inclusion dependencies are database constraints expressing subset relationships the" "unary inclusion dependencies are database constraints expressing subset relationships the decidability of implication for these dependencies together with embedded implicational dependencies such as functional dependencies are investigated as shown by casanova et al the unrestricted and finite implication problems are different for the class of functional and unary inclusion dependencies also for this class and for any fixed k finite implication has no kary complete axiomatization for both of these problems complete axiomatizations and polynomialtime decision procedures are provided linear time for unrestricted implication and cubic time for finite implication it follows that functional and unary inclusion dependencies form a semantically natural class of firstorder sentences with equality which although not finitely controllable is efficiently solvable and docile generalizing from these results it is shown that the interaction between functional and inclusion dependencies characterizes unrestricted implication of unary inclusion and all embedded implicational dependencies finite implication of unary inclusion and all full implicational dependencies finite implication of unary inclusion and all embedded tuplegenerating dependencies as a direct consequence of this analysis most of the applications of dependency implication are extended within polynomialtime to database design problems involving unary inclusion dependencies such examples are tests for lossless joins and tests for complementarity of projective views finally if one additionally requires that"
514 "the fundamental satisfiability problem for word equations has been solved" "the fundamental satisfiability problem for word equations has been solved recently by makanin however this algorithm is purely a decision algorithm the main result of this paper solves the complementary problem of generating the set of all solutions specifically the algorithm in this paper generates given a word equation a minimal and complete set of unifiers it stops if this set is finite"
515 "this paper treats languages whose operational semantics is given by" "this paper treats languages whose operational semantics is given by a set of rewrite rules for such languages it is important to be able to determine that there are enough rules to be able to compute the correct meaning of all expressions but not so many that the system of rules is inconsistent a formal framework is developed in which to give a precise treatment of these completeness and soundness issues which are then investigated in the context of an extended version of the functional programming language fp the rewrite rules of fp are shown to be sound and complete with respect to three different notions of completeness the latter half of the paper considers rewrite strategies in order to implement a language based on rewrite rules it does not suffice to know that there are enough rules in the language a good strategy for determining the order in which to apply them is also needed but what is good corresponding to each notion of completeness there is a notion of a good rewrite strategy these notions of goodness are examined and characterized and examples of a number of natural good strategies are given although these results are presented in the context of fp the techniques some of which are nontrivial extensions of techniques first used in the context of lgrcalculus should apply well beyond the realm of fp rewriting systems"
516 "in this paper a new asymptotic method is developed for" "in this paper a new asymptotic method is developed for analyzing closed bcmp queuing networks with a single class chain consisting of a large number of customers a single infinite server queue and a large number of single server queues with fixed stateindependent service rates asymptotic approximations are computed for the normalization constant partition function starting directly from a recursion relation of buzen the approach of the authors employs the ray method of geometrical optics and the method of matched asymptotic expansions the method is applicable when the servers have nearly equal relative utilizations or can be divided into classes with nearly equal relative utilizations numerical comparisons are given that illustrate the accuracy of the asymptotic approximations"
517 "a semantic or model theoretic approach is proposed to study" "a semantic or model theoretic approach is proposed to study the problems p np and np conp this approach seems to avoid the difficulties that recursiontheoretic approaches appear to face in view of the result of baker et al on relativizations of the p np question moreover semantical methods are often simpler and more powerful than syntactical ones the connection between the existence of certain partial extensions of nonstandard models of arithmetic and the question np conp is discussed several problems are stated about nonstandard models and a possible link between the davismatijaseviputnamrobinson theorem on diophantine sets and the np conp question is mentioned"
518 "lower bounds on the complexity of orthogonal range searching in" "lower bounds on the complexity of orthogonal range searching in the static case are established specifically we consider the following dominance search problem given a collection of n weighted points in dspace and a query point q compute the cumulative weight of the points dominated in all coordinates by q it is assumed that the weights are chosen in a commutative semigroup and that the query time measures only the number of arithmetic operations needed to compute the answer it is proved that if m units of storage are available then the query time is at least proportional to log nlogmnd in both the worst and average cases this lower bound is provably tight for m ohgrnlog n degr and any fixed egr a lower bound of ohgrnlog log nd on the time required for executing n inserts and queries is also established authors"
519 "an onltime algorithm is introduced for constructing an optimal huffman" "an onltime algorithm is introduced for constructing an optimal huffman code for a weighted alphabet of size n where each code string must have length no greater than l the algorithm uses on space"
520 "the question is a given join dependency equivalent to some" "the question is a given join dependency equivalent to some set of multivalued dependencies led to the development of acyclicity theory the central question of this paper is is a given equalitygenerating dependency equivalent to a set of functional dependencies an algorithm is presented that answers that question in polynomial time without using the chase process and in the case of a yes answer can be used to find a cover of the set of functional dependencies involved this question is also related to the similar question about join dependencies and multivalued dependencies by proving a result about the hypergraph representation of an egd it is interesting to note that a minimal representation of an egd must be bgracyclic for the egd to be equivalent to a set of fds in contrast to the jdmvd case in which only agr acyclicity is needed the bgracyclicity of an egd not necessarily minimal is always sufficient for the egd to be equivalent to a set of fds as shown finally the algorithm is extended for a single egd to answer the question whether a set of egds with the same righthandside column is equivalent to a set of fds"
521 "a detailed model of a transaction processing system with dynamic" "a detailed model of a transaction processing system with dynamic locking is developed and analyzed transaction classes are distinguished on the basis of the number of data items accessed and the access mode readonly update the performance of the system is affected by transaction blocking and restarts due to lock conflicts that do not or do cause deadlocks respectively the probability of these events is determined by the characteristics of transactions and the database access pattern hardware resource contention due to concurrent transaction processing is taken into account by specifying the throughput characteristic of the computer system for processing transactions when there is no data contention a solution method based on decomposition is developed to analyze the system and also used as the basis of an iterative scheme with reduced computational cost the analysis to estimate the probability of lock conflicts and deadlocks is based on the mean number of locks held by transactions these probabilities are used to derive the state transition probabilities for the markov chain specifying the transitions among the system states the decomposition solution method and the associated iterative scheme are shown to be more accurate than previously defined methods for dynamic locking through validation against simulation results several important conclusions regarding the behavior of dynamic locking systems are derived from parametric studies"
522 "this paper is concerned with the solvability of the problem" "this paper is concerned with the solvability of the problem of processor renaming in unreliable completely asynchronous distributed systems fischer et al prove in that nontrivial consensus cannot be attained in such systems even when only a single benign processor failure is possible in contrast this paper shows that problems of processor renaming can be solved even in the presence of up to t n faulty processors contradicting the widely held belief that no nontrivial problem can be solved in such a system the problems deal with renaming processors so as to reduce the size of the initial name space when only uniqueness of the new names is required we present a lower bound of n on the size of the new name space and a renaming algorithm that establishes an upper bound on n t if the new names are required also to preserve the original order a tight bound of n t is obtained"
523 "reasoning about knowledge seems to play a fundamental role in" "reasoning about knowledge seems to play a fundamental role in distributed systems indeed such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols communication in a distributed system can be viewed as the act of transforming the systems state of knowledge this paper presents a general framework for formalizing and reasoning about knowledge in distributed systems it is shown that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols in particular distributed knowledge corresponds to knowledge that is distributed among the members of the group while common knowledge corresponds to a fact being publicly known the relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated furthermore it is shown that formally speaking in practical systems common knowledge cannot be attained a number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated"
524 "many problems in the area of symbolic computing can be" "many problems in the area of symbolic computing can be solved by iterative algorithms implementations of these algorithms on multiprocessors can be synchronous or asynchronous asynchronous implementations are potentially more efficient because synchronization is a major source of performance degradation in most multiprocessor systems in this paper sufficient conditions for the convergence of asynchronous iterations to desired solutions are given the main sufficient condition is shown to be also necessary for the case of finite data domains the results are applied to prove the convergence of three asynchronous algorithms for the allpairs shortest path problem the consistent labeling problem and a neural net model"
525 "a simple extension of the relational model is introduced to" "a simple extension of the relational model is introduced to study the effects of dynamic constraints on database evolution both static and dynamic constraints are used in conjunction with the model the static constraints considered here are functional dependencies fds the dynamic constraints involve global updates and are restricted to certain analogs of fds called dynamic fds the results concern the effect of the dynamic constraints on the static constraints satisfied by the database in the course of time the effect of the past history of the database on the static constraints is investigated using the notions of age and age closure the connection between the static constraints and the potential future evolution of the database is briefly discussed using the notions of survivability and survivability closure"
526 "the problem of electing a leader in a synchronous ring" "the problem of electing a leader in a synchronous ring of n processors is considered both positive and negative results are obtained on the one hand if processor ids are chosen from some countable set then there is an algorithm that uses only on messages in the worst case on the other hand any algorithm that is restricted to use only comparisons of ids requires ohgrn log n messages in the worst case alternatively if the number of rounds is required to be bounded by some t in the worst case and ids are chosen from any set having at least fn t elements for a certain very fastgrowing function f then any algorithm requires ohgrn log n messages in the worst case"
527 "we give a simple and new randomized primality testing algorithm" "we give a simple and new randomized primality testing algorithm by reducing primality testing for number n to testing if a specific univariate identity over zn holdswe also give new randomized algorithms for testing if a multivariate polynomial over a finite field or over rationals is identically zero the first of these algorithms also works over zn for any n the running time of the algorithms is polynomial in the size of arithmetic circuit representing the input polynomial and the error parameter these algorithms use fewer random bits and work for a larger class of polynomials than all the previously known methods for example the schwartzzippel test schwartz zippel chenkao and lewinvadhan tests chen and kao lewin and vadhan"
528 "this article introduces the sieve a novel building block that" "this article introduces the sieve a novel building block that allows to adapt to the number of simultaneously active processes the point contention during the execution of an operation we present an implementation of the sieve in which each sieve operation requires ok log k steps where k is the point contention during the operationthe sieve is the cornerstone of the first waitfree algorithms that adapt to point contention using only read and write operations specifically we present efficient algorithms for longlived renaming timestamping and collecting information"
529 "the theory of generalized functions is the foundation of the" "the theory of generalized functions is the foundation of the modern theory of partial differential equations pde as computers are playing an everlarger role in solving pdes it is important to know those operations involving generalized functions in analysis and pde that can be computed on digital computers in this article we introduce natural concepts of computability on test functions and generalized functions as well as computability on schwartz test functions and tempered distributions type turing machines are used as the machine model weihrauch it is shown here that differentiation and integration on distributions are computable operators and various types of fourier transforms and convolutions are also computable operators as an application it is shown that the solution operator of the distributional inhomogeneous three dimensional wave equation is computable"
530 "we describe a slightly subexponential time algorithm for learning parity" "we describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise a problem closely related to several cryptographic and coding problems our algorithm runs in polynomial time for the case of parity functions that depend on only the first olog n log log n bits of input which provides the first known instance of an efficient noisetolerant algorithm for a concept class that is not learnable in the statistical query model of kearns thus we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the pac modelin codingtheory terms what we give is a polyn time algorithm for decoding linear k n codes in the presence of random noise for the case of k c log n log log n for some c the case of k olog n is trivial since one can just individually check each of the k possible messages and choose the one that yields the closest codeworda natural extension of the statistical query model is to allow queries about statistical properties that involve ttuples of examples as opposed to just single examples the second result of this article is to show that any class of functions learnable strongly or weakly with twise queries for t olog n is also weakly learnable with standard unary queries hence this natural extension to the statistical query model does not increase the set of weakly learnable functions"
531 "in this article we develop a general methodology mainly based" "in this article we develop a general methodology mainly based upon lyapunov functions to derive bounds on average delays and on averages and variances of queue lengths in complex systems of queues we apply this methodology to cellbased switches and routers considering first outputqueued oq architectures in order to provide a simple example of our methodology and then both inputqueued iq and combined inputoutput queued cioq architectures these latter switching architectures require a scheduling algorithm to select at each slot a subset of inputbuffered cells that can be transferred toward output ports although the stability properties ie the limit throughput of iq and cioq cellbased switches were already studied for several classes of scheduling algorithms very few analytical results concerning cell delays or queue lengths are available in the technical literature we concentrate on maximum weight matching mwm and maximal size matching msm scheduling algorithms while the former was proved to maximize throughput the latter allows simpler implementation the derived bounds are shown to be rather tight when compared to simulation results"
532 "we consider the problem of scheduling a collection of dynamically" "we consider the problem of scheduling a collection of dynamically arriving jobs with unknown execution times so as to minimize the average flow time this is the classic cpu scheduling problem faced by timesharing operating systems where preemption is allowed it is easy to see that every algorithm that doesnt unnecessarily idle the processor is at worst ncompetitive where n is the number of jobs yet there was no known nonclairvoyant algorithm deterministic or randomized with a competitive ratio provably o nepsi in this article we give a randomized nonclairvoyant algorithm rmlf that has competitive ratio olog n log log n against an oblivious adversary rmlf is a slight variation of the multilevel feedback mlf algorithm used by the unix operating system further justifying the adoption of this algorithm it is known that every randomized nonclairvoyant algorithm is log ncompetitive and that every deterministic nonclairvoyant algorithm is ncompetitive"
533 "this article deals with randomized allocation processes placing sequentially n" "this article deals with randomized allocation processes placing sequentially n balls into n bins we consider multiplechoice algorithms that choose d locations bins for each ball at random inspect the content of these locations and then place the ball into one of them for example in a location with minimum number of balls the goal is to achieve a good load balancing this objective is measured in terms of the maximum load that is the maximum number of balls in the same binmultiplechoice algorithms have been studied extensively in the past previous analyses typically assume that the d locations for each ball are drawn uniformly and independently from the set of all bins we investigate whether a nonuniform or dependent selection of the d locations of a ball may lead to a better load balancing three types of selection resulting in three classes of algorithms are distinguished uniform and independent nonuniform and independent and nonuniform and dependentour first result shows that the wellstudied uniform greedy algorithm class does not obtain the smallest possible maximum load in particular we introduce a nonuniform algorithm class that obtains a better load balancing surprisingly this algorithm uses an unfair tiebreaking mechanism called alwaysgoleft resulting in an asymmetric assignment of the balls to the bins our second result is a lower bound showing that a dependent allocation class cannot yield significant further improvementour upper and lower bounds on the maximum load are tight up to additive constants proving that the alwaysgoleft algorithm achieves an almost optimal load balancing among all sequential multiplechoice algorithm furthermore we show that the results for the alwaysgoleft algorithm can be generalized to allocation processes with more balls than bins and even to infinite processes in which balls are inserted and deleted by an oblivious adversary"
534 "the power of sharedmemory in models of parallel computation is" "the power of sharedmemory in models of parallel computation is studied and a novel distributed data structure that eliminates the need for shared memory without significantly increasing the run time of the parallel computation is described more specifically it is shown how a complete network of processors can deterministically simulate one pram step in olog nlog log n time when both models use n processors and the size of the prams shared memory is polynomial in n the best previously known upper bound was the trivial on it is established that this upper bound is nearly optimal and it is proved that an online simulation of t pram steps by a complete network of processors requires ohgrtlog n log log n time a simple consequence of the upper bound is that an ultracomputer the currently feasible generalpurpose parallel machine can simulate one step of a pram the most convenient parallel model to program in olog nlog log n steps"
535 "allens interval algebra is one of the best established formalisms" "allens interval algebra is one of the best established formalisms for temporal reasoning this article provides the final step in the classification of complexity for satisfiability problems over constraints expressed in this algebra when the constraints are chosen from the full allens algebra this form of satisfiability problem is known to be npcomplete however eighteen tractable subalgebras have previously been identified we show here that these subalgebras include all possible tractable subsets of allens algebra in other words we show that this algebra contains exactly eighteen maximal tractable subalgebras and reasoning in any fragment not entirely contained in one of these subalgebras is npcomplete we obtain this dichotomy result by giving a new uniform description of the known maximal tractable subalgebras and then systematically using a general algebraic technique for identifying maximal subalgebras with a given property"
536 "we consider the traveling salesman problem when the cities are" "we consider the traveling salesman problem when the cities are points in numxdd for some fixed d and distances are computed according to geometric distances determined by some norm we show that for any polyhedral norm the problem of finding a tour of maximum length can be solved in polynomial time if arithmetic operations are assumed to take unit time our algorithms run in time onf log n where f is the number of facets of the polyhedron determining the polyhedral norm thus for example we have on log n algorithms for the cases of points in the plane under the rectilinear and sup norms this is in contrast to the fact that finding a minimum length tour in each case is nphard our approach can be extended to the more general case of quasinorms with a not necessarily symmetric unit ball where we get a complexity of onf log nfor the special case of twodimensional metrics with f which includes the rectilinear and sup norms we present a simple algorithm with on running time the algorithm does not use any indirect addressing so its running time remains valid even in comparison based models in which sorting requires n log n time the basic mechanism of the algorithm provides some intuition on why polyhedral norms allow fast algorithmscomplementing the results on simplicity for polyhedral norms we prove that for the case of euclidean distances in numxdd for d the maximum tsp is nphard this sheds new light on the wellstudied difficulties of euclidean distances"
537 "we characterize the performance of difference coding for compressing sets" "we characterize the performance of difference coding for compressing sets and database relations through an analysis of the problem of estimating the number of bits needed for storing the spacings between values in sets of integers we provide analytical expressions for estimating the effectiveness of difference coding when the elements of the sets or the attribute fields in database tuples are drawn from the uniform and zipf distributions we also examine the case where a uniformly distributed domain is combined with a zipf distribution and with an arbitrary distribution we present limit theorems for most cases and probabilistic convergence results in other cases we also examine the effects of attribute domain reordering on the compression ratio our simulations show excellent agreement with theory"
538 "we study analogs of classical relational calculus in the context" "we study analogs of classical relational calculus in the context of strings we start by studying string logics taking a classical modeltheoretic approach we fix a set of string operations and look at the resulting collection of definable relations these form an algebraa class of nary relations for every n closed under projection and boolean operations we show that by choosing the string vocabulary carefully we get string logics that have desirable properties computable evaluation and normal forms we identify five distinct models and study the differences in their modeltheory and complexity of evaluation we identify a subset of these models that have additional attractive properties such as finite vc dimension and quantifier eliminationonce you have a logic the addition of free predicate symbols gives you a string query language the resulting languages have attractive closure properties from a database point of view while sql does not allow the full composition of string patternmatching expressions with relational operators these logics yield compositional query languages that can capture common stringmatching queries while remaining tractable for each of the logics studied in the first part of the article we study properties of the corresponding query languages we give bounds on the data complexity of queries extend the normal form results from logics to queries and show that the languages have corresponding algebras expressing safe queries"
539 "the state explosion problem remains a major hurdle in applying" "the state explosion problem remains a major hurdle in applying symbolic model checking to large hardware designs state space abstraction having been essential for verifying designs of industrial complexity is typically a manual process requiring considerable creativity and insightin this article we present an automatic iterative abstractionrefinement methodology that extends symbolic model checking in our method the initial abstract model is generated by an automatic analysis of the control erroneous or spurious counterexamples we devise new symbolic techniques that analyze such counterexamples and refine the abstract model correspondingly we describe asmv a prototype implementation of our methodology in nusmv practical experiments including a large fujitsu ip core design with about latches and lines of smv code confirm the effectiveness of our approach"
540 "exponential lower bounds are proved for the lengthofresolution refutations of" "exponential lower bounds are proved for the lengthofresolution refutations of sets of disjunctions constructed from expander graphs using the method of tseitin since these sets of clauses encode biconditionals they have short polynomiallength refutations in a standard axiomatic formulation of propositional calculus"
541 "in this article we will formalize the method of dual" "in this article we will formalize the method of dual fitting and the idea of factorrevealing lp this combination is used to design and analyze two greedy algorithms for the metric uncapacitated facility location problem their approximation factors are and with running times of om log m and on respectively where n is the total number of vertices and m is the number of edges in the underlying complete bipartite graph between cities and facilities the algorithms are used to improve recent results for several variants of the problem"
542 "we study the problem of compressing massive tables within the" "we study the problem of compressing massive tables within the partitiontraining paradigm introduced by buchsbaum et al in which a table is partitioned by an offline training procedure into disjoint intervals of columns each of which is compressed separately by a standard online compressor like gzip we provide a new theory that unifies previous experimental observations on partitioning and heuristic observations on column permutation all of which are used to improve compression rates based on this theory we devise the first online training algorithms for table compression which can be applied to individual files not just continuously operating sources and also a new offline training algorithm based on a link to the asymmetric traveling salesman problem which improves on prior work by rearranging columns prior to partitioning we demonstrate these results experimentally on various test files the online algorithms provide percnt improvement over gzip with negligible slowdown the offline reordering provides up to percnt further improvement over partitioning alone we also show that a variation of the table compression problem is maxsnp hard"
543 "we prove that three apparently unrelated fundamental problems in distributed" "we prove that three apparently unrelated fundamental problems in distributed computing cryptography and complexity theory are essentially the same problem these three problems and brief descriptions of them follow the selective decommitment problem an adversary is given commitments to a collection of messages and the adversary can ask for some subset of the commitments to be opened the question is whether seeing the decommitments to these open plaintexts allows the adversary to learn something unexpected about the plaintexts that are unopened the power of round weak zeroknowledge arguments the question is what can be proved in a possibly weakened form of zeroknowledge in a round argument in particular is there a language outside of bpp that has a round publiccoin weak zeroknowledge argument the fiatshamir methodology this is a method for converting a round publiccoin argument viewed as an identification scheme to a round signature scheme the method requires what we call a magic function that the signer applies to the firstround message of the argument to obtain a secondround message queries from the verifier an open question here is whether every round publiccoin argument for a language outside of bpp has a magic functionit follows easily from definitions that if a round publiccoin argument system is zeroknowledge in the standard fairly strong sense then it has no magic function we define a weakening of zeroknowledge such that zeroknowledge nomagicfunction still holds for this weakened form of zeroknowledge we give a partial converse informally if a round publiccoin argument system is not weakly zeroknowledge then some form of magic is possible for this argument system we obtain our definition of weak zeroknowledge by a sequence of weakenings of the standard definition forming a hierarchy intermediate forms of zeroknowledge in this hierarchy are reasonable ones and they may be useful in applications finally we relate the selective decommitment problem to publiccoin proof systems and arguments at an intermediate level of the hierarchy and obtain several positive security results for selective decommitment"
544 "this article introduces and explores the conditionbased approach to solve" "this article introduces and explores the conditionbased approach to solve the consensus problem in asynchronous systems the approach studies conditions that identify sets of input vectors for which it is possible to solve consensus despite the occurrence of up to f process crashes the first main result defines acceptable conditions and shows that these are exactly the conditions for which a consensus protocol exists two examples of realistic acceptable conditions are presented and proved to be maximal in the sense that they cannot be extended and remain acceptable the second main result is a generic consensus sharedmemory protocol for any acceptable condition the protocol always guarantees agreement and validity and terminates at least when the inputs satisfy the condition with which the protocol has been instantiated or when there are no crashes an efficient version of the protocol is then designed for the message passing model that works when f n and it is shown that no such protocol exists when f n it is also shown how the protocols safety can be traded for its liveness"
545 "the main result is a characterization of the generating sequences" "the main result is a characterization of the generating sequences of the length of words in a regular language on k symbols we say that a sequence s of integers is regular if there is a finite graph g with two vertices i t such that sn is the number of paths of length n from i to t in g thus the generating sequence of a regular language is regular we prove that a sequence s is the generating sequence of a regular language on k symbols if and only if both sequences s snn and t kn snn are regular"
546 "xpath is a language for navigating an xml document and" "xpath is a language for navigating an xml document and selecting a set of element nodes xpath expressions are used to query xml data describe key constraints express transformations and reference elements in remote documents this article studies the containment and equivalence problems for a fragment of the xpath query language with applications in all these contextsin particular we study a class of xpath queries that contain branching label wildcards and can express descendant relationships between nodes prior work has shown that languages that combine any two of these three features have efficient containment algorithms however we show that for the combination of features containment is conpcomplete we provide a sound and complete algorithm for containment that runs in exponential time and study parameterized ptime special cases while we identify one parameterized class of queries for which containment can be decided efficiently we also show that even with some bounded parameters containment remains conpcomplete in response to these negative results we describe a sound algorithm that is efficient for all queries but may return false negatives in some cases"
547 "declustering schemes allocate data blocks among multiple disks to enable" "declustering schemes allocate data blocks among multiple disks to enable parallel retrieval given a declustering scheme d its response time with respect to a query q rtq is defined to be the maximum number of data blocks of the query stored by the scheme in any one of the disks if q is the number of data blocks in q and m is the number of disks then rtq is at least qm one way to evaluate the performance of d with respect to a set of range queries q is to measure its additive errorthe maximum difference of rtq from qm over all range queries q qin this article we consider the problem of designing declustering schemes for uniform multidimensional data arranged in a ddimensional grid so that their additive errors with respect to range queries are as small as possible it has been shown that for a fixed dimension d any declustering scheme on an md grid a grid with length m on each dimension will always incur an additive error with respect to range queries of log m when d and logd m when d asymptotically optimal declustering schemes exist for dimensional data however the best general upper bound known so far for the worstcase additive errors of ddimensional declustering schemes d is omd which is large when compared to the lower bound in this article we propose two declustering schemes based on lowdiscrepancy points in ddimensions when d is fixed both schemes have an additive error of ologd m with respect to range queries provided that certain conditions are satisfied the first scheme requires that the side lengths of the grid grow at a rate polynomial in m while the second scheme requires d and m pt where d p c c a constant and t is a positive integer such that td these are the first multidimensional declustering schemes with additive errors proven to be near optimal"
548 "research on information extraction from web pages wrapping has seen" "research on information extraction from web pages wrapping has seen much activity recently particularly systems implementations but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping in this paper we first study monadic datalog over trees as a wrapping language we show that this simple language is equivalent to monadic second order logic mso in its ability to specify wrappers we believe that mso has the right expressiveness required for web information extraction and propose mso as a yardstick for evaluating and comparing wrappers along the way several other results on the complexity of query evaluation and query containment for monadic datalog over trees are established and a simple normal form for this language is presented using the above results we subsequently study the kernel fragment elog of the elog wrapping language used in the lixto system a visual wrapper generator curiously elog exactly captures mso yet is easier to use indeed programs in this language can be entirely visually specified"
550 "two different kinds of byzantine agreement for distributed systems with" "two different kinds of byzantine agreement for distributed systems with processor faults are defined and compared the first is required when coordinated actions may be performed by each participant at different times this kind is called simultaneous byzantine agreement sba this paper deals with the number of rounds of message exchange required to reach byzantine agreement of either kind ba if an algorithm allows its participants to reach byzantine agreement in every execution in which at most t participants are faulty then the algorithm is said to tolerate t faults it is well known that any ba algorithm that tolerates t faults with t n where n denotes the total number of processors must run at least t rounds in some execution however it might be supposed that in executions where the number f of actual faults is small compared to t the number of rounds could be correspondingly small a corollary of our first result states that when t n any algorithm for sba must run t rounds in some execution where there are no faults for eba with t n a lower bound of mint f rounds is proved finally an algorithm for eba is presented that achieves the lower bound provided that t is on the order of the square root of the total number of processors"
551 "this paper examines the unification problem in the class of" "this paper examines the unification problem in the class of primal algebras and the varieties they generate an algebra is called primal if every function on its carrier can be expressed just in terms of the basic operations of the algebra the twoelement boolean algebra is the simplest nontrivial example every truthfunction can be realized in terms of the basic connectives for example negation and conjunction it is shown that unification in primal algebras is unitary that is if an equation has a solution it has a single most general one two unification algorithms based on equationsolving techniques for boolean algebras due to boole and lowenheim are studied in detail applications include certain finite post algebras and matrix rings over finite fields the former are algebraic models for manyvalued logics the latter cover in particular modular arithmetic then unification is extended from primal algebras to their direct powers which leads to unitary unification algorithms covering finite post algebras finite semisimple artinian rings and finite semisimple nonabelian groups finally the fact that the variety generated by a primal algebra coincides with the class of its subdirect powers is used this yields unitary unification algorithms for the equational theories of post algebras and prings"
552 "a generalization of horn clauses to a higherorder logic is" "a generalization of horn clauses to a higherorder logic is described and examined as a basis for logic programming in qualitative terms these higherorder horn clauses are obtained from the firstorder ones by replacing firstorder terms with simply typed lgrterms and by permitting quantification over all occurrences of function symbols and some occurrences of predicate symbols several prooftheoretic results concerning these extended clauses are presented one result shows that although the substitutions for predicate variables can be quite complex in general the substitutions necessary in the context of higherorder horn clauses are tightly constrained this observation is used to show that these higherorder formulas can specify computations in a fashion similar to firstorder horn clauses a complete theoremproving procedure is also described for the extension this procedure is obtained by interweaving higherorder unification with backchaining and goal reductions and constitutes a higherorder generalization of sldresolution these results have a practical realization in the higherorder logic programming language called lgrprolog"
553 "the reduction algorithm is a technique for improving a decision" "the reduction algorithm is a technique for improving a decision tree in the abseence of aproecise cost criterion the result of applying the algorithm is an irreducible tree that is no less efficient than the original and may be more efficient irreducible trees arise in discrete decision theory as an algebraic form for decision trees this form has significant computational properties in fact every irreducible is optimal with respect to some expected testing cost criterion and is strictly better than any given distinct tree with respect to some criterion many irreducibles are decision equivalent to a given tree onely some of these are reductions of the tree the reduction algorithm is a particular way of finding one of these it tends to preserve the overall structure of the tree by reducing the subtrees first a bound on the complexity of this algorithm with input tree t is ohgtt usizet is the uniform size of the tree the number of leaves less one and hgtt is the height of the tree this means that decision tree reduction has the same worstcase order of complexity as most heuristic methods for building suboptimal trees while the purpose of using heuristics is often rather different such comparisons are an indication of the efficiency of the reduction algorithms"
554 "the polynomiality of nonlinear separable convex concave optimization problems on" "the polynomiality of nonlinear separable convex concave optimization problems on linear constraints with a matrix with small subdeterminants and the polynomiality of such integer problems provided the inteter linear version of such problems ins polynomial is proven this paper presents a generalpurpose algorithm for converting procedures that solves linear programming problems the conversion is polynomial for constraint matrices with polynomially bounded subdeterminants among the important corollaries of the algorithm is the extension of the polynomial solvability of integer linear programming problems with totally unimodular constraint matrix to integerseparable convex programming an algorithm for finding a egr accurate optimal continuous solution to the nonlinear problem that is polynomial in logegr and the input size and the largest subdeterminant of the constraint matrix is also presented these developments are based on proximity results between the continuous and integral optimal solutions for problems with any nonlinear separable convex objective function the practical feature of our algorithm is that is does not demand an explicit representation of the nonlinear function only a polynomial number of function evaluations on a prespecified grid"
555 "a major component of a parallel machine is its interconnection" "a major component of a parallel machine is its interconnection network in which provides concurrent communication between the processing elements it is common to use a multistage interconnection network min that is constructed using crossbar switches and introduces contention not only for destination addresses but also for internal links both types of contention are increased when nonlocal communication across a min becomes concentrated on a certain destination address the hotspot this paper considers analytical models of asynchronous circuitswitched ins in which partial paths are held during path building beginning with a single crossbar and extending recursively to mins since a path must be held between source and destination processors before data can be transmitted switching networks are passive resources and queuing networks that include them do not therefore have productform solutions using decomposition techniques the flowequivalent server fes that represents a bank of devices transmitting through a switching network is determined under mild approximating assumptions in the case of a full crossbar the fes can be solved directly and the result can be applied recursively to model the min two cases are considered one in which there is uniform routing and the other where there is a hotspot at one of the output pins validation with respect to simulation for mins with up to six stages way switching indicated a high degree of accuracy in the models"
